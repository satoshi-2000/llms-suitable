QuestionId,QuestionTitle,QuestionBody,AnswerId,AnswerBody,ChatGPT
76378346,"How to modify a regular expression to validate a text box with letters and numbers only, excluding certain characters like !@#$%^&*+=","Regular Expression for text box validation. I need to validate textbox. conditions like below

can enter numbers and letters.
need to avoid these characters !@#$%^&*+=

Currently i am using below regular expression. but it is not working.
 <input type=""text"" pattern=""^[\w-\.]+@([\w-]+\.)+[\w-]{2,4}$"" />
can u help me to find correct regular expression
",76378431,"The regulation expression you are currently using is for validate email addresses.
So you have to use a regular expression to validate textboxes, make it only contain letters and numbers and avoid the characters (!@#$%^&*+=).
^[a-zA-Z0-9]+$

This will match any strings that is a letter or a number. It will not match any strings that any characters !@#$%^&*+=.


<input type=""text"" pattern=""^[a-zA-Z0-9]+$"" />



Hope this will help. Thank you!
Refer to this link if you need more clarifications https://laasyasettyblog.hashnode.dev/validating-username-using-regex
Improving my answer with the question u asked,


<!DOCTYPE html>
<html>
<head>
<title>Test Pattern</title>
</head>
<body>
<input type=""text"" id=""input"" pattern='^[a-zA-Z0-9./|()?~<>"":;]+$' />
<button onclick=""validate()"">Validate</button>
<script>
function validate() {
 var input = document.getElementById(""input"").value;
 var pattern = /[a-zA-Z0-9./|()?~<>"":;]+/;
 if (pattern.test(input)) {
  alert(""Valid"");
 } else {
  alert(""Invalid"");
 }
}
</script>
</body>
</html>



","To modify the regular expression to validate a text box with letters and numbers only, excluding certain characters like !@#$%^&*+=, you can use the following regular expression pattern:

```regex
^[a-zA-Z0-9]+$
```

Here's an explanation of the pattern:
- `^` asserts the start of the string.
- `[a-zA-Z0-9]` matches any lowercase letter, uppercase letter, or digit.
- `+` allows for one or more occurrences of the preceding pattern (letters or digits).
- `$` asserts the end of the string.

You can use this regular expression in your HTML code like this:

```html
<input type=""text"" pattern=""^[a-zA-Z0-9]+$"" />
```

This will ensure that the input in the text box consists only of letters and numbers, excluding the specified characters."
76378408,How can i add new key and value in a existing list,"I'm trying to create a blind auction. So basically it will ask for your name and then your bid. After that, it will ask if there are any bidders, if yes it will ask you the name and the bid price. But after you said yes the terminal will be cleaned. So that the other bidder can't see how much the other person bid, if I run print on the [data_base] it can't print more than two keys and value.
Here is the output:
What is your name?: Gael
What is your bid: $560
Are there any other bidders? Type 'yes or 'no'.
yes

\[({'Gael': \['560'\]},)\]
What is your name?: Mikey
What is your bid: $350
Are there any other bidders? Type 'yes or 'no'.
yes

\[({'Mikey': \['350'\]},)\]
What is your name?: Josh
What is your bid: $298
Are there any other bidders? Type 'yes or 'no'.
no

Here is the final output:
[({'Mikey': ['350']},), ({'Josh': ['298']},)]
Gael's name and his bid are missing.
Here is the code:
import os

while True:
name = input(""What is your name?: "")
bid = input(""What is your bid: $"")

    other_user = input(""Are there any other bidders? Type 'yes or 'no'.\n"")
    
    if other_user == 'yes':
         os.system('cls')
         data_base = [
    
             ]
    def new_user(name, bid):
        brandnew_user = {
            name: [bid]
            
            },
        
        data_base.append(brandnew_user)
        
    new_user(name, bid)
    print(data_base)
    
    
    if other_user == 'no':
        break

Thank you!!
I was expecting that Gael's name and bid will be recorded. But it did not, it only recorded, Mikey and Josh.
",76378435,"Here's a better way to organize things.  Also, I'm not sure why you are creating a list of tuples of dictionaries.  Why not just make data_base a dictionary and store the new entries as keys?
import os

data_base = []
while True:
    name = input(""What is your name?: "")
    bid = input(""What is your bid: $"")

    data_base.append( {name: [bid]} )
    print(data_base)

    other_user = input(""Are there any other bidders? Type 'yes or 'no'.\n"")
    if other_user == 'no':
        break

Here's what I'm talking about:
import os

data_base = {}
while True:
    name = input(""What is your name?: "")
    bid = input(""What is your bid: $"")

    data_base[name] = [bid]
    print(data_base)

    other_user = input(""Are there any other bidders? Type 'yes or 'no'.\n"")
    if other_user == 'no':
        break

","The issue with your code lies in the placement of the `data_base` list. You are initializing it inside the `if` block, which means it gets reset to an empty list every time a new bidder is detected. To fix this, you need to move the initialization of `data_base` outside the `while` loop so that it maintains its state across iterations. Here's the modified code:

```python
import os

data_base = []

while True:
    name = input(""What is your name?: "")
    bid = input(""What is your bid: $"")

    other_user = input(""Are there any other bidders? Type 'yes' or 'no'.\n"")
    
    if other_user == 'yes':
        os.system('cls')
        
        def new_user(name, bid):
            brandnew_user = {
                name: [bid]
            }
            data_base.append(brandnew_user)
        
        new_user(name, bid)
    
    if other_user == 'no':
        break

print(data_base)
```

In this code, the `data_base` list is defined before the `while` loop, allowing it to store the information for each bidder throughout the execution of the program. Now all the names and bids should be recorded correctly, including Gael's."
76378340,How to repeat video with start and end time in Android Studio?,"I'm getting error in Android Studio on second ""cannot resolve symbol second"" how to fix it so that it loops from 358 to 331 in this example?
package com.example.myapp;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.os.Bundle;
import android.view.View;
import android.widget.RelativeLayout;

import com.pierfrancescosoffritti.androidyoutubeplayer.core.player.YouTubePlayer;
import com.pierfrancescosoffritti.androidyoutubeplayer.core.player.listeners.AbstractYouTubePlayerListener;
import com.pierfrancescosoffritti.androidyoutubeplayer.core.player.views.YouTubePlayerView;

public class FingerStretching extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_finger_stretching);

        YouTubePlayerView youTubePlayerView = findViewById(R.id.youtube_player_view);
        getLifecycle().addObserver(youTubePlayerView);

        youTubePlayerView.addYouTubePlayerListener(new AbstractYouTubePlayerListener() {
            String videoId = ""mSZWSQSSEjE"";
            @Override
            public void onReady(@NonNull YouTubePlayer youTubePlayer) {
                youTubePlayer.loadVideo(videoId, 331);
            }
            
            public void onCurrentSecond(@NonNull YouTubePlayer youTubePlayer) {
                if(second == 358) youTubePlayer.seekTo(331);
            }
        });



    }
}

tried creating local variable second
",76378439,"According to the source code, the signature of onCurrentSecond is
override fun onCurrentSecond(youTubePlayer: YouTubePlayer, second: Float)

You are not overriding it. It should be
@Override
public void onCurrentSecond(@NonNull YouTubePlayer youTubePlayer, float second) {
    if(second >= 358) youTubePlayer.seekTo(331);
}

Such kind of error is easily avoidable if you make use of the auto complete feature in the IDE. Typing onC within the AbstractYouTubePlayerListener should give you auto complete option for onCurrentSecond, selecting it should automatically write the override function for you with correct signature.
","To fix the error ""cannot resolve symbol second"" in your code and make the video loop from 358 to 331, you need to declare and use a local variable named ""second"" in your `onCurrentSecond` method. Here's the modified code:

```java
public class FingerStretching extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_finger_stretching);

        YouTubePlayerView youTubePlayerView = findViewById(R.id.youtube_player_view);
        getLifecycle().addObserver(youTubePlayerView);

        youTubePlayerView.addYouTubePlayerListener(new AbstractYouTubePlayerListener() {
            String videoId = ""mSZWSQSSEjE"";
            int startSecond = 331;
            int loopEndSecond = 358;

            @Override
            public void onReady(@NonNull YouTubePlayer youTubePlayer) {
                youTubePlayer.loadVideo(videoId, startSecond);
            }

            @Override
            public void onCurrentSecond(@NonNull YouTubePlayer youTubePlayer, float second) {
                if (second >= loopEndSecond) {
                    youTubePlayer.seekTo(startSecond);
                }
            }
        });
    }
}
```

In the modified code:
1. I added two variables, `startSecond` and `loopEndSecond`, to specify the start and end times for looping.
2. In the `onCurrentSecond` method, I added a condition to check if the current second is greater than or equal to the `loopEndSecond`. If it is, the video is seeked back to the `startSecond`, thus creating a loop.

Make sure to replace the existing video ID (""mSZWSQSSEjE"") with your desired YouTube video ID."
76378344,how to use react function in codepen?,"How to use React functions in CodePen?
I wrote a react function in CodePem to test React hooks, however it constantly keeps reporting errors: Uncaught ReferenceError: require is not defined.
My Code:
import {useState, useEffect,useRef } from 'react';

function Test() {
  const [count, setCount] = useState(0);
  const prevRef = useRef();
  
  useEffect(() => {
    // const ref = useRef();
    console.log('ref----', prevRef.current);
    prevRef.current = count;
  })
  
  return (
    <div>
      <div onClick={() => setCount(count+1)}>+1</div>
      <div>{`count: ${count}`}</div>
      <div>{`precount: ${prevRef.current}`}</div>
    </div>
  )
}

ReactDOM.render(<Test />, document.getElementById(""app""));


",76378496,"You can add a package by adjusting the settings in your Pen.
Take a look at the following image for reference:

By doing so, it will automatically generate the necessary import statement:
import React, { useState, useEffect, useRef } from 'https://esm.sh/react@18.2.0';
import ReactDOM from 'https://esm.sh/react-dom@18.2.0';

To help you understand this process, I've created a sample code on CodePen. You can refer to this example to implement it yourself.
Here is the codepen link to the sample code: https://codepen.io/camel2243/pen/ExdBRar
","To use React functions in CodePen, you need to make a few adjustments to your code. CodePen uses a specific setup to run React code.

Here's how you can modify your code to make it work in CodePen:

1. Remove the import statement for React since it's already included in the CodePen environment.

2. Wrap your code inside a functional component.

3. Add the necessary HTML structure to render your React component.

Here's the updated code:

```jsx
function Test() {
  const [count, setCount] = React.useState(0);
  const prevRef = React.useRef();

  React.useEffect(() => {
    prevRef.current = count;
  });

  return (
    <div>
      <div onClick={() => setCount(count + 1)}>+1</div>
      <div>{`count: ${count}`}</div>
      <div>{`precount: ${prevRef.current}`}</div>
    </div>
  );
}

ReactDOM.render(<Test />, document.getElementById(""app""));
```

In CodePen, you need to create an HTML structure with a `<div>` element having the id ""app"" to render your React component. Make sure to set the JavaScript settings to include the React library.

Here's how you can set up your CodePen environment:

1. Go to https://codepen.io/.

2. Click on ""Create"" to create a new pen.

3. In the HTML section, add the following code:

```html
<div id=""app""></div>
```

4. In the JavaScript section, paste the modified code from above.

5. In the JavaScript settings (click on the gear icon), add ""react"" and ""react-dom"" to the ""Add External Scripts/Pens"" section.

6. Click on ""Save"" to run your React code.

With these changes, your React component should work correctly in CodePen, and you should no longer see the ""Uncaught ReferenceError: require is not defined"" error."
76378323,Search Customers that are part of the logged on User's Business?,"The code I currently have is this, in my views.py I can't figure out how to set up my search function. All other functions work.
models.py
class User(AbstractUser):
    """"""User can be Employee or Customer""""""

class Business(models.Model):
    business = models.CharField(max_length=50)

class BusinessOwner(models.Model):
    user = models.OneToOneField(User, on_delete=models.CASCADE, null=True )
    business = models.ForeignKey(Business, on_delete=models.CASCADE, null=True)

class Customer(models.Model):
    """""" Customer-specific information """"""
    user = models.OneToOneField(User, on_delete=models.CASCADE, null=True )
    business = models.ForeignKey(Business, on_delete=models.CASCADE, null=True)

class Employee(models.Model):
    """""" Employee-specific information """"""
    user = models.OneToOneField(User, on_delete=models.CASCADE, null=True)
    business = models.ForeignKey(Business,  on_delete=models.CASCADE, null=True, blank=True)`

forms.py
class UserForm(UserCreationForm):
    class Meta:
        model = User
        fields = ( ""username"", ""email"", ""password1"", ""password2"", ""first_name"", ""last_name"", )


class BusinessOwnerForm(forms.ModelForm):
. . . no fields
class EmployeeForm(forms.ModelForm):
. . . no fields

class CustomerForm(forms.ModelForm):
. . . no fields

class BusinessForm(forms.ModelForm):
    class Meta:
        model = Business
        fields = ( ""business"",  )

views.py (user creation process)
def searchUsers(request):
    qs_owned_businesses = BusinessOwner.objects.filter(user = request.user).values('business_id')
    qs_biz_customers = Customer.objects.filter(business_id__in=qs_owned_businesses)
    if request.method == ""GET"":
        query = request.GET.get('search')
        if query == '':
            query = 'None'
    results = User.objects.filter(username__icontains=query, id__in=qs_biz_customers)
    return render(request, 'search_users.html', {'query': query, 'results': results})
    
    


#example of hows employees and customers are created in my views:
def employeeCreation(request):
    """"""Creates an Employee""""""
    if request.method == ""POST"":
        employee_form = EmployeeForm(request.POST)
        user_creation_form = UserForm(request.POST)
        if (user_creation_form.is_valid() and employee_form.is_valid()):
            employee_form.instance.business = request.user.businessowner.business
            new_user = user_creation_form.save(commit=False)
            employee_form.instance.user = new_user
            user_creation_form.save()
            employee_form.save()
            messages.success(request, ""You Have Created An Employee"" )
            return redirect(""user-homepage"")
        else:
            messages.error(request, ""Try creating an Employee Again something went wrong."")
    employee_form = EmployeeForm()
    user_creation_form = UserForm()
    return render (request, ""registration/employee_creation.html"", 
        context={""user_creation_form"": user_creation_form, 
                ""employee_form"": employee_form,
                    })


def customerCreation(request):
. . . functions is exactly the same as employee creation just for a customer. The Business owner's business is used as a starting point to build employees off of. I didn't incldue that view because it's not necessary for this and stack overflow limits how much code I put here.


search_users.html


{% if results %}
 you Searched for {{ query }}  . . 
    {% for x in results %}
        {{ x }}<p></p>
    {% endfor %}

{%endif %}```
I have tried using Q, icontain ,.filter() and django-filter, but this is a tricky search criteria that I can't get to work.
navbar search feature:
       <form action=""{% url 'search-users' %}"" class=""form-inline"" method=""get"">
      <div class=""form-group mx-sm-3 mb-2"">
      <label for="""" class=""sr-only"">search</label>
      <input name=""search"" type="""" class=""form-control"" id="""" placeholder=""Keyword"">
      </div>
     <button type=""submit"" class=""btn btn-success btn-lg mb-2"">Search</button>
</form>```

",76378505,"Let's break this down into tasks. I'm using values() to limit the request to what we're interested in, as I can then use that result to filter further.
#First you want to get all the businesses the logged in user owns
#(Currently they can only own one, so you could use get rather than filter,
#but you might change that later and this approach will still work)
qs_owned_businesses = BusinessOwner.objects.filter(user = request.user).values('business_id')

#next you want to get all the customers of those businesses
qs_biz_customers = Customer.objects.filter(business_id__in= qs_owned_businesses).values('user_id')

#finally you want to filter those customers further based on your form field
#remember, the icontains criteria needs to refer to a field
#here we're looking at username, but you might use last_name or something else.
results = User.objects.filter(username__icontains=query, id__in=qs_biz_customers)

results should now be a list of users you can cycle through in your template to show names, usernames etc.
","Based on the provided code, it seems like you are trying to implement a search functionality to retrieve customers who are part of the logged-in user's business. However, there are a few issues with your code. Here's an updated version of the `searchUsers` function in your views.py file:

```python
from django.db.models import Q

def searchUsers(request):
    owned_businesses = BusinessOwner.objects.filter(user=request.user).values_list('business_id', flat=True)
    business_customers = Customer.objects.filter(business_id__in=owned_businesses)
    
    query = request.GET.get('search')
    if query == '':
        query = 'None'
    
    results = User.objects.filter(
        Q(username__icontains=query) & Q(id__in=business_customers)
    )
    
    return render(request, 'search_users.html', {'query': query, 'results': results})
```

Here's an explanation of the changes:

1. Instead of using `values('business_id')`, you can use `values_list('business_id', flat=True)` to get a flat list of business IDs.
2. In the `results` query, you need to use the `Q` object to perform a complex query combining the conditions for username and ID.
3. I've removed the check for the request method being ""GET"" because it's unnecessary. The code will work regardless of the request method.
4. The updated code fetches the value of the `search` parameter directly from the request using `request.GET.get('search')`.

Make sure you have imported the `Q` object from `django.db.models` at the top of your views.py file:

```python
from django.db.models import Q
```

These changes should help you implement the search functionality correctly."
76378468,Remove remote commits on the branch in GitHub,"ALL,
I made a local branch in my fork a long time ago and pushed some changes to it. I then submitted a PR which was passed the CI build.
Now after some time I came back to the same machine I produced the PR but for some reason I didn't check which branch I was on and made couple of commits on the old branch and pushed them therefore screwing up the PR (it was not yet merged, due to the lack of the test code).
Now what I'd like to do is go to Github Web interface, remove those commits, but keep them locally, because I can just generate a patch on my local machine, remove those commits, switch to the new branch and apply the patch to it.
Or maybe even there is a better solution?
So how do I solve this mess?
Keep in mind - I intend to finish the PR with the test, but those are 2 completely unrelated things.
TIA!!
EDIT:
Everythig worked fine and my old branch on the original laptop is back to normal and the PR is now good.
However, in order to put the unit test I had to go to a different machine and do a git pull. For some unknown reason after that the git tree on that machine becomes clogged with everything including the bad commit.
I was able to revoke bad commits with git reset --hard N, but I fear that the same happen when I try to test my unit test on all platforms/different laptops which means my changes will be lost and I will need to redo them again for the UT on all different machines.
Can you help me here as well?
TIA!!
",76378523,"After some thought, my original answer is more complicated than strictly necessary, but I'll leave it below.
The easiest way to get your original branch back to its old state and keep the new commits is to create a new branch then reset the old branch and force push. It looks like this:
git checkout old-branch
git branch new-branch
git reset --hard <hash of commit you want to keep in old-branch>
git push -f

Alternatively you can use
git reset --hard HEAD~n

where n is the number of commits you want to remove from the old branch.
Now you can do whatever you wish with the new branch, such as rebase it onto main. This might not be entirely necessary. If for example, your PR is merged, you will need to pull those changes into the new branch anyway before making the second PR. However, if you want to make a 2nd PR before the 1st is merged, then it is better to keep them separate until one of them is merged.
TLDR
The easiest way to fix a remote repository is to first make the changes locally and then push, possibly force push, to GitHub or other remote.
Details
You can do this all locally first, then push to GitHub to fix the PR. First, you should create a new branch and git cherry-pick the commits that you want to keep but remove from the other branch.
Start by getting the hashes of the commits you want:
git checkout old-branch
git log --oneline --graph

Copy the commit hashes for the commits you want to move. Then do
git checkout -b new-branch main

and for each of the hashes you copied:
git cherry-pick <hash>

Alternatively, you can do this more easily with git rebase. You only need the hash of the oldest commit you want to keep:
git checkout -b new-branch old-branch
git rebase --onto main <hash of oldest commit>~

Now go back to your old branch and get rid of all the commits you no longer want:
git checkout old-branch
git reset --hard <hash of the first commit you want to keep on this branch>

Finally force push:
git push -f

This will automatically update the PR back to its original state, if you used the correct hash for the git reset command.
","To remove the remote commits from the branch in GitHub, you can follow these steps:

1. Open the repository in your GitHub Web interface.
2. Navigate to the branch containing the unwanted commits.
3. Click on the ""Commits"" tab to view the list of commits on that branch.
4. Identify the commits you want to remove and note down their commit hashes.
5. In your local machine, make sure you have the latest version of the branch with the unwanted commits by performing a `git pull`.
6. Create a backup of your local branch by creating a new branch based on the current state. You can use the following command:
   ```
   git branch backup_branch
   ```
7. Switch to the branch containing the unwanted commits using the following command:
   ```
   git checkout branch_name
   ```
8. Use the `git reset` command to remove the unwanted commits locally while keeping the changes intact. Replace `commit_hash` with the appropriate commit hash:
   ```
   git reset --hard commit_hash
   ```
   Repeat this step for each unwanted commit, using the corresponding commit hash.
9. Once the unwanted commits are removed locally, force-push the branch to the remote repository to update it with the local changes:
   ```
   git push origin branch_name --force
   ```
   Note: Be cautious when using the `--force` flag, as it overwrites the remote branch history.
10. After force-pushing, the remote branch will no longer contain the unwanted commits.

Now, you can switch to the new branch where you want to apply your changes, generate a patch on your local machine, switch to the new branch, and apply the patch as you mentioned in your initial question.

Regarding the issue you mentioned with the git tree becoming clogged on a different machine after a `git pull`, it's likely because the remote branch history still contains the unwanted commits. You'll need to perform the same steps mentioned above on that machine to remove the unwanted commits from the remote branch. Make sure to create a backup branch before making any changes."
76378419,How to use async properly to get chrome.storage?,"I am creating a google chrome extension. On the popup, I am displaying a leaderboard. However, I am new to JavaScript so I don't know how to properly use async. I am using chrome.storage to get stored scores to display on the leaderboard, then sending them from background.js to score.js. My issue is that, since chrome.storage.get happens asynchronously, my findScores method does not wait for chrome.storage.get to finish before incorrectly returning a default empty score.
Here is my code:
background.js
chrome.runtime.onMessage.addListener(
    function(request, sender, sendResponse) {
      console.log(sender.tab ?
                  ""from a content script:"" + sender.tab.url :
                  ""from the extension"");
      if (request.type === ""request"") {
        var scoresVar = findScores(request.table, ""All"");
        console.log(""Sending response "" + scoresVar);
        sendResponse({scores: scoresVar})
      } 
      else if (request.type === ""score"") {
        saveScore(request.website, request.score, request.tab);
        sendResponse(""Finished adding score "" + request.score);
      }
    }
);

function findScores(table, website) {
    const categories = table.split(""-"");
    if (categories.includes(""personal"")) {
        chrome.storage.sync.get([website], function(response) {
            if (!(typeof response[website] === 'undefined')) {
                console.log(""Found "" + response[website]);
                return response[website];
            }
        });
    } else if (categories.includes(""global"")){
        // TODO: Add global leaderboards
        return [""-""];
    }
    console.log(""Didn't find, on default"");
    return [""-""];
}

popup.js
async function requestScores(tableID) {
  var url = ""All""
  if (tableID.includes(""current"")) {
    var url = await getCurrentTab();
  }
  console.log(""Sending message to load scores to "" + url);
  (async () => {
    const response = await chrome.runtime.sendMessage({type: ""request"", request: ""load scores"", table: tableID, tab: url});
    console.log(""Received: "" + response);
    // add scores to HTML DOM
    });
  })();
}


My console messages reveal that I first return a default score, which is sent to popup.js. I have tried throwing async keywords in front of functions (as well as ""await"" in front of variables like scoresVar = await findScores(request.table, ""All"") but it just caused more issues, where findScores still returned a default value, but background.j instead sent an undefined promise.
How can I fix my code?
",76378558,"It is simpler to work with Promises and async/await instead of callbacks. chrome.storage.sync.get returns a Promise if you do not pass a callback.
async function findScores(table, website) {
    // ...
    if (categories.includes(""personal"")) {
        const response = await chrome.storage.sync.get([website]);
        if (response[website] !== undefined) {
            console.log(""Found "" + response[website]);
            return response[website];
        }
    }
    // ...
}
// ...
chrome.runtime.onMessage.addListener(function(request, sender, sendResponse) {
    // ...
    findScores(request.table, ""All"").then(scores => {
        console.log(""Sending response "" + scores);
        sendResponse({scores});
    });
    return true; // keep the messaging channel open for sendResponse
});

Note that the callback of onMessage should return a literal true value (documentation) in order to keep the internal messaging channel open so that sendResponse can work asynchronously.
","To properly use `async` and `await` in your code, you need to ensure that all asynchronous operations are awaited. Currently, your `findScores` function does not wait for the `chrome.storage.sync.get` operation to complete before returning the default value. Here's how you can modify your code to correctly use `async` and `await`:

background.js:
```javascript
chrome.runtime.onMessage.addListener(
  async function(request, sender, sendResponse) {
    console.log(sender.tab ?
                ""from a content script: "" + sender.tab.url :
                ""from the extension"");
    if (request.type === ""request"") {
      var scoresVar = await findScores(request.table, ""All"");
      console.log(""Sending response "" + scoresVar);
      sendResponse({scores: scoresVar})
    } 
    else if (request.type === ""score"") {
      saveScore(request.website, request.score, request.tab);
      sendResponse(""Finished adding score "" + request.score);
    }
  }
);

function findScores(table, website) {
  return new Promise((resolve) => {
    const categories = table.split(""-"");
    if (categories.includes(""personal"")) {
      chrome.storage.sync.get([website], function(response) {
        if (!(typeof response[website] === 'undefined')) {
          console.log(""Found "" + response[website]);
          resolve(response[website]);
        } else {
          console.log(""Didn't find, returning default"");
          resolve([""-""]);
        }
      });
    } else if (categories.includes(""global"")) {
      // TODO: Add global leaderboards
      resolve([""-""]);
    } else {
      console.log(""Invalid table type, returning default"");
      resolve([""-""]);
    }
  });
}
```

popup.js:
```javascript
async function requestScores(tableID) {
  var url = ""All"";
  if (tableID.includes(""current"")) {
    var url = await getCurrentTab();
  }
  console.log(""Sending message to load scores to "" + url);
  const response = await new Promise((resolve) => {
    chrome.runtime.sendMessage({type: ""request"", request: ""load scores"", table: tableID, tab: url}, (response) => {
      resolve(response);
    });
  });
  console.log(""Received: "" + response);
  // add scores to HTML DOM
}
```

In the `findScores` function, we wrap the `chrome.storage.sync.get` operation in a `Promise` and resolve it when the data is available. This allows us to `await` the `findScores` function call in the `chrome.runtime.onMessage` listener, ensuring that it waits for the storage operation to complete.

In `popup.js`, we wrap the `chrome.runtime.sendMessage` call in a `Promise` and resolve it with the response from the background script. By `await`ing this promise, we can ensure that the `requestScores` function waits for the response before proceeding.

Remember to include the modified code snippets in their respective files and test your extension to ensure it works as expected."
76383950,How do I change the focus of the text field on Submit?,"I have a text field and it has an onSubmit method, inside which I check for validation and then focus on another field, but for some reason the focus does not work
onSubmitted: (value) {
        //print(""ga test"");

        if (!widget.validator?.call(value)) {
          setState(() {
            showError = true;
          });
        }
        if (widget.nextFocus != null) {
          FocusScope.of(context).requestFocus(widget.nextFocus);
        }

      },

",76384028,"I did so and it worked
if (widget.validator != null) {
          setState(() {
            showError = !widget.validator?.call(value);
          });
        }
        if (widget.nextFocus != null) {
          FocusScope.of(context).requestFocus(widget.nextFocus);
        }

","The code snippet you provided seems to be using Flutter, a framework for building user interfaces for mobile and web applications. To change the focus of a text field on submit, you can follow these steps:

1. Make sure you have the `focus` package added as a dependency in your `pubspec.yaml` file:

```yaml
dependencies:
  flutter:
    sdk: flutter
  focus: ^x.x.x
```

Replace `^x.x.x` with the latest version of the `focus` package.

2. Import the necessary packages in your file:

```dart
import 'package:flutter/material.dart';
import 'package:focus/focus.dart';
```

3. Wrap the part of your code that handles the text field focus with a `FocusScope` widget. This widget is used to manage the focus of multiple fields in a given scope:

```dart
FocusScope(
  child: Builder(
    builder: (BuildContext context) {
      return TextFormField(
        // ... your existing code ...
        onSubmitted: (value) {
          // ... your existing code ...

          if (widget.nextFocus != null) {
            // Request focus for the next field
            FocusScope.of(context).requestFocus(widget.nextFocus);
          }
        },
      );
    },
  ),
);
```

Make sure to wrap the `TextFormField` widget or any other widget that should receive focus inside the `FocusScope` widget.

4. In the code above, `widget.nextFocus` represents the next field you want to focus on. Ensure that `widget.nextFocus` is correctly set to the focus node of the desired next field. You can create a `FocusNode` for each field and set the `nextFocus` property accordingly.

Here's an example of how to create a `FocusNode` and set it as the next focus for a field:

```dart
// Define the focus nodes
final FocusNode firstFieldNode = FocusNode();
final FocusNode secondFieldNode = FocusNode();

// Set the next focus
firstFieldNode.nextFocus = secondFieldNode;

// Use the focus nodes in your text fields
TextFormField(
  focusNode: firstFieldNode,
  // ... other properties ...
);

TextFormField(
  focusNode: secondFieldNode,
  // ... other properties ...
);
```

By setting the `nextFocus` property of a `FocusNode`, you can control the focus order when calling `FocusScope.of(context).requestFocus()`. Ensure that you correctly set the `focusNode` property of each text field and update `widget.nextFocus` accordingly.

With these steps, you should be able to change the focus of the text field on submit in your Flutter application."
76380624,Testng test are ignored after upgrading to Sprint Boot 3 and maven-surefire-plugin 3.1.0,"I have an application that was executing TestNG tests perfectly with maven, for example, when using a mvn clean install command.
Currently I have updated the application to start using Spring Boot 3.1.0, and now the tests are completely ignored. No tests are executed.
I am using a classic testng.xml file defined on the maven-surefire-plugin:
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>${maven-surefire-plugin.version}</version>
                <configuration>
                    <suiteXmlFiles>
                        <suiteXmlFile>src/test/resources/testng.xml</suiteXmlFile>
                    </suiteXmlFiles>
                </configuration>
            </plugin>

All solutions I have found are related about the java classes ending on *Test.java but this is not applied as I am using the testng suite file. And before the update, the tests are working fine.
What has been changed into Spring Boot 3 to skip my tests?
",76380646,"Ok, I have found the ""issue"". Seems that the new versions of maven-surefire-plugin needs to include a surefire-testng extra plugin for executing it:
           <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>3.1.0</version>
                <configuration>
                    <suiteXmlFiles>
                        <suiteXmlFile>src/test/resources/testng.xml</suiteXmlFile>
                    </suiteXmlFiles>
                </configuration>
                <dependencies>
                    <dependency>
                        <groupId>org.apache.maven.surefire</groupId>
                        <artifactId>surefire-testng</artifactId>
                        <version>3.1.0</version>
                    </dependency>
                </dependencies>
            </plugin>

After including the dependency on the plugin, now is working fine.
","As of my knowledge cutoff in September 2021, there is no Spring Boot version 3.0.0 released yet. The latest stable version of Spring Boot is 2.5.x. It's possible that you're referring to a future version that is not covered in my training data. However, I can provide some general troubleshooting steps that you can try to address the issue you're facing.

1. Verify TestNG Dependency: Ensure that the TestNG dependency is correctly included in your project's `pom.xml` file. Make sure the version matches your requirements.

2. Check Maven Surefire Plugin Configuration: Verify that the Maven Surefire Plugin configuration is set correctly in your `pom.xml` file. The configuration you provided seems correct, but make sure the `suiteXmlFile` path points to the correct location of your TestNG suite XML file.

3. Check Test Naming Convention: Although you mentioned that your tests are defined in a TestNG suite XML file, it's still worth checking if the test class names follow the TestNG naming conventions. Test classes should end with ""Test"" or ""Tests"" for TestNG to automatically pick them up. For example, `MyTest.java` or `MyTests.java`. Ensure that your test class names follow this convention.

4. Verify TestNG Suite XML: Double-check your TestNG suite XML file (`testng.xml`) to ensure that it correctly includes the test classes or packages you want to run. Check that the class names or package names mentioned in the XML file match the actual test classes in your codebase.

5. Check Test Class Annotations: Ensure that your test classes or methods are annotated properly with TestNG annotations. For example, `@Test` annotation should be used to mark the methods that represent individual test cases.

6. Update Maven and Plugins: Make sure you are using the latest version of Maven and the Maven Surefire Plugin. Update your project's `pom.xml` file with the latest versions and try running the tests again.

7. Debug Configuration: If the issue persists, you can try debugging the Maven Surefire Plugin execution. Use the `-X` option with the Maven command to enable debug output and check if there are any error messages or warnings related to test execution.

It's also worth noting that changes in Spring Boot itself shouldn't directly cause TestNG tests to be ignored. However, if there are any changes in your application's dependencies, plugins, or build configurations, they might indirectly affect the test execution.

Please note that the information I provided is based on the available information up to September 2021. If there have been any significant changes or updates since then, it's advisable to consult the official documentation or community resources for the specific versions you are using."
76380600,Terragrunt - make dynamic group optional,"I'm using Okta provider to create okta_app_oauth and okta_app_group_assignments. My module looks like:
resource ""okta_app_oauth"" ""app"" {

  label                      = var.label
  type                       = var.type
  grant_types                = var.grant_types
  redirect_uris              = var.type != ""service"" ? var.redirect_uris : null
  response_types             = var.response_types
  login_mode                 = var.login_mode
  login_uri                  = var.login_uri
  post_logout_redirect_uris  = var.post_logout_redirect_uris
  consent_method             = var.consent_method
  token_endpoint_auth_method = var.token_endpoint_auth_method
  pkce_required              = var.token_endpoint_auth_method == ""none"" ? true : var.pkce_required
  lifecycle {
    ignore_changes = [
      client_basic_secret, groups
    ]
  }
}

resource ""okta_app_group_assignments"" ""app"" {
  app_id = okta_app_oauth.app.id
  dynamic ""group"" {
    for_each = var.app_groups
    content {
      id       = group.value[""id""]
      priority = group.value[""priority""]
    }
  }
}

And it works when I assign groups to application, but when I don't want to assign groups, I get error:
│ Error: Invalid index
│ 
│   on main.tf line 26, in resource ""okta_app_group_assignments"" ""app"":
│   26:       id       = group.value[""id""]
│     ├────────────────
│     │ group.value is empty map of dynamic
│ 
│ The given key does not identify an element in this collection value.


in addition, my app_groups variable looks like:
variable ""app_groups"" {
  description = ""Groups assigned to app""
  type        = list(map(any))
  default     = [{}]
}

I was trying to use lookup(group, ""priority"", null), but it wasn't resolving my problem. Can somebody help me with solving this?
",76380785,"You can make the block optional as follows:
  dynamic ""group"" {
    for_each = length(var.app_groups) > 0 : var.app_groups : []
    content {
      id       = group.value[""id""]
      priority = group.value[""priority""]
    }
  }

also your default value for app_groups should be:
variable ""app_groups"" {
  description = ""Groups assigned to app""
  type        = list(map(any))
  default     = []
}

","The error you're encountering is due to the fact that the `var.app_groups` variable is set to `[{}]` by default, which means it contains an empty map. When the `for_each` loop in the `okta_app_group_assignments` resource encounters an empty map, the `group.value` expression results in an empty map as well. Consequently, trying to access the `""id""` key in the empty map generates the ""Invalid index"" error.

To make the group assignments optional, you can modify your code to handle the scenario when `var.app_groups` is empty. Here's an updated version of your code that incorporates this change:

```hcl
resource ""okta_app_group_assignments"" ""app"" {
  app_id = okta_app_oauth.app.id

  dynamic ""group"" {
    for_each = var.app_groups != [{}] ? var.app_groups : []
    content {
      id       = group.value[""id""]
      priority = group.value[""priority""]
    }
  }
}
```

In the updated code, the `for_each` expression checks if `var.app_groups` is not equal to `[{}]`. If it's not empty, the loop iterates over `var.app_groups` as before. However, if `var.app_groups` is empty, the loop is skipped entirely by setting `for_each` to an empty list (`[]`).

This modification ensures that the `okta_app_group_assignments` resource won't attempt to iterate over an empty map, avoiding the ""Invalid index"" error."
76378487,Group by and select rows based on if value combinations exist,"I have a table PetsTable:




Id
Type
key
value




1
""Cat""
10
5


1
""Cat""
9
2


2
""dog""
10
5


1
""Cat""
8
4


1
""Cat""
6
3


2
""dog""
8
4


2
""dog""
6
3


3
""Cat""
13
5


3
""Cat""
10
0


3
""Cat""
8
0




How to insert this data into a new table MyPets from PetsTable with these conditions:

Group by Id
Only select rows when in the group exists (key = 10 and value = 5) and (key = 8 and value = 4) and (key = 6 and value = 3)
If exists key = 9, then mark hasFee = 1 else hasFee = 0

Final table should look like:




Id
Type
hasFee




1
""Cat""
1


2
""dog""
0



",76378586,"One approach is to use window functions to evaluate your conditions, which you can then apply as conditions using a CTE.
This creates the data you desire, its then trivial to insert into a table of your choice.
create table Test (Id int, [Type] varchar(3), [Key] int, [Value] int);

insert into Test (Id, [Type], [Key], [Value])
values
(1, 'Cat', 10, 5),
(1, 'Cat', 9,  2),
(2, 'Dog', 10, 5),
(1, 'Cat', 8,  4),
(1, 'Cat', 6,  3),
(2, 'Dog', 8,  4),
(2, 'Dog', 6,  3),
(3, 'Cat', 13, 5),
(3, 'Cat', 10, 0),
(3, 'Cat', 8,  0);

with cte as (
  select *
    , sum(case when [Key] = 10 and [Value] = 5 then 1 else 0 end) over (partition by Id) Cond1
    , sum(case when [Key] = 8 and [Value] = 4 then 1 else 0 end) over (partition by Id) Cond2
    , sum(case when [Key] = 6 and [Value] = 3 then 1 else 0 end) over (partition by Id) Cond3
    , sum(case when [Key] = 9 then 1 else 0 end) over (partition by Id) HasFee
  from Test
)
select Id, [Type], HasFee
from cte
where Cond1 = 1 and Cond2 = 1 and Cond3 = 1
group by Id, [Type], HasFee;

Returns:




Id
Type
HasFee




1
Cat
1


2
Dog
0




Note: If you provide your sample data in this format (DDL+DML) you make it much easier for people to assist.
db<>fiddle
","To insert the data into the new table ""MyPets"" with the given conditions, you can use the following SQL query:

```sql
INSERT INTO MyPets (Id, Type, hasFee)
SELECT Id, Type, 
       CASE WHEN EXISTS (SELECT 1 FROM PetsTable WHERE key = 9 AND PetsTable.Id = p.Id) THEN 1 ELSE 0 END AS hasFee
FROM (
  SELECT Id, Type
  FROM PetsTable
  WHERE (key = 10 AND value = 5)
    AND (key = 8 AND value = 4)
    AND (key = 6 AND value = 3)
  GROUP BY Id, Type
  HAVING COUNT(DISTINCT key) = 3
) AS p;
```

This query performs a subquery to filter the rows based on the given conditions using the `WHERE` clause. It groups the rows by `Id` and `Type` and checks if the distinct count of keys in the group is 3, ensuring that all three key-value combinations exist in the group.

The outer query then inserts the selected rows into the ""MyPets"" table, along with the derived column `hasFee`, which is set to 1 if there exists a row with `key = 9` in the group, and 0 otherwise.

After executing this query, the ""MyPets"" table should contain the desired data according to the specified conditions."
76380579,How to store multiple commands in a bash variable (similar to cat otherscript.sh),"For work I'm needing to connect to test nodes and establish a vnc connection so you can see the desktop remotely. It's a manual process with a bunch of commands that need to be executed in order. Perfect for automation using a bash script. The problem is that some commands need to be executed on the remote node after an ssh connection is established.
Currently I've got it working like this, where startVNC is a seperate bash file which stores the commands that need to be executed on the remote node after an ssh connection is established.
cat startVNC | sed -e ""s/\$scaling/$scaling/"" -e ""s/\$address/$address/"" -e ""s/\$display/$display/"" | ssh -X maintain@$host

For my question the contents of startVNC don't really matter, just that multiple commands can be executed in order. It could be:
echo ""hello""
sleep 1
echo ""world""

While for personal use this solution is fine, I find it a bit of a bother that this needs to be done using two separate bash files. If I want to share this file (which I do) it'd be better if it was just one file. My question is, is it possible to mimic the output from cat in some way using a variable?
",76380826,"Well, you could do:
a=""echo 'hello'\nsleep 2\necho world\n""
echo -e $a
#  output-> echo 'hello'
#  output-> sleep 2
#  output-> echo world
echo -e $a | bash
#  output-> hello
#  waiting 2 secs
#  output-> world

The -e in echo enables the interpretation of the \n.
","Yes, it is possible to store multiple commands in a bash variable and execute them in order. One way to achieve this is by using a here document. Here's an example:

```bash
#!/bin/bash

# Store the commands in a variable using a here document
commands=$(cat << EOF
echo ""hello""
sleep 1
echo ""world""
EOF
)

# Execute the commands
eval ""$commands""
```

In this example, the commands are stored in the `commands` variable using a here document. The `<< EOF` syntax allows you to specify multiple lines of input until the `EOF` delimiter is encountered. The commands are then executed using the `eval` command, which evaluates the contents of the variable as if they were written directly in the script.

You can modify the content of the `commands` variable to match your specific commands, including any necessary replacements or variable substitutions."
76383957,How to set ID header in Spring Integration Kafka Message?,"I have a demo Spring Integration project which is receiving Kafka messages, aggregating them, and then releasing them. I'm trying to add JdbcMessageStore to the project. The problem is that it failing with error:
Caused by: java.lang.IllegalArgumentException: Cannot store messages without an ID header
    at org.springframework.util.Assert.notNull(Assert.java:201) ~[spring-core-5.2.15.RELEASE.jar:5.2.15.RELEASE]
    at org.springframework.integration.jdbc.store.JdbcMessageStore.addMessage(JdbcMessageStore.java:314) ~[spring-integration-jdbc-5.3.8.RELEASE.jar:5.3.8.RELEASE]

After debugging I found that it requires the UUID header id in this message. But the problem is that I can't manually set the Kafka header id - it is forbidden (the same as timestamp header) - I tried to do this in Kafka producer in different project.
If I'm using IDEA plugin named Big Data Tools and send a message from there I'm able to set id header but it is received by my project as an array of bytes and it is failing with error
IllegalArgumentException Incorrect type specified for header 'id'. Expected [UUID] but actual type is [B]

I can't find any solution on how to resolve this issue. I need to set somehow this id header to be able to store messages in the database.
Thanks in advance
",76384041,"The KafkaMessageDrivenChannelAdapter has an option:
/**
 * Set the message converter to use with a record-based consumer.
 * @param messageConverter the converter.
 */
public void setRecordMessageConverter(RecordMessageConverter messageConverter) {

Where you can set a MessagingMessageConverter with:
/**
 * Generate {@link Message} {@code ids} for produced messages. If set to {@code false},
 * will try to use a default value. By default set to {@code false}.
 * @param generateMessageId true if a message id should be generated
 */
public void setGenerateMessageId(boolean generateMessageId) {
    this.generateMessageId = generateMessageId;
}

/**
 * Generate {@code timestamp} for produced messages. If set to {@code false}, -1 is
 * used instead. By default set to {@code false}.
 * @param generateTimestamp true if a timestamp should be generated
 */
public void setGenerateTimestamp(boolean generateTimestamp) {
    this.generateTimestamp = generateTimestamp;
}

set to true.
This way the Message created from a ConsumerRecord will have respective id and timestamp headers.
You also simply can have a ""dummy"" transformer to return incoming payload and the framework will create a new Message where those headers are generated.
","To set the ID header in a Spring Integration Kafka message, you can use a custom `KafkaHeaderMapper` implementation. The `KafkaHeaderMapper` is responsible for mapping headers between Kafka messages and Spring Integration messages.

Here's an example of how you can configure the `KafkaHeaderMapper` to set the ID header:

1. Create a custom `KafkaHeaderMapper` implementation:

```java
import org.apache.kafka.common.header.Headers;
import org.springframework.integration.kafka.support.DefaultKafkaHeaderMapper;
import org.springframework.kafka.core.KafkaHeaders;
import org.springframework.messaging.MessageHeaders;

public class CustomKafkaHeaderMapper extends DefaultKafkaHeaderMapper {

    @Override
    public void fromHeaders(MessageHeaders headers, Headers target) {
        super.fromHeaders(headers, target);
        Object id = headers.getOrDefault(""id"", null);
        if (id != null) {
            target.remove(KafkaHeaders.MESSAGE_KEY);
            target.add(""id"", id.toString().getBytes());
        }
    }

    @Override
    public void toHeaders(Headers source, MessageHeaders target) {
        super.toHeaders(source, target);
        byte[] idBytes = source.lastHeader(""id"").value();
        target.put(""id"", new String(idBytes));
    }
}
```

2. Configure the `KafkaHeaderMapper` in your Spring Integration Kafka configuration:

```java
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter;
import org.springframework.integration.kafka.support.KafkaHeaders;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.listener.KafkaMessageListenerContainer;
import org.springframework.kafka.listener.config.ContainerProperties.AckMode;

@Configuration
public class KafkaConfiguration {

    @Bean
    public KafkaMessageDrivenChannelAdapter<String, String> kafkaMessageDrivenChannelAdapter(
            KafkaMessageListenerContainer<String, String> container) {
        KafkaMessageDrivenChannelAdapter<String, String> adapter =
                new KafkaMessageDrivenChannelAdapter<>(container);
        adapter.setOutputChannelName(""inputChannel"");
        return adapter;
    }

    @Bean
    public KafkaHeaderMapper kafkaHeaderMapper() {
        return new CustomKafkaHeaderMapper();
    }

    @Bean
    public KafkaMessageListenerContainer<String, String> kafkaMessageListenerContainer() {
        ContainerProperties containerProperties = new ContainerProperties(""yourTopic"");
        containerProperties.setAckMode(AckMode.MANUAL);
        containerProperties.setMessageListener((AcknowledgingMessageListener<String, String>) (message, ack) -> {
            // Process the received message
            ack.acknowledge();
        });

        DefaultKafkaConsumerFactory<String, String> consumerFactory = new DefaultKafkaConsumerFactory<>(consumerConfigs());
        return new KafkaMessageListenerContainer<>(consumerFactory, containerProperties);
    }

    // Define your Kafka consumer configuration here
    // ...

}
```

In the `CustomKafkaHeaderMapper` implementation, we override the `fromHeaders` method to set the ID header in the Kafka message. We extract the ""id"" header from the Spring Integration message headers and convert it to a byte array before adding it as a custom header to the Kafka message.

In the `toHeaders` method, we retrieve the custom ""id"" header from the Kafka message and convert it back to a string before setting it in the Spring Integration message headers.

Make sure to replace ""yourTopic"" with the actual topic name you're using.

By configuring the `KafkaHeaderMapper` as a bean in your application context, it will be automatically used by the Spring Integration Kafka adapter to map the headers between Kafka messages and Spring Integration messages, allowing you to set the ID header and store the messages in the database successfully."
76383902,Concatenate onto Next Row,"I have some SQL that does some manipulation to the data i.e. filling in empty columns.
SELECT *,
    ModifiedLineData = CASE
        WHEN Column2 = '' AND LineData NOT LIKE ',,,0,,,,0'
            THEN CONCAT(STUFF(LineData, CHARINDEX(',', LineData, CHARINDEX(',', LineData) + 1), 0, '""No PO Number""'), ',""""')
        ELSE CONCAT(LineData, ',""""')
    END
FROM (
    SELECT
        *,
        Column2 = CONVERT(XML, '<s>' + REPLACE((SELECT ISNULL(LineData, '') FOR XML PATH('')), ',', '</s><s>') + '</s>').value('/s[2]', 'varchar(100)')
    FROM [dbo].[Temp_Raw_Data]
    WHERE LineData NOT LIKE ',,,0,,,,0'
) AS Subquery

Now lets say this returns




FileName
LineNumber
LineData
Column2
ModifiedLineData




file1
4
1232,,""product-1"", 1,0

1232,NA,""product-1"", 1,0


file2
7
""failed""
NULL
""failed""


file3
8
1235,,""product-2"", 1,0

1235,NA,""product-2"", 1,0




How can I modify this query so that if Column2 is NULL then it would concatenate the LineData onto the next row (ModifiedLineData) else just concatenate a ,"""" and then remove that NULL result (if possible else it doesnt matter) so that my result would look like:




FileName
LineNumber
LineData
Column2
ModifiedLineData




file1
4
1232,,""product-1"", 1,0

1232,NA,""product-1"", 1,0,""""


file3
8
1235,,""product-2"", 1,0

1235,NA,""product-2"", 1,0,""failed""




I tried playing around with LEAD() but couldn't get it how i wanted.
Note: Two null rows are not possible to be together. This is due to the nature of the data. The next row should simply be the next available row when selecting all rows as they are imported one by 1.
Updated Query that isn't concatenating:
SELECT * 
  FROM (SELECT FileName, LineNumber, LineData, Column2, 
               CASE WHEN LAG(Column2) OVER(ORDER BY LineNumber) IS NULL
                    THEN CONCAT_WS(', ',
                                ModifiedLineData, 
                                LAG(ModifiedLineData) OVER(ORDER BY LineNumber))
                    ELSE ModifiedLineData
               END AS ModifiedLineData
        FROM (
        SELECT *,
            ModifiedLineData = CASE
                WHEN Column2 = '' AND LineData NOT LIKE ',,,0,,,,0'
                    THEN CONCAT(STUFF(LineData, CHARINDEX(',', LineData, CHARINDEX(',', LineData) + 1), 0, '""No PO Number""'), '')
                ELSE CONCAT(LineData, '')
            END
        FROM (
            SELECT *,
                Column2 = CONVERT(XML, '<s>' + REPLACE((SELECT ISNULL(LineData, '') FOR XML PATH('')), ',', '</s><s>') + '</s>').value('/s[2]', 'varchar(100)')
            FROM [backstreet_WMS_Optimizer].[dbo].[Temp_GoodsIn_Raw_Data]
            WHERE LineData NOT LIKE ',,,0,,,,0'
        ) AS Subquery
    ) AS cte
) AS Subquery
WHERE Column2 IS NOT NULL
order by FileName, LineNumber

",76384109,"Given that you can't have consecutive NULL values, using LEAD/LAG should be suitable for this task. Without knowledge of your original data, we can work on your query and add on top two subqueries, last of which is optional:

the inner adds the information needed to the record successive to ""Column2=NULL"" records
the outer removes records having those null values

SELECT * 
  FROM (SELECT FileName, LineNumber, LineData, Column2, 
               CASE WHEN LAG(Column2) OVER(ORDER BY LineNumber) IS NULL
                    THEN CONCAT_WS(', ',
                                ModifiedLineData, 
                                LAG(ModifiedLineData) OVER(ORDER BY LineNumber))
                    ELSE ModifiedLineData
               END AS ModifiedLineData
        FROM <your query>) cte
WHERE Column2 IS NOT NULL 

Output:




FileName
LineNumber
LineData
Column2
ModifiedLineData




file1
4
1232,,""product-1"", 1,0

1232,NA,""product-1"", 1,0


file3
8
1235,,""product-2"", 1,0

1235,NA,""product-2"", 1,0""failed""




Check the demo here.
","To modify the query to concatenate the LineData onto the next row's ModifiedLineData when Column2 is NULL, you can use the LAG() function to check the previous row's value of Column2. Here's an updated version of the query:

```sql
SELECT *
FROM (
    SELECT FileName, LineNumber, LineData, Column2, 
           CASE
               WHEN LAG(Column2) OVER (ORDER BY LineNumber) IS NULL
               THEN CONCAT_WS(', ', LAG(ModifiedLineData) OVER (ORDER BY LineNumber), LineData)
               ELSE ModifiedLineData
           END AS ModifiedLineData
    FROM (
        SELECT *,
            ModifiedLineData = CASE
                WHEN Column2 = '' AND LineData NOT LIKE ',,,0,,,,0'
                THEN CONCAT(STUFF(LineData, CHARINDEX(',', LineData, CHARINDEX(',', LineData) + 1), 0, '""""No PO Number""""'), ',""""""""')
                ELSE CONCAT(LineData, ',""""""""')
            END
        FROM (
            SELECT *,
                Column2 = CONVERT(XML, '<s>' + REPLACE((SELECT ISNULL(LineData, '') FOR XML PATH('')), ',', '</s><s>') + '</s>').value('/s[2]', 'varchar(100)')
            FROM [dbo].[Temp_Raw_Data]
            WHERE LineData NOT LIKE ',,,0,,,,0'
        ) AS Subquery
    ) AS cte
) AS Subquery
WHERE Column2 IS NOT NULL
ORDER BY FileName, LineNumber
```

In this modified query, the LAG(Column2) function is used to get the value of Column2 from the previous row. If the previous row's Column2 is NULL, then the LineData is concatenated with the previous row's ModifiedLineData using CONCAT_WS(). Otherwise, the original ModifiedLineData is used.

Make sure to adjust the table and column names as per your actual database schema."
76378480,How do I get my main content to take up the rest of the space left over after the header and footer?,"I'm working through The Odin Project and I'm having trouble making my main content take up the rest of the space of the browser.
Right now it looks like this:

The 1px solid red border is as far as the main content goes. I have tried this but it's not allowing for a fixed header and footer. I have also tried some other flex solutions. Those are commented out in the code.
Am I just doing this whole thing wrong? Is there a standard way that I don't know about?
index.html:
<body>
    <div class=""header"">
        <h1>
            MY AWESOME WEBSITE
        </h1>
    </div>

    <div class=""main-content"">
        <div class=""sidebar"">
            <ul>
                <li><a href=""#"">⭐ - link one</a></li>
                <li><a href=""#"">🦸🏽‍♂️ - link two</a></li>
                <li><a href=""#"">🖌️ - link three</a></li>
                <li><a href=""#"">👌🏽 - link four</a></li>
            </ul>
        </div>
        <div class=""content"">
            <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Tempora, eveniet? Dolorem
                dignissimos
                maiores non delectus possimus dolor nulla repudiandae vitae provident quae, obcaecati ipsam unde impedit
                corrupti veritatis minima porro?</div>
            <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Quasi quaerat qui iure ipsam
                maiores
                velit tempora, deleniti nesciunt fuga suscipit alias vero rem, corporis officia totam saepe excepturi
                odit
                ea.
            </div>
            <div class=""card"">Lorem ipsum dolor sit amet consectetur, adipisicing elit. Nobis illo ex quas, commodi
                eligendi
                aliquam ut, dolor, atque aliquid iure nulla. Laudantium optio accusantium quaerat fugiat, natus officia
                esse
                autem?</div>
            <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Necessitatibus nihil impedit eius
                amet
                adipisci dolorum vel nostrum sit excepturi corporis tenetur cum, dolore incidunt blanditiis. Unde earum
                minima
                laboriosam eos!</div>
            <div class=""card"">Lorem ipsum dolor sit amet consectetur, adipisicing elit. Nobis illo ex quas, commodi
                eligendi
                aliquam ut, dolor, atque aliquid iure nulla. Laudantium optio accusantium quaerat fugiat, natus officia
                esse
                autem?</div>
            <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Necessitatibus nihil impedit eius
                amet
                adipisci dolorum vel nostrum sit excepturi corporis tenetur cum, dolore incidunt blanditiis. Unde earum
                minima
                laboriosam eos!</div>
        </div>
    </div>

    <div class=""footer"">
        The Odin Project ❤️
    </div>
</body>

</html>

style-07.css:
:root{
    --header-height: 72px;
}
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    margin: 0;
    min-height: 100vh;
    height: 100%;
}

.main-content{
    display: flex;
    height: 100%; /* If I use px units it will force the main content to go down but I know that is not ideal. */
    padding-top: var(--header-height); 
    flex-direction: row;
    border: 1px solid red;
    /* Things I have tried from other answers*/
    /* flex: 1 1 auto; */
    /* height: calc(100% - var(--header-height)); */
}

.sidebar{
    flex-shrink: 0;
}

.content {
    padding: 32px;
    display: flex;
    flex-wrap: wrap;
}

.card {
    width: 300px;
    padding: 16px;
    margin: 16px;
}

.header {
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    display: flex;
    align-items: center;
    height: var(--header-height);
    background: darkmagenta;
    color: white;
    padding: 0px 15px;
}

h1 {
    font-weight: 1000;
}

.footer {
    height: var(--header-height);
    background: #eee;
    color: darkmagenta;
    position: fixed;
    bottom: 0;
    left: 0;
    right: 0;
    width: 100%;
    height: 5%;
    display: flex;
    justify-content: center;
    align-items: center;
}

.sidebar {
    width: 300px;
    background: royalblue;
    box-sizing: border-box;
    padding: 16px;
}

.card {
    border: 1px solid #eee;
    box-shadow: 2px 4px 16px rgba(0, 0, 0, .06);
    border-radius: 4px;
}

ul{
    list-style-type: none;
    margin: 0;
    padding: 0;
}

a {
    text-decoration: none;
    color: white;
    font-size: 24px;
}

li{
    margin-bottom: 16px;
}

",76378588,"You can use flex diplay on body instead of using instead of fixed on header and footer and make the body display flex with column direction, then for main-content all you need is to set flex: 1 and remove padding top, flex: 1 will make sure that main-content take any remaining space in the parent. Set the body height to height: 100vh and overflow: hidden, for man-content, set overflow: auto.
Additionally, To make sidebar sticky when scrolling,  I added position: relative; to main-content and position: sticky; to the sidebar.
To force header and footer heights and prevent them to be squeezed by the flex position, use min-height instead of height as I modified in the code.
Try to view the run code in full page, if you have any further questions, comment below.


:root {
  --header-height: 72px;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  margin: 0;
  height: 100vh;
  overflow:hidden;
  
  display: flex; 
  flex-direction: column;
}

.main-content {
  flex: 1;
  display: flex;
  overflow-y: auto;
  /* If I use px units it will force the main content to go down but I know that is not ideal. */
  flex-direction: row;
  border: 1px solid red;
  /* Things I have tried from other answers*/
  /* flex: 1 1 auto; */
  /* height: calc(100% - var(--header-height)); */
  
  position: relative;
}



.content {
  padding: 32px;
  display: flex;
  flex-wrap: wrap;
}

.card {
  width: 300px;
  padding: 16px;
  margin: 16px;
}

.header {

 
  display: flex;
  align-items: center;
  min-height: var(--header-height);
  background: darkmagenta;
  color: white;
  padding: 0px 15px;
}

h1 {
  font-weight: 1000;
}

.footer {
  min-height: var(--header-height);
  background: #eee;
  color: darkmagenta;
 
  width: 100%;
  height: 5%;
  display: flex;
  justify-content: center;
  align-items: center;
}

.sidebar {
  width: 300px;
  background: royalblue;
  box-sizing: border-box;
  padding: 16px;
  
  position: sticky;
  top: 0;
  
  white-space: nowrap;
  min-height: 250px;
}

.card {
  border: 1px solid #eee;
  box-shadow: 2px 4px 16px rgba(0, 0, 0, .06);
  border-radius: 4px;
}

ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

a {
  text-decoration: none;
  color: white;
  font-size: 24px;
}

li {
  margin-bottom: 16px;
}
<body>
  <div class=""header"">
    <h1>
      MY AWESOME WEBSITE
    </h1>
  </div>

  <div class=""main-content"">
    <div class=""sidebar"">
      <ul>
        <li><a href=""#"">⭐ - link one</a></li>
        <li><a href=""#"">🦸🏽‍♂️ - link two</a></li>
        <li><a href=""#"">🖌️ - link three</a></li>
        <li><a href=""#"">👌🏽 - link four</a></li>
      </ul>
    </div>
    <div class=""content"">
      <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Tempora, eveniet? Dolorem dignissimos maiores non delectus possimus dolor nulla repudiandae vitae provident quae, obcaecati ipsam unde impedit corrupti veritatis minima porro?</div>
      <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Quasi quaerat qui iure ipsam maiores velit tempora, deleniti nesciunt fuga suscipit alias vero rem, corporis officia totam saepe excepturi odit ea.
      </div>
      <div class=""card"">Lorem ipsum dolor sit amet consectetur, adipisicing elit. Nobis illo ex quas, commodi eligendi aliquam ut, dolor, atque aliquid iure nulla. Laudantium optio accusantium quaerat fugiat, natus officia esse autem?
      </div>
      <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Necessitatibus nihil impedit eius amet adipisci dolorum vel nostrum sit excepturi corporis tenetur cum, dolore incidunt blanditiis. Unde earum minima laboriosam eos!</div>
      <div class=""card"">Lorem ipsum dolor sit amet consectetur, adipisicing elit. Nobis illo ex quas, commodi eligendi aliquam ut, dolor, atque aliquid iure nulla. Laudantium optio accusantium quaerat fugiat, natus officia esse autem?
      </div>
      <div class=""card"">Lorem ipsum dolor sit amet consectetur adipisicing elit. Necessitatibus nihil impedit eius amet adipisci dolorum vel nostrum sit excepturi corporis tenetur cum, dolore incidunt blanditiis. Unde earum minima laboriosam eos!</div>
    </div>
  </div>

  <div class=""footer"">
    The Odin Project ❤️
  </div>
</body>

</html>



","To make your main content take up the remaining space after the header and footer, you can use flexbox with a combination of `flex-grow` and `flex-shrink` properties. Here's how you can modify your CSS:

```css
body {
  display: flex;
  flex-direction: column;
  min-height: 100vh;
  margin: 0;
}

.main-content {
  flex-grow: 1; /* Allow main content to grow and take up remaining space */
  display: flex;
  flex-direction: row;
}

.content {
  flex-grow: 1; /* Allow content to grow and take up remaining space */
  padding: 32px;
  display: flex;
  flex-wrap: wrap;
}

/* Remove the height property from .main-content */
.main-content {
  /* height: 100%; Remove this line */
}

/* Remove the flex-shrink property from .sidebar */
.sidebar {
  /* flex-shrink: 0; Remove this line */
}
```

By setting `flex-grow: 1` on both the `.main-content` and `.content` elements, you allow them to expand and occupy the remaining space. Additionally, you can remove the `height` property from `.main-content` and remove the `flex-shrink` property from `.sidebar`.

With these changes, your main content should take up the remaining space between the header and footer."
76384080,Can't initialize variables inside of when statement in Kotlin,"For whatever reason, my Kotlin program won't initialize variables assigned inside a when statement. Here's the code:
import kotlin.random.Random
import kotlin.random.nextInt

val mood: String

when(Random.nextInt(1..2)) {
    1 -> {
        mood = ""loud""
        println(""$mood"")
    }
    2 -> {
        mood = ""quiet""
        println(""$mood"")
    }
}

println(""$mood"")

The lines inside the when statement are printed, but when I run the last line, I get a ""Variable 'mood' must be initialized"" error.
I don't know what I could possibly be doing wrong here...
",76384128,"In Kotlin, variables declared with the val keyword must be initialized at the point of declaration or in the constructor of the class. In your code, the mood variable is declared without an initial value, and you are trying to assign values to it inside the when statement. However, the compiler is unable to determine if either of the branches will be executed at runtime, so it doesn't consider the variable as fully initialized.
To fix this issue, you can either declare the mood variable as a var instead of a val or assign an initial value to it when declaring it. Here's an updated version of your code using a var:
import kotlin.random.Random
import kotlin.random.nextInt
var mood: String

when (Random.nextInt(1..2)) {
    1 -> {
        mood = ""loud""
        println(""$mood"")
    }
    2 -> {
        mood = ""quiet""
        println(""$mood"")
    }
}

println(""$mood"")

By using a var instead of a val, you indicate that the variable can be reassigned later. Since the mood variable is assigned within both branches of the when statement, the compiler no longer complains about it being uninitialized.
Note that the order of the when branches should cover all possible cases, otherwise you might encounter a ""when expression must be exhaustive"" warning. In your case, the range of nextInt is 1 to 2, so the two branches should be sufficient.
","In Kotlin, the `when` statement is an expression, which means that it should return a value. However, in your code, you're trying to assign a value to the `mood` variable inside each branch of the `when` statement, which makes it impossible for the compiler to guarantee that the variable will be initialized before it's used outside the `when` statement.

To fix this issue, you can initialize the `mood` variable with a default value and then update it inside the `when` statement. Here's an example:

```kotlin
import kotlin.random.Random
import kotlin.random.nextInt

val mood: String = when (Random.nextInt(1..2)) {
    1 -> {
        ""loud""
    }
    2 -> {
        ""quiet""
    }
    else -> {
        // Handle any other possible values
        ""unknown""
    }
}

println(mood)
```

In this code, the `mood` variable is initialized with a default value of `""unknown""`. Inside the `when` statement, the variable is assigned the appropriate value based on the random number generated. This ensures that the `mood` variable is always initialized before it's used.

Note that in this example, the `else` branch is added to handle any unexpected values from the `when` expression. You can decide how you want to handle those cases in your program."
76380728,Flutter Deep Link Firebase in iOS,"My deep link works fine on Android and transfers information to the app, but it doesn't work on iOS
Firebase Link
https://dvzpl.com

my short link
https://dvzpl.com/6BG2

my domain
https://dovizpanel.com/

my associated domain
<dict>
    <key>aps-environment</key>
    <string>development</string>
    <key>com.apple.developer.associated-domains</key>
    <array>
        <string>webcredentials:dvzpl.com</string>
        <string>applinks:dvzpl.com</string>
    </array>
</dict>

how to fix ?
When I open the short link in the browser, it goes into the app but does not transfer the data in ios , android working not problams
<key>FirebaseDynamicLinksCustomDomains</key>
<array>
    <string>https://dovizpanel.com/blog</string>
    <string>https://dovizpanel.com/exchanger</string>
    <string>https://dovizpanel.com/link</string>
</array>

",76380908,"If you are using a custom domain for firebase dynamic links follow the instructions below:
In your Xcode project's Info.plist file, create a key called FirebaseDynamicLinksCustomDomains and set it to your app's Dynamic Links URL prefixes. For example:
<key>FirebaseDynamicLinksCustomDomains</key>
<array>
  <string>https://dvzpl.com</string>
</array>

You can find more details directly in the Firebase documentation.
","To troubleshoot the issue with deep linking not working on iOS while it works fine on Android, you can follow these steps:

1. Check the Associated Domains: Make sure that the `Associated Domains` capability is enabled in your Xcode project. This capability allows your app to handle incoming deep links. Verify that the correct domains are listed in the `com.apple.developer.associated-domains` array in your Xcode project's entitlements file.

2. Verify the Custom Domains in Firebase Console: In the Firebase console, under your project settings, make sure that the custom domains you're using (`https://dovizpanel.com/blog`, `https://dovizpanel.com/exchanger`, `https://dovizpanel.com/link`) are added to the `FirebaseDynamicLinksCustomDomains` array in your `Info.plist` file. This step ensures that Firebase recognizes these domains as valid for deep linking.

3. Test Associated Domains: Test the associated domains by opening the links in Safari on your iOS device. If the links don't open in your app, ensure that the entitlements file and `Info.plist` file are configured correctly with the associated domains.

4. Check URL Handling: Verify that you have implemented the URL handling code in your app correctly. In your app delegate, implement the `application(_:open:options:)` method to handle the deep link URL and extract any necessary data. Make sure the method is correctly implemented for iOS. You can refer to the Firebase documentation for the proper implementation.

5. Test with Debugging: Test your app with Xcode's debugging tools. Run your app on a physical device, click on the deep link, and check if there are any errors or warnings logged in the Xcode console. This can help identify any issues with the deep link handling process.

6. Ensure Universal Links Setup: If you're using Universal Links for deep linking on iOS, make sure you have correctly set up the `apple-app-site-association` file on your web server. The file should be accessible at `https://dvzpl.com/apple-app-site-association` and contain the necessary app association details.

By going through these steps, you should be able to identify and resolve any issues with deep linking on iOS."
76384218,How to toggle/display content individually in ReactJS,"my question is how can I toggle/display the ""Some text"" content on onClick individually?.
I can use different function and state for every div an it is working but I know this is not the correct way to do it .
Can you help me with this guys? Thanks
This is my code
function App() {
  const [loaded, setLoaded] = useState(true);
  const [show, setShow] = useState(false);

  const handleShow = () => {
    setShow(!show);
  };

  return (
    <div className={styles.App}>
      {loaded && (
        <div className={styles.cards_container}>
          <div className={styles.card_container} onClick={handleShow}>
            <h3>Title</h3>
            {show && (
              <div>
                <p>Some text</p>
              </div>
            )}
          </div>
          <div className={styles.card_container} onClick={handleShow}>
            <h3>Title</h3>
            {show && (
              <div>
                <p>Some text</p>
              </div>
            )}
          </div>
          <div className={styles.card_container} onClick={handleShow}>
            <h3>Title</h3>
            {show && (
              <div>
                <p>Some text</p>
              </div>
            )}
          </div>
        </div>
      )}
    </div>
  );
}

",76384269,"You could create a custom component for your card that handles the state for each card:
function Card() {
    const [show, setShow] = useState(false);

    const handleShow = () => {
      setShow(state => !state);
    };

    return <div className={styles.card_container} onClick={handleShow}>
      <h3>Title</h3>

      {show && (
          <div>
            <p>Some text</p>
          </div>
      )}
    </div>
}

And use it in your app:
function App() {
  const [loaded, setLoaded] = useState(true);

  return (
    <div className={styles.App}>
      {loaded && (
        <div className={styles.cards_container}>
          <Card />
          <Card />
          <Card />
        </div>
      )}
    </div>
  );
}

","In your current code, you are using a single `show` state variable to control the visibility of all the ""Some text"" content. If you want to toggle/display the content individually for each div, you can create an array of boolean values to track the visibility of each div's content. Here's an example of how you can modify your code to achieve this:

```jsx
function App() {
  const [loaded, setLoaded] = useState(true);
  const [show, setShow] = useState([false, false, false]); // Array of boolean values

  const handleShow = (index) => {
    setShow((prevShow) => {
      const newShow = [...prevShow];
      newShow[index] = !newShow[index];
      return newShow;
    });
  };

  return (
    <div className={styles.App}>
      {loaded && (
        <div className={styles.cards_container}>
          <div className={styles.card_container} onClick={() => handleShow(0)}>
            <h3>Title</h3>
            {show[0] && (
              <div>
                <p>Some text</p>
              </div>
            )}
          </div>
          <div className={styles.card_container} onClick={() => handleShow(1)}>
            <h3>Title</h3>
            {show[1] && (
              <div>
                <p>Some text</p>
              </div>
            )}
          </div>
          <div className={styles.card_container} onClick={() => handleShow(2)}>
            <h3>Title</h3>
            {show[2] && (
              <div>
                <p>Some text</p>
              </div>
            )}
          </div>
        </div>
      )}
    </div>
  );
}
```

In the modified code, I've replaced the single `show` state variable with an array `show` containing three boolean values, each representing the visibility of the content for each div. The `handleShow` function is updated to take an index parameter corresponding to the index of the div being clicked. When a div is clicked, it toggles the corresponding boolean value in the `show` array using the index. Finally, in each div, we use the respective `show` array element to conditionally render the ""Some text"" content for that div.

This way, each div maintains its own state for toggling/displaying the content individually."
76383839,Revoking permission to install plugins?,"The following query was used as part of a security audit to identify users with access to install/uninstall server plugins at the database level.
SELECT user, host FROM mysql.db WHERE db = 'mysql' and (insert_priv='y') or (delete_priv='y') or (insert_priv='y' and delete_priv='y');

I need to revoke that permission from the users that are listed.  Is there a specific privilege I revoke to do this? If so, I can't find it.  Or would I simply UPDATE the insert_priv and delete_priv fields directly in the mysql.db table?  I'm not a DBA but the closest thing we have at the moment.
",76384284,"You are able to install plugins when you have INSERT permissions on the mysql.plugin table, see INSTALL PLUGIN:

To use INSTALL PLUGIN, you must have the INSERT privilege for the mysql.plugin table.

So when you have database wide INSERT permissions on the (internal administrative) database mysql, then you can install plugins.
The same goes for the UNINSTALL PLUGIN statement, see UNINSTALL PLUGIN

To use UNINSTALL PLUGIN, you must have the DELETE privilege for the mysql.plugin table.

Remove the insert_priv and delete_priv privileges for the mysql database, your ""normal"" MySQL user accounts shouldn't be able to write in this database anyway.
","To revoke the permission to install/uninstall server plugins for the users listed in the query, you can update the `mysql.db` table directly. Here's the approach you can take:

1. Identify the users that you want to revoke the permission from based on the results of the query you mentioned.
2. Connect to your MySQL database server with administrative privileges.
3. Execute an `UPDATE` statement to modify the `insert_priv` and `delete_priv` fields for those users in the `mysql.db` table. Set both `insert_priv` and `delete_priv` to `'n'` to revoke the corresponding privileges.

Here's an example of how you can perform the update:

```sql
UPDATE mysql.db
SET insert_priv = 'n', delete_priv = 'n'
WHERE db = 'mysql'
  AND (
    insert_priv = 'y' OR
    delete_priv = 'y' OR
    (insert_priv = 'y' AND delete_priv = 'y')
  )
  AND user = 'username' AND host = 'userhost';
```

Make sure to replace `'username'` and `'userhost'` with the actual values for the user you want to revoke the permission from. Repeat this `UPDATE` statement for each user you want to modify.

By setting both `insert_priv` and `delete_priv` to `'n'`, you effectively revoke the privileges to install/uninstall server plugins for the specified users."
76378670,pandas dataframe query not working with where,"I am new to pandas, I have this data frame:
df['educ1']
which gives
1        4
2        3
3        3
4        4
5        1
        ..
28461    3
28462    2
28463    3
28464    2
28465    4
Name: educ1, Length: 28465, dtype: int64

when I try querying with
dt=df[df.educ1 > 1]

It's working fine returning multiple rows, but when I try
college_grad_mask=(df.educ1 > 1)
df.where(college_grad_mask).dropna().head()

It gives 0 rows, I wonder what is wrong here?
",76378715,"You likely have NaNs in many columns, try to subset:
df.where(college_grad_mask).dropna(subset=['educ1']).head()

Or better:
df[college_grad_mask].head()

","The issue with your code is that you're using the `df.where()` function, which replaces the values where the condition is False with NaN (missing values). When you subsequently call `dropna()`, it removes all the rows containing NaN values, resulting in an empty DataFrame.

To achieve your desired result using `df.where()`, you can modify your code as follows:

```python
college_grad_mask = (df.educ1 > 1)
df.where(college_grad_mask, inplace=True)
df.dropna(inplace=True)
df.head()
```

In this code, the `inplace=True` parameter ensures that the modifications are made directly to the original DataFrame, rather than creating a new DataFrame. By passing `college_grad_mask` to the `where()` function, it replaces the rows where the condition is False with NaN values. Finally, calling `dropna()` removes the rows containing NaN values, giving you the desired result.

However, a simpler and more efficient way to achieve the same result is by using boolean indexing directly, as you did in your first code snippet:

```python
dt = df[df.educ1 > 1]
dt.head()
```

This code filters the DataFrame using boolean indexing, selecting only the rows where the condition `df.educ1 > 1` is True. This approach is straightforward and more commonly used in pandas."
76378383,Problem when scoring new data -- tidymodels,"I'm learning tidymodels. The following code runs nicely:
library(tidyverse)
library(tidymodels)

# Draw a random sample of 2000 to try the models

set.seed(1234)

diamonds <- diamonds %>%    
  sample_n(2000)
  
diamonds_split <- initial_split(diamonds, prop = 0.80, strata=""price"")

diamonds_train <- training(diamonds_split)
diamonds_test <- testing(diamonds_split)

folds <- rsample::vfold_cv(diamonds_train, v = 10, strata=""price"")

metric <- metric_set(rmse,rsq,mae)

# Model KNN 

knn_spec <-
  nearest_neighbor(
    mode = ""regression"", 
    neighbors = tune(""k""),
    engine = ""kknn""
  ) 

knn_rec <-
  recipe(price ~ ., data = diamonds_train) %>%
  step_log(all_outcomes()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

knn_wflow <- 
  workflow() %>% 
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

knn_grid = expand.grid(k=c(1,5,10,30))

knn_res <- 
  tune_grid(
    knn_wflow,
    resamples = folds,
    metrics = metric,
    grid = knn_grid
  )

collect_metrics(knn_res)
autoplot(knn_res)

show_best(knn_res,metric=""rmse"")

# Best KNN 

best_knn_spec <-
  nearest_neighbor(
    mode = ""regression"", 
    neighbors = 10,
    engine = ""kknn""
  ) 

best_knn_wflow <- 
  workflow() %>% 
  add_model(best_knn_spec) %>%
  add_recipe(knn_rec)

best_knn_fit <- last_fit(best_knn_wflow, diamonds_split)

collect_metrics(best_knn_fit)


But when I try to fit the best model on the training set and applying it to the test set I run into problems. The following two lines give me the error : ""Error in step_log():
! The following required column is missing from new_data in step 'log_mUSAb': price.
Run rlang::last_trace() to see where the error occurred.""
# Predict Manually

f1 = fit(best_knn_wflow,diamonds_train)
p1 = predict(f1,new_data=diamonds_test)

",76378734,"This problem is related to log transform outcome variable in tidymodels workflow
For log transformations to the outcome, we strongly recommend that those transformation be done before you pass them to the recipe(). This is because you are not guaranteed to have an outcome when predicting (which is what happens when you last_fit() a workflow) on new data. And the recipe fails.
You are seeing this here as when you predict on a workflow() object, it only passes the predictors, as it is all that it needs. Hence why you see this error.
Since log transformations isn't a learned transformation you can safely do it before.
diamonds_train$price <- log(diamonds_train$price)

if (!is.null(diamonds_test$price)) {
  diamonds_test$price <- log(diamonds_test$price)
}

","The error you're encountering is due to the missing ""price"" column in the new_data when trying to apply the best model to the test set. This occurs because the recipe used to create the best_knn_wflow object includes a step_log() transformation that requires the ""price"" column to be present.

To resolve this issue, you can modify the recipe by removing the step_log() transformation. Here's the modified code:

```R
best_knn_rec <-
  recipe(price ~ ., data = diamonds_train) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())

best_knn_wflow <- 
  workflow() %>% 
  add_model(best_knn_spec) %>%
  add_recipe(best_knn_rec)

best_knn_fit <- last_fit(best_knn_wflow, diamonds_split)

collect_metrics(best_knn_fit)

# Predict Manually
f1 <- fit(best_knn_wflow, diamonds_train)
p1 <- predict(f1, new_data = diamonds_test)
```

By removing the step_log() transformation from the recipe used to create the best_knn_wflow, you won't encounter the error related to the missing ""price"" column when making predictions on the test set.

Note that you should use the modified recipe (`best_knn_rec`) when creating the best_knn_wflow object."
76380693,How to name a term created in the formula when calling `lm()`?,"Is it possible to name a term created in a formula? This is the scenario:
Create a toy dataset:
set.seed(67253)
n <- 100
x <- sample(c(""A"", ""B"", ""C""), size = n, replace = TRUE)
y <- sapply(x, switch, A = 0, B = 2, C = 1) + rnorm(n, 2)
dat <- data.frame(x, y)
head(dat)
#>   x         y
#> 1 B 4.5014474
#> 2 C 4.0252796
#> 3 C 2.4958761
#> 4 C 0.6725571
#> 5 B 4.3364206
#> 6 C 3.9798909

Fit a regression model:
out <- lm(y ~ x, dat)
summary(out)
#> 
#> Call:
#> lm(formula = y ~ x, data = dat)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -2.07296 -0.52161 -0.03713  0.53898  2.12497 
#> 
#> Coefficients:
#>             Estimate Std. Error t value Pr(>|t|)    
#> (Intercept)   2.1138     0.1726  12.244  < 2e-16 ***
#> xB            1.6772     0.2306   7.274 9.04e-11 ***
#> xC            0.5413     0.2350   2.303   0.0234 *  
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 0.9297 on 97 degrees of freedom
#> Multiple R-squared:  0.3703, Adjusted R-squared:  0.3573 
#> F-statistic: 28.52 on 2 and 97 DF,  p-value: 1.808e-10

Fit the model again, but use ""C"" as the reference group:
out2 <- lm(y ~ relevel(factor(x), ref = ""C""), dat)
summary(out2)
#> 
#> Call:
#> lm(formula = y ~ relevel(factor(x), ref = ""C""), data = dat)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -2.07296 -0.52161 -0.03713  0.53898  2.12497 
#> 
#> Coefficients:
#>                                Estimate Std. Error t value Pr(>|t|)    
#> (Intercept)                      2.6551     0.1594  16.653  < 2e-16 ***
#> relevel(factor(x), ref = ""C"")A  -0.5413     0.2350  -2.303   0.0234 *  
#> relevel(factor(x), ref = ""C"")B   1.1359     0.2209   5.143 1.41e-06 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 0.9297 on 97 degrees of freedom
#> Multiple R-squared:  0.3703, Adjusted R-squared:  0.3573 
#> F-statistic: 28.52 on 2 and 97 DF,  p-value: 1.808e-10

The variable, x, was re-leveled in the second call to lm(). This is done in the formula and so the name of this term is relevel(factor(x), ref = ""C"").
Certainly, we can create the term before calling lm(), e.g.:
dat$x2 <- relevel(factor(x), ref = ""C"")
out3 <- lm(y ~ x2, dat)
summary(out3)
#> 
#> Call:
#> lm(formula = y ~ x2, data = dat)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -2.07296 -0.52161 -0.03713  0.53898  2.12497 
#> 
#> Coefficients:
#>             Estimate Std. Error t value Pr(>|t|)    
#> (Intercept)   2.6551     0.1594  16.653  < 2e-16 ***
#> x2A          -0.5413     0.2350  -2.303   0.0234 *  
#> x2B           1.1359     0.2209   5.143 1.41e-06 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 0.9297 on 97 degrees of freedom
#> Multiple R-squared:  0.3703, Adjusted R-squared:  0.3573 
#> F-statistic: 28.52 on 2 and 97 DF,  p-value: 1.808e-10

However, can I create a term and name it in the formula? If yes, how?
",76380922,"adapted from the info in this comment : Rename model terms in lm object for forecasting
set.seed(67253)
n <- 100
x <- sample(c(""A"", ""B"", ""C""), size = n, replace = TRUE)
y <- sapply(x, switch, A = 0, B = 2, C = 1) + rnorm(n, 2)
dat <- data.frame(x, y)

out <- lm(y ~ x, dat)
summary(out)

out2 <- lm(y ~ x2, transform(dat,
                             x2=relevel(factor(x), ref = ""C"")))
summary(out2)

","Yes, it is possible to create a term and name it directly in the formula when calling `lm()`. In R, you can use the `I()` function to create a named term within the formula.

Here's an example of how you can create and name a term in the formula:

```R
out4 <- lm(y ~ I(x2 := relevel(factor(x), ref = ""C"")), dat)
summary(out4)
```

In this example, `x2` is the name given to the term created using the `I()` function. The `:=` operator is used to assign the result of `relevel(factor(x), ref = ""C"")` to `x2`. The formula `y ~ I(x2)` specifies that `y` is regressed on the term `x2`. By naming the term in the formula, it will appear with the specified name in the model output and summary.

Note that the `:=` operator is not a standard operator in R but is used here to illustrate the naming of the term within the formula. You can use any valid R assignment operator to achieve the same effect.

Please note that the `:=` operator is not available in all R packages or versions. If you encounter any issues with it, you can use other assignment operators such as `=` or `<-` to create and name the term before calling `lm()`, as shown in your example with `x2`."
76378708,Translating Stata to R yields different results,"I am trying to translate a Stata code from a paper into R.
The Stata code looks like this:
g tau = year - temp2 if temp2 > temp3 & (bod<. | do<. | lnfcoli<.)

My R translation looks like this:
data <- data %>%
  mutate(tau = if_else((temp2 > temp3) & 
                         (is.na(bod) | is.na(do) | is.na(lnfcoli)), 
                       year - temp2,
                       NA_integer_))

The problem is that when I run each code I get different results.
This is the result I get when I run the code in Stata:
1   Year |  temp2  |  temp3 | bod | do  | lnfcoli | tau |
2   1986 |  1995   |  1986  | 3.2 | 7.2 | 2.1.    |  -9 |

This is the result I get when I run the code in R:
1   Year |  temp2  |  temp3 | bod | do  | lnfcoli | tau |
2   1986 |  1995   |  1986  | 3.2 | 7.2 | 2.1.    |  NA |

Do you know what might be wrong with my R code or what should I modify to get the same output?
",76378750,"None of bod, do or lnfcoli are missing (NA), so your logic returns FALSE and returns NA_integer_ (false= in the if_else). Stata treats . or missing values as positive infinity, so that check is actually looking for not missing.
So the equivalent in R/dplyr is probably:
data %>%
    mutate(
        tau = if_else(
        (temp2 > temp3) & (!(is.na(bod) | is.na(do) | is.na(lnfcoli))),
        year-temp2,
        NA_integer_
        )
    )

#  year temp2 temp3 bod  do lnfcoli tau
#1 1986  1995  1986 3.2 7.2     2.1  -9

","In the Stata code, the condition `(bod<. | do<. | lnfcoli<.)` checks if any of the variables `bod`, `do`, or `lnfcoli` have missing values. The `.<.` operator in Stata is used to check for missing values.

However, in your R translation, you are using the `is.na()` function to check for missing values. This is not equivalent to the Stata code. The `is.na()` function in R checks if each individual value is missing, not if any of the variables have missing values.

To replicate the Stata behavior in R, you can use the `rowSums()` function along with the `is.na()` function to check if any of the variables have missing values. Here's an updated version of your R code:

```R
data <- data %>%
  mutate(tau = if_else((temp2 > temp3) & 
                         (rowSums(is.na(c(bod, do, lnfcoli))) > 0), 
                       year - temp2,
                       NA_integer_))
```

In this updated code, `rowSums(is.na(c(bod, do, lnfcoli))) > 0` checks if any of the variables `bod`, `do`, or `lnfcoli` have missing values. If any of the variables have missing values, the condition is `TRUE`, and the value `year - temp2` is assigned to `tau`. Otherwise, `NA` is assigned.

This modification should produce the same results as the Stata code."
76383859,Why sometimes local class cannot access constexpr variables defined in function scope,"This c++ code cannot compile:
#include <iostream>

int main()
{
    constexpr int kInt = 123;
    struct LocalClass {
        void func(){
            const int b = std::max(kInt, 12); 
            //                     ^~~~  
            // error: use of local variable with automatic storage from containing function
            std::cout << b;
        }
    };
    LocalClass a;
    a.func();
    return 0;
}

But this works:
#include <iostream>
#include <vector>

int main()
{
    constexpr int kInt = 123;
    struct LocalClass {
        void func(){
            const int b = std::max((int)kInt, 12); // added an extra conversion ""(int)""
            std::cout << b;
            const int c = kInt; // this is also ok
            std::cout << c;
            const auto d = std::vector{kInt}; // also works
            std::cout << d[0];
        }
    };
    LocalClass a;
    a.func();
    return 0;
}

Tested under C++17 and C++20, same behaviour.
",76384297,"1. odr-using local entities from nested function scopes
Note that kInt still has automatic storage duration - so it is a local entity as per:

6.1 Preamble [basic.pre]
(7) A local entity is a variable with automatic storage duration, [...]


In general local entities cannot be odr-used from nested function definitions (as in your LocalClass example)
This is given by:

6.3 One-definition rule [basic.def.odr]
(10) A local entity is odr-usable in a scope if:
[...]
(10.2) for each intervening scope between the point at which the entity is introduced and the scope (where *this is considered to be introduced within the innermost enclosing class or non-lambda function definition scope), either:

the intervening scope is a block scope, or
the intervening scope is the function parameter scope of a lambda-expression that has a simple-capture naming the entity or has a capture-default, and the block scope of the lambda-expression is also an intervening scope.

If a local entity is odr-used in a scope in which it is not odr-usable, the program is ill-formed.

So the only times you can odr-use a local variable within a nested scope are nested block scopes and lambdas which capture the local variable.
i.e.:
void foobar() {
    int x = 0;

    {
        // OK: x is odr-usable here because there is only an intervening block scope
        std::cout << x << std::endl;
    }

    // OK: x is odr-usable here because it is captured by the lambda
    auto l = [&]() { std::cout << x << std::endl; };

    // NOT OK: There is an intervening function definition scope
    struct K {
      int bar() { return x; }
    };
}

11.6 Local class declarations [class.local] contains a few examples of what is and is not allowed, if you're interested.

So if use of kInt constitutes an odr-use, your program is automatically ill-formed.
2. Is naming kInt always an odr-use?
In general naming a variable constitutes an odr-use of that variable:

6.3 One-definition rule [basic.def.odr]
(5) A variable is named by an expression if the expression is an id-expression that denotes it. A variable x that is named by a potentially-evaluated expression E is odr-used by E unless [...]

But because kInt is a constant expression the special exception (5.2) could apply:

6.3 One-definition rule [basic.def.odr]
(5.2) x is a variable of non-reference type that is usable in constant expressions and has no mutable subobjects, and E is an element of the set of potential results of an expression of non-volatile-qualified non-class type to which the lvalue-to-rvalue conversion is applied, or

So naming kInt is not deemed an odr-use as long as it ...

is of non-reference type (✓)
is usable in constant expressions (✓)
does not contain mutable members (✓)

and the expression that contains kInt ...

must produce a non-volatile-qualified non-class type (✓)
must apply the lvalue-to-rvalue conversion (?)

So we pass almost all the checks for the naming of kInt to not be an odr-use, and therefore be well-formed.
The only condition that is not always true in your example is the lvalue-to-rvalue conversion that must happen.
If the lvalue-to-rvalue conversion does not happen (i.e. no temporary is introduced), then your program is ill-formed - if it does happen then it is well-formed.
// lvalue-to-rvalue conversion will be applied to kInt:
// (well-formed)
const int c = kInt;  
std::vector v{kInt}; // vector constructor takes a std::size_t

// lvalue-to-rvalue conversion will NOT be applied to kInt:
// (it is passed by reference to std::max)
// (ill-formed)
std::max(kInt, 12); // std::max takes arguments by const reference (!)

This is also the reason why std::max((int)kInt, 12); is well-formed - the explicit cast introduces a temporary variable due to the lvalue-to-rvalue conversion being applied.
","The reason for the compilation error in the first code snippet is related to the strict scoping rules in C++. In C++, a local class defined within a function has limitations on accessing variables from the enclosing function. This is known as ""name hiding.""

In the first code snippet, the `constexpr int kInt` is defined in the `main()` function, and the `LocalClass` is defined within `main()`. When you try to access `kInt` inside `LocalClass::func()`, you encounter a compilation error because the local class cannot directly access variables with automatic storage duration (i.e., variables defined within the function scope). This restriction is imposed to ensure that the local class remains valid even after the function execution ends and the automatic variables are destroyed.

In the second code snippet, you introduced an explicit conversion `(int)kInt` when comparing it with `12` using `std::max`. This works because the explicit conversion causes a temporary value to be created, effectively extending the lifetime of the converted value until the end of the full-expression. As a result, you avoid the direct access to `kInt`, and the code compiles successfully.

Similarly, when you assign `kInt` to `const int c` or initialize `std::vector` with `kInt`, the lifetime of `kInt` is effectively extended until the end of the full-expression, allowing the code to compile without errors.

To summarize, the ability to access variables defined in the enclosing function from a local class depends on the scoping rules and the lifetime of the variables. By introducing explicit conversions or extending the lifetime of the variable in some way, you can overcome the compilation error caused by name hiding in local classes."
76380850,How do I keep and append placeholder text into the selected value in React Select?,"Let's say I have a React Select with a placeholder ('Selected Value: '), and I want to keep the placeholder and append it into the selected value so that it looks something like ('Selected Value: 1'). Is there any way to do it?
import Select from ""react-select"";

export default function App() {
  const options = [
    { value: 1, label: 1 },
    { value: 2, label: 2 },
    { value: 3, label: 3 },
    { value: 4, label: 4 }
  ];
  const placeholder = ""Selected Value: "";
  return (
    <div className=""App"">
      <Select options={options} placeholder={placeholder} />
    </div>
  );
}

codesandbox: https://codesandbox.io/s/brave-chatterjee-pjol2d?file=/src/App.js:23-385
EDIT: Sorry, forget to mention, I do not want the placeholder to directly be in the labels of the options
",76380929,"you can accept my answer
import Select from ""react-select"";
import { useState } from ""react"";

export default function App() {
  const [selectBoxValue, setSelectBoxValue] = useState('')
  const options = [
    { value: 1, label: 1 },
    { value: 2, label: 2 },
    { value: 3, label: 3 },
    { value: 4, label: 4 }
  ];
  const placeholder = `Selected Value: ${selectBoxValue}`;
  return (
    <div className=""App"">
      <Select 
      options={options} 
      placeholder={placeholder} 
      value={placeholder}
      onChange={(event) => setSelectBoxValue(event.value)} />
    </div>
  );
}

","To achieve the desired behavior of keeping the placeholder text and appending it to the selected value in React Select, you can use the `formatOptionLabel` prop. This prop allows you to customize the appearance of each option label in the dropdown menu and the selected value.

Here's an updated version of your code with the modified `formatOptionLabel` function:

```jsx
import React from 'react';
import Select from 'react-select';

export default function App() {
  const options = [
    { value: 1, label: 'Option 1' },
    { value: 2, label: 'Option 2' },
    { value: 3, label: 'Option 3' },
    { value: 4, label: 'Option 4' },
  ];

  const placeholder = 'Selected Value: ';

  const formatOptionLabel = ({ label }) => (
    <div>
      {placeholder}
      <span>{label}</span>
    </div>
  );

  return (
    <div className=""App"">
      <Select options={options} placeholder={placeholder} formatOptionLabel={formatOptionLabel} />
    </div>
  );
}
```

In the `formatOptionLabel` function, we're creating a custom label for each option by wrapping the `placeholder` and the actual option label (`{label}`) within a `div`. This way, each option label will display the combined text of the placeholder and the option label.

Now, when you select an option, it will appear in the format: ""Selected Value: {selectedOptionLabel}"". The placeholder text will always be present, whether an option is selected or not."
76380934,"Method not allowed, flask, python","Installed FlareSolverr in docker.
cURL work correctly and return the correct response.
curl -L -X POST 'http://localhost:8191/v1' -H 'Content-Type: application/json' --data-raw '{
  ""cmd"": ""request.get"",
  ""url"":""http://google.com"",
  ""maxTimeout"": 60000
}'

but when using from python + flask I get an error - 405 Method is not allowed
def get_parsed_page(url, delay=0.5):
data = {
    ""cmd"": ""request.get"",
    ""url"": url,
    ""maxTimeout"": 60000
}
headers = {""Content-Type"": ""application/json""}
time.sleep(delay)
print(requests.get(""***:8191/v1"", headers=headers, data=data))
return BeautifulSoup(requests.get(""***:8191/v1"", headers=headers, data=data).text, 'lxml')

",76380982,"you are using a GET request in your python code. It should be a POST request. Use requests.post
","The error you're encountering, ""405 Method Not Allowed,"" typically occurs when you're trying to access a resource using an HTTP method that is not allowed or supported by the server. In this case, it seems that the Flask server is rejecting the POST request you're sending.

There are a few things you can check to resolve this issue:

1. Verify the Flask route: Ensure that you have defined the appropriate route in your Flask application to handle the POST request. Make sure the route is configured to accept POST requests.

2. Check the URL: Double-check the URL you're using in the `requests.get()` call. It should match the URL you tested with cURL (`http://localhost:8191/v1`).

3. Use the correct HTTP method: In your `requests.get()` call, change the method from `GET` to `POST` since you're sending a JSON payload in the request body. Update the line to `requests.post()`.

Here's an updated version of your code with these modifications:

```python
import requests
from bs4 import BeautifulSoup
import time

def get_parsed_page(url, delay=0.5):
    data = {
        ""cmd"": ""request.get"",
        ""url"": url,
        ""maxTimeout"": 60000
    }
    headers = {""Content-Type"": ""application/json""}
    time.sleep(delay)
    response = requests.post(""http://localhost:8191/v1"", headers=headers, json=data)
    print(response)
    return BeautifulSoup(response.text, 'lxml')
```

In this updated code, `requests.post()` is used instead of `requests.get()`, and the payload data is sent as JSON using the `json` parameter. Also, make sure to replace `""***""` in the URL with the appropriate hostname or IP address.

These changes should help resolve the ""405 Method Not Allowed"" error and allow your Flask application to make the POST request to the FlareSolverr API successfully."
76378592,Can't not write three value in 1 line,"I was having this problem about last week in this code
a = int(input())
b = int(input())
c = int(input())
print(min(a+b,b+c,c+a))


so when I enter three input like this: 2 5 6 (three interger in 1 line)
It show me a error:

File ""c:\Users\Administrator\Documents\Code\Python\baitap(LQDOJ)\EZMIN.py"", line 1, in <module>
    a = int(input())
ValueError: invalid literal for int() with base 10: '2 5 6' 

and I see that it only identify 'a' but not identify 'b' , 'c' so can you show me how to fix it or are there other ways to write it in 1 line?
",76378760,"Method 1
The error you're encountering is because you're trying to convert the entire string '2 5 6' into an integer using the int() function. However, the int() function expects a single integer value, not a string containing multiple numbers.
code:
a = int(input())
b = int(input())
c = int(input())

x = a + b
y = b + c
z = c + a

min_value = x
if y < min_value:
    min_value = y
if z < min_value:
    min_value = z

print(""The minimum value is:"", min_value)

you'll be prompted to enter the values for a, b, and c separately, and the code will correctly calculate and display the minimum value among the three sums.
Method 2
Using This one is more optimize solution
input_values = input()
input_list = list(map(int, input_values.split()))

min_value = min(input_list[0] + input_list[1], input_list[1] + input_list[2], input_list[2] + input_list[0])

print(""The minimum value is:"", min_value)


The split() method splits the input string at spaces, creating a list of string elements.
The map() function applies the int() function to each element of the split list, converting them into integers.
list() is used to convert the resulting map object into a list of integers.
The resulting list is stored in input_list for further calculations.

","The error you're encountering is because when you enter the input values separated by spaces, the `input()` function reads the entire line as a single string. You need to split the string into separate values before converting them to integers.

Here's an updated version of your code that splits the input string and assigns the values to variables `a`, `b`, and `c`:

```python
a, b, c = map(int, input().split())
print(min(a+b, b+c, c+a))
```

In this code, `input().split()` splits the input string at each space, creating a list of individual number strings. The `map(int, ...)` applies the `int()` function to each element of the list, converting them into integers. Finally, the three values are assigned to variables `a`, `b`, and `c` using unpacking. Now you can calculate the minimum value as expected.

With this modification, you can enter the three integers on a single line separated by spaces, like this: `2 5 6`, and the code will work correctly."
76383945,Typescript type extension,"I try to define a custom interfaces like this :
export interface IAPIRequest<B extends any, P extends any, Q extends any>
{
  body: B;
  params: P;
  query: Q;
}

This type is supposed to be extended in a lot of other types for each request mu API is supposed to handle.
For example :
export interface ILoginRequest extends IAPIRequest<{ email: string; password: string; }>, undefined, undefined> {}

It works a little but everytime I use this interface, I must provide all the properties even if they are undefined.
Example:
const login = async ({ body }: ILoginRequest) => 
{
  ...
}

const response = await login({ body: { email: 'mail@test.com', password: 'verystrongpassword' }, params: undefined, query: undefined });

It doesn't work if I don't provide the undefined properties.
How can I define an abstract type for IAPIRequest that would avoid me from providing undefined values ?
PS : I've tried this as well
export interface IAPIRequest<B extends any, P extends any, Q extends any>
{
  body?: B;
  params?: P;
  query?: Q;
}

Even for IAPIRequest<B, P, Q> where none of B, P, or Q allow undefined, I still get that the properties might be undefined
",76384326,"TypeScript doesn't automatically treat properties that accept undefined to be optional (although the converse, treating optional properties as accepting undefined, is true, unless you've enabled --exactOptionalPropertyTypes).  There is a longstanding open feature request for this at microsoft/TypeScript#12400 (the title is about optional function parameters, not object properties, but the issue seems to have expanded to include object properties also).  Nothing has been implemented there, although the discussion describes various workarounds.
Let's define our own workaround; a utility type UndefinedIsOptional<T> that produces a version of T such that any property accepting undefined is optional.  It could look like this:
type UndefinedIsOptional<T extends object> = (Partial<T> &
    { [K in keyof T as undefined extends T[K] ? never : K]: T[K] }
) extends infer U ? { [K in keyof U]: U[K] } : never

That's a combination of Partial<T> which turns all properties optional, and a key remapped type that suppresses all undefined-accepting properties.  The intersection of those is essentially what you want (an intersection of an optional prop and a required prop is a required prop) but I use a technique described at How can I see the full expanded contract of a Typescript type? to display the type in a more palatable manner.
Then we can define your type as
type IAPIRequest<B, P, Q> = UndefinedIsOptional<{
    body: B;
    params: P;
    query: Q;
}>

and note that this must be a type alias and not an interface because the compiler needs to know exactly which properties will appear (and apparently their optional-ness) to be an interface.  This won't matter much with your example code but you should be aware of it.
Let's test it out:
type ILR = IAPIRequest<{ email: string; password: string; }, undefined, undefined>
/* type ILR = {
    body: {
        email: string;
        password: string;
    };
    params?: undefined;
    query?: undefined;
} */

That looks like what you wanted, so you can define your ILoginRequest interface:
interface ILoginRequest extends IAPIRequest<
    { email: string; password: string; }, undefined, undefined> {
}

Also, let's just look at what happens when the property includes undefined but is not only undefined:
type Other = IAPIRequest<{ a: string } | undefined, number | undefined, { b: number }>;
/* type Other = {
    body?: {
        a: string;
    } | undefined;
    params?: number | undefined;
    query: {
        b: number;
    };
} */

Here body and params are optional because undefined is possible, but query is not because undefined is impossible.
Playground link to code
","If you want to avoid providing undefined values for the properties of `IAPIRequest`, you can make use of TypeScript's utility types to define optional properties. Instead of using `undefined` as the default value for properties, you can mark them as optional by adding a question mark (`?`) after the property name.

Here's an example of how you can modify the `IAPIRequest` interface to make the properties optional:

```typescript
export interface IAPIRequest<B, P, Q> {
  body?: B;
  params?: P;
  query?: Q;
}
```

By making the properties optional, you won't be required to provide all properties explicitly when using `IAPIRequest` or its extended interfaces. You can provide only the properties that are relevant to the specific request.

Here's an example usage with the modified interface:

```typescript
const login = async ({ body }: ILoginRequest) => {
  // ...
}

const response = await login({ body: { email: 'mail@test.com', password: 'verystrongpassword' } });
```

In this example, the `params` and `query` properties are not required to be provided since they are optional in the `IAPIRequest` interface."
76380868,How to configure the Quarkus Mailer extension to allow dynamic 'from' email addresses based on user?,"This Quarkus mailer guide requires that the sending email is preconfigured in property file: quarkus.mailer.from=YOUREMAIL@gmail.com. However, my use case for email includes unique originator email based on user. Using the provided method looks something like:
public void sendEmail(EmailSender emailSender) {

    // Send to each recipient
    emailMessageRepository.findByEmailSenderId(emailSender.getId())
        .forEach(emailMessage ->
                mailer.send(
                    Mail.withText(emailMessage.getEmail(),
                    emailSender.getSubject(),
                    emailSender.getMessage())
            );
    );
}

How can I include the sender's email address (i.e. 'from') when the Mail.withText() method only provides for recipient email?
",76380985,"The documention showcases how to use multimailer (Multiple From Addresses)
quarkus.mailer.from=your-from-address@gmail.com 
quarkus.mailer.host=smtp.gmail.com

quarkus.mailer.aws.from=your-from-address@gmail.com 
quarkus.mailer.aws.host=${ses.smtp}
quarkus.mailer.aws.port=587

quarkus.mailer.sendgrid.from=your-from-address@gmail.com 
quarkus.mailer.sendgrid.host=${sendgrid.smtp-host}
quarkus.mailer.sendgrid.port=465

So you would write:
quarkus.mailer.from=default@gmail.com 

quarkus.mailer.aws.from=your_aws@gmail.com 

quarkus.mailer.sendgrid.from=your_sendgrid@gmail.com 

Then you would inject them as shown below and use them based on whom you want to send with:
@Inject
@MailerName(""aws"") 
Mailer mailer;


@Inject
@MailerName(""sendgrid"") 
Mailer mailer;

aws and sendgrid at the names between quarkus.mailer.xxx.from
https://quarkus.io/guides/mailer-reference#multiple-mailer-configurations


The Quarkus Mailer is implemented on top of the Vert.x Mail Client,
providing an asynchronous and non-blocking way to send emails.

If you need fine control on how the mail is sent, for instance if you need to retrieve the message ids, you can inject the underlying client, and use it directly:
@Inject MailClient client;

Then use it:
MailMessage message = new MailMessage();
message.setFrom(""user@example.com (Example User)"");
message.setTo(""recipient@example.org"");
message.setCc(""Another User <another@example.net>"");
message.setText(""this is the plain message text"");
message.setHtml(""this is html text <a href=\""http://vertx.io\"">vertx.io</a>"");

To send using MailClient:
mailClient.sendMail(message)
  .onSuccess(System.out::println)
  .onFailure(Throwable::printStackTrace);

https://quarkus.io/guides/mailer-reference#using-the-underlying-vert-x-mail-client
https://vertx.io/docs/vertx-mail-client/java/
","To configure the Quarkus Mailer extension to allow dynamic 'from' email addresses based on the user, you can use the `MailerImpl` class provided by Quarkus Mailer. Instead of using the `Mail.withText()` method directly, you can create a `MailerImpl` instance and set the 'from' email address dynamically before sending the email.

Here's an example of how you can modify your code to include the sender's email address ('from'):

```java
import io.quarkus.mailer.Mail;
import io.quarkus.mailer.MailerImpl;
import io.quarkus.mailer.reactive.ReactiveMailer;

import javax.inject.Inject;
import javax.transaction.Transactional;

// Assuming this class is a CDI bean
public class EmailService {

    @Inject
    MailerImpl mailer; // Inject the MailerImpl instance

    @Transactional
    public void sendEmail(EmailSender emailSender) {

        emailMessageRepository.findByEmailSenderId(emailSender.getId())
            .forEach(emailMessage -> {
                Mail mail = Mail.withText(emailMessage.getEmail(),
                    emailSender.getSubject(),
                    emailSender.getMessage());

                // Set the 'from' email address dynamically based on the sender
                mail.setFrom(emailSender.getEmail());

                mailer.send(mail);
            });
    }
}
```

In this example, we assume that you have the necessary dependencies configured in your project, including the Quarkus Mailer extension. The `MailerImpl` instance is injected into the `EmailService` class, and you can use it to send emails.

The `Mail` object is created using the `Mail.withText()` method, as before. However, after creating the `Mail` object, you can use the `setFrom()` method to set the 'from' email address dynamically based on the sender's email address (`emailSender.getEmail()` in this example).

By setting the 'from' email address on the `Mail` object before sending it with the `mailer.send(mail)` method, you ensure that each email is sent with the appropriate 'from' address based on the user."
76380847,"Why Drop trait is only executed at the end of the scope, instead of after the last use?","This is a question from rust onomicon # lifetime
The first example can compile, as x is a reference and the compiler can infer its lifetime as minimal as the last use here :println!(), so x is dropped after this line.
let mut data = vec![1, 2, 3];
let x = &data[0];
println!(""{}"", x);
// This is OK, x is no longer needed
data.push(4);

But the case is different when x is a struct implemented Drop trait.
#[derive(Debug)]
struct X<'a>(&'a i32);

impl Drop for X<'_> {
    fn drop(&mut self) {}
}

let mut data = vec![1, 2, 3];
let x = X(&data[0]);
println!(""{:?}"", x);
data.push(4);
// Here, the destructor is run and therefore this'll fail to compile.

The onomicon says in this case, drop() is only executed at the very end of a scope, so x keeps valid until the last line.
But why the compiler cannot minimize the lifetime of x to the last use? And is applying drop() just after the last use has some nontrivial side effects when x is implemented Drop trait?
",76380988,"The primary reason is that it was once defined to be like that, and now changing it isn't possible any more because it wouldn't be backwards-compatible and might break stuff.
Your code is easily fixable by introducing a nested scope, though, which is (to my understanding) best practice in those situations:
#[derive(Debug)]
struct X<'a>(&'a i32);

impl Drop for X<'_> {
    fn drop(&mut self) {}
}

fn main() {
    let mut data = vec![1, 2, 3];
    {
        let x = X(&data[0]);
        println!(""{:?}"", x);
    }
    data.push(4);
}

X(1)

Alternatively, you could drop it manually:
#[derive(Debug)]
struct X<'a>(&'a i32);

impl Drop for X<'_> {
    fn drop(&mut self) {}
}

fn main() {
    let mut data = vec![1, 2, 3];

    let x = X(&data[0]);
    println!(""{:?}"", x);
    drop(x);

    data.push(4);
}

X(1)

","In Rust, the `Drop` trait is designed to provide a way to run some cleanup code when a value goes out of scope. The `Drop` trait's `drop` method is automatically called by the Rust runtime when a value is being dropped. 

The reason the `drop` method is executed at the end of the scope instead of immediately after the last use is primarily to ensure deterministic destruction. Rust aims to provide memory safety without relying on a garbage collector, and by running the `drop` method at the end of the scope, Rust guarantees that the cleanup code is always executed in a predictable order.

If the `drop` method were executed immediately after the last use, it could lead to unexpected behavior and potential memory safety issues. Consider the following example:

```rust
let x = X(&data[0]);
let y = X(&data[1]);

// Some code that doesn't use x or y

data.push(4);

// If `drop` were executed immediately, x and y would be dropped here,
// and the references stored inside them would become invalid.
```

By deferring the execution of the `drop` method until the end of the scope, Rust ensures that all the variables within the scope are dropped in a consistent and predictable order, preventing the use of invalid references.

It's worth noting that implementing the `Drop` trait with an empty `drop` method, as shown in your example, effectively disables the automatic execution of any cleanup code. This is generally discouraged unless you have a specific reason to do so."
76384211,When it's necessary to create a custom spring boot starter rather than create a new microservice?,"I've a microservice architecture, and need some common logic.
When it's necessary to create a custom spring boot starter rather than create a new microservice?
",76384348,"In my experience, creating a new microservice from the ground up is generally due to preventing any monoliths occurring. Microservices should generally have one job and then do it well. You don't want to muddy up the implementation and purpose of your microservice by adding unrelated operations.
There are many design patterns for the ""types"" you could be creating but I won't go into too much detail there. Overall, based on what business purpose you are solving you can select your design and begin development. Different designs should be separated and not combined into monolithic styles. Here is a good article showcasing design options: https://www.openlegacy.com/blog/microservices-architecture-patterns/
If you find your self having to re-create multiple microservice serving different use cases you can always utilize a tool such as yeoman to speed up creating these new projects. You can build a generator that will give you a working template so you don't have to spend the time re developing from the ground up each time you need a different service.
Here is a guide that I wrote recently on creating your own yeoman generator: https://medium.com/@dylanlamott/building-a-yeoman-generator-line-by-line-6966debb39a3
","Creating a custom Spring Boot starter or developing a new microservice depends on the specific requirements and goals of your project. Both approaches serve different purposes and can be beneficial in different scenarios. Let's discuss each option:

1. Custom Spring Boot Starter:
A custom Spring Boot starter is essentially a set of pre-configured dependencies, configurations, and utilities bundled together to simplify the setup and usage of certain functionality within your Spring Boot applications. You would typically create a custom starter when you have common logic or functionality that is used across multiple microservices in your architecture. Some scenarios where creating a custom starter might be appropriate include:

- Reusability: If you have a set of common libraries, configurations, or utilities that are shared across multiple microservices, creating a custom starter can help enforce consistency and reduce duplication. This allows you to encapsulate the common logic into a reusable module that can be easily included in multiple projects.

- Simplified Configuration: If you have complex or repetitive configuration setups that are required across multiple microservices, a custom starter can provide pre-configured defaults and simplify the configuration process for developers. This promotes consistency and reduces the likelihood of configuration errors.

- Standardization: If you want to enforce certain coding standards, architectural patterns, or best practices across your microservices, a custom starter can be a convenient way to provide a standardized set of libraries, configurations, and utilities to ensure consistency throughout the codebase.

2. New Microservice:
Creating a new microservice is appropriate when you have distinct functionality or business requirements that warrant a separate, independently deployable service. Some scenarios where creating a new microservice might be necessary include:

- Separation of Concerns: If the common logic or functionality you need to implement is independent of the existing microservices and requires separate deployment, scaling, or maintenance considerations, it's better to create a new microservice. This ensures loose coupling and allows each microservice to focus on its specific domain.

- Scalability and Performance: If the common logic requires additional computing resources or specialized infrastructure to handle the expected load, separating it into a dedicated microservice can provide better scalability and performance. This way, you can scale the common logic independently from other microservices.

- Independent Lifecycle: If the common logic has its own development lifecycle, release schedule, or versioning requirements that differ from the existing microservices, it's better to create a new microservice. This allows for independent development, testing, and deployment of the common logic without impacting other microservices.

Ultimately, the decision between creating a custom Spring Boot starter or a new microservice depends on factors such as code reusability, the need for standardized configurations, separation of concerns, scalability requirements, and independent lifecycle considerations. Analyzing these factors and understanding the specific requirements of your project will help guide your decision-making process."
76378628,How to fix 'int' object has no attribute 'astype' error when sending WhatsApp messages to large number of contacts using Python and pandas?,"AttributeError: 'int' object has no attribute 'astype' in automatic WhatsApp message sender script

The following is an automated WhatsApp message sender script I partially developed. I tried the following script and it worked fine with an excel with 5 numbers in it. However, I tried upscaling it to 1700+ numbers, and I get the following traceback:
Traceback (most recent call last):
  File ""c:\Users\MSI\Desktop\AutoSenderPY\main.py"", line 9, in <module>
    cellphone = data.loc[i,'Cellphone'].astype(str)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'int' object has no attribute 'astype'*

The script is the following:
import pandas as pd
import webbrowser as web
import pyautogui as pg
import time

data = pd.read_excel(""book1.xlsx"", sheet_name='sheet1')

for i in range(len(data)):
    cellphone = data.loc[i,'Cellphone'].astype(str) 
    
    message = ""Test Message""
    
    web.open(""https://web.whatsapp.com/send?phone="" + cellphone + ""&text="" + message)
    
    time.sleep(5.5)           
    pg.click(1230,964)      
    time.sleep(1)            
    pg.press('enter')       
    time.sleep(2)         
    pg.hotkey('ctrl', 'w') 
    time.sleep(1)

Why is that happening, and how can I get it working for those 1700+ numbers?
",76378769,"Try using -
cellphone = str(data.loc[i,'Cellphone'])

I think loc returns a single element of type ""numpy.int64"", calling the ""str"" should be enough.
","The error you are encountering, `'int' object has no attribute 'astype'`, suggests that the `astype()` method is being called on an integer object instead of a Pandas DataFrame or Series object. This issue can occur if the value retrieved from the `data` DataFrame is not of the expected type.

To resolve this issue, you can try the following steps:

1. Check the data type of the 'Cellphone' column: Verify that the 'Cellphone' column in your Excel file contains string values. If it contains integer values, you can convert them to strings using the `astype()` method before entering the loop. For example:
   ```
   data['Cellphone'] = data['Cellphone'].astype(str)
   ```

2. Validate the data in the 'Cellphone' column: Ensure that the values in the 'Cellphone' column do not contain any null or non-integer values. You can use the `isnull()` function to check for null values and the `apply()` function to check for non-integer values. For example:
   ```
   non_integer_values = data['Cellphone'].apply(lambda x: not isinstance(x, int))
   print(data[non_integer_values])
   ```

   If any non-integer values are present, you may need to clean or correct them in your Excel file before running the script.

3. Ensure the 'Cellphone' column exists in the Excel sheet: Double-check that the 'Cellphone' column exists in the 'sheet1' of your Excel file. Make sure the column name is spelled correctly and matches the case exactly.

By performing these checks and modifications, you should be able to resolve the `'int' object has no attribute 'astype'` error and process a larger number of contacts in your script successfully."
76378370,SQL How to return record ID's not included in table 2 from table 1 based off of user ID in table 2,"I have two tables, one has course name and course ID. The second table has the ID of the students and the course ID they have taken. I need to find all the class ID’s of the classes a student hasn’t taken. For example, in table 2 student 03 has taken classes 01 and 02 but not 03 and 04 from table one. The course ID’s 03 and 04 from table one are what I need to return (all the classes student 03 hasn't taken). I've tried numerous queries and the last one I tried is:
SELECT table1.* FROM table1
LEFT JOIN table2
ON
    table1.course_ID = table2.course_ID
WHERE
    table2.course_ID IS NULL
AND 
    table2.user_ID != 3

Appreciate your help!
table 1




course_ID
courseName




01
math


02
English


03
art


04
music




table 2




cert_Id
course_ID
user_ID




01
01
03


02
02
03



",76378800,"As per your current requirement below query will work
    SELECT * FROM table1 t1 
    WHERE course_ID 
    NOT IN (SELECT course_ID FROM table2 WHERE user_ID =3)

If you have more records in table2 and if you need to populate more than one student's details then you have to use other logic
If you want to modify your query then use as below
    SELECT table1.* FROM table1 
         LEFT JOIN table2 ON table1.course_ID = table2.course_ID 
         AND table2.user_ID = 3 
    WHERE table2.course_ID IS NULL

","To find all the course IDs from table 1 that a student (with user ID 03) hasn't taken, you can use a combination of a cross join and a left join. Here's an example query to achieve that:

```sql
SELECT table1.course_ID, table1.courseName
FROM table1
CROSS JOIN (SELECT DISTINCT user_ID FROM table2 WHERE user_ID = 3) AS students
LEFT JOIN table2 ON table1.course_ID = table2.course_ID AND table2.user_ID = students.user_ID
WHERE table2.cert_Id IS NULL;
```

This query first generates a temporary table called `students` that contains the distinct user IDs from table 2 for the student you're interested in (in this case, user ID 03). Then, it performs a cross join between table 1 and the `students` table to get a combination of all course IDs from table 1 with the student ID.

Next, it performs a left join with table 2, matching on both the course ID and the user ID, to find the courses the student has taken. Finally, the `WHERE` clause filters out the rows where the `cert_Id` (assuming it's a unique identifier in table 2) is `NULL`, which indicates that the student hasn't taken that particular course.

The result will include the course IDs and names from table 1 that the student hasn't taken."
76380967,Why is SQL Server Pivot being case sensitive on TabTypeId instead of treating it as the actual column name?,"In T-Sql I am parsing JSON and using PIVOT.
Select * from (select [key],convert(varchar,[value])[value] 
from openjson ('{""Name"":""tew"",""TabTypeId"":9,""Type"":3}'))A
    pivot(max(value) for [key] in ([Name],tabTypeId,[Type]))b

It is not treating tabTypeId as equal to TabTypeId. I am getting NULL for tabTypeId.
If I use TabTypeId I get the value 9.
Why is it happening?
",76381059,"It's not PIVOT that is case sensitive, it's the data returned from OPENJSON that is. If you check the data returned from it, you'll see that the column key is a binary collation:
SELECT name, system_type_name, collation_name
FROM sys.dm_exec_describe_first_result_set(N'SELECT [key], CONVERT(varchar, [value]) AS [value] FROM OPENJSON(''{""Name"":""tew"",""TabTypeId"":9,""Type"":3}'');',NULL,NULL)





name
system_type_name
collation_name




key
nvarchar(4000)
Latin1_General_BIN2


value
varchar(30)
SQL_Latin1_General_CP1_CI_AS




For binary collations the actual bytes of the characters must match. As such N'tabTypeId' and N'TabTypeId' are not equal as N'T' and N't' have the binary values 0x5400 and 0x7400.
Though I am unsure why you are using PIVOT at all; just define your columns in your OPENJSON call:
SELECT name, --Columns are intentionally demonstrating non-case sensitivity
       tabTypeId,
       type
FROM OPENJSON('{""Name"":""tew"",""TabTypeId"":9,""Type"":3}')
        WITH (Name varchar(3),
              TabTypeId int,
              Type int);

Note that in the WITH clause of OPENJSON the column names are still case sensitive. tabTypeId int would also yield NULL. If you ""had"" to have a column called tabTypeId defined prior to the SELECT you would use tabTypeId int '$.TabTypeId' instead.
","In SQL Server, column names are by default case-insensitive unless they are enclosed in double quotation marks. When you specify column names without quotation marks, SQL Server treats them as case-insensitive.

In your query, you are using the column name ""TabTypeId"" as a pivot column. Since you have not enclosed it in double quotation marks, SQL Server considers it case-insensitive and matches it with the actual column name ""TabTypeId"" in the result set.

However, when you reference ""tabTypeId"" in the pivot clause, SQL Server treats it as a different column name than ""TabTypeId"" because it is not enclosed in double quotation marks. Since there is no column named ""tabTypeId"" in your result set, it returns NULL for that pivot column.

To resolve this issue, you can enclose the column name ""TabTypeId"" in double quotation marks consistently throughout your query. This ensures that SQL Server treats it as a case-sensitive column name and matches it correctly in the pivot clause. Here's an updated version of your query:

```sql
SELECT *
FROM (
    SELECT [key], CONVERT(VARCHAR, [value]) AS [value]
    FROM OPENJSON ('{""""Name"""":""""tew"""",""""TabTypeId"""":9,""""Type"""":3}')
) A
PIVOT (
    MAX([value]) FOR [key] IN ([Name], ""TabTypeId"", [Type])
) B
```

Now, both ""TabTypeId"" and ""tabTypeId"" will be treated as separate column names, and you should get the desired result."
76384091,PSQL / SQL: Is it possible to further optimize this query with requiring write access to the database?,"I have a query here that uses four subqueries inside a single CTE, and each subquery is scanning every row of another CTE for each row in itself. I would think that this is very inefficient.
Are there any SQL optimizations that I can implement now that the proof of concept is finished?
I don't have write access to the database, so optimizations would be required within the select clause.
WITH datetable AS (
    SELECT generate_series(
        DATE_TRUNC('week', (SELECT MIN(created_at) FROM org_accounts.deleted_users)),
        DATE_TRUNC('week', now()),
        '1 week'::INTERVAL
    )::DATE AS week_start
), all_users AS (
    SELECT
        id,
        registered_at,
        NULL AS deleted_at
    FROM org_accounts.users
    WHERE status = 'active'
        AND org_accounts.__user_is_qa(id) <> 'Y'
        AND email NOT LIKE '%@org%'
    
    UNION ALL
    
    SELECT
        id,
        created_at AS registered_at,
        deleted_at
    FROM org_accounts.deleted_users
    WHERE deleter_id = id
        AND email NOT LIKE '%@org%'
), weekly_activity AS (
    SELECT
        DATE_TRUNC('week', date)::DATE AS week_start,
        COUNT(DISTINCT user_id) AS weekly_active_users
    FROM (
      SELECT user_id, date
      FROM org_storage_extra.stats_user_daily_counters 
      WHERE type in ('created_file', 'created_folder', 'created_secure_fetch')
      
      UNION ALL
      
      SELECT user_id, date
      FROM ipfs_pinning_facility.stats_user_daily_counters
      WHERE type <> 'shares_viewed_by_others'
      ) activity_ids_dates
    WHERE EXISTS(SELECT 1 from all_users WHERE id = user_id)
    GROUP BY week_start
), preprocessed AS (
    SELECT
        week_start,
        (
            SELECT COUNT(DISTINCT id)
            FROM all_users
            WHERE registered_at < week_start
                AND (deleted_at IS NULL OR deleted_at > week_start)
        ) AS actual_users,
        (
            SELECT COUNT(DISTINCT id)
            FROM all_users
            WHERE deleted_at < week_start + '1 week'::INTERVAL
        ) AS cumulative_churned_users,
        (
            SELECT COUNT(DISTINCT id)
            FROM all_users
            WHERE registered_at >= week_start
                AND registered_at < week_start + '1 week'::INTERVAL
        ) AS weekly_new_users,
        (
            SELECT COUNT(DISTINCT id)
            FROM all_users
            WHERE deleted_at >= week_start
                AND deleted_at < week_start + '1 week'::INTERVAL
        ) AS weekly_churned_users,
        COALESCE(weekly_active_users, 0) AS weekly_active_users
    FROM datetable dt
    LEFT JOIN weekly_activity USING (week_start)
    ORDER BY week_start DESC
)
SELECT
    week_start AS for_week_of, 
    actual_users + cumulative_churned_users AS cumulative_users,
    cumulative_churned_users,
    cumulative_churned_users::FLOAT / NULLIF((actual_users + cumulative_churned_users)::FLOAT, 0) AS cumulated_churn_rate,
    actual_users,
    weekly_new_users,
    weekly_churned_users,
    weekly_active_users,
    weekly_churned_users::FLOAT / NULLIF(actual_users::FLOAT, 0) AS weekly_churn_rate 
FROM preprocessed;

Results of query analysis:
QUERY PLAN
Subquery Scan on preprocessed  (cost=40875.45..7501783.95 rows=1000 width=68) (actual time=1553.471..13613.116 rows=231 loops=1)
  Output: preprocessed.week_start, (preprocessed.actual_users + preprocessed.cumulative_churned_users), preprocessed.cumulative_churned_users, ((preprocessed.cumulative_churned_users)::double precision / NULLIF(((preprocessed.actual_users + preprocessed.cumulative_churned_users))::double precision, '0'::double precision)), preprocessed.actual_users, preprocessed.weekly_new_users, preprocessed.weekly_churned_users, preprocessed.weekly_active_users, ((preprocessed.weekly_churned_users)::double precision / NULLIF((preprocessed.actual_users)::double precision, '0'::double precision))
  Buffers: shared hit=287734 read=1964, temp read=274840 written=873
  CTE all_users
    ->  Append  (cost=0.00..30953.99 rows=70293 width=32) (actual time=0.099..1313.372 rows=71228 loops=1)
          Buffers: shared hit=285995 read=1964
          ->  Seq Scan on org_accounts.users  (cost=0.00..27912.65 rows=70009 width=32) (actual time=0.099..1289.469 rows=70007 loops=1)
                Output: users.id, users.registered_at, NULL::timestamp with time zone
                Filter: ((users.email !~~ '%@mailinator%'::text) AND (users.email !~~ '%@org%'::text) AND (users.email !~~ '%testaccnt%'::text) AND (users.status = 'active'::text) AND ((org_accounts.__user_is_qa(users.id))::text <> 'Y'::text))
                Rows Removed by Filter: 9933
                Buffers: shared hit=285269 read=1964
          ->  Seq Scan on org_accounts.deleted_users  (cost=0.00..1986.94 rows=284 width=32) (actual time=0.014..14.267 rows=1221 loops=1)
                Output: deleted_users.id, deleted_users.created_at, deleted_users.deleted_at
                Filter: ((deleted_users.email !~~ '%@mailinator%'::text) AND (deleted_users.email !~~ '%@org%'::text) AND (deleted_users.email !~~ '%testaccnt%'::text) AND (deleted_users.deleter_id = deleted_users.id))
                Rows Removed by Filter: 61826
                Buffers: shared hit=726
  ->  Merge Left Join  (cost=9921.47..7470794.97 rows=1000 width=44) (actual time=1553.467..13612.496 rows=231 loops=1)
        Output: (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date), (SubPlan 2), (SubPlan 3), (SubPlan 4), (SubPlan 5), COALESCE(weekly_activity.weekly_active_users, '0'::bigint)
        Inner Unique: true
        Merge Cond: ((((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date) = weekly_activity.week_start)
        Buffers: shared hit=287734 read=1964, temp read=274840 written=873
        ->  Sort  (cost=1601.45..1603.95 rows=1000 width=4) (actual time=10.108..10.250 rows=231 loops=1)
              Output: (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date)
              Sort Key: (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date) DESC
              Sort Method: quicksort  Memory: 35kB
              Buffers: shared hit=726
              ->  Result  (cost=1514.10..1541.62 rows=1000 width=4) (actual time=9.986..10.069 rows=231 loops=1)
                    Output: ((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date
                    Buffers: shared hit=726
                    InitPlan 6 (returns $5)
                      ->  Aggregate  (cost=1514.09..1514.10 rows=1 width=8) (actual time=9.974..9.975 rows=1 loops=1)
                            Output: min(deleted_users_1.created_at)
                            Buffers: shared hit=726
                            ->  Seq Scan on org_accounts.deleted_users deleted_users_1  (cost=0.00..1356.47 rows=63047 width=8) (actual time=0.006..4.332 rows=63047 loops=1)
                                  Output: deleted_users_1.id, deleted_users_1.email, deleted_users_1.created_at, deleted_users_1.deleter_id, deleted_users_1.deleted_at, deleted_users_1.registration_app
                                  Buffers: shared hit=726
                    ->  ProjectSet  (cost=0.00..5.03 rows=1000 width=8) (actual time=9.984..10.030 rows=231 loops=1)
                          Output: generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)
                          Buffers: shared hit=726
                          ->  Result  (cost=0.00..0.01 rows=1 width=0) (actual time=0.000..0.001 rows=1 loops=1)
        ->  Sort  (cost=8320.02..8320.52 rows=200 width=12) (actual time=1475.315..1475.418 rows=159 loops=1)
              Output: weekly_activity.weekly_active_users, weekly_activity.week_start
              Sort Key: weekly_activity.week_start DESC
              Sort Method: quicksort  Memory: 32kB
              Buffers: shared hit=287008 read=1964, temp read=412 written=872
              ->  Subquery Scan on weekly_activity  (cost=8050.90..8312.37 rows=200 width=12) (actual time=1466.686..1475.279 rows=159 loops=1)
                    Output: weekly_activity.weekly_active_users, weekly_activity.week_start
                    Buffers: shared hit=287008 read=1964, temp read=412 written=872
                    ->  GroupAggregate  (cost=8050.90..8310.37 rows=200 width=12) (actual time=1466.685..1475.254 rows=159 loops=1)
                          Output: ((date_trunc('week'::text, (""*SELECT* 1"".date)::timestamp with time zone))::date), count(DISTINCT ""*SELECT* 1"".user_id)
                          Group Key: ((date_trunc('week'::text, (""*SELECT* 1"".date)::timestamp with time zone))::date)
                          Buffers: shared hit=287008 read=1964, temp read=412 written=872
                          ->  Sort  (cost=8050.90..8136.22 rows=34130 width=20) (actual time=1466.668..1468.872 rows=23005 loops=1)
                                Output: ((date_trunc('week'::text, (""*SELECT* 1"".date)::timestamp with time zone))::date), ""*SELECT* 1"".user_id
                                Sort Key: ((date_trunc('week'::text, (""*SELECT* 1"".date)::timestamp with time zone))::date)
                                Sort Method: quicksort  Memory: 2566kB
                                Buffers: shared hit=287008 read=1964, temp read=412 written=872
                                ->  Hash Join  (cost=1586.09..5481.12 rows=34130 width=20) (actual time=1411.350..1462.022 rows=23005 loops=1)
                                      Output: (date_trunc('week'::text, (""*SELECT* 1"".date)::timestamp with time zone))::date, ""*SELECT* 1"".user_id
                                      Inner Unique: true
                                      Hash Cond: (""*SELECT* 1"".user_id = all_users.id)
                                      Buffers: shared hit=287008 read=1964, temp read=412 written=872
                                      ->  Append  (cost=0.00..3080.17 rows=68261 width=20) (actual time=0.010..25.441 rows=68179 loops=1)
                                            Buffers: shared hit=1013
                                            ->  Subquery Scan on ""*SELECT* 1""  (cost=0.00..1018.43 rows=21568 width=20) (actual time=0.008..7.895 rows=21532 loops=1)
                                                  Output: ""*SELECT* 1"".date, ""*SELECT* 1"".user_id
                                                  Buffers: shared hit=372
                                                  ->  Seq Scan on org_storage_extra.stats_user_daily_counters  (cost=0.00..802.75 rows=21568 width=20) (actual time=0.008..5.910 rows=21532 loops=1)
                                                        Output: stats_user_daily_counters.user_id, stats_user_daily_counters.date
                                                        Filter: (stats_user_daily_counters.type = ANY ('{created_file,created_folder,created_secure_fetch}'::text[]))
                                                        Rows Removed by Filter: 9795
                                                        Buffers: shared hit=372
                                            ->  Subquery Scan on ""*SELECT* 2""  (cost=0.00..1720.44 rows=46693 width=20) (actual time=0.009..12.460 rows=46647 loops=1)
                                                  Output: ""*SELECT* 2"".date, ""*SELECT* 2"".user_id
                                                  Buffers: shared hit=641
                                                  ->  Seq Scan on ipfs_pinning_facility.stats_user_daily_counters stats_user_daily_counters_1  (cost=0.00..1253.51 rows=46693 width=20) (actual time=0.009..8.209 rows=46647 loops=1)
                                                        Output: stats_user_daily_counters_1.user_id, stats_user_daily_counters_1.date
                                                        Filter: (stats_user_daily_counters_1.type <> 'shares_viewed_by_others'::text)
                                                        Rows Removed by Filter: 2354
                                                        Buffers: shared hit=641
                                      ->  Hash  (cost=1583.59..1583.59 rows=200 width=16) (actual time=1411.250..1411.251 rows=71228 loops=1)
                                            Output: all_users.id
                                            Buckets: 131072 (originally 1024)  Batches: 2 (originally 1)  Memory Usage: 3073kB
                                            Buffers: shared hit=285995 read=1964, temp read=100 written=717
                                            ->  HashAggregate  (cost=1581.59..1583.59 rows=200 width=16) (actual time=1383.986..1398.270 rows=71228 loops=1)
                                                  Output: all_users.id
                                                  Group Key: all_users.id
                                                  Batches: 5  Memory Usage: 4161kB  Disk Usage: 1544kB
                                                  Buffers: shared hit=285995 read=1964, temp read=100 written=560
                                                  ->  CTE Scan on all_users  (cost=0.00..1405.86 rows=70293 width=16) (actual time=0.102..1351.241 rows=71228 loops=1)
                                                        Output: all_users.id
                                                        Buffers: shared hit=285995 read=1964, temp written=296
        SubPlan 2
          ->  Aggregate  (cost=1777.05..1777.06 rows=1 width=8) (actual time=20.197..20.197 rows=1 loops=231)
                Output: count(DISTINCT all_users_1.id)
                Buffers: temp read=68607 written=1
                ->  CTE Scan on all_users all_users_1  (cost=0.00..1757.33 rows=7888 width=16) (actual time=0.883..10.874 rows=27239 loops=231)
                      Output: all_users_1.id, all_users_1.registered_at, all_users_1.deleted_at
                      Filter: ((all_users_1.registered_at < (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date)) AND ((all_users_1.deleted_at IS NULL) OR (all_users_1.deleted_at > (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date))))
                      Rows Removed by Filter: 43989
                      Buffers: temp read=68607 written=1
        SubPlan 3
          ->  Aggregate  (cost=1815.90..1815.91 rows=1 width=8) (actual time=11.215..11.215 rows=1 loops=231)
                Output: count(DISTINCT all_users_2.id)
                Buffers: temp read=68607
                ->  CTE Scan on all_users all_users_2  (cost=0.00..1757.33 rows=23431 width=16) (actual time=11.009..11.150 rows=231 loops=231)
                      Output: all_users_2.id, all_users_2.registered_at, all_users_2.deleted_at
                      Filter: (all_users_2.deleted_at < ((((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date) + '7 days'::interval))
                      Rows Removed by Filter: 70997
                      Buffers: temp read=68607
        SubPlan 4
          ->  Aggregate  (cost=1933.94..1933.95 rows=1 width=8) (actual time=14.515..14.515 rows=1 loops=231)
                Output: count(DISTINCT all_users_3.id)
                Buffers: temp read=68607
                ->  CTE Scan on all_users all_users_3  (cost=0.00..1933.06 rows=351 width=16) (actual time=2.264..14.424 rows=308 loops=231)
                      Output: all_users_3.id, all_users_3.registered_at, all_users_3.deleted_at
                      Filter: ((all_users_3.registered_at >= (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date)) AND (all_users_3.registered_at < ((((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date) + '7 days'::interval)))
                      Rows Removed by Filter: 70920
                      Buffers: temp read=68607
        SubPlan 5
          ->  Aggregate  (cost=1933.94..1933.95 rows=1 width=8) (actual time=6.556..6.556 rows=1 loops=231)
                Output: count(DISTINCT all_users_4.id)
                Buffers: temp read=68607
                ->  CTE Scan on all_users all_users_4  (cost=0.00..1933.06 rows=351 width=16) (actual time=6.441..6.547 rows=5 loops=231)
                      Output: all_users_4.id, all_users_4.registered_at, all_users_4.deleted_at
                      Filter: ((all_users_4.deleted_at >= (((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date)) AND (all_users_4.deleted_at < ((((generate_series(date_trunc('week'::text, $5), date_trunc('week'::text, now()), '7 days'::interval)))::date) + '7 days'::interval)))
                      Rows Removed by Filter: 71223
                      Buffers: temp read=68607
Planning Time: 0.612 ms
Execution Time: 13615.054 ms

",76384360,"An obvious optimization is to eliminate redundant table scans. There isn't any need in preprocessed to query from all_users more than once. The following query uses COUNT with FILTER to gather the same statistics:
WITH datetable AS (SELECT GENERATE_SERIES(
                            DATE_TRUNC('week', (SELECT MIN(created_at) FROM org_accounts.deleted_users)),
                            DATE_TRUNC('week', NOW()),
                            '1 week'::INTERVAL
                            )::DATE AS week_start),
     all_users AS (SELECT id,
                          registered_at,
                          NULL AS deleted_at
                     FROM org_accounts.users
                     WHERE status = 'active'
                       AND org_accounts.__user_is_qa(id) <> 'Y'
                       AND email NOT LIKE '%@org%'
                   UNION ALL
                   SELECT id,
                          created_at AS registered_at,
                          deleted_at
                     FROM org_accounts.deleted_users
                     WHERE deleter_id = id
                       AND email NOT LIKE '%@org%'),
     weekly_activity AS (SELECT DATE_TRUNC('week', date)::DATE AS week_start,
                                COUNT(DISTINCT user_id)        AS weekly_active_users
                           FROM (SELECT user_id, date
                                   FROM org_storage_extra.stats_user_daily_counters
                                   WHERE type IN ('created_file', 'created_folder', 'created_secure_fetch')
                                 UNION ALL
                                 SELECT user_id, date
                                   FROM ipfs_pinning_facility.stats_user_daily_counters
                                   WHERE type <> 'shares_viewed_by_others') activity_ids_dates
                           WHERE EXISTS(SELECT 1 FROM all_users WHERE id = user_id)
                           GROUP BY week_start),
     preprocessed AS (SELECT week_start,
                             us.actual_users,
                             us.cumulative_churned_users,
                             us.weekly_new_users,
                             us.weekly_churned_users,
                             COALESCE(weekly_active_users, 0) AS weekly_active_users
                        FROM datetable dt
                          CROSS JOIN LATERAL (SELECT
                                                 COUNT(DISTINCT u.id) FILTER (WHERE u.registered_at < dt.week_start AND
                                                                                    (u.deleted_at IS NULL OR u.deleted_at > dt.week_start)) AS actual_users,
                                                 COUNT(DISTINCT u.id)
                                                 FILTER (WHERE u.deleted_at < dt.week_start + '1 week'::INTERVAL)                           AS cumulative_churned_users,
                                                 COUNT(DISTINCT u.id)
                                                 FILTER (WHERE u.registered_at >= dt.week_start AND u.registered_at <
                                                                                                    dt.week_start +
                                                                                                    '1 week'::INTERVAL)                     AS weekly_new_users,
                                                 COUNT(DISTINCT u.id)
                                                 FILTER (WHERE u.deleted_at >= dt.week_start AND u.deleted_at <
                                                                                                 dt.week_start +
                                                                                                 '1 week'::INTERVAL)                        AS weekly_churned_users
                                               FROM all_users u
                                               WHERE u.registered_at < dt.week_start + '1 week'::INTERVAL
                                                  OR (u.deleted_at >= dt.week_start AND
                                                      u.deleted_at < dt.week_start + '1 week'::INTERVAL)) us
                          LEFT JOIN weekly_activity
                                    USING (week_start)
  ORDER BY week_start DESC)
SELECT week_start                                                   AS for_week_of,
       actual_users + cumulative_churned_users                      AS cumulative_users,
       cumulative_churned_users,
       cumulative_churned_users::FLOAT /
       NULLIF((actual_users + cumulative_churned_users)::FLOAT, 0)  AS cumulated_churn_rate,
       actual_users,
       weekly_new_users,
       weekly_churned_users,
       weekly_active_users,
       weekly_churned_users::FLOAT / NULLIF(actual_users::FLOAT, 0) AS weekly_churn_rate
  FROM preprocessed;

There are probably other optimizations possible, but this one was immediately evident.
","The query you provided appears to be complex and involves multiple subqueries. While it is difficult to optimize the query without having access to the database and schema information, I can suggest some general optimization techniques that you can consider.

1. Indexing: Ensure that the relevant columns used in joins and filters are indexed. This can significantly improve query performance by reducing the need for full table scans. However, since you mentioned that you don't have write access to the database, you may not be able to create new indexes.

2. Subquery Refactoring: Look for opportunities to simplify or rewrite subqueries using alternative techniques such as joins or window functions. Subqueries can sometimes be performance bottlenecks, so optimizing them can lead to better overall query performance.

3. CTE Materialization: By default, Common Table Expressions (CTEs) are not materialized, which means they are re-evaluated each time they are referenced. However, if a CTE is referenced multiple times or used in complex calculations, materializing it (i.e., storing the intermediate result in a temporary table) can improve performance. You can try adding the `MATERIALIZED` keyword before each CTE name to suggest materialization, although its effectiveness depends on the database system you are using.

4. Analyze Query Execution Plan: Examine the query execution plan to identify any performance bottlenecks or areas where the query optimizer may not be making optimal choices. Look for high-cost operations, sequential scans, or expensive sorts, and consider whether there are alternative ways to achieve the same results more efficiently.

5. Denormalization: If performance is a significant concern and you have the flexibility to denormalize the schema, you can consider creating summary tables or pre-aggregating data that is frequently used in complex queries. This approach trades off storage space for improved query performance.

6. Query Relevance: Review the business requirements and the purpose of the query. Are all the subqueries and calculations necessary for the desired results? Eliminating unnecessary calculations or reducing the scope of the query can improve performance.

Remember that the effectiveness of these optimization techniques can vary depending on the database system, data size, and specific query patterns. It's always a good practice to test the impact of any optimization on a representative dataset to ensure that it improves performance without introducing unexpected side effects."
76378322,How can I convert an int to a generic type containing complex128 in Go?,"I cannot work out how to convert an int to a generic type containing complex128. Here is an example which doesn't compile unless the complex128 is commented out:
package main

import ""fmt""

type val interface {
    int64 | float64 | complex128
}

func f[V val](a, b V) (c V) {
    q := calc()
    return a * b * V(q)
}

func calc() int {
    // lengthy calculation that returns an int
    return 1
}

func main() {
    fmt.Printf(""%v\n"", f(int64(1), int64(2)))
}

This is simplified from a much larger calculation. I've tried using a switch but every syntax I have attempted seems to meet resistance of one kind or another.
How can I multiply a and b with an integer?
I have tried using a switch on the type of the return variable such as any(c).(type) but for example if I have case complex128: then it refuses to allow the complex builtin since it doesn't return a V.
Without the complex128 the above will compile.
",76378801,"This one works but it needs to list every type in the switch statement:
func f[V val](a, b V) (c V) {
    q := calc()

    var temp any
    switch any(c).(type) {
    case complex128:
        temp = complex(float64(q), 0)
    case int64:
        temp = int64(q)
    default:
        temp = float64(q)
    }
    return a * b * (temp.(V))
}

","In Go, you cannot directly convert an `int` to a generic type containing `complex128` because the generic type system introduced in Go 1.18 does not support numeric types like `int`, `float64`, or `complex128` as type parameters. The current implementation of generics in Go focuses on generic functions and data structures rather than numeric types.

However, you can work around this limitation by using type assertions and converting the `int` to `complex128` explicitly. Here's an updated version of your code that demonstrates this approach:

```go
package main

import ""fmt""

type val interface {
        int64 | float64 | complex128
}

func f[V val](a, b V) V {
        q := calc()
        switch v := q.(type) {
        case int:
                return a.(complex128) * b.(complex128) * complex(float64(v), 0)
        default:
                return a.(complex128) * b.(complex128)
        }
}

func calc() int {
        // lengthy calculation that returns an int
        return 1
}

func main() {
        fmt.Printf(""%v\n"", f(int64(1), int64(2)))
}
```

In this updated code, we first check the type of `q` in the `calc()` function using a type switch. If it's an `int`, we convert `a` and `b` to `complex128` and perform the multiplication by constructing a complex number using `complex(float64(v), 0)`. If `q` is not an `int`, we directly multiply `a` and `b` as `complex128` values.

Please note that this workaround assumes that you have a good reason for using generics in this particular scenario. If your requirements allow, you might want to consider alternative approaches that don't rely on generics or re-evaluate the design to see if a different solution can be achieved without generics."
76378721,Hide 'Display Cart' button in WooCommerce mini cart widget,"enter image description here
in wordpress and woocommerce Plugin
is there anyway to hide ""Display Cart"" button in wordpress mini card widget ?
i can hide ""checkout"" button individually but it seems theres no Special Css Class Fot ""Display Card"" buttun. ?!?!
",76378829,"you can try this
 
add_action( 'woocommerce_widget_shopping_cart_buttons', 'bbloomer_remove_view_cart_minicart', 1 );
 
function bbloomer_remove_view_cart_minicart() {
   remove_action( 'woocommerce_widget_shopping_cart_buttons', 'woocommerce_widget_shopping_cart_button_view_cart', 10 );
}

OR
.widget .woocommerce-mini-cart__buttons a:not(.checkout) {
display: none;
}

","To hide the ""Display Cart"" button in the WooCommerce mini cart widget in WordPress, you can use CSS to target and hide the button. Here's how you can do it:

1. Go to your WordPress dashboard and navigate to ""Appearance"" -> ""Customize"".
2. In the Customizer, click on ""Additional CSS"" or ""Custom CSS"" (the exact label may vary depending on your theme).
3. Add the following CSS code to the text area:

```css
.widget_shopping_cart_content .woocommerce-mini-cart__buttons .woocommerce-mini-cart__buttons--showcart {
    display: none;
}
```

4. Click the ""Publish"" or ""Save"" button to apply the changes.

The CSS code above targets the ""Display Cart"" button specifically and sets its display property to ""none"", effectively hiding it from view."
76378661,"The fastest way to convert a UInt64 hex string to a UInt32 value preserving as many leading digits as possible, i.e. truncation","I'm looking for the fastest way to parse a hex string representing a ulong into a uint keeping as many leading digits as a uint can handle and discarding the rest. For example,
string hex = ""0xab54a9a1df8a0edb"";  // 12345678991234567899
Should output: uint result = 1234567899;
I can do this by simply parsing the hex into a ulong, getting the digits using ToString and then just taking as many of them as would fit into uint without overflowing but I need something much faster. Thanks. C# code preferred but any would do.
",76378944,"For decimal truncation, all the high bits of the hex digit affect the low 9 or 10 decimal digits, so you need to convert the whole thing.  Is there an algorithm to convert massive hex string to bytes stream QUICKLY? asm/C/C++ has C++ with SSE intrinsics.  I commented there with some possible improvements to that, and to https://github.com/zbjornson/fast-hex .  This could be especially good if you're using SIMD to find numeric literals in larger buffers, so you might have the hex string in a SIMD register already.  (Not sure if SIMDJSON does that.)
Hex-string to 64-bit integer is something SIMD certainly can speed up, e.g. do something to map each digit to a 0-15 integer, combine pairs of bytes to pack nibbles (e.g. with x86 pmaddubsw), then shuffle those 8-bit chunks to the bottom of a register.  (e.g. packuswb or pshufb).  x86 at least has efficient SIMD to GP-integer movq rax, xmm0, although the ARM equivalent is slow on some ARM CPUs.
(Getting a speedup from SIMD for ASCII hex -> uint is much easier if your strings are fixed-length, and probably if you don't need to check for invalid characters that aren't hex digits.)

Decimal truncation of u64 (C# ulong) to fit in u32 (C# uint)
Modulo by a power of 10 truncates to some number of decimal digits.
(uint)(x % 10000000000) works for some numbers, but 10000000000 (1e10 = one followed by 10 zeros) is larger than 2^32-1. Consider an input like 0x2540be3ff (9999999999).  We'd get (uint)9999999999 producing 1410065407 = 0x540be3ff (keeping the low 32 bits of that 34-bit number.)
So perhaps try modulo 1e10, but if it's too big for u32 then modulo 1e9.
  ulong tendigit = x % 10000000000;  // 1e10
  uint truncated = tendigit <= (ulong)0xffffffff ? tendigit : (x % 1000000000);  // % 1e9 keeps 9 decimal digits

If this isn't correct C# syntax or the literals need some decoration to make them ulong (like C 10000000000uLL for good measure), please let me know.
It's probably at least as efficient to just modulo the original number two different ways than to try to get the leading decimal digit of x % 1e10 and subtract it or whatever.  The asm is going to need two 64-bit multiplicative inverse constants, and starting from the original number again keeps critical-path latency shorter for out-of-order exec if branch prediction predicts that it needs to calculate the nine-digit truncation.

Binary truncation
@Matthew Whited deleted his answer (due to a bug in the decimal truncation part), but his binary truncation part based on substrings of the original hex input could perhaps be more efficient in some cases than doing the full conversion and then casting to a narrower type or masking with AND.

If you want the last 8 bytes of the hex string
uint.Parse(hex[^8..],NumberStyles.HexNumber)

If you want the first 8 bytes
uint.Parse(hex[2..10], NumberStyles.HexNumber);


","To convert a UInt64 hex string to a UInt32 value while preserving as many leading digits as possible, you can use bitwise operations and shift operations to extract the desired bits. Here's an example implementation in C#:

```csharp
string hex = ""0xab54a9a1df8a0edb"";

// Remove the ""0x"" prefix if it exists
if (hex.StartsWith(""0x""))
    hex = hex.Substring(2);

// Convert the hex string to a UInt64
ulong value = ulong.Parse(hex, System.Globalization.NumberStyles.HexNumber);

// Get the upper 32 bits
uint result = (uint)(value >> 32);

// Output the result
Console.WriteLine(result);
```

In this code, we remove the ""0x"" prefix if it exists and parse the hex string into a UInt64 value using `ulong.Parse`. Then, we extract the upper 32 bits of the UInt64 value using a right shift (`>>`) operation and cast it to a UInt32.

Please note that if the original UInt64 value has leading digits that cannot fit into a UInt32, they will be discarded. This solution assumes that the input hex string always represents a valid UInt64 value.

Keep in mind that the performance of this code will largely depend on the performance of the underlying hardware and the optimizations provided by the compiler."
76384304,How to backup a full project of firebase,"I am faced a problem. I have a project which is in firebase. I have used there firebase Authenticate, Firebase realtime database, Firebase function and some more. Now I have changed my decision. I want to make my own server where I will set up and manage everything.
So that I want to backup my project to move all data to other framework like spring boot project.
In this situation how can I get the whole project? User Auth data, Firebase Realtime database, Firestore etc.
",76384361,"You'll have to write code or use the CLI to query all of the data you want, and write it to a place you want.  Firebase does not provide a tool to do all this automatically for an entire project. You will need to deal with each product's data separately.
You can use the Firebase Admin SDK or the Firebase CLI to access data from the products you listed.
See also:

Is it possible to backup Firebase DB?
https://firebase.google.com/docs/firestore/manage-data/export-import
https://firebase.google.com/docs/cli/auth

","To backup your Firebase project and move it to another framework like a Spring Boot project, you'll need to handle each component separately. Here's a step-by-step guide on how to backup different parts of your Firebase project:

1. Firebase Authentication:
   - Export User Data: You can export user data from Firebase Authentication using the Firebase Admin SDK. Write a script or use an existing Firebase Admin SDK client library to retrieve and export user data to a format suitable for your new server. The exported data may include user IDs, email addresses, display names, etc.

2. Firebase Realtime Database:
   - Export Data: Use Firebase's data export feature to create a backup of your Firebase Realtime Database. You can find this feature in the Firebase console. It allows you to download your data as a JSON file.

3. Firestore:
   - Export Data: Firestore also provides a data export feature in the Firebase console. Use this feature to export your Firestore data to a suitable format, such as a JSON file.

4. Firebase Functions:
   - Backup Code: Make sure to back up the code for your Firebase Functions. Copy the source code of your Firebase Functions to a local directory or a version control system like Git.

Once you have backed up all the necessary components, you can proceed with setting up your new server using Spring Boot or any other framework of your choice. Here are the steps to restore the backup:

1. Set up User Authentication: Implement user authentication in your new server using the authentication data exported from Firebase. You'll need to create user records, store their credentials securely, and handle authentication flows.

2. Import Data to Database: Depending on the database technology you choose to use with your Spring Boot project (e.g., SQL-based, NoSQL-based), you'll need to import the data you exported from Firebase Realtime Database and Firestore into your new database system. Write scripts or use existing tools to convert the data into the appropriate format and import it into your new database.

3. Migrate Firebase Functions: Rewrite or modify your Firebase Functions to work within the context of your new server framework. Adjust the code as necessary to integrate with your chosen server technology.

By following these steps, you should be able to backup and migrate your Firebase project to your own server using a different framework like Spring Boot. Remember to test thoroughly after the migration to ensure everything works as expected."
76378577,"Why does ruby recognise a method outside of a class, but not inside?","I am trying to build a simple language translating program. I imported the 'language_converter' gem to aid with this goal. I wrote the following code:
require 'language_converter'

class Translator
    def initialize
        @to = 'ja';  
        @from = 'en';   
    end

    def translate text
        lc(text, @to,@from)
    end
end

#puts lc('welcome to Japan!', 'ja','en');
 
t = Translator.new

p t.translate('welcome to Japan!');

This code results in the error: undefined method 'lc' for #<Translator:0x0000000101167a90 @to=""ja"", @from=""en""> (NoMethodError)
However, when i uncomment the code on line 15, ruby can access the lc method and return some japanese. Does anyone know why the method is 'defined' outside of the class but not inside?
Edit: the language-converter gem is not my own. also, I cannot find the source code on its homepage.
I have also tried adding two semicolons before the lc method like so: ::lc(text, @to,@from). This results in the error: syntax error, unexpected local variable or method, expecting constant
",76378945,"The gem is more than 10 years old and only has one method. And that method is implemented as a class method.
You are properly better off with just rewriting that method in your application with a modern Ruby syntax and proper error handling.
For reference, this it how lib/language_converter.rb in the gem looks like:
require 'net/http'
require 'rubygems'
require ""uri""
require 'json'

class UnSupportedLanguage < RuntimeError

  def initialize(message='')
    @msg = ""not supported.""
  end
end


  def self.lc( text, to, from='en' )

    begin

      uri = URI.parse(""http://mymemory.translated.net/api/get"")

      response = Net::HTTP.post_form(uri, {""q"" => text,""langpair""=>""#{from.to_s.downcase}|#{to.to_s.downcase}"", ""per_page"" => ""50""})

      json_response_body = JSON.parse( response.body )

      if json_response_body['responseStatus'] == 200
        json_response_body['responseData']['translatedText']
      else
        puts json_response_body['responseDetails']
        raise StandardError, response['responseDetails']
      end
    rescue UnSupportedLanguage
      raise UnSupportedLanguage.new
    rescue => err_msg
      puts ""#{err_msg}""
    end

  end

",
76384270,How to override parent's styles in css?,"In this example, I want the purple rectangle to change its opacity to 100% regardless of the value of the parent. I tried using all: unset/initial and !important but it doesn't seem to work.


.rect {
    width: 500px;
    height: 600px;
    margin-top: 200px;
    margin-left: 300px;
    background-color: black;
    /* this V */
    opacity: 37%;
    z-index: -1;
}

.rect1 {
    all: unset;
    position: absolute;
    z-index: 10;
    width: 259px;
    height: 300px;
    margin-top: 500px;
    margin-left: 50px;
    background-color: purple;
    /* to this V */
    opacity: 100% !important;
}
<div class=""rect"">
  <div class=""rect1""></div>
</div>



",76384365,"So like Haworth pointed out, using opacity on the element itself brings all children under the influence of the pixelshading used to make the opacity effect.
If you want to get the same effect while retaining your html structure I'd recommend a different approach for the same result using RGBA or hex with an alpha channel on the background-color property directly. See example below.


body {
  height: 100%;
  width: 100%;
  background: url(https://picsum.photos/800) no-repeat;
  background-size: cover;
}

.rect {
    width: 500px;
    height: 600px;
    margin-top: 200px;
    margin-left: 300px;
    background-color: rgba(0,0,0,.37);
    /* this V
    opacity: 37%;*/
    z-index: -1;
}

.rect1 {
    position: absolute;
    z-index: 10;
    width: 259px;
    height: 300px;
    margin-top: 500px;
    margin-left: 50px;
    background-color: purple;
    /* to this V */
    opacity: 100% !important;
}
<div class=""rect"">
  <div class=""rect1""></div>
</div>



","In Ruby, methods defined outside of a class are considered global methods and can be accessed from anywhere in your code. However, methods defined inside a class are considered instance methods and can only be accessed through an instance of that class.

In your code, the error occurs because you're trying to call the `lc` method directly inside the `Translator` class without specifying the receiver. Ruby interprets it as an instance method call on the current object (`self`), which is an instance of the `Translator` class. Since there is no method named `lc` defined within the `Translator` class, it raises a `NoMethodError`.

To fix this issue, you need to make sure that the `lc` method is accessible within the `Translator` class. Since you imported the 'language_converter' gem, it's possible that the `lc` method is defined as a global method, which is why it works when called outside the class.

To access the `lc` method from inside the `Translator` class, you can use the scope resolution operator (`::`) to explicitly specify the global namespace. Here's how you can modify your code:

```ruby
require 'language_converter'

class Translator
  def initialize
    @to = 'ja'
    @from = 'en'
  end

  def translate(text)
    ::lc(text, @to, @from)
  end
end

t = Translator.new
p t.translate('welcome to Japan!')
```

By using `::lc`, you explicitly tell Ruby to look for the `lc` method in the global namespace, where the 'language_converter' gem is expected to define it.

Note that this assumes the 'language_converter' gem does define the `lc` method as a global method. If that's not the case, you may need to consult the gem's documentation or source code to understand how to correctly access and use its functionality within your `Translator` class."
76378347,How to generate a log file of the windows prompt when I run a bat file,"I'm running a bat file in windows. I'm trying to generate a log file of all the output that appears in the command prompt, to have as a document.
Note, Not a log file of the contents of the bat file but of the command prompt that it outputs.
How would I do this? Thanks
",76378974,"Redirecting to output is done by using > or appending to file using >>
for batch-file, we typically call them.
(call script.cmd)2>&1>""logfile.log""

or append
(call script.cmd)2>&1>>""logfile.log""

Note, 2>&1 2>&1 is redirecting the stderr stream 2 to the stdout stream 1, it is important here, seeing as you said you want to log all of the output results to logfile.
So that should also give the clue that you can in fact redirect success (stdout) results to one file and failures (stderr) to another, i.e
(call script.cmd) 1>""Output.log"" 2>""Errors.log""

Note, some commands and executables sends everything to the stdout stream and nothing to stderr, example ping.exe.
","To generate a log file of the Windows command prompt output when running a batch file, you can redirect the command prompt output to a text file. Here's how you can do it:

1. Open Notepad or any other text editor.
2. Create a new file and enter the following command:
   ```
   your_batch_file.bat > output.log
   ```
   Replace ""your_batch_file.bat"" with the actual name of your batch file.
   The `>` symbol redirects the standard output of the command to a file called ""output.log"". You can choose any name for the log file.
3. Save the file with a `.bat` extension, such as `log.bat`. Make sure to save it as a batch file.
4. Double-click the batch file (`log.bat`) to run it.

When the batch file executes, the command prompt output will be written to the `output.log` file in the same directory as the batch file. You can open the log file with any text editor to view the output.

Note: If you want to append the output to an existing log file instead of creating a new one each time, you can use `>>` instead of `>`. For example:
```
your_batch_file.bat >> output.log
```

Using `>>` will append the output to the existing log file if it exists, or create a new file if it doesn't."
76384255,calculate an object property based on the value of another property of the same object,"I need to find out the value of ""name"" inside on the obj object. How can I find it without function invocation?
I wanna use just obj.isActive not obj.isActive()
let obj = {
  name: ""X Æ A-12 Musk"",
  isActive: function () {
    return this.name.length > 4;
  },
};

// and after a while I need to check if is active:

console.log(obj);

// { 
//   name: 'X Æ A-12 Musk',
//   isActive: [Function: isActive]  <--------- NOT COOL ! 
// }



If use an IFEE:
let obj = {
  name: ""X Æ A-12 Musk"",
  isActive: (function () {
    return this.name.length > 4;
  })(),
};

I get:
return this.name.length > 4;
                ^
TypeError: Cannot read properties of undefined (reading 'length') 

",76384399,"If you do not want to have to call isActive as a function, you can use a getter.


const obj = {
  name: ""X Æ A-12 Musk"",
  get isActive () {
    return this.name.length > 4;
  },
};

console.log(obj.isActive);



","In order to access the `name` property inside the `isActive` function without invoking it, you can use a getter method instead of a regular function. Here's an updated version of your code:

```javascript
let obj = {
  name: ""X Æ A-12 Musk"",
  get isActive() {
    return this.name.length > 4;
  }
};

console.log(obj.isActive); // Output: true
```

In this code, `isActive` is defined as a getter method using the `get` keyword. It allows you to access `isActive` as a property without invoking it explicitly with parentheses. Instead, when you access `obj.isActive`, the code inside the getter method is executed, and the result is returned.

Now, when you log `obj.isActive`, it will directly output the result of the `isActive` condition, rather than displaying the function itself."
76384220,Code to format JSON data and append hardcoded data to create a flat .txt file,"Source Data::
json_data = [{""studentid"": 1, ""name"": ""ABC"", ""subjects"": [""Python"", ""Data Structures""]},
             {""studentid"": 2, ""name"": ""PQR"", ""subjects"": [""Java"", ""Operating System""]}]

Hardcoded_Val1 = 10
Hardcoded_Val2 = 20
Hardcoded_Val3 = str(datetime.datetime.now())

Need to create a flat .txt file with the below data.
ID,DEPT,""studentid|name|subjects"",execution_dt
10,20,""1|ABC|Python,Data Structures"",2023-06-01
10,20,""2|PQR|Java,Operating System"",2023-06-01

I am very new in python. Have already tried to figure it out to achieve it but couldn't. Your help will be much appreciated.
import datetime
import pandas as pd
import json


json_data = [{""studentid"": 1, ""name"": ""ABC"", ""subjects"": [""Python"", ""Data Structures""]},
             {""studentid"": 2, ""name"": ""PQR"", ""subjects"": [""Java"", ""Operating System""]}]

Hardcoded_Val1 = 10
Hardcoded_Val2 = 20
Hardcoded_Val3 = str(datetime.datetime.now())

profile = str(Hardcoded_Val1) + ',' + str(Hardcoded_Val2) + ',""' + str(json_data) + '"",' + Hardcoded_Val3
        
print(profile)
#data = json.dumps(profile, indent=True)
#print(data)
data_list = []
for data_info in profile:
   data_list.append(data_info.replace("", '"", '|'))
data_df = pd.DataFrame(data=data_list)
data_df.to_csv(r'E:\DataLake\api_fetched_sample_output.txt', sep='|', index=False, encoding='utf-8')


",76384400,"I would bypass using pandas for this and just build the string manually primarily using a list comprehension and join().
import datetime
import csv

Hardcoded_Val1 = 10
Hardcoded_Val2 = 20
Hardcoded_Val3 = str(datetime.date.today())
json_data = [
    {""studentid"": 1, ""name"": ""ABC"", ""subjects"": [""Python"", ""Data Structures""]},
    {""studentid"": 2, ""name"": ""PQR"", ""subjects"": [""Java"", ""Operating System""]}
]

csv_data = []
for row in json_data:
    keys = ""|"".join(row.keys())
    values = ""|"".join([
        "","".join(value) if isinstance(value, list) else str(value)
        for value in row.values()
    ])
    csv_data.append(dict([
        (""ID"", Hardcoded_Val1),
        (""DEPT"", Hardcoded_Val2),
        (keys, values),
        (""execution_dt"", Hardcoded_Val3)
    ]))

with open(""out.csv"", ""w"", encoding=""utf-8"", newline="""") as file_out:
    writer = csv.DictWriter(file_out, fieldnames=list(csv_data[0].keys()))
    writer.writeheader()
    writer.writerows(csv_data)

This will produce a file with the following contents:
ID,DEPT,studentid|name|subjects,execution_dt
10,20,""1|ABC|Python,Data Structures"",2023-06-02
10,20,""2|PQR|Java,Operating System"",2023-06-02

","To achieve the desired result, you can use the following code:

```python
import datetime
import pandas as pd
import json

json_data = [{""studentid"": 1, ""name"": ""ABC"", ""subjects"": [""Python"", ""Data Structures""]},
             {""studentid"": 2, ""name"": ""PQR"", ""subjects"": [""Java"", ""Operating System""]}]

Hardcoded_Val1 = 10
Hardcoded_Val2 = 20
Hardcoded_Val3 = str(datetime.datetime.now().date())

data_list = []
for item in json_data:
    student_id = item[""studentid""]
    name = item[""name""]
    subjects = "","".join(item[""subjects""])
    data = f""{student_id}|{name}|{subjects}""
    data_list.append(data)

profile = f""{Hardcoded_Val1},{Hardcoded_Val2},\""{'|'.join(data_list)}\"",{Hardcoded_Val3}""

header = ""ID,DEPT,\""studentid|name|subjects\"",execution_dt""
output_data = f""{header}\n{profile}""

with open(""output.txt"", ""w"") as file:
    file.write(output_data)
```

This code will create a flat text file named ""output.txt"" with the desired data format:

```
ID,DEPT,""studentid|name|subjects"",execution_dt
10,20,""1|ABC|Python,Data Structures"",2023-06-01
10,20,""2|PQR|Java,Operating System"",2023-06-01
```

The code iterates over the `json_data` list and extracts the necessary values for each student, formatting them as `""studentid|name|subjects""`. These formatted strings are then joined with the `|` separator. The other hardcoded values are added to create the `profile` string.

Finally, the code writes the `header` and `profile` strings to the ""output.txt"" file."
76380911,Expect function Parameter to be Key of Object with Dynamic Properties,"Im making an application multi Language.
I want to build typing as strict and simpel as possible. My Code is the following:
//=== Inside my Hook: ===//
interface ITranslation {
   [key:string]:[string, string]
}

const useTranslator = (translations:ITranslation) => {
   const language = useLanguage() // just getting the language setting from another hook

   const translate = (key:keyof typeof translations) => {
      // mapping and returning the right translation
   }

   return translate;
}


//=== Inside the component: ===//
const translation:ITranlation = {
   ""something in english"": [ ""something in german"", ""something in spanish"" ],
   ""anotherthing in english"": [""anotherthing in german"", ""anotherthing in spanish""]
}

const translate = useTranslation(translation)

return(
   <Text>{translate(""something in english"")}</Text>
)


What i want to achieve:

When passing the translation Object, with Dynamic Keys to the Hook: useTranslation(translations), there should be a typecheck validating, that both languages are provided (any property has an Array with 2 Strings)

When using the translate function (inside the Text component) typescript should bring an error, if a key is not matching the Dynamic Keys inside the translations object. So this should throw an error: tranlate(""not a key in object"")


But i can't get it to work properly. I can either set the translations object as const, but then there is no typecheck when passing the object to the Hook.
Or i set it as shown above with translation:ITranslation but then there is no typechecking for the parameter in the ´translate´ function inside the component.
Is it possible to achive that? (If yes, how?)
Thanks in advance!
",76381092,"This solution will work only for Typescript >= 4.9 since it uses the satisfies operator introduced in the 4.9.
Adding as const is the approach we will go with, and satisfies will allow us to type-check it.
const translation = {
  'something in english': ['something in german', 'something in spanish'],
  'anotherthing in english': ['anotherthing in german', 'anotherthing in spanish'],
} as const satisfies ITranslation;

Since we added as const the values in the ITranslation will be readonly [string, string], thus we have to update the ITranslation to the following:
interface ITranslation {
  [key: string]: readonly [string, string];
}

Next, we need to add a generic parameter to useTranslator so it works over the specific instance of ITranslation.  The same goes for the translate function. It should accept the generic parameter for the key of ITranslation and return the value for that specific key:
const useTranslator = <T extends ITranslation>(translations: T) => {
  const language = useLanguage(); // just getting the language setting from another hook

  const translate = <K extends keyof T>(key: K): T[K][number] => {
    // return retrieved value
  };

  return translate;
};

Since it is not asked in the question translate will return a union of the translations for the specific key, which is achieved by T[K][number]
Usage:
const Component = () => {
  const translate = useTranslator(translation);
  
  // ""something in german"" | ""something in spanish""
  const case1 = translate('something in english');

  // ""anotherthing in german"" | ""anotherthing in spanish""
  const case2 = translate( 'anotherthing in english');

  return null;
};

playground
","To achieve the desired type checking for both the `translations` object and the keys passed to the `translate` function, you can use a combination of generic types and keyof operator. Here's an updated version of your code that incorporates these changes:

```typescript
//=== Inside my Hook: ===//
interface ITranslation {
  [key: string]: [string, string];
}

const useTranslator = <T extends ITranslation>(translations: T) => {
  const language = useLanguage(); // just getting the language setting from another hook

  const translate = (key: keyof T) => {
    // mapping and returning the right translation
    return translations[key][0]; // assuming you want to return the first translation string
  };

  return translate;
};

//=== Inside the component: ===//
const translations = {
  ""something in english"": [""something in german"", ""something in spanish""],
  ""anotherthing in english"": [""anotherthing in german"", ""anotherthing in spanish""],
};

const translate = useTranslator(translations);

return <Text>{translate(""something in english"")}</Text>;
```

In this updated code:

1. The `useTranslator` function is now defined as a generic function using `<T extends ITranslation>`, which means it can accept any subtype of `ITranslation` as the `translations` parameter.

2. The `translate` function inside `useTranslator` is now defined to accept `key` of type `keyof T`, which ensures that the key passed to the function is a valid key of the `translations` object.

3. When using `useTranslator`, you can pass the `translations` object directly, and TypeScript will perform type checking based on the provided object.

4. The `translate` function inside the component will now throw a TypeScript error if you pass a key that is not present in the `translations` object.

With these changes, you should get type checking both when passing the `translations` object to the `useTranslator` function and when calling the `translate` function inside the component."
76381023,jquery above and below screen sizes,"I have added a script for showing a div before different divs in different screen size. This is the code I used:
jQuery(function($){ 
jQuery(document).ready(function(){
    jQuery(window).on('resize', function(){
        if(jQuery(window).width() <= 1024){
            jQuery( "".checkout.woocommerce-checkout .woocommerce-shipping-fields__wrapper"" ).insertBefore( "".checkout.woocommerce-checkout .flux-step.flux-step--2 .flux-checkout__shipping-table"" );
        }
        else if(jQuery(window).width() >= 1025){
            jQuery( "".checkout.woocommerce-checkout .woocommerce-shipping-fields__wrapper"" ).insertBefore( "".checkout.woocommerce-checkout .flux-checkout__content-right #order_review"" );
        }
    }); 
}); 
});

But the code is not working when I open the site. It only works if I resize the screen. May be due to the resize function is used.
Can anyone please guide me how to make it so that it'll show the 2 conditions even without resizing the screen and one'll work above 1024px and another below 1024px.
TIA
",76381114,"Just put your code in a function and call it on the document ready:


$(function(){
  
  resize();
  
  $(window).on('resize', resize);

  function resize(){
    $( "".checkout.woocommerce-checkout .woocommerce-shipping-fields__wrapper"" )
      .insertBefore(
        $(window).width() <= 1024 ? 
        "".checkout.woocommerce-checkout .flux-step.flux-step--2 .flux-checkout__shipping-table"" : 
        "".checkout.woocommerce-checkout .flux-checkout__content-right #order_review""
      );
   }
   
}); 



","To achieve the desired behavior without relying on the `resize` event, you can extract the code that needs to be executed into a separate function and call it both on page load and when the window is resized. Here's an updated version of your code:

```javascript
jQuery(function($){
  function rearrangeDiv() {
    if ($(window).width() <= 1024) {
      $("".checkout.woocommerce-checkout .woocommerce-shipping-fields__wrapper"").insertBefore("".checkout.woocommerce-checkout .flux-step.flux-step--2 .flux-checkout__shipping-table"");
    } else if ($(window).width() >= 1025) {
      $("".checkout.woocommerce-checkout .woocommerce-shipping-fields__wrapper"").insertBefore("".checkout.woocommerce-checkout .flux-checkout__content-right #order_review"");
    }
  }

  $(document).ready(function(){
    rearrangeDiv(); // Call the function on page load

    $(window).on('resize', function(){
      rearrangeDiv(); // Call the function when the window is resized
    });
  });
});
```

In this code, the `rearrangeDiv()` function is defined to handle the logic of moving the div based on the screen size. It is called both on page load and when the window is resized. By separating the logic into a function, you can ensure that it executes correctly in both scenarios."
76378620,How is arbitrary distributed for Int? Why is it limited by so small values?,"I am trying to compare the QuickCheck library to the SmallCheck one. In SmallCheck I can reach particular value manipulating depth parameter. In QuickCheck:
>a<-generate (replicateM 10000 arbitrary) :: IO [Int]
>length a
10000
>maximum a
30

and my question then is: why are 10,000 ""random"" (""arbitrary"") integers limited by 30?! I expected to see more ""widely"" distributed values within the range 0..10,000, maybe the maximum value close to 5,000.
",76378997,"The documentation contains a clue:

The size passed to the generator is always 30

By default QuickCheck works by starting with 'easy' or 'small' inputs to see if it can find counterexamples with those. Only if it finds no problems with the small inputs does it gradually widen the range of generated input. The size value (which runs implicitly throughout everything that QuickCheck does) is the value that controls this behaviour.
When you run QuickCheck (e.g. with quickCheck) it automatically increases the size as it goes.
You're not really supposed to use the generate function directly, but if you do, you can resize it:
ghci> b <- generate (replicateM 10000 (resize 60 arbitrary)) :: IO [Int]
ghci> maximum b
60

That said, how are you supposed to use QuickCheck? The documentation describes quickCheck along with a multitude of variations you can use to evaluate properties.
Personally, I integrate my QuickCheck properties with a unit testing framework with testProperty. You can see examples here: Property-based testing is not the same as partition testing.
","The behavior you observed in QuickCheck is not due to a limitation of the arbitrary distribution for `Int`, but rather a consequence of the default implementation of `arbitrary` for `Int` in QuickCheck. 

In QuickCheck, the `arbitrary` function generates random values for a given type. For `Int`, the default implementation generates random integers in the range of approximately -2^29 to 2^29-1. The specific range depends on the version of QuickCheck and the underlying implementation.

The reason you observed a maximum value of 30 in your example is likely because the default generator for `Int` in QuickCheck is designed to produce smaller values by default to avoid excessively large or slow test cases. This default behavior is intended to make QuickCheck tests faster and more efficient, especially when dealing with large test cases.

If you want to generate `Int` values with a wider range, including values closer to 5,000, you can define a custom instance of `Arbitrary` for `Int` and use a different distribution strategy. For example, you can use the `choose` function from QuickCheck to specify a custom range:

```haskell
import Test.QuickCheck

instance Arbitrary Int where
  arbitrary = choose (0, 10000)
```

With this custom instance, `arbitrary` will generate `Int` values in the range of 0 to 10,000. Keep in mind that generating a larger range of values may result in slower tests due to increased computation time and memory usage."
76384387,How can I resolve the TypeScript error 'Function lacks ending return statement and return type does not include 'undefined'' in my code?,"I have the following simple function (make) that calls the handle function and is supposed to retry a number of times whenever that function throws. If the retries are exhausted, the make function should throw the error.
const handle = async (): Promise<string> => 'hi';

const make = async (): Promise<string> => {
  const MAX_RETRIES = 2;
  for (let idx = 0; idx <= MAX_RETRIES; idx++) {
    try {
      return await handle();
    } catch (err) {
      if (idx < MAX_RETRIES) {
        continue;
      } else {
        throw err;
      }
    }
  }
};

I'm using TypeScript, which is complaining because the return type doesn't include undefined:

Function lacks ending return statement and return type does not include 'undefined'.

For reference, this is the TS Playground for the code above.
I'm looking for guidance on how to handle the return type for the function.
Note that:

I don't want to change my tsconfigs (currently set to strict)
I don't want to modify the return type to Promise<string | undefined>

My understanding is that the make function can only either return a string (inside the try block) or throw an error once the retries have been exhausted. If that's the case then where does the undefined that TS is asking for comes from? Am I missing something?
",76384457,"
My understanding is that the make function can only either return a string (inside the try block) or throw an error once the retries have been exhausted.

I'm fairly sure you're right, but TypeScript can't quite follow logic that complex, so it (incorrectly, I think) sees a path through the function that doesn't do an explicit return and so implicitly returns undefined (wrapped in a promise).
You can solve it in a few ways:

Add a return """"; at the end with a comment noting it'll never happen. (Blech.)
Add a throw new Error(""Logic error, this will never be reached.""); at the end.
Rewrite the function to make the final attempt more obviously a return-or-throw situation by using < instead of <= and then repeating the return await handle(); at the end. (Not great to have to repeat it, but it's very simple.)

I don't think #1 or #2 need examples, but here's what #3 might look like:
const make = async (): Promise<string> => {
    const MAX_RETRIES = 2;
    for (let idx = 0; idx < MAX_RETRIES; idx++) {
        try {
            return await handle();
        } catch (err) {
            continue; // I guess technically we don't need this, since
                      // the loop doesn't do anything else
        }
    }
    return await handle();
};

For me, #2 is the winner (or jcalz's rewrite), but any of them will make TypeScript happy, it's really a style choice.
","The error message you're encountering is due to TypeScript's control flow analysis, which determines the possible return types of a function based on its code paths. In your case, TypeScript is inferring that the function `make` may reach the end without encountering a `return` statement, resulting in a potential return type of `undefined` (since you haven't specified `string | undefined` explicitly).

To resolve this issue without modifying the return type to include `undefined`, you can add a `throw` statement at the end of the function to explicitly indicate that the function will always throw an error if the retries are exhausted. Here's an updated version of your code:

```typescript
const handle = async (): Promise<string> => 'hi';

const make = async (): Promise<string> => {
  const MAX_RETRIES = 2;
  for (let idx = 0; idx <= MAX_RETRIES; idx++) {
    try {
      return await handle();
    } catch (err) {
      if (idx < MAX_RETRIES) {
        continue;
      } else {
        throw err;
      }
    }
  }
  throw new Error('Retries exhausted'); // Add this line
};
```

By adding the `throw new Error('Retries exhausted')` statement at the end, you're ensuring that the function has a guaranteed error throw as its final code path. TypeScript's control flow analysis will recognize this and infer the return type of `make` as `Promise<string>`, without including `undefined`.

Note that this change will not affect the actual behavior of your code, as the `throw err;` statement inside the `else` block will already throw the error when the retries are exhausted. The additional `throw` statement at the end is purely for TypeScript's benefit in terms of type inference."
76384356,How can i get all the metrics where two label have same values using promql?,"I am new to promql. So not sure if promql supports my requirement or not.
max_over_time(cbnode_systemstats_cpu_utilization_rate{instance=""a"",node=""a""}[6h])

This above query gives me result  of max cpu utilization in past 6 hr for instance a single instnace a.
However I want a query which fetches all metrics for all the instances where instance and node has same value. Something similar to below:
max_over_time(cbnode_systemstats_cpu_utilization_rate{instance = node}[6h])

",76384460,"There is no easy elegant way to do that.
But you can utilize label_replace, logic of label matching for binary operations and a pinch of ingenuity.
label_replace(cbnode_systemstats_cpu_utilization_rate{}, ""pseudoid"", ""$1"", ""instance"", ""(.*)"")
 == label_replace(cbnode_systemstats_cpu_utilization_rate{}, ""pseudoid"", ""$1"", ""node"", ""(.*)"")

Here we add to LHS metric new label called pseudoid with value of instance, and same for RHS, but with value of node.
Result will be returned only if all labels are the same, and in turn it will mean that instance == pseudoid == node.
Demo of similar query can be seen here.
Notice that since it is not the instant vector selector, you'll need to use subquery syntax to pass it into max_over_time.
You resulting query should look like this:
max_over_time(
 (
   label_replace(cbnode_systemstats_cpu_utilization_rate{}, ""pseudoid"", ""$1"", ""instance"", ""(.*)"")
     == label_replace(cbnode_systemstats_cpu_utilization_rate{}, ""pseudoid"", ""$1"", ""node"", ""(.*)"")
 )[6h:]
)

","To retrieve metrics where the `instance` and `node` labels have the same values using PromQL, you can use the `on()` operator to perform a join operation on the two labels. Here's an example query:

```
max_over_time(cbnode_systemstats_cpu_utilization_rate{instance = node}[6h:] offset 5m) * on(instance, node) group_left()
```

Let's break down the query:

- `cbnode_systemstats_cpu_utilization_rate{instance = node}`: This selects the metrics where the `instance` and `node` labels have the same values.
- `max_over_time(...)`: This calculates the maximum value over time for each selected metric.
- `[6h:] offset 5m`: This specifies the time range, in this case, the past 6 hours, with an offset of 5 minutes to align with the evaluation interval.
- `* on(instance, node) group_left()`: This performs a join operation on the `instance` and `node` labels, ensuring that both labels have the same value. The `group_left()` ensures that the resulting query includes all time series with the same labels.

Keep in mind that the exact label names and metric names may vary based on your specific Prometheus setup. Adjust the query accordingly to match your metric and label names.

PromQL is a powerful query language, and it's recommended to refer to the official Prometheus documentation for more details and advanced usage: https://prometheus.io/docs/prometheus/latest/querying/basics/"
76381015,How to Save data to the Database using Laravel 8?,"This is ItemManufactureController file
class ItemManufactureController extends Controller
{
    public function index(){

        return view('item_manufacture');
    }
    // Save category data into the database
    public function store(Request $request){

        $newManufacture = new ItemManufacture;
        $newManufacture->name = $request->input('txtManufactureName');
        $newManufacture->status = $request->input('status', 'available');

        dd($newManufacture);

        $newManufacture->save();
        return redirect('/item_manufacture');

    }
}

This is item_manufacture.blade.php file
{{--this page add to layout   --}}
@extends('layout.layout_01')

{{--identity the content form the layout--}}
@section('content')

    <div class=""container"">
        <div class=""row"">
            <div class=""col-md-4""></div>

            <div class=""col-md-4"">
                <div class=""card"">
                    <h5 class=""card-header"">Add Item Manufacture Details</h5>
                    <div class=""card-body"">

                        <div class=""input-field p-3"">
                            <label for=""txtManufactureName"">Manufacture Name :</label>
                            <div class=""col-sm-8 p-2"">
                                <input type=""text"" placeholder=""Item Name"" name=""txtManufactureName"" id=""txtManufactureName"">
                            </div>
                        </div>

                        <div class=""input-field p-3"">

                            <div class=""col-sm-8 p-2"">

                            </div>
                        </div>

                        <a href=""/save_manufacture"" class=""btn btn-primary mb-2"" id=""btnAdd"">ADD</a>
                    </div>
                </div>
            </div>

            <div class=""col-md-4""></div>
        </div>

    </div>

@endsection

This is route file

//save manufacture
Route::get('/save_manufacture', [ItemManufactureController::class, 'store'])->name('saveManufacture');
Route::get('/item_manufacture', function (){
    return view('pages.item_manufacture');
});

This is Model file
class ItemManufacture extends Model
{
    use HasFactory;
//    public $timestamps=false;
    protected $connection = 'mysql';
    protected $primaryKey = 'id';
    protected $table = 'item_manufacture';
    protected $fillable = [
        'name',
        'status'];
}

when add data into the form and click ""ADD"" button array comes null value
I used Laravel 8 framework, when I add data into the input field of the item_manufacture form, data will not pass the array. If there any error of my code, Please correct it.
How to save data and get values from the input fields using Laravel framework?
",76381115,"Please make your route as post since you're storing the data and change your route in chaining name() method as saveManufacture.store
Route::post('/save_manufacture', [ItemManufactureController::class, 'store'])->name('saveManufacture.store');

And in your blade file wrap your inputs inside form tag and set named route in your action.
And then replace a tag (anchor tag) with input type submit since we have added action in our form tag. so your blade file will look this.
{{--this page add to layout   --}}
@extends('layout.layout_01')

{{--identity the content form the layout--}}
@section('content')
    <div class=""container"">
        <div class=""row"">
            <div class=""col-md-4""></div>
            <div class=""col-md-4"">
                <div class=""card"">
                    <h5 class=""card-header"">Add Item Manufacture Details</h5>
                    <div class=""card-body"">
                        <form action=""{{ route('saveManufacture.store') }}"" method=""post"">
                        <div class=""input-field p-3"">
                            <label for=""txtManufactureName"">Manufacture Name :</label>
                            <div class=""col-sm-8 p-2"">
                                <input type=""text"" placeholder=""Item Name"" name=""txtManufactureName"" id=""txtManufactureName"">
                            </div>
                        </div>
                        <div class=""input-field p-3"">
                            <div class=""col-sm-8 p-2"">
                            </div>
                        </div>
                        <input type=""submit"" class=""btn btn-primary mb-2"" id=""btnAdd"" value=""ADD"">
                        </form>
                    </div>
                </div>
            </div>
            <div class=""col-md-4""></div>
        </div>
    </div>
@endsection

Now you'll able to get the request param in your store() function, please try to debug dd($request->post());
","Based on the code you provided, it seems like you're missing the `POST` method in your form submission. Here's how you can modify your code to save data to the database using Laravel 8:

1. Update the `item_manufacture.blade.php` file:

```html
@extends('layout.layout_01')

@section('content')
    <div class=""container"">
        <div class=""row"">
            <div class=""col-md-4""></div>

            <div class=""col-md-4"">
                <div class=""card"">
                    <h5 class=""card-header"">Add Item Manufacture Details</h5>
                    <div class=""card-body"">
                        <form method=""POST"" action=""{{ route('saveManufacture') }}"">
                            @csrf

                            <div class=""input-field p-3"">
                                <label for=""txtManufactureName"">Manufacture Name:</label>
                                <div class=""col-sm-8 p-2"">
                                    <input type=""text"" placeholder=""Item Name"" name=""txtManufactureName"" id=""txtManufactureName"">
                                </div>
                            </div>

                            <button type=""submit"" class=""btn btn-primary mb-2"" id=""btnAdd"">ADD</button>
                        </form>
                    </div>
                </div>
            </div>

            <div class=""col-md-4""></div>
        </div>
    </div>
@endsection
```

In the updated code, we added a `<form>` element with the `POST` method and included the `@csrf` directive to add a CSRF token to the form.

2. Update the `routes/web.php` file:

```php
// Save manufacture
Route::post('/save_manufacture', [ItemManufactureController::class, 'store'])->name('saveManufacture');
Route::get('/item_manufacture', function () {
    return view('pages.item_manufacture');
});
```

We changed the route method to `POST` for the `/save_manufacture` route to match the form submission.

3. Update the `ItemManufactureController`:

```php
use App\Models\ItemManufacture;
use Illuminate\Http\Request;

class ItemManufactureController extends Controller
{
    public function index()
    {
        return view('item_manufacture');
    }

    // Save category data into the database
    public function store(Request $request)
    {
        $newManufacture = new ItemManufacture;
        $newManufacture->name = $request->input('txtManufactureName');
        $newManufacture->status = $request->input('status', 'available');
        $newManufacture->save();

        return redirect('/item_manufacture');
    }
}
```

Make sure to import the `ItemManufacture` model at the top of the file.

With these changes, when you submit the form, it will use the `POST` method to send the data to the `store` method in the `ItemManufactureController`. The data will be saved to the database, and then the user will be redirected to the `/item_manufacture` page."
76378362,Prevent webpack from auto-incrementing project version,"I am working with a chrome extension which uses webpack to build.
To build I use this : cross-env NODE_ENV=production yarn webpack -c webpack.config.js --mode production
webpack.config.js
const HTMLPlugin = require('html-webpack-plugin');
const CopyPlugin = require('copy-webpack-plugin');
const path = require('path');
const UglifyJSPlugin = require('uglifyjs-webpack-plugin');
const BrowserExtensionPlugin = require(""extension-build-webpack-plugin"");

module.exports = {
  entry: {
    options: './src/options.tsx',
    popup: './src/popup.tsx',
    content: './src/content.tsx',
    background: './src/background.tsx',
  },
  output: {
    filename: '[name].js',
    path: path.resolve(__dirname, 'build'),
  },
  resolve: {
    extensions: ['.js', '.jsx', '.ts', '.tsx', '.css'],
    modules: [path.resolve(__dirname, 'src'), 'node_modules'],
    alias: {
      react: 'preact/compat',
      'react-dom': 'preact/compat',
    },
  },
  module: {
    rules: [
      {
        test: /\.(tsx|jsx|ts|js)x?$/,
        exclude: /node_modules/,
        use: [
          {
            loader: 'babel-loader',
            options: {
              presets: [
                ""@babel/preset-env"",
                ""@babel/preset-react"",
                ""@babel/preset-typescript"",
              ],
            },
          },
        ],
      },
      {
        test: /\.svg$/,
        use: ['@svgr/webpack'],
      },
    ],
  },
  plugins: [
    new HTMLPlugin({
      chunks: ['options'],
      filename: 'options.html',
      title: 'Options page title',
    }),
    new HTMLPlugin({
      chunks: ['popup'],
      filename: 'popup.html',
    }),
    new CopyPlugin([
      { from: './src/_locales/', to: './_locales' },
      { from: './src/assets', to: './assets' },
      { from: './src/manifest.json', to: './manifest.json' },
    ]),
    new BrowserExtensionPlugin({devMode: false, name: ""build/chromium.zip"", directory: ""src"", updateType: ""minor""}),
  ],
  optimization: {
    minimizer: [
      new UglifyJSPlugin({
        uglifyOptions: {
          compress: {
            drop_console: true,
            drop_debugger: true,
          }
        }
      })
    ]
  },
  mode: 'production',
  stats: 'minimal',
  performance: {
    hints: false,
    maxEntrypointSize: 512000,
    maxAssetSize: 512000
  }
};


manifest.json:
{
    ""manifest_version"": 3,
    ""name"": ""__MSG_appName__"",
    ""description"": ""__MSG_appDesc__"",
    ""default_locale"": ""en"",
    ""version"": ""0.1.0"",
    ....
    ....
}

If I run cross-env NODE_ENV=production yarn webpack -c webpack.config.js --mode production again it increments the version from  0.1.0 to   0.2.0 automatically not just in build folder but in src folder as well. How can I prevent this auto increment functionality.
I suspect it's due to one of the webpack plugins I am using.
",76378998,"This is caused by extension-build-webpack-plugin which you really shouldn't have struggled to find, as there's a total of 4 plugins there to look at.
No, it does not offer any method of avoiding version bumps. You can only configure if you want it to bump the major or minor version number, defaulting to minor.
It's a really weird library to be using, it gets few downloads and is unmaintained. There's probably better alternatives out there.
","The auto-increment functionality you're experiencing is not related to webpack or any of the plugins you're using. The version increment is happening because of the configuration in your `manifest.json` file.

In your `manifest.json`, you have the `""version""` field set to `""0.1.0""`. When you build your project using webpack, it copies the `manifest.json` file from the source (`src`) directory to the output (`build`) directory. Therefore, the version in the `manifest.json` file in the output directory will also be incremented.

To prevent the auto-increment functionality, you can manually set the version in your `manifest.json` file to a fixed value instead of relying on the auto-incrementing behavior. Here's an example:

```json
{
  ""manifest_version"": 3,
  ""name"": ""__MSG_appName__"",
  ""description"": ""__MSG_appDesc__"",
  ""default_locale"": ""en"",
  ""version"": ""1.0.0"",
  ...
}
```

By setting the version to `""1.0.0""` (or any specific version you desire), it will remain the same in both the source and output directories during the build process."
76384281,Javascript: run specific parts of strings through a function,"I want to parse some data that's in a string format. Anything enclosed in parenthesis in the string to parse should be replaced with itself run through a function. This is what I want:
function foo(str) {
    return parseInt(str) + 1; // Example function, not actually what the function will be
}

function parse(str) {
    // everything in str that is enclosed in parenthesis should be replaced with itself ran through foo();

    // Example
    // Input: ""My name is foo and I am (0) year old.""
    // Output: ""My name is foo and I am 1 year old.""
    // ""(0)"" has been replaced with the result of foo(""0"")
}

I have thought up a couple bad workarounds, but I want something more robust. For example:
function parse(str) {
    // Input: ""My name is foo and I am (0) year old.""
    str = str.replaceAll(""("", ""${foo('"");
    str = str.replaceAll("")"", ""')}"");
    str = ""`"" + str + ""`""
    // Here str will be ""`My name is foo and I am ${foo(0)} year old.`""
    // And I can use eval() or something to treat it like I've typed that
}

This, however, is kind of a bad way of doing it.
EDIT: I tested it, it works, but it is quite vulnerable.
I can't think of anything else and I'm not very good with RegEx. (although I'd accept a solution using it)
",76384463,"Here's what I would do. I would match the string with a RegEx that would match anything inside parenthesis in the string. With that, I would then use str.replaceAll() to replace the matched string with the result of the foo() function.
const regex = /\((\d*)\)/gm;

function foo(str) {
    return parseInt(str) + 1;
}

function parse(str) {
  
  // Loop all match the regex find in the string
  let m;
  while ((m = regex.exec(str)) !== null) {
    
    // This is necessary to avoid infinite loops with zero-width matches
    if (m.index === regex.lastIndex) {
        regex.lastIndex++;
    }
    
    // Replace all instance of the match with the operation of the match
    str = str.replaceAll(m[0], foo(m[1]))
  }
  return str;
}

let p = parse('My name is foo and I am (0) year old and I want (54) apples');

// The result will be: My name is foo and I am 1 year old and I want 55 apples

With that, you won't need to use eval() as it potentially pose a risk for your application.
I hope that would work for you. If I missed anything, tell me, I will edit my answer.
","You can achieve the desired result by using regular expressions and the `replace()` method in JavaScript. Here's an example of how you can modify the `parse()` function to accomplish this task:

```javascript
function foo(str) {
    return parseInt(str) + 1; // Example function, not actually what the function will be
}

function parse(str) {
    return str.replace(/\((.*?)\)/g, (match, p1) => {
        return foo(p1).toString();
    });
}

// Example usage:
const input = ""My name is foo and I am (0) year old."";
const output = parse(input);
console.log(output); // Output: ""My name is foo and I am 1 year old.""
```

In the `parse()` function, we use the `replace()` method with a regular expression pattern `/\((.*?)\)/g` to match any substring enclosed in parentheses. The `(.*?)` part captures the content within the parentheses as a group, and the `g` flag ensures that all occurrences are replaced, not just the first one.

The replacement function `(match, p1) => {...}` takes each matched substring (including the parentheses) and the captured group as arguments. It then calls the `foo()` function with the captured group (`p1`), applies the necessary processing, and returns the result as a string to replace the matched substring.

Note that this approach is more robust than using `eval()` and avoids the potential security risks associated with it."
76381105,Find unique date from existing dataframe and make a new CSV with corresponding column values,"I have a time series every which looks like this :




Time
Volume every minute




2023-05-25T00:00:00Z
284


2023-05-25T00:01:00Z
421


.
.


.
.


2023-05-27T23:58:00Z
894


2023-05-27T23:59:00Z
357




I have to make new CSV by iterating Time column finding unique date and making new columns with corresponding values of volume every minute. For example desired output:




Date
min1
min2
...
min1440




2023-05-25
284
421
...
578


2023-05-26
512
645
...
114


2023-05-27
894
357
...
765




i am able to fetch unique dates but after that i am clueless. please find my sample codes:
import pandas as pd

train_data = pd.read_csv('date25to30.csv')

print(pd.to_datetime(train_data['time']).dt.date.unique())

",76381147,"First add parameter parse_dates to read_csv for convert Time column to datetimes:
train_data = pd.read_csv('date25to30.csv', parse_dates=['Time'])

Then create minutes by converting HH:MM:SS to timedeltas by to_timedelta and Series.dt.total_seconds, divide 60 and add 1 because python count from 0:
minutes = (pd.to_timedelta(train_data['Time'].dt.strftime('%H:%M:%S'))
             .dt.total_seconds()
             .div(60)
             .astype(int)
             .add(1))

Last pass to DataFrame.pivot_table with DataFrame.add_prefix:
df = (train_data.pivot_table(index=train_data['Time'].dt.date,
                             columns=minutes,
                             values='Volume',
                             aggfunc='sum').add_prefix('min'))
print (df)
Time         min1   min2  min1439  min1440
Time                                      
2023-05-25  284.0  421.0      NaN      NaN
2023-05-27    NaN    NaN    894.0    357.0

","To achieve the desired output, you can use the following code:

```python
import pandas as pd

# Read the CSV file
train_data = pd.read_csv('date25to30.csv')

# Convert the 'Time' column to datetime format
train_data['Time'] = pd.to_datetime(train_data['Time'])

# Extract the date from the 'Time' column
train_data['Date'] = train_data['Time'].dt.date

# Pivot the dataframe to create new columns for each minute
pivot_data = train_data.pivot(index='Date', columns='Time', values='Volume every minute')

# Rename the columns to 'min1', 'min2', ..., 'min1440'
new_columns = ['min' + str(i) for i in range(1, 1441)]
pivot_data.columns = new_columns

# Reset the index
pivot_data = pivot_data.reset_index()

# Save the pivot dataframe to a new CSV file
pivot_data.to_csv('new_data.csv', index=False)
```

This code performs the following steps:
1. Reads the CSV file using `pd.read_csv()`.
2. Converts the 'Time' column to datetime format using `pd.to_datetime()`.
3. Extracts the date from the 'Time' column and stores it in a new column 'Date'.
4. Pivots the dataframe using `pivot()` to create new columns for each minute, with the 'Volume every minute' values as the corresponding values.
5. Renames the columns to 'min1', 'min2', ..., 'min1440'.
6. Resets the index.
7. Saves the pivot dataframe to a new CSV file using `to_csv()`.

Make sure to replace `'date25to30.csv'` with the actual path to your input file and `'new_data.csv'` with the desired path for the output file."
76378633,Cannot properly hide the appbar title on scroll in flutter,"I want to hide the AppBar on scroll. The search icon is hidden properly and also the opacity decreases on scroll. But for the title, it is not working.
import 'package:flutter/material.dart';
import 'package:vet_mobile/screens/chat.dart';
import 'package:vet_mobile/screens/logot.dart';

class HomeScreen extends StatelessWidget {
  const HomeScreen({Key? key}) : super(key: key);

  @override
  Widget build(BuildContext context) {
    return DefaultTabController(
      length: 3,
      child: Scaffold(
        body: NestedScrollView(
          headerSliverBuilder: (BuildContext context, bool innerBoxIsScrolled) {
            return <Widget>[
              SliverAppBar(
                title: Row(
                  mainAxisAlignment: MainAxisAlignment.spaceBetween,
                  children: [
                    Text(
                      'WhatsApp',
                      style: TextStyle(
                        color: Theme.of(context).textTheme.bodyLarge!.color,
                      ),
                    ),
                    IconButton(
                      onPressed: () {},
                      icon: Icon(
                        Icons.search,
                        color: Theme.of(context).textTheme.bodyLarge!.color,
                      ),
                    ),
                  ],
                ),
                pinned: true,
                floating: true,
                elevation: 5,
                bottom: TabBar(
                  indicatorSize: TabBarIndicatorSize.tab,
                  indicatorWeight: 4,
                  indicatorColor: Theme.of(context).textTheme.bodyLarge!.color,
                  labelStyle:
                      TextStyle(fontSize: 13, fontWeight: FontWeight.w600),
                  labelColor: Theme.of(context).textTheme.bodyLarge!.color,
                  unselectedLabelColor:
                      Theme.of(context).textTheme.bodySmall!.color,
                  dividerColor: Colors.transparent,
                  tabs: const [
                    Tab(text: 'CHATS'),
                    Tab(text: 'STATUS'),
                    Tab(text: 'CALLS'),
                  ],
                ),
              ),
            ];
          },
          body: const TabBarView(
            children: [
              Center(child: LogoutScreen()),
              Center(child: ChatScreen()),
              Center(child: Text('Patient')),
            ],
          ),
        ),
      ),
    );
  }
}



As we can see the opacity of the search button decreases slowly as I scroll down but not for the title.
I tried using the preferred height, animation controller, but it messed up more.
",76379006,"Seems that this effect does not work when you set a custom style. Remove the fixed style setting from here:
Text(
  'PawCare',
  // remove this
  /*style: TextStyle(
    color: Theme.of(context).textTheme.bodyLarge!.color,
  ),*/
),

To set the style of the title text, use the titleTextStyle configuration of SliverAppBar:
SliverAppBar(
  titleTextStyle: TextStyle(
   color: Theme.of(context).textTheme.bodyLarge!.color),
...

","To properly hide the AppBar title on scroll in Flutter, you can use a combination of a `ScrollController` and an `Opacity` widget. Here's an updated version of your code with the necessary modifications:

```dart
import 'package:flutter/material.dart';
import 'package:vet_mobile/screens/chat.dart';
import 'package:vet_mobile/screens/logot.dart';

class HomeScreen extends StatefulWidget {
  const HomeScreen({Key? key}) : super(key: key);

  @override
  _HomeScreenState createState() => _HomeScreenState();
}

class _HomeScreenState extends State<HomeScreen> {
  ScrollController _scrollController = ScrollController();
  bool _showTitle = true;
  double _opacity = 1.0;

  @override
  void initState() {
    super.initState();
    _scrollController.addListener(_onScroll);
  }

  @override
  void dispose() {
    _scrollController.removeListener(_onScroll);
    _scrollController.dispose();
    super.dispose();
  }

  void _onScroll() {
    if (_scrollController.offset > 50 && _showTitle) {
      setState(() {
        _showTitle = false;
        _opacity = 0.0;
      });
    } else if (_scrollController.offset <= 50 && !_showTitle) {
      setState(() {
        _showTitle = true;
        _opacity = 1.0;
      });
    }
  }

  @override
  Widget build(BuildContext context) {
    return DefaultTabController(
      length: 3,
      child: Scaffold(
        body: NestedScrollView(
          controller: _scrollController,
          headerSliverBuilder: (BuildContext context, bool innerBoxIsScrolled) {
            return <Widget>[
              SliverAppBar(
                title: Opacity(
                  opacity: _opacity,
                  child: AnimatedOpacity(
                    duration: const Duration(milliseconds: 300),
                    opacity: _showTitle ? 1.0 : 0.0,
                    child: Row(
                      mainAxisAlignment: MainAxisAlignment.spaceBetween,
                      children: [
                        Text(
                          'WhatsApp',
                          style: TextStyle(
                            color: Theme.of(context).textTheme.bodyLarge!.color,
                          ),
                        ),
                        IconButton(
                          onPressed: () {},
                          icon: Icon(
                            Icons.search,
                            color: Theme.of(context).textTheme.bodyLarge!.color,
                          ),
                        ),
                      ],
                    ),
                  ),
                ),
                pinned: true,
                floating: true,
                elevation: 5,
                bottom: TabBar(
                  indicatorSize: TabBarIndicatorSize.tab,
                  indicatorWeight: 4,
                  indicatorColor: Theme.of(context).textTheme.bodyLarge!.color,
                  labelStyle: TextStyle(
                    fontSize: 13,
                    fontWeight: FontWeight.w600,
                  ),
                  labelColor: Theme.of(context).textTheme.bodyLarge!.color,
                  unselectedLabelColor: Theme.of(context).textTheme.bodySmall!.color,
                  dividerColor: Colors.transparent,
                  tabs: const [
                    Tab(text: 'CHATS'),
                    Tab(text: 'STATUS'),
                    Tab(text: 'CALLS'),
                  ],
                ),
              ),
            ];
          },
          body: const TabBarView(
            children: [
              Center(child: LogoutScreen()),
              Center(child: ChatScreen()),
              Center(child: Text('Patient')),
            ],
          ),
        ),
      ),
    );
  }
}
```

In this code, I've added a `ScrollController` to track the scroll position. The `_showTitle` boolean and `_opacity` value are used to control the visibility and opacity of the title, respectively. The `_onScroll` method updates these variables based on the scroll position.

Inside the `SliverAppBar`, I wrapped the `title` widget with an `Opacity` widget,"
76378657,Haskell: cache result of a function in pattern matching,"I have the following algebraic data type:
data Tree a = Empty | Node a (Tree a) (Tree a)
  deriving (Show, Eq)

Also, I have
data Step = StepL | StepR
  deriving (Show, Eq)

Now, I need a function search that takes

a root of the tree
a target value t
... and it must return a path of type [Step] leading to a node with value t. Also, if t is not present in the tree, search must return Nothing. Finally, the input is guaranteed to have the target value at most once.

My best effort, as of now, is:
searchHelper :: Eq a => a -> Tree a -> [Step] -> Maybe [Step]
searchHelper _ Empty _ = Nothing
searchHelper targetValue (Node nodeValue leftChild rightChild) stepsSoFar = 
  if targetValue == nodeValue then Just stepsSoFar 
  else if searchHelper targetValue leftChild (stepsSoFar ++ [StepL]) /= Nothing then searchHelper targetValue leftChild (stepsSoFar ++ [StepL])
  else if searchHelper targetValue rightChild (stepsSoFar ++ [StepR]) /= Nothing then searchHelper targetValue rightChild (stepsSoFar ++ [StepR])
  else Nothing

search :: Eq a => a -> Tree a -> Maybe [Step]
search targetValue root = searchHelper targetValue root []

As you can see, I call the searchHelper too often (else if searchHelper targetValue leftChild (stepsSoFar ++ [StepL]) /= Nothing then searchHelper targetValue leftChild (stepsSoFar ++ [StepL])). I need a machinery that would allow me to cache the results of searchHelper calls and use them
in if ... then ... else.
Q: How can I do it?
",76379177,"The use of the word cache confused me, but if I understand the question correctly, the real problem is the repeated use of the same expression. That could certainly become a readability and maintainability issue in a larger code base, so is worthwhile addressing.
From the context this looks like a 'toy problem'. There's nothing wrong with that - I play with plenty of those myself to learn new stuff. The reason I mention it, though, is that from this and other clues I gather that you're still a Haskell beginner. Again: nothing wrong with that, but it just means that I'm going to skip some of the slightly more advanced Haskell stuff.
Checking for Nothing or Just like in the OP is rarely idiomatic Haskell. Instead you'd use pattern-matching or (more commonly) some of the higher-level APIs for working with Maybe (such as Functor, Applicative, Monad, etc.).
That said, I gather that this isn't quite what you need right now. In order to cut down on the duplication of expressions, you can use let..in syntax in Haskell:
searchHelper :: Eq a => a -> Tree a -> [Step] -> Maybe [Step]
searchHelper _ Empty _ = Nothing
searchHelper targetValue (Node nodeValue leftChild rightChild) stepsSoFar = 
  if targetValue == nodeValue then Just stepsSoFar
  else
    let l = searchHelper targetValue leftChild (stepsSoFar ++ [StepL])
    in if l /= Nothing then l
  else
    let r = searchHelper targetValue rightChild (stepsSoFar ++ [StepR])
    in if r /= Nothing then r
  else Nothing

This enables you to 'declare' 'variables' l and r and reuse them.
As my lengthy preamble suggests, this still isn't idiomatic Haskell, but I hope it adresses the immediate question.
","To cache the results of the `searchHelper` function and avoid redundant computations, you can utilize memoization. Memoization is a technique where the results of function calls are stored in a cache, allowing future calls with the same arguments to retrieve the result from the cache instead of recomputing it.

In Haskell, you can implement memoization using a data structure called a memo table. Here's an updated version of your code that incorporates memoization:

```haskell
import qualified Data.Map as Map

data Tree a = Empty | Node a (Tree a) (Tree a)
  deriving (Show, Eq)

data Step = StepL | StepR
  deriving (Show, Eq)

type MemoTable a = Map.Map (Tree a, a) (Maybe [Step])

searchHelper :: Eq a => a -> Tree a -> [Step] -> MemoTable a -> Maybe [Step]
searchHelper _ Empty _ _ = Nothing
searchHelper targetValue (Node nodeValue leftChild rightChild) stepsSoFar memoTable =
  case Map.lookup (Node nodeValue leftChild rightChild, targetValue) memoTable of
    Just result -> result
    Nothing ->
      let result = if targetValue == nodeValue
                   then Just stepsSoFar
                   else searchHelper targetValue leftChild (stepsSoFar ++ [StepL]) memoTable
                        <|> searchHelper targetValue rightChild (stepsSoFar ++ [StepR]) memoTable
      in Map.insert (Node nodeValue leftChild rightChild, targetValue) result memoTable

search :: Eq a => a -> Tree a -> Maybe [Step]
search targetValue root = searchHelper targetValue root [] Map.empty
```

In this updated code, we introduce the `MemoTable` type alias, which is a mapping from pairs of `(Tree a, a)` (representing the current subtree and the target value) to `Maybe [Step]` (representing the result of the search). We use `Data.Map` from the `containers` package to implement the memo table.

The `searchHelper` function first checks if the result for the given subtree and target value is already present in the memo table using `Map.lookup`. If it is, the cached result is returned. Otherwise, the function proceeds to compute the result and stores it in the memo table using `Map.insert` before returning it.

By using memoization, the intermediate results of `searchHelper` calls are cached and reused when the same subtree and target value combination is encountered again, reducing redundant computations and improving performance.

Note: To use the `(<|>)` operator in the code, you need to import `Control.Applicative` module."
76383893,Implement MultiKeyDict class in Python with alias() method for creating aliases. Existing code fails when original key is deleted. Need fix,"Python OOP problem
MultiKeyDict class, which is almost identical to the dict class. Creating an instance of MultiKeyDict class should be similar to creating an instance of dict class:
multikeydict1 = MultiKeyDict(x=1, y=2, z=3)
multikeydict2 = MultiKeyDict([('x', 1), ('y', 2), ('z', 3)])

print(multikeydict1['x']) # 1
print(multikeydict2['z']) # 3

A feature of the MultiKeyDict class should be the alias() method, which should allow aliases to be given to existing keys. The reference to the created alias should not differ from the reference to the original key, that is, the value has two keys (or more if there are several aliases) when the alias is created:
multikeydict = MultiKeyDict(x=100, y=[10, 20])

multikeydict.alias('x', 'z') # add key 'x' alias 'z'
multikeydict.alias('x', 't') # add alias 't' to key 'x'
print(multikeydict['z']) # 100
multikeydict['t'] += 1
print(multikeydict['x']) # 101

multikeydict.alias('y', 'z') # now 'z' becomes an alias of the key 'y'
multikeydict['z'] += [30]
print(multikeydict['y']) # [10, 20, 30]

The value must remain available by alias even if the original key was removed:
multikeydict = MultiKeyDict(x=100)

multikeydict.alias('x', 'z')
del multikeydict['x']
print(multikeydict['z']) # 100

Keys must take precedence over aliases. If some key and alias are the same, then all operations when accessing them must be performed with the key:
multikeydict = MultiKeyDict(x=100, y=[10, 20])

multikeydict.alias('x', 'y')
print(multikeydict['y']) # [10, 20]

I can't implement such a feature, please give me ideas how it can be done!!!
multikeydict = MultiKeyDict(x=100)

multikeydict.alias('x', 'z')
del multikeydict['x']
print(multikeydict['z']) # 100

my code does not work with this test
multikeydict = MultiKeyDict(x=100)

multikeydict.alias('x', 'z')
del multikeydict['x']
print(multikeydict['z']) #100 

class MultiKeyDict(dict):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.aliases = {}

    def alias(self, key, alias):
        self.aliases[alias] = key

    def __getitem__(self, key):
        if key in self.aliases:
            key = self.aliases[key]
        return super().__getitem__(key)

    def __setitem__(self, key, value):
        if key in self.aliases:
            key = self.aliases[key]
        super().__setitem__(key, value)
    
    def __delitem__(self, key):
        if key in self.aliases:
            del self.aliases[key]
        super().__delitem__(key)


multikeydict = MultiKeyDict(x=100, y=[10, 20])

multikeydict.alias('x', 'z')
multikeydict.alias('x', 't')
print(multikeydict['z'])
multikeydict['t'] += 1
print(multikeydict['x'])

multikeydict.alias('y', 'z')
multikeydict['z'] += [30]
print(multikeydict['y'])

",76384507,"Some remarks:

As the specification says that keys should have precedence over aliases (when both exist), you should first test key membership on self before looking in aliases. Your methods first check for membership in aliases...

As a value must continue to exist when a key is deleted for which there are still alias(es), I would suggest storing the values wrapped in a list (that just has that value as only member). This way you can reference that list in an alias entry. When the key is deleted, the alias will still have the reference to the list and can still act on it.


Here is how that could look:
class MultiKeyDict(dict):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.aliases = {}
        # wrap each value in a list of size 1:
        for key, value in self.items():
            super().__setitem__(key, [value])
            
    def alias(self, key, alias):
        self.aliases[alias] = super().__getitem__(key)

    def __getitem__(self, key):
        if key in self:
            return super().__getitem__(key)[0]
        return self.aliases[key][0]

    def __setitem__(self, key, value):
        if key in self:
            super().__getitem__(key)[0] = value
        elif key in self.aliases:
            self.aliases[key][0] = value
        else:
            super().__setitem__(key, [value])

    def __delitem__(self, key):
        if key in self:
            return super().__delitem__(key)
        del self.aliases[key]

","The issue with the current implementation is that when the original key is deleted, the alias still points to the original key. To fix this, you need to update the `alias` method and override the `__delitem__` method in the `MultiKeyDict` class. Here's an updated version of the code:

```python
class MultiKeyDict(dict):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.aliases = {}

    def alias(self, key, alias):
        if key in self.aliases.values() or alias in self.aliases:
            raise ValueError(""Alias or key already exists"")
        self.aliases[alias] = key

    def __getitem__(self, key):
        if key in self.aliases:
            key = self.aliases[key]
        return super().__getitem__(key)

    def __setitem__(self, key, value):
        if key in self.aliases:
            key = self.aliases[key]
        super().__setitem__(key, value)
    
    def __delitem__(self, key):
        if key in self.aliases.values():
            alias = [k for k, v in self.aliases.items() if v == key][0]
            del self.aliases[alias]
        elif key in self.aliases:
            del self.aliases[key]
        super().__delitem__(key)
```

With this updated implementation, the `alias` method checks if the key or alias already exists and raises a `ValueError` if that's the case. The `__delitem__` method now handles the case when the key is deleted. It checks if the key is an alias and removes the corresponding alias from the `aliases` dictionary.

Now, when you run the problematic test case, it should work as expected:

```python
multikeydict = MultiKeyDict(x=100)
multikeydict.alias('x', 'z')
del multikeydict['x']
print(multikeydict['z'])  # 100
```

This will correctly output `100` even after deleting the original key `'x'`."
76381091,Narrow down literal unions based on previously used values,"The scenario is the following:
type Option = 'a' | 'b' | 'c' | 'd'

type Question = {
  message: string;
  options: Option[];
  default: Option // here's the issue
}

I want the default prop to be the one of the options used inside question.options. For example:
const q1: Question = {
  message: 'first question',
  options: ['a', 'b'],
  default: 'a'
}

const q2: Question = {
  message: 'second question',
  options: ['c', 'd'],
  default: 'a' // I want this to give an error because 'a' is not in 'c' | 'd'
}

How can I achieve this?
",76381163,"It can be done just by using Question; however, it will be a complex type that will cause a horrible time for the compiler since it grows at the speed of power of two, and if you have more options (more than 10), the compiler will reach its limits and won't compile.
Instead, I would suggest adjusting Question to accept the Option[] as a generic parameter and assign the type of the elements of that generic parameter to default:
type Question<T extends Option[]> = {
  message: string;
  options: T;
  default: T[number];
};

Lastly, we will need a generic function that would create a question for us:
const createQuestion = <T extends Option[]>(question: Question<T>) => question;

Usage:
const q1 = createQuestion({
  message: ""first question"",
  options: [""a"", ""b""],
  default: ""a"",
});

const q2 = createQuestion({
  message: ""second question"",
  options: [""c"", ""d""],
  default: ""a"", // Expected error
});

playground
","To achieve this behavior, you can use a union type of the available options in the `Question` type and then create a helper function to infer the correct default value based on the provided options. Here's an updated version of your code:

```typescript
type Option = 'a' | 'b' | 'c' | 'd';

type Question<T extends Option[]> = {
  message: string;
  options: T;
  default: T[number]; // Use indexed access to infer the correct type
};

// Helper function to infer the correct default value based on the provided options
function createQuestion<T extends Option[]>(question: Question<T>): Question<T> {
  return question;
}

// Examples
const q1 = createQuestion({
  message: 'first question',
  options: ['a', 'b'],
  default: 'a', // Works correctly
});

const q2 = createQuestion({
  message: 'second question',
  options: ['c', 'd'],
  default: 'a', // Gives an error because 'a' is not in 'c' | 'd'
});
```

In this example, the `Question` type takes a generic parameter `T` which represents the array of options. The `default` property is then defined as `T[number]`, which allows TypeScript to infer the correct type based on the provided options. The `createQuestion` function is used to create instances of the `Question` type while correctly inferring the type for the `default` property.

With this approach, you can ensure that the `default` value is always one of the options specified in the `options` array, and TypeScript will provide an error if an invalid default value is assigned."
76378693,How to create a transparent Material 3 NavigationBar in Flutter?,"I want to make my NavigationBar transparent. I have tried extendBody: true on Scafold with surfaceTintColor=Colors.transparent to the NavigationBar widget, but nothing changed.
",76379413,"According to the document, SurfaceTintColor is the color of the surface tint overlay applied to the app bar's background color to indicate elevation.
If you want to make the AppBar transparent, just use the property backgroundColor instead.
Scaffold(
      extendBody: true,
      backgroundColor: Colors.white,
      appBar: AppBar(
        backgroundColor: Colors.transparent, // To make appBar transparent
        
        /// This is not necessary. You can play around 
        /// to see surfaceTintColor when the AppBar is transaprent
        surfaceTintColor: Colors.redAccent,
        elevation: 3,
        title: Text(widget.title),
      ),
),

It is also applied to NavigationBar
bottomNavigationBar: NavigationBar(
        surfaceTintColor: Colors.amber, // not neccessary
        backgroundColor: Colors.transparent,
        destinations: [
          Icon(Icons.book, color: Colors.blue,),
          Icon(Icons.map, color: Colors.blue,),
        ],
      ),

","To create a transparent Material 3 NavigationBar in Flutter, you can follow these steps:

1. First, ensure that your project has the required dependencies. Add the `flutter_material_theming` package to your `pubspec.yaml` file:

```yaml
dependencies:
  flutter_material_theming: ^1.0.1
```

2. Import the necessary packages in your Dart file:

```dart
import 'package:flutter/material.dart';
import 'package:flutter_material_theming/flutter_material_theming.dart';
```

3. Use the `MaterialNavigationBar` widget from the `flutter_material_theming` package. Wrap your `Scaffold` with a `MaterialThemed` widget and set the `materialThemeData` property to a transparent background color:

```dart
class MyApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: MaterialThemed(
        materialThemeData: MaterialThemeData(
          surfaceColor: Colors.transparent,
        ),
        child: Scaffold(
          extendBody: true,
          appBar: MaterialNavigationBar(), // Transparent NavigationBar
          body: Container(
            // Your app content goes here
          ),
        ),
      ),
    );
  }
}
```

Make sure you wrap your `Scaffold` with the `MaterialThemed` widget to apply the transparent background color to the entire app.

4. Run your app and verify that the NavigationBar is now transparent.

Note: The `flutter_material_theming` package provides Material 3 theming capabilities for Flutter. Make sure you have the latest version of the package and follow any updates or changes in its usage if necessary."
76378332,How to use tableone to change table percentage by row?,"I am use library(tableone) to make my descriptive statistics for multiple variables
This is my code:
library(tableone)

myVars <- c(""class"", ""age"", ""Sex"", ""bmi"", ""bmi_category"",
            ""drink_freq"", ""smoke_yn"", ""edu_dummy"")

catVars <- c(""class"", ""Sex"", ""bmi_category"",
             ""drink_freq"", ""smoke_yn"", ""edu_dummy"")

tab1_inf <- CreateTableOne(vars = myVars, strata = ""NEWDI"",
                             data = TKA_table1, factorVars = catVars)

a1 <- print(tab1_inf, exact = ""NEWDI"", showAllLevels = TRUE)

This it default for percentage, and I want change it format like this(example):

I checked its description and found no options to set.
https://rdrr.io/cran/tableone/man/print.TableOne.html
How can I do it?
",76379520,"With some clever getting-your-hands dirty, you can manipulate the percentages in the TableOne object. This uses an example dataset called pbc from survival package.
library(tableone)
library(survival)
data(pbc)

## Make categorical variables factors
varsToFactor <- c(""status"",""trt"",""ascites"",""hepato"",""spiders"",""edema"",""stage"")
pbc[varsToFactor] <- lapply(pbc[varsToFactor], factor)

## Create a variable list
vars <- c(""time"",""status"",""age"",""sex"",""ascites"",""hepato"",
          ""spiders"",""edema"",""bili"",""chol"",""albumin"",
          ""copper"",""alk.phos"",""ast"",""trig"",""platelet"",
          ""protime"",""stage"")

## Create Table 1 stratified by trt
tableOne <- CreateTableOne(vars = vars, strata = c(""trt""), data = pbc)

tableOne

Before
                      Stratified by trt
                       1                 2                 p      test
  n                        158               154                      
  time (mean (SD))     2015.62 (1094.12) 1996.86 (1155.93)  0.883     
  status (%)                                                0.894     
     0                      83 (52.5)         85 (55.2)               
     1                      10 ( 6.3)          9 ( 5.8)               
     2                      65 (41.1)         60 (39.0)               
  age (mean (SD))        51.42 (11.01)     48.58 (9.96)     0.018     
  sex = f (%)              137 (86.7)        139 (90.3)     0.421     
  ascites = 1 (%)           14 ( 8.9)         10 ( 6.5)     0.567     
  hepato = 1 (%)            73 (46.2)         87 (56.5)     0.088     
  spiders = 1 (%)           45 (28.5)         45 (29.2)     0.985 
...

You should try to adapt the following code for your own data format:
for (i in 1:length(table1)) {
    sum = tableOne$CatTable[[1]][[i]]$freq + tableOne$CatTable[[2]][[i]]$freq
    tableOne$CatTable[[1]][[i]]$percent = tableOne$CatTable[[1]][[i]]$freq / sum
    tableOne$CatTable[[2]][[i]]$percent = tableOne$CatTable[[2]][[i]]$freq / sum
  }
}

tableOne

After
                      Stratified by trt
                       1                 2                 p      test
  n                        158               154                      
  time (mean (SD))     2015.62 (1094.12) 1996.86 (1155.93)  0.883     
  status (%)                                                0.894     
     0                      83 (0.5)          85 (0.5)                
     1                      10 (0.5)           9 (0.5)                
     2                      65 (0.5)          60 (0.5)                
  age (mean (SD))        51.42 (11.01)     48.58 (9.96)     0.018     
  sex = f (%)              137 (0.5)         139 (0.5)      0.421     
  ascites = 1 (%)           14 (0.6)          10 (0.4)      0.567     
  hepato = 1 (%)            73 (0.5)          87 (0.5)      0.088     
  spiders = 1 (%)           45 (0.5)          45 (0.5)      0.985   

","The `tableone` package provides a way to customize the formatting of the table through the `print.TableOne` function. To change the format of the percentages displayed in the table, you can modify the `fmt` argument of the `print.TableOne` function.

Here's an example of how you can change the format of percentages displayed in the table:

```R
# Assuming you have already created the table using CreateTableOne
a1 <- print(tab1_inf, exact = ""NEWDI"", showAllLevels = TRUE)

# Modify the format of percentages
a1$p[[""p""]] <- paste0(a1$p[[""p""]], ""%"")

# Print the modified table
print(a1, exact = ""NEWDI"", showAllLevels = TRUE)
```

In this example, `a1$p[[""p""]]` represents the percentages in the table, and we use the `paste0` function to append the ""%"" symbol at the end of each percentage. Finally, we print the modified table using `print(a1, exact = ""NEWDI"", showAllLevels = TRUE)`.

By modifying the `fmt` argument in this way, you can change the format of percentages or apply any other formatting you desire to the table."
76384509,Altair: showing the value of the current point in the tooltip,"In the code below, we have a dataset that can be read as: ""two cooks cook1, cook2 are doing a competition. They have to make four dishes, each time with two given ingredients ingredient1, ingredient2. A jury has scored the dishes and the grades are stored in _score.
I want to use Altair to show a graph where the x-axis is each dish (1, 2, 3, 4) and the y-axis contains the scores of the two cooks separately. This currently works but the main issue is that on hover, the tooltip does not include the score of the current point that is being hovered.
import altair as alt
import pandas as pd


df = pd.DataFrame({
    ""ingredient1"": [""potato"", ""onion"", ""carrot"", ""beet""],
    ""ingredient2"": [""tomato"", ""pepper"", ""zucchini"", ""lettuce""],
    ""dish"": [1, 2, 3, 4],
    ""cook1"": [""cook1 dish1"", ""cook1 dish2"", ""cook1 dish3"", ""cook1 dish4""],
    ""cook1_score"": [0.4, 0.3, 0.7, 0.9],
    ""cook2"": [""cook2 dish1"", ""cook2 dish2"", ""cook2 dish3"", ""cook2 dish4""],
    ""cook2_score"": [0.6, 0.2, 0.5, 0.6],
})


value_vars = [c for c in df.columns if c.endswith(""_score"")]
cook_names = [c.replace(""_score"", """") for c in value_vars]
id_vars = [""dish"", ""ingredient1"", ""ingredient2"",] + cook_names
df_melt = df.melt(id_vars=id_vars, value_vars=value_vars,
                  var_name=""cook"", value_name=""score"")

chart = alt.Chart(df_melt).mark_circle().encode(
    x=alt.X(""dish:O"", title=""Dish number""),
    y=alt.Y(""score:Q"", title=""Score""),
    color=""cook:N"",
    tooltip=id_vars
)

chart.show()


I tried explicitly adding the score columns to the tooltip:
    tooltip=id_vars+value_vars

But that yields the following error:

ValueError: cook1_score encoding field is specified without a type; the type cannot be inferred because it does not match any column in the data.

So how can I get altair to also show the score of (only) the currently hovered element?
",76384598,"cook1_score is not a column in df_melt, which is why you see the error. Setting tooltip=id_vars+['score'] will work.
","To include the score of the currently hovered point in the tooltip, you can modify the tooltip encoding in Altair as follows:

```python
tooltip = id_vars + [""cook"", ""score""]
```

Here's the updated code:

```python
import altair as alt
import pandas as pd

df = pd.DataFrame({
    ""ingredient1"": [""potato"", ""onion"", ""carrot"", ""beet""],
    ""ingredient2"": [""tomato"", ""pepper"", ""zucchini"", ""lettuce""],
    ""dish"": [1, 2, 3, 4],
    ""cook1"": [""cook1 dish1"", ""cook1 dish2"", ""cook1 dish3"", ""cook1 dish4""],
    ""cook1_score"": [0.4, 0.3, 0.7, 0.9],
    ""cook2"": [""cook2 dish1"", ""cook2 dish2"", ""cook2 dish3"", ""cook2 dish4""],
    ""cook2_score"": [0.6, 0.2, 0.5, 0.6],
})

value_vars = [c for c in df.columns if c.endswith(""_score"")]
cook_names = [c.replace(""_score"", """") for c in value_vars]
id_vars = [""dish"", ""ingredient1"", ""ingredient2""] + cook_names

df_melt = df.melt(
    id_vars=id_vars,
    value_vars=value_vars,
    var_name=""cook"",
    value_name=""score""
)

tooltip = id_vars + [""cook"", ""score""]

chart = alt.Chart(df_melt).mark_circle().encode(
    x=alt.X(""dish:O"", title=""Dish number""),
    y=alt.Y(""score:Q"", title=""Score""),
    color=""cook:N"",
    tooltip=tooltip
)

chart.show()
```

By adding `""cook""` and `""score""` to the `tooltip` list, you explicitly specify that these columns should be included in the tooltip. This ensures that the score of the currently hovered point will be displayed when you interact with the chart."
76384490,Flutter: Inconsistent column padding on Buttons between Android and Windows,"I have created a simple material app in flutter with:
flutter create --platforms=android,windows columntest
When I run the program on Android and Windows, I get some kind of padding between the ElevatedButtons on Android, but not on Windows. Do you know where this comes from and how I can make the design consistent?
The behavior seems to occur only with buttons (TextButton, OutlinedButton, ElevatedButton).
I have also tested this with container (with border), there it does not occur.
Here the code from the small app:
import 'package:flutter/material.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Flutter Demo',
      home: Scaffold(
        body: Center(
          child: Column(
            crossAxisAlignment: CrossAxisAlignment.center,
            mainAxisAlignment: MainAxisAlignment.center,
            children: [
              ElevatedButton(child: const Text(""Foobar1""), onPressed: () {}),
              ElevatedButton(child: const Text(""Foobar2""), onPressed: () {}),
            ],
          ),
        ),
      ),
    );
  }
}


Here is a screenshot at runtime:

Here my flutter version:
$ flutter --version
Flutter 3.10.0 • channel stable • https://github.com/flutter/flutter.git
Framework • revision 84a1e904f4 (3 weeks ago) • 2023-05-09 07:41:44 -0700
Engine • revision d44b5a94c9
Tools • Dart 3.0.0 • DevTools 2.23.1

My Android Emulator is an: Pixel_3a_API_33_x86_64
But the behaviour also occurs on my physical Pixel 6 (with android UpsideDownCake)
I look forward to your responses.
best regards
Michael
",76384624,"So, this implementation is done by flutter.
This is behaviour is because of the ThemeData.materialTapTargetSize parameter for the MaterialApp.
This feature decides what should be touchable dimensions of Material Button, in your case ElevatedButton.
You have 2 potential solutions

Change padding from ElevatedButton like below

    ElevatedButton(
              onPressed: () {},
              style: const ButtonStyle(padding: MaterialStatePropertyAll(EdgeInsets.zero)),
              child: const Icon(Icons.abc),
            ),


Change value from material app

    MaterialApp(
      title: 'Flutter Demo',
      theme: ThemeData(
        primarySwatch: Colors.blue,
        materialTapTargetSize: MaterialTapTargetSize.shrinkWrap),
      home: CupertinoPickerExample(),
    )

Reference :  https://stackoverflow.com/a/67580951
","The inconsistency in padding between the ElevatedButtons on Android and Windows is likely due to differences in default button styles and theme configurations on different platforms. Flutter provides default styles and themes for buttons on different platforms to match the native look and feel.

To make the design consistent across platforms, you can customize the button padding explicitly. You can use the `padding` property of the `ButtonStyle` to set the padding for your ElevatedButtons. Here's an updated version of your code that sets explicit padding for the buttons:

```dart
import 'package:flutter/material.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({Key? key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Flutter Demo',
      home: Scaffold(
        body: Center(
          child: Column(
            crossAxisAlignment: CrossAxisAlignment.center,
            mainAxisAlignment: MainAxisAlignment.center,
            children: [
              ElevatedButton(
                child: const Text(""Foobar1""),
                onPressed: () {},
                style: ElevatedButton.styleFrom(
                  padding: const EdgeInsets.all(16.0), // Set the desired padding
                ),
              ),
              ElevatedButton(
                child: const Text(""Foobar2""),
                onPressed: () {},
                style: ElevatedButton.styleFrom(
                  padding: const EdgeInsets.all(16.0), // Set the desired padding
                ),
              ),
            ],
          ),
        ),
      ),
    );
  }
}
```

In this updated code, the `padding` property of the `ElevatedButton.styleFrom` is set to `EdgeInsets.all(16.0)`, but you can adjust the value according to your preference. By explicitly setting the padding, you ensure that the buttons have consistent padding on both Android and Windows platforms.

Make sure to replace your existing code with the updated code and check if it resolves the inconsistency in padding between the buttons."
76378581,Perl Mojolicious: Passing arguments to a code ref,"In my Mojolicious Controller, I have:
my @promise;
foreach my $code (\&doit1, \&doit2,) {
    my $prom = Mojo::Promise->new;
    Mojo::IOLoop->subprocess(
    sub {
        my $r = $code->(""Hello"");
        return $r;
    },
    sub {
        my ($subprocess, $err, @res) = @_;
        return $prom->reject($err) if $err;
        $prom->resolve(@res);
    },
    );
    push @promise, $prom;
}

Mojo::Promise
    ->all(@promise)
    ->then(
    sub {
    my ($result1, $result2) = map {$_->[0]} @_;
    });

This works, and I can pass arguments (e.g. Hello) to my sub.
Now I converted doti1() and doit2() as helpers. So the code looks like:
foreach my $code (sub {$self->myhelper->doit1(""Goodbye"")},
                  sub {$self->myhelper->doit2(""Good night"")},
    ) {
    my $prom = Mojo::Promise->new;
    Mojo::IOLoop->subprocess(
    sub {
        my $r = $code->(""Hello""); # this is ignored?
        return $r;
    },
    sub {
        my ($subprocess, $err, @res) = @_;
        return $prom->reject($err) if $err;
        $prom->resolve(@res);
    },
    );
    push @promise, $prom;
}

How can I continue to pass the same set of arguments inside the loop (e.g. Hello), without having to specify them in each code ref (i.e. avoid Goodbye & Good night)? I like the idea of passing the same arguments for each code ref: $code->(""Hello"")
",76379917,"
Now I converted doti1() and doit2() as helpers. So the code looks like:

foreach my $code (sub {$self->myhelper->doit1(""Goodbye"")},
                  sub {$self->myhelper->doit2(""Good night"")},
    ) {
    #....
}

Yes but you are calling the helpers from another anonymous sub,

How can I continue to pass the same set of arguments inside the loop (e.g. Hello), without having to specify them in each code ref

so to recover the argument and pass it on the the helper, you just do:
foreach my $code (sub {my $arg = shift; $self->myhelper->doit1($arg)},
                  sub {my $arg = shift; $self->myhelper->doit2($arg)},
) {...}

or more generally as @Dada pointed out in the comments:
foreach my $code (sub {$self->myhelper->doit1(@_)},
                  sub {$self->myhelper->doit2(@_)},
) {...}

","In order to pass the same set of arguments to each code reference without specifying them individually, you can modify your code as follows:

```perl
my @promise;
my @args = (""Hello"");  # Set the common arguments here

foreach my $code (
    sub { $self->myhelper->doit1(@args) },
    sub { $self->myhelper->doit2(@args) },
) {
    my $prom = Mojo::Promise->new;
    Mojo::IOLoop->subprocess(
        sub {
            my $r = $code->(@args);  # Pass the arguments to the code reference
            return $r;
        },
        sub {
            my ($subprocess, $err, @res) = @_;
            return $prom->reject($err) if $err;
            $prom->resolve(@res);
        },
    );
    push @promise, $prom;
}

Mojo::Promise
    ->all(@promise)
    ->then(sub {
        my ($result1, $result2) = map { $_->[0] } @_;
        # Handle the results here
    });
```

By defining the common arguments in the `@args` array, you can pass them to each code reference using `$code->(@args)`. This way, you only need to specify the arguments once and they will be passed to each code reference within the loop."
76378589,How can I parse the certificate information output from the security command in Mac?,"I need to retrieve the attributes of a certificate that is stored in the keychain on my Mac from the command line. I can collect them manually from the Keychain Access app, but I want to do that with a script.

I used the security command to get a certificate and ""grep"" to inspect the ""subject"" section:
security find-certificate -c ""Apple Development"" login.keychain | grep ""subj""

and then got the following output (some omitted by ""..."").
""subj""<blob>=0x3081943...553  ""0\201\2241\0320\03...02US""

In the output above, what format is the data following ""subj""<blob>= and how can I parse it? I found that decoding the first half of the hexadecimal sequence(0x30...) with UTF-8 yields the second half of the string (0\201...), but I don't know what 0\201\2241\... means. I have tried other character codes, but they just give me garbled characters.
",76381943,"As for the format, the certificates are stored in DER/PEM format, which is a representation of ASN.1 encoded data. What you see in the output is the hexadecimal representation of the ASN.1 binary data. The blob indicates that the value or attribute is stored as binary data.
As for exporting (for certificates), I would highly recommend combining security with openssl as follows:
security find certificate -p -c ""Apple Development"" login.keychain | openssl x509 -noout -subject

The -p option in the security command exports the found certificate in PEM format, which is something openssl can use. You can then pipe the PEM data into the openssl command, where one can easily extract the subject using the -subject option.
You can check out both the man page of security and the man page of openssl x509.
","The output you obtained from the `security` command represents a hexadecimal-encoded string that contains the subject information of the certificate. To parse this data and extract the desired attributes, you need to decode the hexadecimal string and interpret it according to the ASN.1 (Abstract Syntax Notation One) format used in X.509 certificates.

Here's an example of how you can parse the subject information using Python and the `cryptography` library:

```python
import binascii
from cryptography import x509
from cryptography.hazmat.backends import default_backend

# Hexadecimal-encoded subject string
subject_hex = ""3081943...553""

# Convert hexadecimal to bytes
subject_bytes = binascii.unhexlify(subject_hex)

# Parse the subject bytes
cert = x509.load_der_x509_certificate(subject_bytes, default_backend())

# Extract desired attributes
common_name = cert.subject.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)[0].value
organization = cert.subject.get_attributes_for_oid(x509.oid.NameOID.ORGANIZATION_NAME)[0].value
# Add more attribute extraction as needed

# Print the extracted attributes
print(""Common Name:"", common_name)
print(""Organization:"", organization)
```

In this example, you'll need to install the `cryptography` library if you haven't already (`pip install cryptography`). This library provides cryptographic primitives and X.509 certificate parsing capabilities.

You can adapt this Python code to integrate it into your script or further automate the process as needed."
76384082,error messages fitting a non-linear exponential model between two variables,"I have two variables that I'm trying to model the relationship between and extract the residuals. The relationship between the two variables is clearly a non-linear exponential relationship. I've tried a few different approaches with nls, but I keep getting different error messages.

# dataset
df <- structure(list(y = c(464208.56, 334962.43, 361295.68, 426535.68, 258843.93, 272855.46, 
   166322.72, 244695.28, 227003.03, 190728.4, 156025.45, 72594.24, 56911.4, 175328.95, 161199.76, 
   152520.77, 190610.57, 60734.34, 31620.9, 74518.86, 45524.49, 2950.58, 2986.38, 15961.77, 12484.05, 
   6828.41, 2511.72, 1656.12, 5271.4, 7550.66, 3357.71, 3620.43, 3699.85, 3337.56, 4106.55, 3526.66, 
   2996.79, 1649.89, 4561.64, 1724.25, 3877.2, 4426.69, 8557.61, 6021.61, 6074.17, 4072.77, 4032.95, 
   5280.16, 7127.22), 
   x = c(39.23, 38.89, 38.63, 38.44, 38.32, 38.27, 38.3, 38.4, 38.56, 38.79, 39.06, 39.36, 39.68, 
   40.01, 40.34, 40.68, 41.05, 41.46, 41.93, 42.48, 43.14, 43.92, 44.84, 45.9, 47.1, 48.4, 49.78, 
   51.2, 52.62, 54.01, 55.31, 56.52, 57.6, 58.54, 59.33, 59.98, 60.46, 60.78, 60.94, 60.92, 60.71, 
   60.3, 59.69, 58.87, 57.86, 56.67, 55.33, 53.87, 52.33)), 
   row.names = c(NA, -49L), 
   class = c(""tbl_df"", ""tbl"", ""data.frame""), 
   na.action = structure(c(`1` = 1L, `51` = 51L), 
   class = ""omit""))

# initial model
m <- nls(y ~  a * exp(r * x), 
         start = list(a = 0.5, r = -0.2), 
         data = df)
Error in nls(y ~ a * exp(r * x), start = list(a = 0.5, r = -0.2), data = df,  : singular gradient

# add term for alg
m <- nls(y ~  a * exp(r * x), 
         start = list(a = 0.5, r = -0.2), 
         data = df,
         alg = ""plinear"")
Error in nls(y ~ a * exp(r * x), start = list(a = 0.5, r = -0.2), data = df,  : 
  step factor 0.000488281 reduced below 'minFactor' of 0.000976562

",76384628,"log-Gaussian GLM
As @Gregor Thomas suggests you could linearize your problem (fit a log-linear regression), at the cost of changing the error model.  (Basic model diagnostics, i.e. a scale-location plot, suggest that this would be a much better statistical model!) However, you can do this efficiently without changing the error structure by fitting a log-link Gaussian GLM:
m1 <- glm(y ~ x, family = gaussian(link = ""log""), data = df)

The model is y ~ Normal(exp(b0 + b1*x), s), so a = exp(b0), r = b1.
I tried using list(a=exp(coef(m1)[1]), r=coef(m1)[2]) as starting values, but even this was too finicky for nls().
There are two ways to get nls to work.
shifted exponential
As @GregorThomas suggests, shifting the x-axis to x=38 also works fine (given a sensible starting value):
m <- nls(y ~  a * exp(r * (x-38)), 
         start = list(a = 3e5, r = -0.35), 
         data = df)

provide nls with a gradient
The deriv function will generate a function with the right structure for nls (returns the objective function, with a "".grad"" attribute giving a vector of derivatives) if you ask it nicely. (I'm also using the exponentiated intercept from the log-Gaussian GLM as a starting value ...)
f <- deriv( ~ a*exp(r*x), c(""a"", ""r""), function.arg = c(""x"", ""a"", ""r""))
m2 <- nls(y ~  f(x, a, r),
         start = list(a = exp(coef(m1)[1]), r = -0.35),
         data = df)

We can plot these to compare the predictions (visually identical):
par(las = 1, bty = ""l"")
xvec <- seq(38, 60, length = 101)
plot(y ~ x, df)
lines(xvec, predict(m1, newdata = data.frame(x=xvec), type = ""response""),
      col = 2)
lines(xvec, predict(m, newdata = data.frame(x=xvec)), col = 4,  lty = 2)
lines(xvec, predict(m2, newdata = data.frame(x=xvec)), col = 5,  lty = 2)


With a little bit of extra work (exponentiating the intercept for the Gaussian GLM, shifting the x-origin back to zero for the nls fit) we can compare the coefficients (only equal up to a tolerance of 2e-4 but that should be good enough, right?)
a1 <- exp(coef(m1)[[1]])
a2 <- coef(m)[[1]]*exp(-38*coef(m)[[2]])
all.equal(c(a = a1, r = coef(m)[[2]]),
          c(a = a2, r = coef(m1)[[2]]), tolerance = 1e-4)
all.equal(c(a = a1, r = coef(m)[[2]]),
          coef(m2), tolerance = 2e-4)

","The error messages you encountered suggest issues with the fitting of the non-linear exponential model using the `nls()` function. Let's discuss the possible causes of these errors and potential solutions:

1. Singular gradient error:
   The ""singular gradient"" error indicates that the algorithm encountered difficulty in estimating the parameters of the model. This can happen when the starting values are not appropriate for the data or when the model is not identifiable. In your case, the starting values of `a = 0.5` and `r = -0.2` might not be suitable for the data.

   Solution: You can try providing better starting values that are closer to the expected values. Here's an example with updated starting values:

   ```R
   m <- nls(y ~ a * exp(r * x),
            start = list(a = 1e5, r = -0.1),
            data = df)
   ```

2. Step factor reduced below 'minFactor' error:
   The ""step factor reduced below 'minFactor'"" error typically occurs when the algorithm encounters difficulties in converging to a solution. The default convergence criteria might not be appropriate for your data.

   Solution: You can try adjusting the convergence criteria by specifying the `control` argument in the `nls()` function. Here's an example:

   ```R
   m <- nls(y ~ a * exp(r * x),
            start = list(a = 1e5, r = -0.1),
            data = df,
            control = nls.control(tol = 1e-6, minFactor = 1e-8))
   ```

   In the above code, `tol` sets the tolerance for convergence, and `minFactor` sets the minimum factor by which the parameter estimates can be reduced.

By applying these changes, you should be able to fit the non-linear exponential model successfully without encountering the aforementioned error messages."
76382271,Function call as a parameter inside insert values statement,"I'm trying to insert the data inside a forall loop. For this case, I cannot use a temporary variable and set result of the function beforehand.
The function just maps a number to a string:
create or replace function GetInvoiceStatus(status number)
    return nvarchar2
as
begin
    case status
        when 0 then return 'New';
        when 200 then return 'Sent';
        when 300 then return 'Accepted';
        end case;

    return '';
end; 

when I call this function like:
select GetInvoiceStatus(200) from dual;

I get the appropriate result.
However, when I try to insert the data I get errors.
The forall insert:
forall i in 1.. INVOICE_DATA.COUNT
insert into ""InvoiceAudit""
(""PropertyName"", ""OldValue"", ""NewValue"" (
            VALUES ('Status', (GetInvoiceStatus(invoice_data(i).status)),
                    ((GetInvoiceStatus((select ""Status"" from ""Invoice"" where ""InvoiceId"" = invoice_data(i).invoiceId)))));

However, I get the following error:

[2023-06-01 15:02:57] [65000][6592] [2023-06-01 15:02:57]     ORA-06592:
CASE not found while executing CASE statement [2023-06-01 15:02:57]
ORA-06512: at ""PUBLIC.GETINVOICESTATUS"", line 9 [2023-06-01 15:02:57]
ORA-06512: at ""PUBLIC.INVOICESSP"", line 63 [2023-06-01 15:02:57]
Position: 5

I have double checked, and the results from invoice_data(i).Status and the other select value are both valid parameters (and have their cases covered) and return appropriate string when called outside the stored procedure.
Is the syntax somewhere wrong?
I would like to remain using forall if at all possible because it is much faster than a regular for loop.
",76382378,"This error means that the parameter value (status) is not one of the cases in the case expression (which are 0, 200, 300).
If you executed this code select GetInvoiceStatus(555) as dd from dual you will get the same error. So, add ELSE clause like this:
create or replace function GetInvoiceStatus(status number)
    return nvarchar2
as
begin
    case status
        when 0 then return 'New';
        when 200 then return 'Sent';
        when 300 then return 'Accepted';
        else return '';
    end case;
end; 

","The issue you're facing is related to the usage of a function call within the `forall` loop's `VALUES` clause. Unfortunately, you cannot directly call a function within the `VALUES` clause in a `forall` statement. The `VALUES` clause expects a literal value or a subquery, not a function call.

To work around this limitation, you can assign the function call result to a temporary variable before the `forall` loop and then use that variable in the `VALUES` clause. Here's an example of how you can modify your code:

```sql
DECLARE
  -- Declare a temporary variable to store the function call result
  l_status_newvalue NVARCHAR2(100);
BEGIN
  FORALL i IN 1..INVOICE_DATA.COUNT
    -- Assign the function call result to the temporary variable
    l_status_newvalue := GetInvoiceStatus(invoice_data(i).status);
    INSERT INTO ""InvoiceAudit"" (""PropertyName"", ""OldValue"", ""NewValue"")
    VALUES ('Status', l_status_newvalue, (GetInvoiceStatus((SELECT ""Status"" FROM ""Invoice"" WHERE ""InvoiceId"" = invoice_data(i).invoiceId))));
END;
```

By assigning the function call result to the `l_status_newvalue` variable outside the `forall` loop, you can use it in the `VALUES` clause without directly calling the function there.

Note that I've assumed the `l_status_newvalue` variable's data type as `NVARCHAR2(100)`. Please adjust it to match the appropriate data type based on your requirements.

This approach should resolve the error you encountered and allow you to insert the data using the `forall` loop while utilizing the function call."
76384531,pivot returning blank instead of 0 google sheet,"I have a spreadsheet where I have an importrange and vlookup to another file where its looking up to a pivot table. Some data is blank in the pivot table and when I lookup in the formula, I have a result of blank even though I have set it to return to 0 by iferror.
Here's my formula:
=iferror(VLOOKUP(A5,importrange(""12PaJfEC7Q7gOcCx2zlMHG3YybQuk1TSsNjZDw26qFRg"",""Converted Pivot!A:E""),3,false),0)
",76384635,"You may try:
=let(Σ,ifna(vlookup(A5,importrange(""12PaJfEC7Q7gOcCx2zlMHG3YybQuk1TSsNjZDw26qFRg"",""Converted Pivot!A:E""),3,),""no_match_found""),
       if(Σ="""",0,Σ))


blank_value will now be shown as 0 & a non-match output error will be prompted with no_match_found

","The formula you provided seems to be correct, and it should return 0 when the VLOOKUP function encounters an error. However, if the result is still blank instead of 0, there might be a few reasons why this is happening. Here are some troubleshooting steps you can try:

1. Check if the pivot table source data contains actual blank cells: Sometimes, cells may appear blank in a pivot table, but they might contain empty strings or other non-visible characters. Ensure that the cells in the pivot table source range are truly blank.

2. Verify if the VLOOKUP range includes the correct column: Ensure that the VLOOKUP range you are using (`importrange(""""12PaJfEC7Q7gOcCx2zlMHG3YybQuk1TSsNjZDw26qFRg"""",""""Converted Pivot!A:E"""")`) includes the correct column number (3 in your case) for the value you want to retrieve. Double-check that the desired value is indeed in the third column of the range.

3. Confirm that the formula is applied correctly: Make sure the formula is applied to the appropriate cell and that it's not being overridden by other formulas or formatting. Check that cell A5 contains a valid lookup value.

4. Ensure the formula is recalculated: If you've made changes to the pivot table or the data it's based on, ensure that the formula is recalculated. You can force a recalculation by selecting the cell and pressing the Enter key.

5. Check if there are any other factors affecting the result: Consider if there are any other formulas or conditional formatting rules that could be causing the unexpected behavior. Evaluate the surrounding cells and their dependencies.

By going through these steps, you should be able to identify any potential issues that may be causing the formula to return a blank instead of 0."
76380577,Make an element not scroll horizontally,"I am trying to  make a layout with:

A header (gray block in the snippet)
A body (lime borrder)
Main body content ( blocks with red border)

If you scroll horizontally, then the header should not scroll, it should be full width and stay in view. If you scroll vertically, then the header should scroll off the page as usual. The height of the header is dynamic, and fits the content within it (this SO answer works with a fixed height)..
The <main> element is allowed to be wider than the viewport, but the header is always the viewport width.
The reason I dont add max-width: 100%; overflow-x: auto on the <main> element (like this SO answer, is because then the horizontal scroll appears at the bottom of the element, and then say one is reading the first block, and you wish to scroll horizontally, you have to scroll to the bottom of the main element to see the horizontal scroll bar, scroll to the side, then scroll back up. I wish to have the horizontal scroll bar always present if main is wider than the view port.
I have tried position: sticky/fixed on the header but could not get it to work.
I would prefer not to use JavaScript if possible.


header {
  padding: 32px;
  background: gray;
  width: 100%;
}
main {
  border: 2px solid lime;
  min-width: 100%;
}
div {
  height: 200px;
  width: 120%; /* make it overflow horizontally */
  display: flex;
  align-items: center;
  justify-content: center;
  border: 2px solid red;
}
<header>The Header should not scroll horizntally<br>(is dynamic height)</header>
<main>
  <div>content 1</div>
  <div>content 2</div>
  <div>content 3</div>
  <div>content 4</div>
  <div>content 5</div>
  <div>content 6</div>
</main>



",76381169,"What I have done here is make header sticky to the left part of the screen. Its parent element must be aware of size of your content to allow header to move. So I set body min-width to min-content and same with main so it can transfer its children's size to body.
You also may notice I used box-sizing: border-box; in the header, its so padding size is taken into account when element size is calculated(100vw in this case). You don´t want to use % on header width because it won´t have room to slide.
Also div sizes must not be dependent on parent size, so you can´t use % here either.


body{
  min-width: min-content;
}

header {
  box-sizing: border-box;
  position: sticky;
  left: 0;
  padding: 32px;
  background: gray;
  width: 100vw;
}
main {
  min-width: min-content;
  border: 2px solid lime;
}
div {
  height: 200px;
  width: 120vw; /* make it overflow horizontally */
  display: flex;
  align-items: center;
  justify-content: center;
  border: 2px solid red;
}
<body>

<header>The Header should not scroll horizntally<br>(is dynamic height)</header>
<main>
  <div>content 1</div>
  <div>content 2</div>
  <div>content 3</div>
  <div>content 4</div>
  <div>content 5</div>
  <div>content 6</div>
</main>
</body>



","To achieve the desired effect of keeping the header fixed and preventing horizontal scrolling, you can use a combination of CSS properties. Here's an updated version of your code that incorporates these changes:

```html
<style>
  body {
    margin: 0;
    padding: 0;
    overflow-x: hidden; /* Hide horizontal scrollbar on the body */
  }
  header {
    padding: 32px;
    background: gray;
    position: sticky;
    top: 0;
    z-index: 1; /* Ensure the header stays on top */
  }
  main {
    border: 2px solid lime;
    overflow-x: auto; /* Show horizontal scrollbar on the main element */
  }
  div {
    height: 200px;
    width: 120%; /* make it overflow horizontally */
    display: flex;
    align-items: center;
    justify-content: center;
    border: 2px solid red;
  }
</style>

<body>
  <header>The Header should not scroll horizontally<br>(is dynamic height)</header>
  <main>
    <div>content 1</div>
    <div>content 2</div>
    <div>content 3</div>
    <div>content 4</div>
    <div>content 5</div>
    <div>content 6</div>
  </main>
</body>
```

In this code, the header is made sticky using `position: sticky` and `top: 0`, which ensures that it remains fixed at the top of the viewport while scrolling vertically. The `z-index: 1` property ensures that the header stays on top of other elements.

The `overflow-x: auto` property on the `main` element allows a horizontal scrollbar to appear if its content overflows horizontally. This way, you can always scroll horizontally even when the main content is wider than the viewport.

The `overflow-x: hidden` property on the `body` element hides the horizontal scrollbar of the entire page.

With these changes, the header will remain fixed at the top of the viewport while scrolling vertically, and you will be able to scroll horizontally whenever the main content overflows horizontally."
76382239,"""Unused CSS selector"" when using a SASS themify mixin with Svelte and Vite:","I'm trying to create a small web application using Svelte.
One of the requirements is to be able to change the application ""theme"" on demand, for example - dark theme, light theme, high contrast, and so on.
I've been using an online mixin snippet to help me with that -
https://medium.com/@dmitriy.borodiy/easy-color-theming-with-scss-bc38fd5734d1
However, this doesn't work consistently, and I often get errors like:
[vite-plugin-svelte] /path/to/svelte/component.svelte:61:0 Unused CSS selector ""main.default-theme div.some.element.identification""
even tho the selector is used and is receiving it's non-themed attributes.
Inside a themes.scss file:
@mixin themify($themes) {

    @each $theme,
    $map in $themes {
        main.#{$theme}-theme & {
            $theme-map: () !global;

            @each $key,
            $submap in $map {
                $value: map-get(map-get($themes, $theme), '#{$key}');
                $theme-map: map-merge($theme-map, ($key: $value)) !global;
            }

            @content;
            $theme-map: null !global;
        }
    }
}

@function themed($key) {
    @return map-get($theme-map, $key);
}

$themes: (
    default: (
        strokeColor: green,
        fillColor: red,
    ),
);

and inside another scss file that is importing themes.scss:
div.some.element.identification {
    some-non-themed-attribute: some-value;

    @include themify($themes) {
        stroke: themed('strokeColor');
        fill: themed('fillColor');
    }
}

now the punchline - when using this methodology, some elements are receiving their appropriate themed attributes, and others dont.
I am also seeing the following error:
[vite-plugin-svelte] /path/to/svelte/component.svelte:61:0 Unused CSS selector ""main.default-theme div.some.element.identification""
the issue doesn't seem to be in the css selectors - since the elements that dont receive the themed attributes, still receive the other non-themed attributes in the same css clause.
Two final observations -

When I'm building the project (using vite build), I can see that the css asset file being created doesn't include the css selectors that are missing their themed attributes.
When i'm using the devtools to locate the supposedly unused selectors (whose themed attributes are not present), they can be found - despite the error message.

I've been trying different way to solve this issue and nothing works consistently.
Thank you in advance for your help!
",76382400,"You could try checking these different items:

If you use svelte-preprocess, try to add scss: { prependData: `@import 'src/styles/theme.scss';` } or whatever the path to your theme is, to the config object.
If it still does not work, maybe try to swap svelte-preprocess with vite-preprocess
Disable any potential css purge plugin

","The error message you're seeing, ""Unused CSS selector,"" typically occurs when a CSS selector is not being used anywhere in your code. However, in your case, it seems like the error is misleading because the selectors are indeed being used and are receiving non-themed attributes.

The issue might be related to the way Vite handles CSS and dynamic styles in Svelte. Vite's CSS handling is based on static analysis, which means it tries to optimize and remove unused CSS selectors during the build process. Since your styles are being generated dynamically using SASS mixins, Vite may mistakenly identify some of the selectors as unused.

To work around this issue, you can try the following approaches:

1. Disable CSS tree shaking: Vite provides an option to disable CSS tree shaking, which can prevent the removal of supposedly unused CSS selectors. You can try adding the following configuration in your Vite configuration file (vite.config.js):

```javascript
export default {
  // ...
  build: {
    rollupOptions: {
      output: {
        manualChunks: undefined,
      },
    },
  },
};
```

2. Use dynamic CSS classes instead of dynamic selectors: Instead of generating dynamic selectors, you can generate dynamic CSS classes and apply them to your elements in Svelte. For example, you can define different theme classes in your SCSS file and conditionally apply them to the elements in Svelte based on the selected theme.

3. Pre-generate styles for all themes: Another approach is to pre-generate styles for all possible themes during the build process. This way, Vite will not consider any of the selectors as unused. You can define CSS classes for each theme in your SCSS file and include them in your HTML or Svelte components based on the selected theme.

By adopting one of these approaches, you should be able to resolve the issue with the supposedly unused CSS selectors and ensure consistent application of themed attributes to your elements in Svelte."
76384567,Is it faster to use push_back(x) or using an index (capacity)?,"I learned 2 ways of inserting elements into a vector.
And I've been wondering which way is faster since I'm working with time limits.
Method 1:
int n;
cin>>n;
vector<int> v(n);
for(int i = 0;i<n;i++){
  cin>>v[i];
}

Method 2:
int n;
cin>>n;
vector<int> v;
for(int i = 0;i<n;i++){
  int x;
  cin>>x;
  v.push_back(x);
}

If you have a better method to suggest, it'd be appreciated!
",76384661,"Both have issues:
You should be using reserve(n)
int n;
cin >>  n;
vector<int> v;
v.reserve(n);
for(int i = 0; i < n; ++i){
    int x;
    cin >> x;
    v.emplace_back(x);
}

In the first version: Setting size.
Here you have the issue that you are constructing all the elements in the array. Now for integers this may be insignificant. But if we extend this to non integer types that have a constructor that needs to be called for each element and then you are using the assignment operator to copy over them.
The second option: push_back
Here you run into the risk of the underlying storage being reallocated (potentially multiple times). Each time you re-allocate you need to copy the data from the old storage to the new storage.
Again this hurts for integers but really hurts for types with constructors and destructors.
Prefer: emplace_back()
Rather than pushing where you need a fully constructed object. You can use emplace_back and pass in the objects used to construct the object. This allows the vector to construct the object in place. If you have simple integers or classes with effecient move semantics then not an issue but worth it as a general habit.
","Both methods you described have different time complexities, and the choice between them depends on the specific requirements of your program.

Method 1:
In this method, you preallocate memory for the vector by specifying the size `n` during initialization. Then, you use the index `i` to directly access and assign values to the vector elements. This method has a time complexity of O(n) for both initialization and assignment.

Method 2:
In this method, you start with an empty vector and use the `push_back()` function to add elements one by one. This method dynamically resizes the vector as needed. The time complexity of `push_back()` is amortized constant time, which means it is usually efficient. However, in the worst case scenario, when the vector needs to be resized, it has a time complexity of O(n), where n is the current size of the vector.

Considering the time complexity, if you know the exact number of elements `n` in advance and have a good estimate of its upper limit, Method 1 (preallocating memory) can be more efficient. It avoids potential resizing operations and provides better memory locality.

On the other hand, if the number of elements is not known in advance or can vary significantly, Method 2 (using `push_back()`) is more flexible and convenient. It automatically handles the memory management and allows you to add elements without worrying about the capacity.

In general, if you have a good estimate of the number of elements and memory is a concern, Method 1 may be a better choice. However, if flexibility and simplicity are more important, Method 2 can be a suitable option.

Additionally, you can further optimize the performance by using the `reserve()` function in Method 2. By reserving memory in advance, you can reduce the number of resizing operations, which can improve the overall efficiency. For example:

```cpp
int n;
cin >> n;
vector<int> v;
v.reserve(n);  // Reserve memory for n elements
for (int i = 0; i < n; i++) {
  int x;
  cin >> x;
  v.push_back(x);
}
```

By reserving memory, you inform the vector about the expected number of elements, allowing it to allocate memory efficiently.

Ultimately, the choice between the methods depends on your specific use case and requirements. It may be helpful to measure the performance of your program using both methods and consider factors such as input size, memory usage, and time constraints to determine which approach is more suitable for your needs."
76382402,Background video in Node.js 13,"I am trying to set up a gif as a background,I get does it not work:
In the code Import GridMatrix and extract the src from it, then I use the video tag to try to render it on fullscreen.
import React from 'react';
import GridMatrix from '../assets/gridMatrix.gif';

function Home() {

    return (
        <div>
            <video
                className=""matrix-bg fixed top-0 left-0 w-full h-full z-[-1] object-cover""
                autoPlay
                loop
                muted
            >
                <source
                    src={GridMatrix.src}
                    type=""video/gif""
                />
            </video>
            <main className=""container mx-auto py-10 px-4 flex flex-col items-center justify-center"">
                <h1 className=""text-4xl font-bold mb-8 text-white text-center"">
                    UNS Demo
                </h1>
                <button className=""bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded"">
                    Login
                </button>

            </main>
        </div>
    );
}

export default Home;


",76382476,"GIF files are not video files and the MIME type for them is image/gif. The <video> tag will not render them.
You can embed images using <img src={GridMatrix.src} /> or set it as a background-image with CSS on an element.
Nowadays, websites often embed what they call 'GIFs' that are actually video files, but notice that those are often .webm or .mp4 files, both being video formats, thus compatible with <video>.
","In your code, you are trying to set a GIF image (`gridMatrix.gif`) as the background video using the HTML `<video>` tag. However, there are a few issues with the code that might be causing it not to work properly. Here are some suggestions to fix the problems:

1. Make sure the path to the `gridMatrix.gif` file is correct. Double-check that the file is located in the correct directory and that the import statement is pointing to the right path.

2. Instead of trying to access `GridMatrix.src` directly, you should use `GridMatrix` itself as the source of the video. Since `GridMatrix` is imported as the default export from `'../assets/gridMatrix.gif'`, you can directly use it in the `src` attribute of the `<source>` tag.

Here's an updated version of your code with the suggested changes:

```jsx
import React from 'react';
import GridMatrix from '../assets/gridMatrix.gif';

function Home() {
  return (
    <div>
      <video
        className=""matrix-bg fixed top-0 left-0 w-full h-full z-[-1] object-cover""
        autoPlay
        loop
        muted
      >
        <source src={GridMatrix} type=""video/gif"" />
      </video>
      <main className=""container mx-auto py-10 px-4 flex flex-col items-center justify-center"">
        <h1 className=""text-4xl font-bold mb-8 text-white text-center"">UNS Demo</h1>
        <button className=""bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded"">
          Login
        </button>
      </main>
    </div>
  );
}

export default Home;
```

Make sure to verify the file path and ensure that the `gridMatrix.gif` file is in the correct location relative to the component file."
76380888,Convert XML File with nested hierarchy placed in Azure Data lake to CSV using C# Azure Function,"I have the following xml file with the below structure to convert to csv using Azure function C#. The XML file is located in Azure Data Lake location. The structure of the file is as follows.
<root id=""1"" created_date=""01/01/2023"" asof_date=""01/01/2023"">
    <level1>
        <data1>sdfs</data1>
        <data2>true</data2>
        <level2 rec=""4"">
            <level_record>
                <groupid>1</groupid>
                <groupname>somegroup</groupname>
                <groupdate>01/01/2023</groudate>
                <groupvalue>5</groupvalue>
                <groupkey>ag55</groupkey>
            </level_record>  
            <level_record>
                <groupid>2</groupid>
                <groupname>somegroup1</groupname>
                <groupdate>02/01/2023</groudate>
                <groupvalue>6</groupvalue>
                <groupkey>ag56</groupkey>
            </level_record> 
       </level2> 
    </level1>
</root> 

How do i read the file from Azure data lake and convert it as a csv file?
",76381205,"Here is the example of Azure Function in C# that reads an XML file from Azure Data Lake Storage and converts it to a CSV file
using Microsoft.Azure.Functions.Worker;
using Microsoft.Extensions.Logging;
using Microsoft.Azure.Storage;
using Microsoft.Azure.Storage.Auth;
using Microsoft.Azure.Storage.Blob;
using System.IO;
using System.Xml.Linq;

namespace YourNamespace
{
    public static class ConvertXmlToCsvFunction
    {
        [Function(""ConvertXmlToCsvFunction"")]
        public static void Run([BlobTrigger(""your-container/{name}"", Connection = ""AzureWebJobsStorage"")] Stream xmlStream, string name, FunctionContext context)
        {
            var logger = context.GetLogger(""ConvertXmlToCsvFunction"");
            logger.LogInformation($""Processing file: {name}"");

            try
            {
                // Read the XML file content
                string xmlContent;
                using (StreamReader reader = new StreamReader(xmlStream))
                {
                    xmlContent = reader.ReadToEnd();
                }

                // Parse the XML content
                XDocument xDoc = XDocument.Parse(xmlContent);

                // Extract data and convert to CSV format
                XElement rootElement = xDoc.Element(""root"");
                XElement level1Element = rootElement.Element(""level1"");
                XElement level2Element = level1Element.Element(""level2"");

                // Create the CSV header
                string csv = ""groupid,groupname,groupdate,groupvalue,groupkey"" + ""\n"";

                // Iterate over level_record elements and extract data
                foreach (XElement recordElement in level2Element.Elements(""level_record""))
                {
                    string groupid = recordElement.Element(""groupid"").Value;
                    string groupname = recordElement.Element(""groupname"").Value;
                    string groupdate = recordElement.Element(""groupdate"").Value;
                    string groupvalue = recordElement.Element(""groupvalue"").Value;
                    string groupkey = recordElement.Element(""groupkey"").Value;

                    // Append the CSV row
                    csv += $""{groupid},{groupname},{groupdate},{groupvalue},{groupkey}"" + ""\n"";
                }

                // Save the CSV content to a file
                string csvFileName = Path.ChangeExtension(name, ""csv"");
                string csvFilePath = Path.Combine(Path.GetTempPath(), csvFileName);
                File.WriteAllText(csvFilePath, csv);

                logger.LogInformation($""CSV file created: {csvFilePath}"");
            }
            catch (Exception ex)
            {
                logger.LogError($""An error occurred: {ex.Message}"");
                throw;
            }
        }
    }
}

","To read an XML file from Azure Data Lake and convert it to a CSV file using C# in an Azure Function, you can use the `Microsoft.Azure.Storage` and `System.Xml.Linq` namespaces to interact with Azure Data Lake Storage and parse XML data, respectively. Here's an example of how you can achieve this:

1. Add the necessary NuGet packages to your Azure Function project:
   - `Microsoft.Azure.Storage.Blob`: This package is required to access Azure Data Lake Storage.
   - `System.Xml.Linq`: This package is used to parse XML data.

2. Install the required packages by running the following command in the Package Manager Console:
   ```bash
   Install-Package Microsoft.Azure.Storage.Blob
   Install-Package System.Xml.Linq
   ```

3. Import the required namespaces in your Azure Function C# code:
   ```csharp
   using Microsoft.Azure.Storage;
   using Microsoft.Azure.Storage.Blob;
   using System.IO;
   using System.Xml.Linq;
   ```

4. Write the code to read the XML file from Azure Data Lake and convert it to a CSV file:
   ```csharp
   public static void ConvertXmlToCsv([TimerTrigger(""0 */5 * * * *"")] TimerInfo timer, ILogger log)
   {
       // Retrieve the connection string for Azure Data Lake Storage
       string connectionString = ""<your-azure-data-lake-connection-string>"";

       // Specify the name of the XML file in Azure Data Lake
       string xmlFileName = ""<xml-file-name.xml>"";

       // Specify the name of the CSV file to be created
       string csvFileName = ""<csv-file-name.csv>"";

       // Retrieve the reference to the Azure Data Lake Storage account
       CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);

       // Create the CloudBlobClient object to interact with blobs
       CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();

       // Retrieve a reference to the XML file in Azure Data Lake
       CloudBlobContainer container = blobClient.GetContainerReference(""<container-name>"");
       CloudBlockBlob xmlBlob = container.GetBlockBlobReference(xmlFileName);

       // Download the XML file to a stream
       MemoryStream xmlStream = new MemoryStream();
       xmlBlob.DownloadToStream(xmlStream);
       xmlStream.Position = 0;

       // Load the XML data from the stream
       XDocument xmlDoc = XDocument.Load(xmlStream);

       // Extract the required data from the XML and convert it to a CSV string
       string csvContent = ExtractCsvContent(xmlDoc);

       // Create the CloudBlockBlob object for the CSV file
       CloudBlockBlob csvBlob = container.GetBlockBlobReference(csvFileName);

       // Upload the CSV content to Azure Data Lake
       csvBlob.UploadText(csvContent);

       log.LogInformation(""Conversion completed successfully."");
   }

   private static string ExtractCsvContent(XDocument xmlDoc)
   {
       // Define the delimiter for the CSV file
       string delimiter = "","";

       // Extract the required data from the XML and convert it to a CSV string
       var dataElements = xmlDoc.Descendants(""level_record"");
       var csvLines = dataElements.Select(e =>
           string.Join(delimiter, e.Elements().Select(x => x.Value))
       );
       string csvContent = string.Join(Environment.NewLine, csvLines);

       // Include a header line with the element names
       string headerLine = string.Join(delimiter, dataElements.Descendants().First().Elements().Select(x => x.Name));
       csvContent = headerLine + Environment.NewLine + csvContent;

       return csvContent;
   }
   ```

   Make sure to replace the placeholders `<your-azure-data-lake-connection-string>`, `<xml-file-name.xml>`, `<csv-file-name.csv>`, and `<container"
76380899,How can we stop vuetify 3 v-combobox from adding new items if the validation checks fail?,"I have tried using the validation-on, rules props, they are able to validate and give me error messages but the new items are still getting appended to the state. Is there any way to change this behaviour so that every time there is a validation error it don't append the item to the state?
ParentComponent.vue
...
 <MultiSelect
    v-model=""form.tags""
    label=""Select Tags""
    :items=""tags""
    item-title=""name""
    item-value=""id""
  />
...

MultiselectComponent.vue
<template>
    <v-combobox 
        multiple 
    chips 
        closable-chips 
        clearable 
        :return-object=""false""
        variant=""outlined""
    />
</template>

What I want
Basically I don't want user to add tags that starts with a number or are all numbers

e.g. 123, 2VueJs, 456890, 68yjkk etc.

",76381218,"Validation is used to show error messages to the user and prevent the form from being submitted. But if you remove invalid values immediately, there is no error message and the values are always going to be valid.
So instead of validation, you can just filter the values coming out of the component. Just replace the v-model with the underlying :modelValue and @update:modelValue and pipe the values through a filter:
<v-combobox
  :model-value=""values""
  @update:model-value=""values = filterInvalid($event)""
  ...
/>

You can also use the filter on the input of :modelValue to filter any invalid values coming in, depending on if there are preset values and how to deal with them if they are invalid.
Here it is in a snippet:


const { createApp, ref } = Vue;
const { createVuetify } = Vuetify
const vuetify = createVuetify()
const app = {
  setup(){
    return {
      values: ref([12, '12n','n']),
      filterInvalid: (inputValues) => inputValues.filter(value => typeof value === 'string' && isNaN(value[0]))
    }
  }

}
createApp(app).use(vuetify).mount('#app')
<link rel=""stylesheet"" type=""text/css"" href=""https://cdn.jsdelivr.net/npm/vuetify@3/dist/vuetify.min.css"" />
<link href=""https://cdn.jsdelivr.net/npm/@mdi/font@5.x/css/materialdesignicons.min.css"" rel=""stylesheet"">
<div id=""app"">
  <v-app>
    <v-main class=""pa-8"">
    
      <v-combobox
        :model-value=""filterInvalid(values)""
        @update:model-value=""values = filterInvalid($event)""
        multiple
        chips
        closable-chips
        clearable
        variant=""outlined""
      ></v-combobox>
      
      <div>Values: {{values}}</div>
      
      </v-main>
  </v-app>
</div>
<script src=""https://unpkg.com/vue@3/dist/vue.global.prod.js""></script>
<script src=""https://cdn.jsdelivr.net/npm/vuetify@3/dist/vuetify.min.js""></script>



Note however that removing values automatically can feel like a bug to users. You might be better off with the validation approach after all.
","To prevent the addition of new items in v-combobox when validation checks fail, you can utilize the `@input` event and manually validate the input value before updating the v-model.

Here's an example of how you can achieve this:

```vue
<template>
  <v-combobox
    multiple
    chips
    closable-chips
    clearable
    :items=""tags""
    item-text=""name""
    item-value=""id""
    v-model=""selectedTags""
    @input=""handleInput""
    :rules=""[validateTags]""
  ></v-combobox>
</template>

<script>
export default {
  data() {
    return {
      selectedTags: [],
      tags: [
        { id: 1, name: ""Vue.js"" },
        { id: 2, name: ""React"" },
        { id: 3, name: ""Angular"" }
      ]
    };
  },
  methods: {
    handleInput(value) {
      // Manually validate the input value
      if (!this.validateTags(value)) {
        // Validation failed, do not update the v-model
        return;
      }

      // Validation passed, update the v-model
      this.selectedTags = value;
    },
    validateTags(value) {
      // Custom validation logic
      // Example: Disallow tags that start with a number or are all numbers
      const tags = Array.isArray(value) ? value : [value];

      for (const tag of tags) {
        if (/^\d/.test(tag) || /^\d+$/.test(tag)) {
          return false;
        }
      }

      return true;
    }
  }
};
</script>
```

In the above code, the `handleInput` method is triggered on the `@input` event of v-combobox. It performs custom validation using the `validateTags` method. If the validation fails, the v-model (`selectedTags`) is not updated, preventing the addition of invalid tags."
76382472,I tried Simple Jolt transformation but its not working,"I want to convert this data using jolt, I tried but it showing null as result
Input :
{
  ""employer"": [
    {
      ""id"": ""98"",
      ""place_id"": ""7871"",
      ""name"": ""Iti-ha-cho""
    }
  ]
}

Expected Output :
{
  ""id"" : ""98"",
  ""place_id"" : ""7871"",
  ""name"" : ""Iti-ha-cho""
}

Jolt Spec I tried but didnt work :
{
  ""operation"": ""shift"",
  ""spec"": {
    ""employer"": {
      ""id"": ""[&1].&"",
      ""place_id"": ""place_id"",
      ""name"": ""name""
    }
  }
}

",76382541,"While the current issue is due to missing square bracket nesting the spec, you don't need to specify those long stuff, just the following spec will suffice
[
  {
    ""operation"": ""shift"",
    ""spec"": {
      ""employer"": {
        ""*"": """"
      }
    }
  }
]

as you only want to extract the sub-content of the employer array.
","The Jolt transformation you attempted is almost correct, but there is a small issue with the spec. In your spec, you used `""[&1].&""` for the `id` field, which is causing the problem. Instead, you should use `""[0].id""` to correctly reference the `id` field inside the `employer` array. Here's the corrected Jolt spec:

```json
{
  ""operation"": ""shift"",
  ""spec"": {
    ""employer"": {
      ""0"": {
        ""id"": ""id"",
        ""place_id"": ""place_id"",
        ""name"": ""name""
      }
    }
  }
}
```

With this spec, the Jolt transformation should produce the desired output. Here's the modified spec applied to your input:

Input:
```json
{
  ""employer"": [
    {
      ""id"": ""98"",
      ""place_id"": ""7871"",
      ""name"": ""Iti-ha-cho""
    }
  ]
}
```

Jolt Spec:
```json
{
  ""operation"": ""shift"",
  ""spec"": {
    ""employer"": {
      ""0"": {
        ""id"": ""id"",
        ""place_id"": ""place_id"",
        ""name"": ""name""
      }
    }
  }
}
```

Output:
```json
{
  ""id"": ""98"",
  ""place_id"": ""7871"",
  ""name"": ""Iti-ha-cho""
}
```

Now, when you apply this corrected Jolt spec to your input data, you should get the expected output."
76382467,form with 2 int inputs where one always has to be lower and one always has to higher,"I think its's pretty clear in my code what I am trying to do. basically I'm trying to use the max and min parameter from the input to make it so that they can never cross each other. This doesn't work of course, I am using react and am using usestate to set the values whenever the form is submitted and pass these variables into my database fetch. I feel like using 2 states one for the temporary value of the input and one to pass the submitted value to the fetch is not a good way of solving this.
const [ LowestPrice, setLowestPrice ] = useState(0)
const [ HighestPrice, setHighestPrice ] = useState(500)

useEffect(() =>{
const getProps = async () => {
const { data, count, error } = await backbase.from('products_2')
      .select('*', { count: 'exact' })
      .gte('price', LowestPrice)
      .lt('price', HighestPrice)
      .range(indexOfFirstItem, indexOfLastItem - 1)
}}, [LowestPrice, HighestPrice])

const handleSubmit = (e) => {
    e.preventDefault()

    setLowestPrice(document.getElementById(""lowest_price"")?.value)
    setHighestPrice(document.getElementById(""highest_price"")?.value)
  }

<form onSubmit={handleSubmit}>
        <label htmlFor=""lowest_price"">minimum price</label>
        <input
          type=""number""
          id=""lowest_price""
          defaultValue={LowestPrice}
          min={0}
          max={document.getElementById(""highest_price"")?.value}
        />
        <label htmlFor=""highest_price"">maximum price</label>
        <input
          type=""number""
          id=""highest_price""
          defaultValue={HighestPrice}
          min={document.getElementById(""lowest_price"")?.value}
          max={500}
        />
        <button type=""submit"">apply filters</button>
      </form>

I left out unessential parts of the code to make it easier to read. It's the min and max in the form that are the most relevant.
",76382555,"Firstly, don't use getElementById to listen to changes. React triggers a re-render on components when their value changes. In order to retain the value, we use useState. Secondly, you can use OnChange to compare between both values in state before deciding whether or not to discard the new value. Try something like this;
const [lowestPrice, setLowestPrice] = useState(0);
const [highestPrice, setHighestPrice] = useState(1);

return (
<form onSubmit={handleSubmit}>
    <label htmlFor=""lowest_price"">minimum price</label>
    <input
      onChange={e => e.target.value <= highestPrice && setLowestPrice(e.target.value)} value={lowestPrice}
      type=""number""
      id=""lowest_price""
      defaultValue={lowestPrice}
      value={lowestPrice}
      min={0}
      max={highestPrice}
    />
    <label htmlFor=""highest_price"">maximum price</label>
    <input
      onChange={e => e.target.value > lowestPrice && setHighestPrice(e.target.value)}
      type=""number""
      id=""highest_price""
      defaultValue={highestPrice}
      value={highestPrice}
      min={lowestPrice}
      max={500}
    />
    <button type=""submit"">apply filters</button>
  </form>
)

","In your code, you are using two separate states, `LowestPrice` and `HighestPrice`, to track the temporary values of the inputs and update them when the form is submitted. However, you want to ensure that the lowest price is always lower than the highest price.

One way to solve this is by handling the validation and setting the input values dynamically, without using separate states. Here's an updated version of your code that achieves this:

```javascript
const handleSubmit = (e) => {
  e.preventDefault();
  
  const lowestPriceInput = document.getElementById(""lowest_price"");
  const highestPriceInput = document.getElementById(""highest_price"");
  
  const lowestPrice = parseInt(lowestPriceInput.value);
  const highestPrice = parseInt(highestPriceInput.value);
  
  if (lowestPrice <= highestPrice) {
    setLowestPrice(lowestPrice);
    setHighestPrice(highestPrice);
  } else {
    // Swap the values if they are not in the correct order
    setLowestPrice(highestPrice);
    setHighestPrice(lowestPrice);
    
    lowestPriceInput.value = highestPrice;
    highestPriceInput.value = lowestPrice;
  }
};

return (
  <form onSubmit={handleSubmit}>
    <label htmlFor=""lowest_price"">minimum price</label>
    <input
      type=""number""
      id=""lowest_price""
      defaultValue={LowestPrice}
      min={0}
      max={HighestPrice}
    />
    <label htmlFor=""highest_price"">maximum price</label>
    <input
      type=""number""
      id=""highest_price""
      defaultValue={HighestPrice}
      min={LowestPrice}
      max={500}
    />
    <button type=""submit"">apply filters</button>
  </form>
);
```

In the updated code, the `handleSubmit` function first parses the input values as integers. If the lowest price is less than or equal to the highest price, it sets the states normally. Otherwise, it swaps the values and also updates the input elements accordingly to maintain the correct order.

This approach ensures that the lowest price is always lower than the highest price, without the need for a separate temporary state."
76384489,Postgresql Need a query that gives me all the parents that don't have child with a specific status value,"This is the parent table named route




id
start_day
end_day




1
2023/05/01
2023/05/07


2
2023/05/01
2023/05/07


3
2023/05/01
2023/05/07


4
2023/05/01
2023/05/07


5
2023/05/01
2023/05/07




Child table named route_detail




id
route_id
visit_status
point_of_delivery_plant_name
point_of_delivery_plant_number




1
1
5
CROP SOLUTIONS S.A.
563


2
1
5
CROP SOLUTIONS S.A.
563


3
1
5
CROP SOLUTIONS S.A.
563


4
2
0
SAMA S.A.
781


5
3
0
WALTER SAMA HARMS
732


6
4
5
AGROSER S.A.
242


7
4
5
AGROSER S.A.
242


8
5
5
AGROFERTIL S.A
287


9
5
5
AGROFERTIL S.A
287


10
5
5
AGROFERTIL S.A
287




and a third child table named event, for each record route_detail there is 1 event. This is child to route_detail




id
route_detail_id
event_type
event_description




50
1
1
start visit


51
2
2
recurrent form


52
3
3
end visit


53
4
1
start visit


54
5
1
start visit


55
6
1
start visit


56
7
2
recurrent form


57
8
1
start visit


58
9
2
recurrent form


59
10
4
harvest advance




What I'm trying to do is to get all the routes with visit_status = 5 and that don't have events with event_type = 3(end visit)
But I can't manage to get that result
I tried something like this after some research but the queries would still return routes with route_details with the event_type = 3 on them
SELECT r.id,
       r.start_day,
       r.end_day,
       de.point_of_delivery_plant_name,
       de.point_of_delivery_plant_number,
       de.visit_status
FROM route r
JOIN route_detail de ON de.route_id = r.id
WHERE NOT EXISTS (SELECT 1 
                  FROM route ro 
                  JOIN route_detail rd ON rd.route_id = ro.id 
                  JOIN event ev ON ev.route_detail_id = rd.id 
                  WHERE rd.route_id = r.id 
                    AND ev.event_type_id !=7 
                    AND rd.visit_status = '5' 
                    AND rd.id = de.id)
   AND de.visit_status = '5'
GROUP BY 1,2,3,4,5,6
ORDER BY r.id DESC;

This is how my results should look like, since only routes 4 and 5 have visit_status = '5' and their route_details don't have event_type =3
Note: I didn't make the tables




id
start_day
end_day




4
2023/05/01
2023/05/07


5
2023/05/01
2023/05/07



",76384691,"If you want to do it with the EXISTS expression, you can use:

one EXISTS to check the existence of route_detail.visit_status = 5
one EXISTS to check the non-existence of event.event_type = 3 when route_detail.visit_status = 5

SELECT r.*
FROM route r
WHERE EXISTS(SELECT 1
             FROM route_detail rd
             WHERE r.id = rd.route_id
               AND rd.visit_status = 5 )
  AND NOT EXISTS(SELECT 1 
                 FROM       route_detail rd
                 INNER JOIN ""event""      e
                         ON rd.id = e.route_detail_id
                 WHERE r.id = rd.route_id
                   AND e.event_type = 3)

Output:




id
start_day
end_day




4
2023-05-01T00:00:00.000Z
2023-05-07T00:00:00.000Z


5
2023-05-01T00:00:00.000Z
2023-05-07T00:00:00.000Z




Check the demo here.
","To retrieve all the routes that have a visit_status of '5' and do not have any route_detail with an event_type of '3' (end visit), you can use the following query:

```sql
SELECT r.id, r.start_day, r.end_day
FROM route r
WHERE r.id NOT IN (
    SELECT rd.route_id
    FROM route_detail rd
    JOIN event ev ON rd.id = ev.route_detail_id
    WHERE rd.visit_status = 5
    AND ev.event_type = 3
)
AND EXISTS (
    SELECT 1
    FROM route_detail rd
    WHERE rd.route_id = r.id
    AND rd.visit_status = 5
)
```

This query uses a subquery to find all route_detail records with event_type '3' and then selects the routes that are not present in that list. The EXISTS clause is used to ensure that only routes with at least one route_detail having visit_status '5' are included in the result.

Note: In your sample data, the visit_status values are stored as integers, so I used the number '5' instead of the string '5'. If your visit_status column is actually a string, you can change the values accordingly."
76381164,Sum cells by colors based on other colum cells,"I wanna sum cells that have the same color. I know there are some VBA functions to do that. But my problem is kinda specific. I want to sum cells values from a single column, based on cells colors on another column.
I add an example and the code I used. I got the ""#VALUE"" error on the line where I try to access the Interior property.

Function SumByColor(CellColor As Range, rRange As Range)
 Dim cSum As Double
 Dim ColIndex As Integer
 Dim compatedCell As Range  
 Debug.Print (""sumbycolor called"")

 ColIndex = CellColor.Interior.ColorIndex

 For Each cl In rRange
  comparedCell = Worksheets(""HA"").Cells(cl.Row, 1)
  Debug.Print (comparedCell.Interior.ColorIndex) #nothing printed

  If comparedCell.Interior.ColorIndex = ColIndex Then
   cSum = WorksheetFunction.Sum(cl, cSum)
  End If
  Next cl

 SumByColor = cSum

End Function

Thx for your help.
",76381242,"You should dim all your variables.
  Dim cl As Range, comparedCell As Range
  For Each cl In rRange
    Set comparedCell = Worksheets(""HA"").Cells(cl.Row, 1)
    Debug.Print (comparedCell.Interior.ColorIndex) 'nothing printed

    If comparedCell.Interior.ColorIndex = ColIndex Then
         cSum = WorksheetFunction.Sum(cl, cSum)
    End If
  Next cl

As comparedCell is a Range-object you have to use Set.
","It seems like you're trying to use VBA to sum cells based on their colors in one column, using the color information from another column. However, accessing the `Interior` property directly from a range object might not work in all cases. To overcome this issue, you can use conditional formatting to assign a unique value to each color in the cells of the second column, and then use the `Find` function to locate cells with the same color in the first column.

Here's an example of how you can modify your code to achieve this:

```vba
Function SumByColor(CellColor As Range, rRange As Range) As Double
    Dim cSum As Double
    Dim ColIndex As Long
    Dim comparedCell As Range
    Dim foundCell As Range
    
    ColIndex = CellColor.Interior.ColorIndex
    
    For Each comparedCell In rRange
        ' Check if the color matches the specified color index
        If comparedCell.Interior.ColorIndex = ColIndex Then
            ' Find the corresponding cell in the first column based on the unique value assigned through conditional formatting
            Set foundCell = rRange.Offset(0, -1).Find(comparedCell.Value, LookIn:=xlValues, LookAt:=xlWhole)
            
            ' If the cell is found, add its value to the sum
            If Not foundCell Is Nothing Then
                cSum = cSum + foundCell.Value
            End If
        End If
    Next comparedCell
    
    SumByColor = cSum
End Function
```

In this modified code, we use the `Find` function to search for the value in the second column that corresponds to the color being checked in the first column. If a match is found, we add the value from the first column cell to the running sum (`cSum`).

Make sure you have applied conditional formatting to assign unique values to each color in the second column. Once you have done that, you can use the `SumByColor` function in your worksheet to sum the values based on the colors."
76382489,How to delete constructors and operators using inheritance in c++?,"Suppose I have two classes, no_copy and no_move which are base classes. From which any class can derive from and hence have their constructors and operators modified.
As the name suggests, no_copy will literally just do the following; (commented out what it should do)
class base : no_copy
{
    /*
       base(const base&)=delete;
       base& operator=(const base&)=delete;
    */
};

And the same for no_move,
class base : no_move
{
    /*
       base(base&&)=delete;
       base& operator=(base&&)=delete;
    */
};

These classes (no_copy & no_move) make it so that the class that derives from it should not be copyable or moveable.
Right now I am just using a macro to do this;
// For classes without a qualified name
#define __NO_COPY__(__class__) __class__::__class__(const __class__&)=delete; __class__& __class__::operator=(const __class__&)=delete;

// For classes with qualified names
#define __NO_COPY_LIB__(__qualified_name__,__class__) __qualified_name__::__class__(const __class__&)=delete; __class__& __qualified_name__::operator=(const __class__&)=delete;

And the same technique for not moving.

EDIT:
Examples of the macros;
suppose a class as such;
namespace wrapper
{
   class my_class {};

   __NO_COPY__(my_class);
}
__NO_COPY_LIB__(::wrapper::my_class,my_class);


The macros work just fine, but it doesn't look pretty. Moreover they are not easy to refactor as these macros can be anywhere in the definition or implementation.
A syntax like the following would mean that all of this in right there in the definition of the class and can be changed or modified very easily;
class base : public attributes<no_copy,no_move> {};

The attributes struct is just defined as;
template <typename... _BCs/*Base Classes*/>
struct attibutes : public _BCs... {};

Now obviously the syntax of inheriting doesn't matter that much. All I want it to be able to define where the class can be copied,moved or not. Also, if there is a method other than inheritance which could work please do suggest it.
As all I am looking for is the method that is easily refactorable.

I have tried finding solutions online about how to overload constructors. But so far I have not found anything about deleting them and hence I am doubting if it is even possible to delete constructors or operators using inheritance.
Any help would be greatly appreciated.
",76382575,"Special member functions delegate to the corresponding functions of their subobjects (bases and members) and will be defaulted as deleted if that delegation finds a deleted or inaccessible function.

https://timsong-cpp.github.io/cppwp/n4868/class.ctor#class.copy.ctor-10
https://timsong-cpp.github.io/cppwp/n4868/class.copy.assign#7

Although you could use a non-static member for this, using a base class is better because it enables the ""Empty Base Optimization"", not costing anything.
struct no_copy
{
protected:
    no_copy() = default;
    no_copy(const no_copy&) = delete;
    no_copy& operator=(const no_copy&) = delete;
};

class important_class : no_copy {};

pre-C++11 version:
class no_copy // members default to ""private""
{
    no_copy(const no_copy&); // no definition needed
    no_copy& operator=(const no_copy&); // no definition needed
protected:
    no_copy() {}
};

This won't stop the derived class from defining a user-provided special member that explicitly avoids calling the base class version, but if you do that it shows intent to make the derived class actually be copyable/whatever after all.
","In C++, it is not possible to delete inherited constructors or operators directly. The `delete` specifier is used to explicitly delete the special member functions in the class where they are declared, but it cannot be used to delete functions inherited from a base class.

However, you can achieve the desired effect by using a combination of techniques such as private inheritance, using declarations, and making the base class constructor and operators private. Here's an example:

```cpp
class no_copy
{
protected:
    no_copy() = default;  // Default constructor is protected

private:
    no_copy(const no_copy&) = delete;  // Private copy constructor (deleted)
    no_copy& operator=(const no_copy&) = delete;  // Private copy assignment operator (deleted)
};

class no_move
{
protected:
    no_move() = default;  // Default constructor is protected

private:
    no_move(no_move&&) = delete;  // Private move constructor (deleted)
    no_move& operator=(no_move&&) = delete;  // Private move assignment operator (deleted)
};

class base : private no_copy, private no_move
{
public:
    // Your class implementation here...
};
```

By using private inheritance, the derived class `base` has access to the members of the base classes `no_copy` and `no_move`, but these members are not accessible from outside the `base` class.

Note that the default constructors of `no_copy` and `no_move` are protected to prevent direct instantiation of these base classes. However, the derived class `base` can still call the default constructor of `no_copy` and `no_move` implicitly.

This technique effectively prevents copying or moving instances of the `base` class, as the copy and move constructors/operators are not accessible outside the `base` class due to private inheritance.

Using this approach, you don't need macros, and the behavior is directly defined within the class definition itself, making it easier to refactor and modify."
76384393,Nest.js empty Body for form data,"I have a simple controller, in this controller I have this endpoint
@Post('/temp')
async asdf(
  @Body() form: Record<string, string>,
  @Res({ passthrough: true }) response: Response,
) {
  this.logger.debug(JSON.stringify(form));
  await response.json({ ok: true, form: JSON.stringify(form) });
}

When I try to POST some form data on it, using cURL or the browser, the object form is empty.
Example:
curl -X POST http://localhost:4000/mycontroller/temp -H ""Content-Type: application/x-www-form-urlencoded"" -d ""param1=value1&param2=value2""

Results in

{""ok"":true,""form"":""{}""}

Other controllers work; I can't see any difference between my controller and the endpoint to others.
What I'm doing wrong or missing?
",76384692,"If you're using form data you need to implement a form data parser, like busboy or multer. Nest integrates with multer and express already via the FileInterceptor and its variants. This will force multer to parse the request. If you don't use any files, just the form data format, I believe there is a NoFileInterceptor or similar.

Looks like there is no NoFileInterceptor. You could use AnyFileInterceptor instead and ignore the req.files, just be aware it could end up having your server taken down if a really nasty set of files comes in for multer to parse
","In your code, you're using the `@Body()` decorator from Nest.js to parse the request body into an object. By default, Nest.js expects JSON data in the request body. However, in your cURL command, you're sending form data in the `application/x-www-form-urlencoded` format.

To handle form data instead of JSON data, you need to use a different decorator: `@Body('key')` with a key parameter specifying the name of the form data field. Here's an updated version of your code to handle form data:

```typescript
import { Body, Controller, Post, Res } from '@nestjs/common';
import { Response } from 'express';

@Controller('mycontroller')
export class MyController {
  @Post('/temp')
  async asdf(
    @Body('param1') param1: string,
    @Body('param2') param2: string,
    @Res({ passthrough: true }) response: Response,
  ) {
    const form = { param1, param2 };
    this.logger.debug(JSON.stringify(form));
    await response.json({ ok: true, form: JSON.stringify(form) });
  }
}
```

With this code, the `param1` and `param2` variables will contain the values sent in the form data. You no longer need to use the `Record<string, string>` type for the `form` parameter.

Now, if you send the same cURL command, you should see the correct form data in the response:

```json
{""ok"":true,""form"":""{\""param1\"":\""value1\"",\""param2\"":\""value2\""}""}
```

Make sure to adjust the code according to your specific needs, such as adding proper error handling or parsing the form data into a different structure if necessary."
76381056,How to construct a graph using a list of tuples in python in networkX?,"I am trying to make a graph from a list of tuples stored in a variable. I found G.add_edges_from(e) for making graph using list of tuples. but the problem is that this does not work and when i try to for example print the graph it returns None. I appreciate answers that solve my problem. I use the code below to make the graph:
import networkx as nx

e = [(1,2),(1,3),(2,3)]
G = nx.Graph()
g1 = G.add_edges_from(e)
print(g1)


Update:
I testes this code but again give None when trying to print:
e = [[(1,2),(1,3),(2,3)],[(10,20),(10,30),(20,30)]]
graph_list = []
for i in e:
    graph_list.append(nx.Graph().add_edges_from(i))

print(graph_list[0].nodes)


",76381247,"Let's break it down shall we?
You assigned a list of edges in e, then you made a graph with G.
However, your issue is you're trying to assign g1 to what the method add_edges_from returns (which is None).
What you actually want is something like this:
import networkx as nx

e = [(1,2),(1,3),(2,3)]
G = nx.Graph()
G.add_edges_from(e)
print(G)

Since the add_edges_from method returns None it is working as intended, you should try printing your original graph instead. I hope this helps and clarifies things for you!
Edit:
If you insist on just using the list of tuples, you can just do away with variables. Use lists to store graph objects instead, keep storing them in a loop as such:
e = [[(1,2),(1,3),(2,3)],[(10,20),(10,30),(20,30)]]
graph_list = []
for i in e:
    G = nx.Graph()
    G.add_edges_from(i)
    graph_list.append(G)

print(graph_list[0].nodes)
print(graph_list[1].nodes)

Then you can use indices to get each specific graph you make (which would be stored separately in the list)
Or you might want to start dumping each graph in a json file ([answered here] (Method to export networkx graph to json graph file?))
Which can solve your RAM issue
","The reason you are getting `None` when printing `g1` or `graph_list[0].nodes` is because both `G.add_edges_from(e)` and `nx.Graph().add_edges_from(i)` return `None`. These methods modify the graph in place and do not return anything. Therefore, assigning the result of `G.add_edges_from(e)` or `nx.Graph().add_edges_from(i)` to a variable will result in `None`.

To construct a graph using a list of tuples in NetworkX, you can follow the code below:

```python
import networkx as nx

e = [(1, 2), (1, 3), (2, 3)]
G = nx.Graph()
G.add_edges_from(e)

print(G.nodes)
```

This code will print the nodes of the graph `G`. Note that you don't need to assign the result of `G.add_edges_from(e)` to a variable because it modifies `G` directly.

If you want to create multiple graphs from a list of tuple lists, you can modify your code as follows:

```python
import networkx as nx

e = [[(1, 2), (1, 3), (2, 3)], [(10, 20), (10, 30), (20, 30)]]
graph_list = []

for i in e:
    G = nx.Graph()
    G.add_edges_from(i)
    graph_list.append(G)

print(graph_list[0].nodes)
```

In this updated code, each sub-list of tuples in `e` is used to create a new graph `G`. The graphs are then appended to `graph_list`. Finally, you can access the nodes of the first graph in `graph_list` using `graph_list[0].nodes`."
76381127,Why is strtol() returning 0x7fffffff instead of the expected 0xAABBCCDD?,"// msvc 17.3.5
// sizeof (long) : 4
#include <stdlib.h>
int main(){
    long i = 0xAABBCCDD;// 0x AABBCCDD in debugger window

    char number[] = ""AABBCCDD EEFF"";
    char* p;
    long var = strtol ( number, &p, 16 );// 0x 7FFF FFFF
    perror ( ""?:"" ); // ?:: Result too large
}

I tried char number[] = ""AABBCC EEFF""; and it works fine.
I expect 0xAABBCCDD inside var instead 0x7fffffff.
What is wrong?
",76381250,"From the C Standard (7.22.1.4 The strtol, strtoll, strtoul, and strtoull functions)

8 The strtol, strtoll, strtoul, and strtoull functions
return the converted value, if any. If no conversion could be
performed, zero is returned. If the correct value is outside the range
of representable values, LONG_MIN, LONG_MAX, LLONG_MIN,
LLONG_MAX, ULONG_MAX, or ULLONG_MAX is returned (according to the return type and sign of the value, if any), and the value of
the macro ERANGE is stored in errno.

The positive hexadecimal constant 0xAABBCCDD can not be represented in an object of the signed type long int provided that sizeof( long int ) is equal to 4.
For example try this demonstration program
#include <stddef.h>
#include <stdio.h>

int main( void )
{
    printf( ""%#X\n"", LONG_MAX );
}

The program output is
0X7FFFFFFF

Note: as in this case sizeof( long ) is equal to sizeof( unsigned int ) and the value is representable in an object of the type unsigned int there is used the conversion specifier X. Otherwise you need to include header <inttypes.h> and to use a macro as for example PRIX32.
As you can see LONG_MAX (the maximum positive value that can be stored in an object of the type long int)  is less than the positive hexadecimal constant 0xAABBCCDD.
Instead of using the function strtol use function strtoul
unsigned long var = strtoul ( number, &p, 16 );

Or if you want to deal with signed integers then use function strtoll.
","The issue in your code lies with the `number` array. 

In C, a string literal enclosed in double quotes (`""""`) is null-terminated, meaning it includes an extra null character at the end. However, in your code, you have defined the `number` array as `char number[] = """"AABBCCDD EEFF"""";`, which includes spaces between the hexadecimal values. 

When `strtol` encounters a non-hexadecimal character, such as a space, it stops parsing and returns the value accumulated so far. In this case, it stops at the first space after `""AABBCCDD""` and returns `0xAABBCCDD`, which is within the expected range.

However, the `p` pointer passed to `strtol` is used to determine the end of the parsed value. Since `strtol` stops at the space character, `p` points to the space character in `number`.

When you assign the return value of `strtol` to `var`, it receives `0xAABBCCDD` as expected, but the `perror` function is called with an empty string `""""?: """"`. According to the C standard, if the input string to `strtol` is not completely consumed (i.e., if there are unprocessed characters remaining), the value returned is the maximum value that can be represented by `long` (0x7FFFFFFF for a 32-bit `long`).

To resolve this issue, you need to remove the spaces from the `number` array definition. Here's the corrected code:

```c
#include <stdlib.h>
#include <stdio.h>

int main() {
    long i = 0xAABBCCDD;
    char number[] = ""AABBCCDDEEFF"";
    char* p;
    long var = strtol(number, &p, 16);
    if (*p != '\0') {
        perror(""Invalid input"");
    } else {
        printf(""var = 0x%lX\n"", var);
    }
    return 0;
}
```

Now, `number` contains the hexadecimal string `""AABBCCDDEEFF""` without any spaces, and the output should correctly display `var = 0xAABBCCDDEEFF`."
76383877,"How to find out which package depends on ""futures"" in requirements.txt","I have defined many pip packages in a requirements.txt, but I have not define the ""futures"" package:
...
future == 0.18.3
six == 1.16.0
joblib == 1.2.0
...

And then download all packages with the following command on Ubuntu 22.04:
pip3.9 download -r ""/home/requirements.txt""

The above command exited with the following error:
...
...
Collecting widgetsnbextension~=4.0.7
  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 3.9 MB/s eta 0:00:00
Collecting branca>=0.5.0
  Downloading branca-0.6.0-py3-none-any.whl (24 kB)
Collecting traittypes<3,>=0.2.1
  Downloading traittypes-0.2.1-py2.py3-none-any.whl (8.6 kB)
Collecting xyzservices>=2021.8.1
  Downloading xyzservices-2023.5.0-py3-none-any.whl (56 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 KB 1.3 MB/s eta 0:00:00
Collecting futures
  Downloading futures-3.0.5.tar.gz (25 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [25 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 14, in <module>
        File ""/python39/lib/python3.9/site-packages/setuptools/__init__.py"", line 18, in <module>
          from setuptools.dist import Distribution
        File ""/python39/lib/python3.9/site-packages/setuptools/dist.py"", line 32, in <module>
          from setuptools.extern.more_itertools import unique_everseen
        File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
        File ""<frozen importlib._bootstrap>"", line 565, in module_from_spec
        File ""/python39/lib/python3.9/site-packages/setuptools/extern/__init__.py"", line 52, in create_module
          return self.load_module(spec.name)
        File ""/python39/lib/python3.9/site-packages/setuptools/extern/__init__.py"", line 37, in load_module
          __import__(extant)
        File ""/python39/lib/python3.9/site-packages/setuptools/_vendor/more_itertools/__init__.py"", line 1, in <module>
          from .more import *  # noqa
        File ""/python39/lib/python3.9/site-packages/setuptools/_vendor/more_itertools/more.py"", line 5, in <module>
          from concurrent.futures import ThreadPoolExecutor
        File ""/tmp/pip-download-jelw4tc2/futures/concurrent/futures/__init__.py"", line 8, in <module>
          from concurrent.futures._base import (FIRST_COMPLETED,
        File ""/tmp/pip-download-jelw4tc2/futures/concurrent/futures/_base.py"", line 357
          raise type(self._exception), self._exception, self._traceback
                                     ^
      SyntaxError: invalid syntax
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> futures

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

How to find out which package depends on the ""futures"" from the ""requirements.txt""?
Here is the dummy code:
# find_out_depends --requirement-file ""/home/requirements.txt"" --find-depends ""futures""

Is there any ""find_out_depends"" command for accepting requirements.txt as argument and then print out the whole dependencies tree?
",76384741,"Create a fresh Python 3.9 venv and install your requirements without dependencies:
python3.9 -m pip install --no-deps requirements.txt

Then run the pip check CLI:
python3.9 -m pip check

It will complain that some package(s) have unmet dependencies, and you should find futures somewhere in there. Not to be confused with future, which is cross-compat.
","To find out which package depends on ""futures"" in your requirements.txt file, you can use the `pipdeptree` package. `pipdeptree` is a Python package that helps you visualize the dependency tree of your installed packages.

First, make sure you have `pipdeptree` installed. You can install it by running the following command:

```
pip3.9 install pipdeptree
```

Once installed, you can use the following command to find out the dependencies:

```
pipdeptree -r /home/requirements.txt
```

This command will display a tree-like structure showing the dependencies of the packages listed in your requirements.txt file. You can look for the package that depends on ""futures"" in the output.

Note that `pipdeptree` analyzes the installed packages on your system, so make sure you have all the required packages installed before running the command.

Unfortunately, there is no built-in command like ""find_out_depends"" that accepts a requirements.txt file as an argument and prints out the entire dependency tree. However, you can use the `pipdeptree` command mentioned above to achieve a similar result."
76384647,Grouping Semesters into Academic Years generalization,"I have about a specific section of my code. The loop inputs semester files, computes new columns and outputs a data set with the new variables. The loop works beautifully, however making the Acad_Year variable is stagnant, I am looking for a way to make it more flexible so that I won't need to go in and re-write the case_when statement every time there is a new dataset. Sample data is available.  Thank you in advance!
{r setup}

require(""knitr"")
 setwd(""~/Downloads/Stack Overflow/"")


library(dplyr)
library(tidyr)
library(writexl)

PhGrad <- rbind(PhGrad_08, PhGrad_SP_23) %>% 
  filter(!BannerID== """")

d <- tibble(
  filename = list.files(), 
  Sem = gsub("".*(Fall|Spring|Summer).*"", ""//1"", filename), 
  Year = gsub("".*(//d{2}).*"", ""//1"", filename), 
  grp = gsub("".*(ASPH|ID).*"", ""//1"", filename)) %>% 
  pivot_wider(names_from = ""grp"", values_from=""filename"")

res <- vector(mode=""list"", length=nrow(d))
names(res) <- paste(d$Sem, d$Year, sep=""_"")

for(i in seq_along(res)){
  ASPH <- rio::import(d$ASPH[i])
  ID <- rio::import(d$ID[i])
  
res[[i]] <- bind_rows(ASPH, ID) %>%
    distinct(ID, Program, .keep_all = T) %>% 
    rowwise() %>% 
    mutate(racecount= sum(c_across(`Race-Am Ind`:`Race- Caucasian`)== ""Y"", na.rm=T)) %>% 
    ungroup() %>% 
    mutate(racecode= case_when(Citizenship %in% list(""NN"", ""NV"") ~ ""foreign_national"",
                               `Race- Hispanic`== ""Y"" ~ ""hispanic_latino"", 
                                racecount >1 ~ ""two_or_more_races"",
                               `Race-Am Ind`== ""Y""  ~ ""american_indian_alaskan_native"",
                               `Race- Asian`== ""Y""  ~ ""asian"",
                               `Race-Afr Amer`== ""Y""  ~ ""black_african_american"",
                               `Race- Hawaiian` == ""Y""  ~ ""native_hawaiian_pacific_islander"",
                               `Race- Caucasian`== ""Y"" ~ ""white"",
                               `Race-Not Rept`== ""Y"" ~ ""race_unknown"",
                               TRUE~ ""race_unknown""),
           gender_long= case_when(Gender== ""F""~ ""Female"",
                                  Gender== ""M""~ ""Male"",
                                  Gender== ""N""~ ""Other"",
                                  TRUE~ ""other""),
           DEPT= case_when(Program %in% list(""3GPH363AMS"", ""3GPH363AMSP"", ""3GPH378AMCD"", ""3GPH378AMS"", ""3GPH379APHD"")~ ""COMD"",
                           Program %in% list(""3GPH593AMPH"", ""3GPH593AMS"", ""3GPH593APHD"", ""3GPH569ACGS"")~ ""ENHS"",
                           Program %in% list(""3GPH596AMS"", ""3GPH596AMSPH"", ""3GPH596APHD"",""3GPH594AMPH"", ""3GPH594AMS"", ""3GPH594AMSPH"", ""3GPH594APHD"", ""3GPH586APBAC"")~ ""EPID/BIOS"", 
                           Program %in% list(""3GPH331AMS"",""3GPH331APHD"",""3GPH334AMS"",""3GPH335ADPT"", ""3GPH377AMS"", ""3GPH388AMS"", ""3GPH588AMPH"", ""3GPHJ331MS"", ""3UPH331ABS"")~ ""EXSC"",
                           Program %in% list(""3GPH568APBAC"",""3GPH592ACGS"",""3GPH592AMPH"", ""3GPH592APHD"", ""3GPH576ACGS"", ""3GPH121ACGS"", ""3GID635ACGS"")~ ""HPEB"",
                           Program %in% list(""3GPH591AMPH"", ""3GPH591APHD"", ""3GPH597AMHA"",""3GPH591ADPH"")~ ""HSPM"",
                           TRUE~ ""Missing""), 
           degree_delivery_type= case_when(`First Concentration`== ""R999"" | `Second Concentration`== ""R999"" ~ ""Distance-based"",
                                           `First Concentration`== ""3853"" | `Second Concentration`== ""3853"" ~ ""Executive"", 
                                           TRUE~ ""Campus-based""),
 # FTE_compute= case_when(Level== ""GR"" & `Course Hours`<9 ~ round(`Course Hours`/9, #digits=2),
 #                                  Level== ""GR"" & `Course Hours`>=9~ 1,
 #                                 Level== ""UG"" & `Course Hours`<12~ round(`Course Hours`/12, 
 #digits=2),
 #                                  Level== ""UG"" & `Course Hours`>=12 ~ 1),
 #          Full_Part_Status=case_when((Level== ""GR"" & `Course Hours` <9)| (Level== ""UG"" & 
 #`Course Hours`<12)~""parttime_status"",
 #                                      (Level==""GR"" & `Course Hours`>=9)|(Level== ""UG"" & `Course 
 #Hours`>=12)~""fulltime_status"",
 #                                       TRUE~ ""other""),
           Sem_Year= paste0(d$Sem[i],""_"",d$Year[i]),
           StudentCount= 1,
      Acad_Year= case_when(Sem_Year %in% list(""Fall_18"", ""Spring_19"", ""Summer_19"")~ ""AY2018-19"",
                                 Sem_Year %in% list(""Fall_19"", ""Spring_20"", ""Summer_20"")~ ""AY2019-20"",
                                 Sem_Year %in% list(""Fall_20"", ""Spring_21"", ""Summer_21"")~ ""AY2020-21"",
                                 Sem_Year %in% list(""Fall_21"", ""Spring_22"", ""Summer_22"")~ ""AY2021-22"",
                                 Sem_Year %in% list(""Fall_22"", ""Spring_23"")~ ""AY2022-23""),
      Deg_group = case_when(Degree %in% list(""DPT"", ""PHD"", ""DPH"")~ ""Doctorate"",
                            Degree %in% list(""MSP"", ""MCD"", ""MPH"", ""MHA"", ""MS"",""MSPH"")~ ""Masters"",
                            Degree %in% list(""CGS"", ""PBACC"")~ ""Certificate"")) %>% 
    left_join(., PhGrad %>% mutate_at(vars(BannerID), ~as.character(.)), by= c(""ID""=""BannerID"", ""DEPT""), unmatched= ""drop"", relationship= ""many-to-many"") %>% 
  mutate(New_Deg= case_when(is.na(Degree.y)== T~ Degree.x,
                     is.na(Degree.y)== F~ Degree.y,
                          TRUE~ ""Error"")) %>% 
  select(-c(ApplicationID:StudentStatus))
}

",76384769,"library(dplyr)
data.frame(Sem_Year = c(""Fall_21"", ""Spring_22"", ""Summer_22"",
                        ""Fall_31"", ""Spring_32"", ""Summer_32"")) %>%
  
  tidyr::separate(Sem_Year, c(""Sem"",""Yr""), convert = TRUE, remove = FALSE) %>%
  mutate(AY_end = Yr + if_else(Sem == ""Fall"", 1, 0),
         Acad_Year = paste0(""AY20"", AY_end - 1, ""-"", AY_end)) %>%
  select(-c(Sem, Yr, AY_end))

Result (Reminder: update in 2099)
   Sem_Year Acad_Year
1   Fall_21 AY2021-22
2 Spring_22 AY2021-22
3 Summer_22 AY2021-22
4   Fall_31 AY2031-32
5 Spring_32 AY2031-32
6 Summer_32 AY2031-32

","In your code, the `Acad_Year` variable is currently hardcoded with specific semesters and years in the `case_when` statement. If you want to make it more flexible so that you don't have to manually update it every time there is a new dataset, you can create a separate lookup table or mapping that defines the mapping between semesters/years and academic years.

Here's an example of how you can modify your code to achieve this:

1. Create a separate data frame that maps semesters/years to academic years. You can define this mapping based on your specific requirements and add more rows as needed. For example:

```R
# Create a mapping between Sem_Year and Acad_Year
year_mapping <- data.frame(
  Sem_Year = c(""Fall_18"", ""Spring_19"", ""Summer_19"", ""Fall_19"", ""Spring_20"", ""Summer_20"", ""Fall_20"", ""Spring_21"", ""Summer_21"", ""Fall_21"", ""Spring_22"", ""Summer_22"", ""Fall_22"", ""Spring_23""),
  Acad_Year = c(""AY2018-19"", ""AY2018-19"", ""AY2018-19"", ""AY2019-20"", ""AY2019-20"", ""AY2019-20"", ""AY2020-21"", ""AY2020-21"", ""AY2020-21"", ""AY2021-22"", ""AY2021-22"", ""AY2021-22"", ""AY2022-23"", ""AY2022-23"")
)
```

2. Modify the `res` data frame creation code to include a join with the `year_mapping` data frame to retrieve the corresponding `Acad_Year` based on the `Sem_Year`. Replace the existing `Acad_Year` assignment in the `mutate` statement with the join operation. Here's the modified code:

```R
res[[i]] <- bind_rows(ASPH, ID) %>%
  distinct(ID, Program, .keep_all = TRUE) %>%
  rowwise() %>%
  mutate(racecount = sum(c_across(`Race-Am Ind`:`Race- Caucasian`) == ""Y"", na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(
    racecode = case_when(
      Citizenship %in% c(""NN"", ""NV"") ~ ""foreign_national"",
      `Race- Hispanic` == ""Y"" ~ ""hispanic_latino"",
      racecount > 1 ~ ""two_or_more_races"",
      `Race-Am Ind` == ""Y"" ~ ""american_indian_alaskan_native"",
      `Race- Asian` == ""Y"" ~ ""asian"",
      `Race-Afr Amer` == ""Y"" ~ ""black_african_american"",
      `Race- Hawaiian` == ""Y"" ~ ""native_hawaiian_pacific_islander"",
      `Race- Caucasian` == ""Y"" ~ ""white"",
      `Race-Not Rept` == ""Y"" ~ ""race_unknown"",
      TRUE ~ ""race_unknown""
    ),
    gender_long = case_when(
      Gender == ""F"" ~ ""Female"",
      Gender == ""M"" ~ ""Male"",
      Gender == ""N"" ~ ""Other"",
      TRUE ~ ""other""
    ),
    DEPT = case_when(
      Program %in% c(""3GPH363AMS"", ""3GPH363AMSP"", ""3GPH378AMCD"", ""3GPH378AMS"", ""3GPH379APHD"") ~ ""COMD"",
      Program %in% c(""3GPH593AMPH"", """
76381002,Is this a race condition in go,"func main() {
    m := map[string]int{
        ""foo"": 42,
        ""bar"": 1337,
    }

    go func() {
        time.Sleep(1 * time.Second)
        tmp := map[string]int{
            ""foo"": 44,
            ""bar"": 1339,
        }

        m = tmp
    }()

    for {
        val := m[""foo""]
        fmt.Println(val)
    }
}

I saw this in many packages.
Why this is not considered as a race condition?
go run -race . giving no error.
",76381254,"As pointed out by @Volker, this is a data race. And since there is only one write, it's hard to be detected. Here is a modified demo to make it easy to trigger the data race error:
package main

import (
    ""fmt""
    ""time""
)

func main() {
    m := map[string]int{
        ""foo"": 42,
        ""bar"": 1337,
    }

    done := make(chan any)

    go func() {
        for i := 0; i < 100; i++ {
            time.Sleep(time.Microsecond)
            tmp := map[string]int{
                ""foo"": 44,
                ""bar"": 1339,
            }

            m = tmp
        }

        close(done)
    }()

    for {
        select {
        case <-done:
            return
        default:
            val := m[""foo""]
            fmt.Println(val)
        }
    }
}

","The code you provided does not contain a race condition because there are no concurrent operations or shared mutable state being accessed concurrently. In Go, a race condition occurs when multiple goroutines access shared data concurrently, and at least one of the accesses is a write operation.

In the code snippet you provided, the only goroutine spawned is the one within the `go` statement. It sleeps for 1 second and then assigns a new map to the variable `m`. However, the `for` loop that accesses the `m` map is executed sequentially and does not modify the map. Since there is no concurrent write access to the `m` map, there is no race condition in this code.

When you run the code with the `-race` flag (`go run -race .`), the Go race detector is invoked. It detects race conditions by instrumenting the code and tracking accesses to shared variables. If a race condition is detected, the race detector will report an error. In this case, the race detector does not report any errors because there are no race conditions present in the code.

It's important to note that although this specific code snippet does not exhibit a race condition, it doesn't mean that all code containing concurrent operations or shared mutable state is free from race conditions. It's still crucial to carefully manage shared state and ensure proper synchronization to avoid race conditions in concurrent Go programs."
76382398,"Condition to filter records with ""in"" and ""and"" operators","



ID
name
isSearchable




1
foo
true


2
bar
true


3
zar
false




I've got some ids and I need to filter records where they have isSearchable = true.
This query give as result ID = 1 because is searchable, but I would to apply the filter isSearchable to the entire result, not row-by-row.
SELECT *
FROM my_table
WHERE id IN (1, 3)
  AND isSearchable = true

So in this case I'm expecting no-results because both records should be in first isSearchable and after that, filter the ids.
I've tried experimenting with sub-query etc but the in operator (or the or operator) but I'm not able to accomplish the result.
Maybe is something really simple, but I've no ideas on how to solve.
Thanks for your help.
",76382590,"One approach using a window function:
SELECT ID 
FROM (SELECT ID,
             MIN(isSearchable::INT) OVER() AS minSearchable
      FROM my_table
      WHERE id IN (1,3)) cte 
WHERE minSearchable = 1

Check the demo here.
","To filter the records based on the `isSearchable` condition for the entire result rather than row-by-row, you can use a subquery or a common table expression (CTE) to first select the IDs that meet the `isSearchable = true` condition, and then join this result with your main table using the `id` column. Here's an example query that achieves this:

```sql
WITH searchable_ids AS (
  SELECT id
  FROM my_table
  WHERE id IN (1, 3)
    AND isSearchable = true
)
SELECT mt.*
FROM my_table mt
JOIN searchable_ids si ON mt.id = si.id;
```

This query creates a CTE named `searchable_ids` that selects the IDs from `my_table` where `id` is in (1, 3) and `isSearchable` is true. Then, it joins this CTE with the `my_table` table on the `id` column to retrieve the desired records.

With the provided sample data, the above query would return no results because neither ID 1 nor ID 3 have `isSearchable` set to true."
76382293,Converting Bash Option String to Unix Environment Variable Name using vscode snippet Transformation,"I am working on a bash script snippet for the sheller extension where I need to convert a Bash option string entered by the user into a valid Unix environment variable name. For example, the user may enter an option string like ""-my-option-name"" or ""--another_option="", and I need to transform it into a valid environment variable name like ""MY_OPTION_NAME"" or ""ANOTHER_OPTION"".
To clarify my requirements and provide a clear understanding of the desired transformation, I have created a JavaScript example on JSFiddle. You can find it here. The example showcases a table of different input strings and their expected output after the transformation.
The javascript function handling the transformation used in the fiddle.
function stripCapitalizeAndSnake(inputString) {
  const regex = /^-+(.*?)(?:=)?$/;
  const result = inputString.replace(regex, (_, selectedString) => {
    const transformedString = selectedString.replace(/-/g, '_');
    return transformedString.toUpperCase();
  });
  return result;
}

I am seeking guidance or preferably code examples on how to implement this transformation using snippet transforms in Visual Studio Code snippets.

Thank you in advance for your assistance, especially if you have experience or knowledge about the vscode, code snippets and transformations!

Here is a simplified version of a snippet, which I hope someone can modify for the expected result.
Current snippet
{
    ""OPTION TO ENVIRONMENT"": {
        ""prefix"": ""option to environment"",
        ""body"": [
            ""#!/usr/bin/env bash"",
            """",
            ""OPTION=\""${1:--quiet-mode}\"""",
            """",
            ""echo \""Option        :\\${OPTION}\"""",
            ""echo \""Variable name :${1/^(\\-+)([^=]+)(=)?$/${2:/upcase}/}\""""
        ],
        ""description"": ""Convert bash option to a valid shell environment variable""
    }
}

Current Result
When triggering the snippet, and leaving the default value as is the Variable name is correct except for the fact that ""-"" is not replaced with ""_"" which is my problem.
""QUIET-MODE"" needs to be transformed to ""QUIET_MODE"".
#!/usr/bin/env bash

OPTION=""--quiet-mode""

echo ""Option        :${OPTION}""
echo ""Variable name :QUIET-MODE""

I have been trying to do this for a while now, so I ask you, do you know how to do this?
Ps.
Do you know of a better documentation about transformations, other than is mentioned here?
",76382615,"use the extension Hypersnips
you can add JavaScript to the snippet

It can be done with VSC standard snippets

capture possible starting - characters, ignore in result
capture all till - or =, UPCASE that group
capture possible -, substitute with _
capture possible =, ignore in result

apply these rules global/repeatedly
{
  ""OPTION TO ENVIRONMENT"": {
    ""prefix"": ""option to environment"",
    ""body"": [
        ""#!/usr/bin/env bash"",
        """",
        ""OPTION=\""${1:--quiet-mode}\"""",
        """",
        ""echo \""Option        :\\${OPTION}\"""",
        ""echo \""Variable name :${1/(^-+)?([^-=]+)(-?)(=?)/${2:/upcase}${3:+_}/g}\""""
    ],
    ""description"": ""Convert bash option to a valid shell environment variable""
}

","To achieve the desired transformation in the Visual Studio Code snippet, you can use the `${1/regex/format}` syntax for transforming the input option string. Here's an updated version of the snippet that performs the necessary transformation:

```json
{
    ""OPTION TO ENVIRONMENT"": {
        ""prefix"": ""option to environment"",
        ""body"": [
            ""#!/usr/bin/env bash"",
            """",
            ""OPTION=\""${1:--quiet-mode}\"""",
            """",
            ""echo \""Option        :\\${OPTION}\"""",
            ""echo \""Variable name :\\${1/^(-+)([^=]+)(=)?$/${2:/downcase}/}\""""
        ],
        ""description"": ""Convert bash option to a valid shell environment variable""
    }
}
```

In the updated snippet, `${2:/downcase}` is used in the last line to convert the transformed string to lowercase. Since environment variable names are typically case-sensitive, using lowercase is a common convention.

By modifying the last line, `${1/^(-+)([^=]+)(=)?$/${2:/downcase}/}`, we use the regular expression `^(-+)([^=]+)(=)?$` to capture the option string and transform it accordingly. The `(-+)` captures one or more hyphens at the start, `([^=]+)` captures one or more characters that are not an equals sign, and `(=)?` captures an optional equals sign. Finally, `${2:/downcase}` converts the captured string to lowercase.

Regarding your question about better documentation on transformations, the official Visual Studio Code documentation on snippets provides a comprehensive guide on snippet transforms, including various examples and use cases. You can find it here: [Visual Studio Code Snippet Syntax](https://code.visualstudio.com/docs/editor/userdefinedsnippets#_snippet-syntax).

I hope this helps! Let me know if you have any further questions."
76381019,How can I create a dynamic list in Terraform that combines a list of strings with a list of maps?,"Merge list with another list of map in terraform
We have a list listA and a list of map, mapA as below
listA = [""cluster-0"",""cluster-1""]


mapA = [
{
  auto_upgrade       = false
  disk_size_gb       = 100
  disk_type          = ""pd-standard""
  node_pool_labels            = {
    agentpool = ""np-1""
  }
},
{
  auto_upgrade       = false
  disk_size_gb       = 50
  disk_type          = ""pd-balanced""
  node_pool_labels            = {
    agentpool = ""np-2""
  }
},
{
 auto_upgrade                = false
 disk_size_gb                = 100
 disk_type                   = ""pd-standard""
 node_pool_labels            = {
    agentpool = ""np-3""
 }
}
]


I am trying to create a new list which should look like
listB = [
""cluster-0"" = [{
  auto_upgrade       = false
  disk_size_gb       = 100
  disk_type          = ""pd-standard""
  node_pool_labels   = {
      agentpool = ""np-1""
   }
 },
 {
  auto_upgrade       = false
  disk_size_gb       = 50
  disk_type          = ""pd-balanced""
  node_pool_labels   = {
      agentpool = ""np-2""
   }
 },
 {
  auto_upgrade                = false
  disk_size_gb                = 100
  disk_type                   = ""pd-standard""
  node_pool_labels            = {
      agentpool = ""np-3""
  }
}], 

""cluster-1""= [{
  auto_upgrade       = false
  disk_size_gb       = 100
  disk_type          = ""pd-standard""
  node_pool_labels   = {
      agentpool = ""np-1""
   }
 },
 {
  auto_upgrade       = false
  disk_size_gb       = 50
  disk_type          = ""pd-balanced""
  node_pool_labels   = {
      agentpool = ""np-2""
   }
 },
 {
  auto_upgrade                = false
  disk_size_gb                = 100
  disk_type                   = ""pd-standard""
  node_pool_labels            = {
      agentpool = ""np-3""
  }
}]
] 

I have tried zipmap which works when listA has two elements and mapA has got two elements like only np-1, np-2 but fails when we add np-3. Trying to make this dynamic listB.
",76381291,"You can use:
listB = [{for idx, value in local.listA: value => local.mapA }]

so iterate over listA to create a new list where the elements are a dict of which the key is the original element in listA, and the value mapA.
","To achieve the desired result of creating a new list `listB` by combining `listA` and `mapA`, you can use the following approach in Terraform:

```hcl
locals {
  listA = [""cluster-0"", ""cluster-1""]

  mapA = [
    {
      auto_upgrade       = false
      disk_size_gb       = 100
      disk_type          = ""pd-standard""
      node_pool_labels   = {
        agentpool = ""np-1""
      }
    },
    {
      auto_upgrade       = false
      disk_size_gb       = 50
      disk_type          = ""pd-balanced""
      node_pool_labels   = {
        agentpool = ""np-2""
      }
    },
    {
      auto_upgrade       = false
      disk_size_gb       = 100
      disk_type          = ""pd-standard""
      node_pool_labels   = {
        agentpool = ""np-3""
      }
    }
  ]

  listB = [
    for cluster in local.listA : {
      cluster_name = cluster
      cluster_data = [for data in local.mapA : data if data.node_pool_labels.agentpool == cluster]
    }
  ]
}
```

In the above code, we define a local variable called `listB` that uses a list comprehension to iterate over `listA`. For each cluster in `listA`, it creates an object with two attributes: `cluster_name` (set to the current cluster value) and `cluster_data` (filtered list of maps from `mapA` where the `agentpool` matches the current cluster). This achieves the desired structure of `listB`.

Note that I made some minor adjustments to your original code to ensure compatibility with Terraform syntax:

- Changed double quotes (`""""`) to single quotes (`""`) for string values within the map elements.
- Removed spaces around the equal sign (`=`) in map assignments.

Feel free to adjust the code according to your specific requirements, and let me know if you have any further questions!"
76382245,UA_Client_Service_read only yields empty responses,"I am trying to create a small program in C which will poll a frame of data over a opc/ua connection using the open62541 library and the forward it to a kafka server.
Everything works fine when fetching the values from the nodes separately but I would like to use a UA_ReadRequest for that. The problem is that I am only receiving empty responses.
The opc/ua server is coded with in python using the freeopc package.
This is the function that tries tu use a UA_ReadResponse to fetch a several values for specified nodeIDs:
void retrieveOPCData(void)
{
    UA_ReadRequest request;
    UA_ReadRequest_init(&request);
    UA_ReadValueId ids[nodeCount];
    
    for (int i = 0; i < nodeCount; i++)
    {
        UA_ReadValueId_init(&ids[i]);
        ids[i].attributeId = UA_ATTRIBUTEID_VALUE;
        ids[i].nodeId = nodesToRead[i];
    }

    request.nodesToRead = ids;

    for (int i = 0; i < nodeCount; i++)
    {
        UA_LOG_INFO(UA_Log_Stdout, UA_LOGCATEGORY_USERLAND, ""ID%i: %s, %i"", i,
            request.nodesToRead[i].nodeId.identifier.string.data,
            request.nodesToRead[i].nodeId.namespaceIndex);
    }

    UA_ReadResponse response = UA_Client_Service_read(client, request);

    UA_LOG_INFO(UA_Log_Stdout, UA_LOGCATEGORY_USERLAND, ""Status: %i"",
        response.responseHeader.serviceResult);
    UA_LOG_INFO(UA_Log_Stdout, UA_LOGCATEGORY_USERLAND, ""Responses: %li"", response.resultsSize);
}

The result value is UA_STATUSCODE_GOOD but the number of responses is 0. It works fine when fetching the values one after the other like this:
void readNodeAtIndex(int index)
{
    if (index >= nodeCount)
    {
        UA_LOG_INFO(UA_Log_Stdout, UA_LOGCATEGORY_USERLAND, ""Index out of Range"");
        return;
    }

    UA_Variant variant;
    UA_Variant_init(&variant);

    const UA_NodeId nodeId = nodesToRead[index];

    UA_StatusCode retval = UA_Client_readValueAttribute(client, nodeId, &variant);

    if (retval == UA_STATUSCODE_GOOD && UA_Variant_hasScalarType(&variant,
        &UA_TYPES[UA_TYPES_DOUBLE]))
    {
        UA_Double value = *(UA_Double*)variant.data;

        UA_LOG_INFO(UA_Log_Stdout, UA_LOGCATEGORY_USERLAND, ""Double-Value: %f"", value);
    }
    else if (retval == UA_STATUSCODE_GOOD && UA_Variant_hasScalarType(&variant,
        &UA_TYPES[UA_TYPES_BOOLEAN]))
    {
        UA_Boolean value = *(UA_Boolean*)variant.data;

        UA_LOG_INFO(UA_Log_Stdout, UA_LOGCATEGORY_USERLAND, ""Boolean-Value: %i"", value);
    }
    

    UA_Variant_clear(&variant);
}

The opc/ua server is setup like this:
server = Server()
space_url = ""opc.tcp://localhost:61032""
server.set_endpoint(space_url)
server.set_security_policy([ua.SecurityPolicyType.NoSecurity])
node = server.get_objects_node()

",76382626,"You need to set nodesToReadSize:
    UA_ReadRequest request;
    UA_ReadRequest_init(&request);
    UA_ReadValueId ids[nodeCount];
    
    for (int i = 0; i < nodeCount; i++)
    {
        UA_ReadValueId_init(&ids[i]);
        ids[i].attributeId = UA_ATTRIBUTEID_VALUE;
        ids[i].nodeId = nodesToRead[i];
    }
    request.nodesToReadSize = nodeCount;
    request.nodesToRead = ids;

","Based on the code you provided, it appears that you are using the open62541 library to create a small program in C that connects to an OPC/UA server and fetches data from specified nodes using the UA_ReadRequest. However, you are receiving empty responses when using UA_Client_Service_read for fetching multiple values.

There could be a few potential issues causing the problem:

1. Verify the correctness of node IDs: Make sure that the node IDs you are using are correct and correspond to the nodes you want to read from the OPC/UA server. Double-check the namespace index and the identifier string for each node ID.

2. Check the server configuration: Ensure that the OPC/UA server is properly configured to allow read operations on the specified nodes. Verify that the server is running and accessible.

3. Debug the request/response process: You can enable additional logging or debug statements to see if there are any errors or issues during the request/response process. Check if the request is being sent correctly and if the response contains the expected data.

4. Confirm the compatibility of open62541 and freeopc: Ensure that the open62541 library and the freeopc Python package are compatible and can work together seamlessly. Check for any known issues or limitations when using these libraries together.

5. Review the server implementation: Examine the OPC/UA server implementation using the freeopc package. Make sure that it properly handles the read requests and returns the expected values for the specified nodes.

By addressing these points and carefully reviewing your code, server configuration, and libraries being used, you should be able to identify and resolve the issue with empty responses when using UA_ReadRequest."
76384502,React Ts trying to create guarded routed but getting an error,"Hi I tried creating guards for my routes but I am getting this error:
[Error] Error: [GuardedRoute] is not a <Route> component. All component children of <Routes> must be a <Route> or <React.Fragment>

This is my code:
GuardedRoute.tsx-
import { Route, Navigate } from 'react-router-dom';
import { useUserInfo } from '../context/UserContext';

export const GuardedRoute = ({ path, element: Element, ...rest }: any) => {
  const { userInfo } = useUserInfo();

  if (userInfo.id !== '') {
    return <Route path={path} element={Element} {...rest} />;
  } else {
    return <Navigate to=""/"" replace />;
  }
};


App.tsx
 <BrowserRouter>
                <Nav></Nav>
                <div className=""w-[100vw] fixed top-[50px] z-[100] flex flex-col"">
                {
                  ErrorMessages.map((message,index)=>(
                      <div className=""w-[300px] h-[50px] mt-[10px] mx-auto bg-red-600/90 rounded-md grid place-content-center"" key={index}>
                        <p className=""font-bold text-gray-200 text-center"">{message}</p>
                      </div>
                  ))
                }
                </div>
                  <Routes>
                    <Route path=""/"" element={<Home />} />
                    <GuardedRoute path=""/profile/:select?"" element={<Profile />} />
                    <GuardedRoute path=""/checkout"" element={<CheckOut />}/>
                    <Route path=""/register"" element={<Register />} />
                    <Route path=""/login"" element={<Login />} />
                    <Route path=""/browse"" element={<Browse />} />
                    <Route path=""/product/:id"" element={<Product />} />
                    <Route path=""/404""  element={<PageNotFound />}/>
                    <Route path=""*""  element={<Navigate to=""/404"" />}/>
                  </Routes>
                </BrowserRouter>

Does anyone have a solution for this error? thanks.
",76384779,"It is better to put all the private routes inside Authguard instead of providing Authguard to each component. you can customize the auth, like if there is cookie exists of particular user, then auth is true, otherwise false.
import { Outlet, Navigate } from 'react-router-dom';

const Authguard = () => {

    let auth = false;
    return(
        auth ? <Outlet/> : <Navigate to=""/""/>
    )
}

export default Authguard

Now you Routes will be like, e.g in App.tsx
  function App(){
    return(
     <Routes>
          <Route path=""/"" element={<Login />} />
    
           <Route element={<Authguard />}> 
               <Route path=""/register"" element={<Register />} />
               <Route path=""/home"" element={<Home />} />
          </Route>
     </Routes>)
    }

This shows your Register and Home components are private, and Login component is public.
","The error message suggests that the `GuardedRoute` component is not a valid child component for the `Routes` component. This is because the `GuardedRoute` component returns either a `<Route>` or `<Navigate>` component, not itself.

To fix this issue, you can modify your code as follows:

```tsx
import { Route, Navigate, Routes } from 'react-router-dom';
import { useUserInfo } from '../context/UserContext';

export const GuardedRoute = ({ path, element: Element, ...rest }: any) => {
  const { userInfo } = useUserInfo();

  if (userInfo.id !== '') {
    return <Route path={path} element={Element} {...rest} />;
  } else {
    return <Navigate to=""/"" replace />;
  }
};
```

Note that the `import` statement now includes `Routes` from `react-router-dom`.

Then, in your `App.tsx` file, make sure to wrap your routes with the `Routes` component:

```tsx
import { BrowserRouter, Routes, Route } from 'react-router-dom';
// ...

<BrowserRouter>
  <Nav></Nav>
  <div className=""w-[100vw] fixed top-[50px] z-[100] flex flex-col"">
    {/* ... */}
  </div>
  <Routes>
    <Route path=""/"" element={<Home />} />
    <GuardedRoute path=""/profile/:select?"" element={<Profile />} />
    <GuardedRoute path=""/checkout"" element={<CheckOut />} />
    <Route path=""/register"" element={<Register />} />
    <Route path=""/login"" element={<Login />} />
    <Route path=""/browse"" element={<Browse />} />
    <Route path=""/product/:id"" element={<Product />} />
    <Route path=""/404"" element={<PageNotFound />} />
    <Route path=""*"" element={<Navigate to=""/404"" />} />
  </Routes>
</BrowserRouter>
```

By importing and using `Routes` instead of `Route` as the top-level component for defining routes, you should no longer encounter the error you mentioned."
76381166,"Object reference not set to an instance of an object, Unity","So when enemy is spawned i try to init him here enemy.Init(); but had error Object reference not set to an instance of an object. But here Debug.Log(enemy._enemyHealth); i get number of enemy health. How i can Init my enemy from here if enemy has class 'Usual Enemy' and it extends from this class 'Enemy' and i wiil have more enemy child cllases?
I vave class Enemy
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.AI;

public abstract class Enemy : MonoBehaviour
{

    private GameObject _player;
    private Animator _animator;
    private Rigidbody _rigidbody;
    private bool _isSpawned;
    private const int _enemySpavnTime = 5;
    [SerializeField] public float _enemyHealth;
    [SerializeField] protected float _rotationSpeed = 10f;
    [SerializeField] protected float _moveSpeed;
    [SerializeField] protected float _damage;
    [SerializeField] protected DamageBox _damageBox;
    [SerializeField] protected TriggerBox _triggerBox;
    private EnemyCondition _enemyCondition = EnemyCondition.Dead;
    
    private enum EnemyCondition 
    {
        Spawn,
        Run,
        Attack,
        Dead
    }


    void Start()
    {
        _rigidbody = GetComponent<Rigidbody>();
        _animator = GetComponent<Animator>();
        _player = GameObject.FindWithTag(""Hero"");
        _damageBox.HeroHited += enemyAttack;
        _triggerBox.HeroInAttackRange += enemyGetAngry;
        _triggerBox.HeroOutAttackRange += enemyGetChill;
    }

    void Update()
    {   
        if (_player != null) 
        {
            if (_enemyCondition == EnemyCondition.Run)
            {
                EnemyPlayerRotation(_player);

            } else if (_enemyCondition == EnemyCondition.Attack)
            {
                EnemyPlayerRotation(_player);
            }
        }
        
    }

    void FixedUpdate() {
        if (_enemyCondition == EnemyCondition.Run)
        {
            EnemyPlayerFollow();
        }   
    }

    public virtual void Init() {
        enemySpawn();
    }

    protected virtual void EnemyPlayerFollow() {
        Vector3 direction = transform.TransformDirection(new Vector3(0, 0, 1));
        _rigidbody.velocity = direction * _moveSpeed;
    }

    protected virtual void EnemyPlayerRotation(GameObject target) {
        Vector3 direction = (target.transform.position - transform.position).normalized;
        Quaternion lookRotation = Quaternion.LookRotation(new Vector3(direction.x, 0, direction.z));
        transform.rotation = Quaternion.Slerp(transform.rotation, lookRotation, Time.deltaTime * _rotationSpeed);
     }

    public virtual void EnemyDamage(float damage)
    {
        _enemyHealth -= damage;
        if (_enemyHealth <= 0) {
            PoolManager.Instanse.Despawn(gameObject);
        }
    }

    public virtual void enemySpawned() {
        _enemyCondition = EnemyCondition.Run;
        _rigidbody.isKinematic = false;
        _animator.SetTrigger(""Run"");
        StopCoroutine(SpawnMoveUp());
        Debug.Log(""enemySpawned"");
    }

    protected virtual void enemyAttack(Hero trigger) 
    {   
        if (_enemyCondition == EnemyCondition.Attack) {
            trigger.HeroDamage(_damage);
        }
    }

    protected virtual void enemyGetAngry() {
        _enemyCondition = EnemyCondition.Attack;
        _animator.SetTrigger(""Attack"");
    }

    protected virtual void enemyGetChill() {
        _enemyCondition = EnemyCondition.Run;
        _animator.SetTrigger(""Run"");
    }

    protected virtual void enemySpawn() {
        _animator.SetTrigger(""Spawn"");
        _animator.speed = 0.0f;
        StartCoroutine(SpawnMoveUp());
    }

    IEnumerator SpawnMoveUp()
    {  
        float spawnStage = 0.0f;
        Vector3 startPoint = transform.position;
        while(spawnStage <= 1.0f) {
            transform.position = Vector3.Slerp(startPoint, new Vector3(transform.position.x, 0, transform.position.z), spawnStage);
            spawnStage += Time.deltaTime/_enemySpavnTime;
            yield return null;
        }
        _animator.speed = 1.0f;
        
    }
    
}

And I have class that spawns enemies
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class EnemiesSpawner : MonoBehaviour
{
    [SerializeField] private Levels[] _levelsList;
    private List<GameObject> _enemiesQueue = new List<GameObject>();
    private List<GameObject> _levelEnemiesList = new List<GameObject>();
    private List<int> _levelEnemiesCount = new List<int>();

    private bool _isBossSpawned = false;
    // private int _currentLevel = PlayerPrefs.GetInt(""currentLevel"") - 1;
    private int _currentLevelNumber = 0;
    private float _reloadPause = 0;
    private int currentEnemiesListPos = 0;
    private int allEnemiesCount = 0;

    private float _reloadTime;
    private float _spawnRadius;
    private float _spawnOfsetRadius = 2f;

    [SerializeField] private GameObject _boss;
    [SerializeField] private GameObject _spawnPoint;

    void Start() {
        Levels currentLevelObj = _levelsList[_currentLevelNumber];
        _levelEnemiesCount.AddRange(currentLevelObj.enemiesCount.ToArray());
        _levelEnemiesList = currentLevelObj.enemiesPrefabs;
        _spawnRadius = currentLevelObj.spawnRadius;
        _reloadTime = currentLevelObj.spawnReload;

        foreach(int num in _levelEnemiesCount) {
            allEnemiesCount += num;
        }

        while(_enemiesQueue.Count < allEnemiesCount){
            int randomIndex = Random.Range(0, _levelEnemiesCount.Count);
            if (_levelEnemiesCount[randomIndex] == 0) {
                continue;
            } else {
                _enemiesQueue.Add(_levelEnemiesList[randomIndex]);
                _levelEnemiesCount[randomIndex] --;
            }
        }
    }

    public void SetPlayerForSpawn (GameObject player) {
        _spawnPoint = player;
    }

    void Update ()
    {           
        if (_spawnPoint != null){
            SpawnEnemies();
        }
        
    }

    public void SpawnEnemies() 
    {
        _reloadPause += Time.deltaTime;
        if (_reloadPause >= _reloadTime && currentEnemiesListPos < allEnemiesCount) {
            GameObject go = PoolManager.Instanse.Spawn(_enemiesQueue[currentEnemiesListPos],new Vector3((Random.value < 0.5f) ? Random.Range(_spawnPoint.transform.position.x + _spawnOfsetRadius, _spawnPoint.transform.position.x + _spawnRadius) : Random.Range(_spawnPoint.transform.position.x - _spawnOfsetRadius, _spawnPoint.transform.position.x - _spawnRadius), -5, (Random.value < 0.5f) ? Random.Range(_spawnPoint.transform.position.z + _spawnOfsetRadius, _spawnPoint.transform.position.z + _spawnRadius) : Random.Range(_spawnPoint.transform.position.z - _spawnOfsetRadius, _spawnPoint.transform.position.z - _spawnRadius)), Quaternion.identity);
            Enemy enemy = go.GetComponent<Enemy>();
            Debug.Log(enemy._enemyHealth);
            enemy.Init();
            _reloadPause = 0;
            currentEnemiesListPos ++;
        } else if (currentEnemiesListPos == allEnemiesCount && _isBossSpawned == true) {
            Instantiate(_boss,new Vector3(0,0,0), Quaternion.identity);
            _isBossSpawned = true;
        }
        
    }
}

So when enemy is spawned i try to init him here enemy.Init(); but had error Object reference not set to an instance of an object. But here Debug.Log(enemy._enemyHealth); i get number of enemy health. How i can Init my enemy from here if enemy has class 'Usual Enemy' and it extends from this class 'Enemy' and i wiil have more enemy child cllases?
",76381309,"The problem is not about ""enemy"" object. It is not null. If you look at the last reference on your call stack, you see that it is ""_animator"" field.
It's probably because you call the ""Init"" function before the ""enemy"" object's ""Start"" method called by Unity engine.
","The error ""Object reference not set to an instance of an object"" typically occurs when you are trying to access a member variable or call a method on an object that has not been initialized or assigned a value. In your case, it seems that the `enemy` variable might not be assigned a valid reference before calling `enemy.Init()`.

To fix this issue, you need to ensure that you are correctly initializing the `enemy` object before calling its `Init()` method. Here are a few possible solutions:

1. Make sure that the enemy object is instantiated and assigned correctly in the `PoolManager.Instanse.Spawn()` method. Verify that the spawned enemy prefab has the `Enemy` component attached to it.

2. Ensure that the `Init()` method is properly implemented in the child classes of `Enemy`, such as `UsualEnemy`. Check if any specific initialization steps are missing or causing the issue.

3. Double-check the initialization order and ensure that the necessary components and references are set up correctly before calling `enemy.Init()`. For example, verify that the `_damageBox` and `_triggerBox` variables are assigned valid references.

If you have multiple child classes of `Enemy`, such as `UsualEnemy`, you can still call the `Init()` method from the `EnemiesSpawner` script. Since `Init()` is a virtual method in the `Enemy` class, it can be overridden in the child classes to provide specific initialization logic for each enemy type."
76382285,Kotlin Error 'unresolved reference' appears when trying to run Java Code from a Kotlin file,"Referencing Problem when Java Class is used in Kotlin. There is the Java class Base32Decoder.java and code from this class is used in the Kotlin file hello.kt.
When I try to run Java code through a Kotlin file, an error occurs because of no reference could be established to the Java class Base32Decoder.
Error message:

hello.kt:4:25: error: unresolved reference: Base32Decoder

Base32Decoder Java class can't resolve the reference to it. Since this class is used inside the Kotlin file, the reference needs to work.
Code
fun main(args: Array<String>){
    val Base32Decoder = Base32Decoder()
   val rectangleArea: String = Base32Decoder.base32Decode(""JBSWY3DPFQQFO33SNRSCC==="")
   println(""inside the Kotlin codes:"" + rectangleArea)
}

How can I reference Java classes when I want to use Java code in Kotlin files?
",76382653,"The code must be accessible for Kotlin. This means you have to compile (If you compile code in terminal: javac SampleFile.java The Base32Decoder.java file to generate the Base32Decoder.class.
Now generate the JAR-file with a link to the Kotlin file in command line:
jar cf app.jar Base32Decoder.class hello.kt

Now since you use kotlinc you can execute the code in command line like this:
kotlinc -classpath app.jar -script hello.kt

Now your code should run fine. The problem is that Kotlin didn't have access to the Base32Decoder.java class.
","To reference Java classes in Kotlin and use Java code within Kotlin files, you need to make sure you have the correct import statement and ensure that the Java class is accessible to the Kotlin file. Here's how you can resolve the ""unresolved reference"" error in your code:

1. Check the package: Ensure that both the Kotlin file (`hello.kt`) and the Java class (`Base32Decoder.java`) are in the same package or that the appropriate package declaration is present in the Java class.

2. Import the Java class: In the Kotlin file, import the Java class using the correct package name. If the Java class is in a different package, you need to import it at the beginning of the Kotlin file.

   ```kotlin
   import com.example.Base32Decoder // Replace with the actual package name

   fun main(args: Array<String>) {
       val base32Decoder = Base32Decoder()
       // Rest of the code
   }
   ```

3. Build the Java class: Make sure the Java class (`Base32Decoder.java`) is compiled and built before running the Kotlin file. If you are using an IDE, ensure that the Java class is included in the build path or module dependencies.

4. Check file locations: Verify that both the Kotlin file and the Java file are located in the correct directories within your project structure.

Once you've made these changes, the ""unresolved reference"" error should be resolved, and you should be able to use the Java class in your Kotlin file without any issues."
76380938,Azure blob storage returns Unauthorized with SAS generated URL,"I am trying to generate a URL where users can access a file that is in a blob storage container on Azure. The blob storage container is heavily restricted by IP address, and then the service I'm building will manage requests and generate a URL where users can access the file directly. Here is the code used to generate the URL (I think the code itself is probably fine).
CloudBlobContainer container = BlobStorage.GetContainer(_accountStatementEmailConfig.BlobStorageContainerName);
CloudBlockBlob blob = container.GetBlockBlobReference(document.StoragePath);

if (!blob.Exists())
{
    return NotFound();
}

TimeSpan sasExpiryTime = TimeSpan.FromMinutes(_apiConfig.PresignedURLExpiryInMinutes);

SharedAccessBlobPermissions permissions = SharedAccessBlobPermissions.Read;

string sasToken = blob.GetSharedAccessSignature(new SharedAccessBlobPolicy
{
    Permissions = permissions,
    SharedAccessExpiryTime = DateTime.UtcNow.Add(sasExpiryTime)
}, new SharedAccessBlobHeaders(), _apiConfig.SharedAccessSignaturePermissionsPolicyName);

string documentUrl = $""{blob.Uri.AbsoluteUri}{sasToken}"";

The code generates the URL fine, but when a user goes to the URL they receive the following error:
<Error>
    <Code>AuthorizationFailure</Code>
    <Message>
        This request is not authorized to perform this operation. RequestId:92d7ca35-501e-0016-2a65-973659000000 Time:2023-06-01T08:43:35.2439678Z
    </Message>
</Error>

It was perhaps my incorrect assumption about SAS that the token would allow me to bypass the IP restrictions since I am generating the URL from a whitelisted IP. Am I taking the incorrect approach, or is there something minor I am overlooking?
",76381339,"As Gaurav Mantri pointed out in comments, the issue is my understanding of how Azure handles permissions since all of my previous experience is from AWS.
In Azure the storage account and the containers have separate access levels. Can achieve similar functionality to AWS's private + presignedURL option by setting the Storage Account to public, and each of the individual containers to private. Then using the generated SAS URL to access the file.
","The issue you're facing is most likely related to the Shared Access Signature (SAS) permissions not being properly configured for the generated URL. By default, SAS tokens inherit the permissions of the container, unless specified otherwise.

In your code snippet, you're setting the `SharedAccessBlobPermissions` to `Read`, which should allow users to read the blob. However, it seems that the SAS token itself is not granting the necessary permissions to access the blob.
"
76384686,Lag of every nth element,"I have data frame as :
df <- data.frame( date =seq(from = as.Date(""2000-01-01""), 
                              to =  as.Date(""2005-01-01""),'month'))


df <-  df %>% mutate(cumsum = seq(1, length.out = length(date)))

I want to create a new column, which is the sum of the value in cumsum and every 12th value (one year back).
EDIT:
I like both your answers! Actually I just found a problem for the solution for me (sorry my explanation was not quite clear.) Your approach gives me the sum of the value now and one year befor. But I do have seveal years and would need the cumsum of all overervation in previous years (so sum(x, lag(x,12), lag(x,24), lag (x,36)). I tried smth. like (rep(lag(cumsu, 12), nrow(df)/12). May you can help. Thanks!
",76384798,"The literal approach is to use lag, and if you are assured of perfectly-spaced data, then @Jamie's answer is the most direct and simplest approach.
However, if there is a chance that you don't have all intermediate months, this could lag incorrectly. One way to guard against this is to self-join with the previous date.
df2 <- df[-20,] # just to impose some missingness
library(lubridate) # %m+%
df2 %>%
  mutate(
    # this is the more direct route, but with missingness it glitches
    rolling_12 = cumsum + lag(cumsum, n = 12),
    lastyear = date %m+% years(-1)
  ) %>%
  left_join(df2, by = c(""lastyear"" = ""date""), suffix = c("""", ""_12"")) %>%
  mutate(cumsum_12 = cumsum + cumsum_12) %>%
  select(-lastyear)
#          date cumsum rolling_12 cumsum_12
# 1  2000-01-01      1         NA        NA
# 2  2000-02-01      2         NA        NA
# 3  2000-03-01      3         NA        NA
# 4  2000-04-01      4         NA        NA
# 5  2000-05-01      5         NA        NA
# 6  2000-06-01      6         NA        NA
# 7  2000-07-01      7         NA        NA
# 8  2000-08-01      8         NA        NA
# 9  2000-09-01      9         NA        NA
# 10 2000-10-01     10         NA        NA
# 11 2000-11-01     11         NA        NA
# 12 2000-12-01     12         NA        NA
# 13 2001-01-01     13         14        14
# 14 2001-02-01     14         16        16
# 15 2001-03-01     15         18        18
# 16 2001-04-01     16         20        20
# 17 2001-05-01     17         22        22
# 18 2001-06-01     18         24        24
# 19 2001-07-01     19         26        26
# 20 2001-09-01     21         29        30  <-- this is where rolling_12 goes wrong
# 21 2001-10-01     22         31        32
# 22 2001-11-01     23         33        34
# 23 2001-12-01     24         35        36
# 24 2002-01-01     25         37        38
# 25 2002-02-01     26         39        40
# 26 2002-03-01     27         41        42
# 27 2002-04-01     28         43        44
# 28 2002-05-01     29         45        46
# 29 2002-06-01     30         47        48
# 30 2002-07-01     31         49        50
# 31 2002-08-01     32         51        NA
# 32 2002-09-01     33         54        54
# 33 2002-10-01     34         56        56
# 34 2002-11-01     35         58        58
# 35 2002-12-01     36         60        60
# 36 2003-01-01     37         62        62
# 37 2003-02-01     38         64        64
# 38 2003-03-01     39         66        66
# 39 2003-04-01     40         68        68
# 40 2003-05-01     41         70        70
# 41 2003-06-01     42         72        72
# 42 2003-07-01     43         74        74
# 43 2003-08-01     44         76        76
# 44 2003-09-01     45         78        78
# 45 2003-10-01     46         80        80
# 46 2003-11-01     47         82        82
# 47 2003-12-01     48         84        84
# 48 2004-01-01     49         86        86
# 49 2004-02-01     50         88        88
# 50 2004-03-01     51         90        90
# 51 2004-04-01     52         92        92
# 52 2004-05-01     53         94        94
# 53 2004-06-01     54         96        96
# 54 2004-07-01     55         98        98
# 55 2004-08-01     56        100       100
# 56 2004-09-01     57        102       102
# 57 2004-10-01     58        104       104
# 58 2004-11-01     59        106       106
# 59 2004-12-01     60        108       108
# 60 2005-01-01     61        110       110

","To create a new column that represents the cumulative sum of the values in the `cumsum` column and every 12th value (one year back) for all previous years, you can use the `lag()` function along with `rowwise()` and `mutate()` from the `dplyr` package. Here's how you can modify your code:

```R
library(dplyr)

df <- df %>% 
  rowwise() %>% 
  mutate(new_column = sum(cumsum, lag(cumsum, 12, default = 0), lag(cumsum, 24, default = 0), lag(cumsum, 36, default = 0))) %>%
  ungroup()
```

In this code, the `rowwise()` function ensures that the calculations are done row by row. Then, `mutate()` creates a new column called `new_column` and calculates the sum of `cumsum`, `lag(cumsum, 12)`, `lag(cumsum, 24)`, and `lag(cumsum, 36)`. The `lag()` function with the appropriate offset (12, 24, 36) is used to fetch the values from the previous years. The `default = 0` argument ensures that missing values (due to insufficient data) are treated as zeros.

After the calculations, `ungroup()` is used to remove the grouping so that the dataframe is returned to its original state.

Now, `df` will have a new column called `new_column` which contains the desired cumulative sum of the `cumsum` column and the corresponding values from the previous years."
76382357,How to do integer arithmetic in Makefile prerequisites?,"I have a file <n>.x and <n-1>.y and I want to create a dependency in my Makefile.
Manually I can write each case as follows:
2.x : 1.y
3.x : 2.y
4.x : 3.y
...

Now I would like to be able to write this more generically:
%.x : <???>.y

Gnu Make doesn't seem to have integer arithmetic on its own and shell expansions ($$(( )), $(shell ) don't seem to work for prerequisites.
Is there a way do this in the Makefile itself or should I use an external script that can generate these dependencies and let the Makefile include them?
Edit: My specific case is dealing with files containing financial transactions. Each file (e.g. 2023.journal) represents transactions for a specific year and includes an opening statement (e.g. 2023_opening.journal), which is based on the closing statement of the year before that. This create a dependency between 2023_opening.journal and 2022.journal.
",76382656,"With GNU make:
.SECONDEXPANSION:
%.x: $$(shell expr $$* + 1).y

All rules after the .SECONDEXPANSION special target have their list of prerequisites (and only that) processed twice by make: a first time, as everything else, when make parses the Makefile, plus a second time when make needs to check the prerequisites of a target. Contrary to the first phase, during the second phase the automatic variables are defined.
So, after the first phase the rule becomes:
%.x: $(shell expr $* + 1).y

$* is the make automatic variable that expands as the stem in pattern rules. In your case it is the part that matches the %.
And then, when make needs the list of prerequisites for 1.x, the rule is processed again, $(shell expr $* + 1).y is expanded, and becomes (step by step):

1.x: $(shell expr 1 + 1).y # $* --> 1
1.x: 2.y                   # $(shell expr 1 + 1) --> 2

","In GNU Make, you can use the `eval` function in combination with a recursive variable definition to achieve integer arithmetic in Makefile prerequisites. Here's an example of how you can do it:

```makefile
# Define the number of files you want to generate
NUM_FILES := 4

# Define a recursive variable to generate the dependencies
define generate_dependencies
ifeq ($$1,1)
$$(eval $$(firstword $$(1:.x=.y)) : $$(firstword $$(lastword $$(1:.x=.y))))
else
$$(eval $$(firstword $$(1:.x=.y)) : $$(firstword $$(shell echo $$(( $$1 - 1 ))).y))
$$(call generate_dependencies,$$(shell echo $$(( $$1 - 1 )))))
endif
endef

# Call the recursive function to generate the dependencies
$(eval $(call generate_dependencies,$(NUM_FILES)))

# Define a generic rule for generating the .x files
%.x: %.y
    # Add your recipe here to generate the .x file from the .y file

# Phony target to build all the files
all: $(shell seq -s' ' -w 1 $(NUM_FILES)).x
```

In this example, the `generate_dependencies` function is defined using the `define` directive. It takes an argument `1` representing the current file number. The function uses `ifeq` and `eval` to conditionally generate the dependencies based on the value of `1`. It recursively calls itself with `$$(( $$1 - 1 ))` to generate dependencies for previous files until it reaches the base case of `1`.

The `$(eval $(call generate_dependencies,$(NUM_FILES)))` line calls the `generate_dependencies` function with the value of `NUM_FILES` to generate the dependencies for all the files.

Finally, a generic rule is defined to generate `.x` files from `.y` files, and the `all` target is defined to build all the `.x` files using the `seq` command to generate a list of file numbers.

You can modify this example to suit your specific case with financial transactions by adjusting the file naming conventions and adding your own recipe in the `%.x: %.y` rule."
76380520,How can I add multiple non-consecutive cells to Name Manager in Excel using Python?,"I am trying to add multiple cells, which are not always consecutive, in only one Name in Excel Name Manager using Python. You can see an example of what I want to do in the attached screenshot.

I have tried the Python libraries openpyxl and XlsxWriter, but both libraries can only define a specific cell or a specific range.
Examples

openpyxl

specific_cell = DefinedName('specific_cell', attr_text='Sheet1!$C$8')
specific_range = DefinedName('specific_range', attr_text='Sheet1!$C$8:$J$13')


XlsxWriter

workbook.define_name('specific_cell', '=Sheet1!$G$1')
workbook.define_name('specific_range', '=Sheet1!$G$1:$H$10')

Is there any way to add to Name Manager something more complicated than the above?
Based on the attached screenshot something like
workbook.define_name('complex_range','=Sheet1!$B$3:$E$8;Sheet1!$B$12:$C$16;Sheet1!$B$19;Sheet1!$H$12:$I$16')

",76381361,"These will work for each module, the common factor being comma rather than semi-colon?

Xlsxwriter:
workbook.define_name(""test"", ""=Sheet1!$B$3:$E$8,Sheet1!$B$12:$C$16,Sheet1!$H$12:$I$16,Sheet1!$B$19"") 

Xlwings:
workbook.names.add(name=""test"", refers_to=""=Sheet1!$B$3:$E$8,Sheet1!$B$12:$C$16,Sheet1!$H$12:$I$16,Sheet1!$B$19"") 

Openpyxl:
workbook.defined_names.add(DefinedName(""test"", attr_text=""Sheet1!$B$3:$E$8,Sheet1!$B$12:$C$16,Sheet1!$H$12:$I$16,Sheet1!$B$19""))

","To add multiple non-consecutive cells to the Name Manager in Excel using Python, you can use the `pyxlsb` library, which provides more flexibility in defining complex ranges. Here's an example of how you can achieve this:

```python
import pyxlsb
from pyxlsb import open_workbook

def add_complex_range_to_name_manager(file_path, sheet_name, range_address, name):
    with open_workbook(file_path) as wb:
        with wb.get_sheet(sheet_name) as sheet:
            defined_name = f'{sheet_name}!{range_address}'
            sheet.book.names.add(name, refersTo=defined_name)

# Example usage
file_path = 'example.xlsx'
sheet_name = 'Sheet1'
range_address = ""'Sheet1'!$B$3:$E$8;'Sheet1'!$B$12:$C$16;'Sheet1'!$B$19;'Sheet1'!$H$12:$I$16""
name = 'complex_range'

add_complex_range_to_name_manager(file_path, sheet_name, range_address, name)
```

In this example, the `pyxlsb` library is used to open the Excel file and access the desired sheet. The `add_complex_range_to_name_manager` function takes the file path, sheet name, range address, and the name you want to assign to the defined name. The `range_address` parameter accepts a semicolon-separated list of cell ranges, allowing you to define complex non-consecutive ranges.

Note: Make sure you have installed the `pyxlsb` library using `pip install pyxlsb` before running the code."
76382439,How to fix Multer middleware when it fails to create upload folder and change filename?,"i am trying to upload pdf to server using nodejs and multer but there is a problem.
a send a file from ejs templet and it must be saved at disk on folder named upload and file name changed to specific name .
but what happen that Multer middleware does not work no folder created no filename changed
There are parts of code .
ejs file
 <form enctype=""multipart/form-data"">
        <input type=""text"" placeholder=""Book Name"" id=""name"" />
        <input type=""text"" placeholder=""Author "" id=""author"" />
        <input type=""text"" placeholder=""Buy link"" id=""link"" />
        <input type=""text"" placeholder=""Book description"" id=""desc"" />
        <input type=""file"" name=""pdf"" id=""pdf"" placeholder=""upload file"" />
        <button type=""submit"">Add</button>
      </form>
<script>
      // const multer = import(""multer"");
      // const upload = multer({ dest: ""./public/data/uploads/"" });
      let form = document.querySelector(""form"");
      form.addEventListener(""submit"", async (e) => {
        let bookName = document.getElementById(""name"").value;
        let bookAuthor = document.getElementById(""author"").value;
        let bookLink = document.getElementById(""link"").value;
        let bookDesc = document.getElementById(""desc"").value;
        let pdf = document.getElementById(""myfile"").files[0].name;
        e.preventDefault();
        try {
          const res = await fetch(""/addBooks"", {
            method: ""POST"",
            body: JSON.stringify({
              bookName,
              bookAuthor,
              bookDesc,
              bookLink,
              pdf,
            }),
            headers: { ""Content-Type"": ""application/json"" },
          });
</script>


middleware:
onst storage = multer.diskStorage({
  destination: function (req, file, cb) {
    cb(null, ""upload"");
  },
  filename: function (req, file, cb) {
    cb(null, Date.now() + ""-"" + file.originalname);
    console.log(destination, filename);
  },
});
const upload = multer({ storage });

route.post(""/addBooks"", upload.single(""pdf""), addBook);


post func
let addBook = async (req, res) => {
  console.log(""reqbody >> "", req.body);
  let { bookName, bookAuthor, bookDesc, bookLink, pdf } = req.body;
  try {
    let _book = await books.create({
      name: bookName,
      author: bookAuthor,
      description: bookDesc,
      buyLink: bookLink,
      pdf:pdf,
    });
    if (_book) {
      res.status(200).send({ msg: ""success"" });
    }
  } catch (error) {
    logger.error(""system crashed try again "");
    res.status(400).send({ msg: ""Wrong"" });
  }
};

",76382678,"When sending a file, you must send it in the form, and the information comes in req.file, not in req.body
","From the code you provided, I can see a couple of issues that might be causing the problem with Multer not working as expected. Here are some suggestions to fix those issues:

1. In your EJS template, there seems to be a typo in the `enctype` attribute of the form tag. It should be `multipart/form-data` instead of an empty string. Update it as follows:

```ejs
<form enctype=""multipart/form-data"">
  <!-- form inputs -->
</form>
```

2. In your JavaScript code, you are trying to access the file input element using `getElementById(""myfile"")`. However, the actual ID of the file input is ""pdf"". Update the line to:

```javascript
let pdf = document.getElementById(""pdf"").files[0].name;
```

3. In the Multer middleware, you are using a disk storage configuration to specify the destination folder and filename. However, it seems like the destination folder is not correctly defined. Update the destination function to provide the complete path to the destination folder. For example:

```javascript
const storage = multer.diskStorage({
  destination: function (req, file, cb) {
    cb(null, ""public/data/uploads/"");
  },
  filename: function (req, file, cb) {
    cb(null, Date.now() + ""-"" + file.originalname);
  },
});
```

Make sure the ""public/data/uploads/"" folder exists in your project directory.

4. Ensure that you have properly set up the routes and the Multer middleware. It seems like you're missing the route configuration for the ""/addBooks"" endpoint. Add the route configuration before the `addBook` function:

```javascript
route.post(""/addBooks"", upload.single(""pdf""), addBook);
```

With these changes, the file should be uploaded to the ""public/data/uploads/"" folder, and the filename will be changed to a timestamped name."
76381321,How do I make the label text of an element bold in javascript?,"My HTML :
<div id=""PDF"">
    <input type=""checkbox"" id=""pdf"" name=""pdf"" value=""pdf"">
    <label for=""pdf"">PDF</label>
</div>

My JS :
document.addEventListener('change',function(event){
    if(event.target.id == ""pdf""){
        if(event.target.checked == true){
            event.target.label.style.fontWeight = ""bold"";
        }
        else{
            event.target.label.style.fontWeight = ""normal"";
        }
    }
});

To my utter dismay, when I execute the code, I am greeted with this error:

Uncaught TypeError: Cannot read properties of undefined (reading
'style')

How do I fix this issue?
",76381368,"There are two main issues here. Firstly, the event target has no attribute of label. You should just select the label explicitly and change the styles. Secondly, I would recommend adding the event listener to only the #pdf div rather than the whole document. This way, you won't need to check for the ID and the event listener won't be fired on every change. For example:
document.getElementById(""pdf"").addEventListener(""click"", event => {
  const label = document.querySelector(""label[for=pdf]"");
  if(event.target.checked == true){
    label.style.fontWeight = ""bold"";
  }
  else{
    label.style.fontWeight = ""normal"";
  }
})

","The issue in your code is that `event.target` refers to the checkbox element, not the label element. The `label` property is not directly accessible through the checkbox element.

To fix this issue, you can use the `querySelector` method to select the label element based on its `for` attribute. Here's an updated version of your code:

```javascript
document.addEventListener('change', function(event) {
  if (event.target.id === ""pdf"") {
    var label = document.querySelector('label[for=""pdf""]');
    if (event.target.checked) {
      label.style.fontWeight = ""bold"";
    } else {
      label.style.fontWeight = ""normal"";
    }
  }
});
```

In this code, `document.querySelector('label[for=""pdf""]')` selects the label element with the `for` attribute set to `""pdf""`. Then, you can modify the `fontWeight` style property of the label element accordingly."
76384755,Oracle SQL Case with Null,"I have a simple query that is taking an input from a bind variable.
CREATE TABLE ""FRUITS"" 
(   ""FRUIT_NAME"" VARCHAR2(100), 
""COLOR"" VARCHAR2(100)
) ;

insert into fruits (fruit_name, color)
values ('Banana', 'Yellow')
insert into fruits (fruit_name, color)
values ('Lemon', '')
insert into fruits (fruit_name, color)
values ('Apple', 'Red')

SELECT * FROM FRUITS
WHERE
COLOR =     case
            when :P1_ITEM is null then null
            else :P1_ITEM
            end

If the input is 'Yellow' the result would be 'Banana' (when 'Red' then 'Apple'). However, if the input happens to be null the result is 'no data found'.
How can this be avoided knowing that null is not a null value?
If the input is null on color then how can I return the null color row? meaning 'Lemon' + null
Thanks
",76384803,"Something like this might be one option:
SELECT * FROM FRUITS
WHERE
nvl(COLOR, 'x') = case
                    when :P1_ITEM is null then 'x'
                    else :P1_ITEM
                  end;

","In Oracle SQL, comparing a column to `NULL` using the equality operator (`=`) will result in `NULL` or no match. This is why you're getting ""no data found"" when the input is `NULL` for the `COLOR` column.

To include the rows with `NULL` values in the `COLOR` column when the input is `NULL`, you can modify your query to handle the `NULL` condition separately. Here's an updated version of your query:

```sql
SELECT *
FROM FRUITS
WHERE
  (COLOR = :P1_ITEM OR (:P1_ITEM IS NULL AND COLOR IS NULL))
```

In this query, we're using a combination of the equality operator and `IS NULL` check to handle the `NULL` condition. If `:P1_ITEM` is not `NULL`, it will match the non-null values in the `COLOR` column. If `:P1_ITEM` is `NULL`, it will match the rows where the `COLOR` column is also `NULL`.

With this modification, when `:P1_ITEM` is `NULL`, the query will return the row with `('Lemon', NULL)` along with any other rows where `COLOR` matches `NULL`."
76380594,A local object disappeared in ABAP from $TMP package,"There was a local report in the $TMP that is ""disappeared"" somehow. I did not delete it, but can't see it in SE80 anymore.
What could be the reason?
Somebody else has deleted it. The system has been resetted somehow to an older backup state. Are any other reasons possible?
Is there a possibility to see in the traces what has happened or are there any other (better) tracking possibilities?
Unfortunately I don't know what was the exact name of the disappeared report, but I know the beginning of its name (like Z_ABCD_...)
",76381374,"You cannot restore or track local objects if they were deleted. There could  be any reason from that you have mentioned. Better ask collegues / basic team if they made some changes.
Try to look up the table TADIR (Directory of Repository Objects) to check which local development objects exist. Use SE16 / SE16N transaction with condition DEVCLASS = $TMP, additionally AUTHOR to include only objects from specific user, OBJ_NAME = Z_ABCD* to restrict the program name.
You can also check the table REPOSRC (Report Source Code), where reports source code in RAWSTRING (DATA field) is stored. Filter on PROGNAME, CNAM (username) to check if the source sode is available on the system (there are also several views avaiable for this table, TRDIR, D010SINF).
If the program were assigned to a package / transport and deleted, than you could find it in TADIR with a deletion flag DELFLAG = X, and also in the table E071 with OBJFUNC = D in case of a transport assignment. Local objects just get deleted from the repository tables.
","There could be multiple reasons for the local report disappearing from the $TMP package in ABAP. Here are a few possibilities:

1. Deletion by someone else: As you mentioned, it's possible that another user with sufficient authorization deleted the report. You can check with your ABAP team or system administrators to see if anyone else has access to delete objects in the $TMP package.

2. System reset to an older backup state: If the system was somehow reset to an older backup state, any changes made after that point in time would be lost, including the local report. This can happen in situations where system administrators restore the system from a backup.

3. Transport or migration issues: If the local report was part of a transport or migration, there could have been issues during the process that caused the report to disappear. This could happen due to transport failures, inconsistencies, or errors during migration activities.

Regarding tracing or tracking possibilities, there are a few options you can explore:

1. SAP system logs: Check the system logs in transaction code SM21 or ST22. These logs may provide information about any critical errors or actions performed in the system, including object deletions.

2. Change documents: If your system is configured to track changes, you can use transaction code SCU3 to access the change documents related to the $TMP package. This can provide information about who made changes and when they were made.

3. Version management: If the local report was under version control, you may be able to retrieve previous versions of the report using transaction code SE80. Check the version history to see if the report can be recovered from a previous version.

4. ABAP development tools: If you have access to ABAP development tools like Eclipse or SAPGUI, you can check the transport history, object logs, or other tracking mechanisms available in those tools. These tools often provide more detailed information about object changes and deletions.

It's important to note that the availability and extent of tracking options may vary depending on your system configuration and the specific actions taken. It's recommended to involve your ABAP team or system administrators for assistance in investigating the disappearance of the local report."
76382531,Cannot convert value of type 'Image' to expected argument type 'String',"I make a category included image scroll butI can't put my variable named tabBarItemName name into image, it keeps giving error like this : Cannot convert value of type 'Image' to expected argument type 'String'
struct ContentView: View {
        @State var currentTab: Int = 0
        var body: some View {
            ZStack(alignment:.top) {
                        TabView(selection:self.$currentTab){
                            Image1View().tag(0)
                            Image2View().tag(1)
                            Image3View().tag(2)
                        }
                        .tabViewStyle(.page(indexDisplayMode: .never))
                        .edgesIgnoringSafeArea(.all)
                        .padding(.top,76)
                        CategoryScroll(currentTab: self.$currentTab)
                    }   
       }
    }

    struct CategoryScroll:  View {
        var tabBarOptions: [Image] = [Image(""bim"")]
        @Binding var currentTab : Int
        var body: some View {
              ScrollView(.horizontal){
                  HStack(spacing:20){
                      ForEach(Array(zip(self.tabBarOptions.indices, self.tabBarOptions)),
                              id:\.0,
                              content: {
                          index, name in
                          CategoryView(currentTab: self.$currentTab, tab: index, tabBarItemName: name)
              })
            }
        }.padding(7)
        .background(Color.white)
    }

    struct CategoryView: View {
        @Binding var currentTab: Int
        var tab : Int
        var tabBarItemName: Image
        var body: some View{
            Button{
                self.currentTab = tab
            } label: {
                VStack{
                    Image(tabBarItemName) // I'm trying to make changes right here
                        .frame(width: 12, height: 12)
                        .padding(.leading, 65)
    
                    if currentTab == tab {
                        Color.init( red: 0.965, green: 0.224, blue: 0.49)
                            .frame(height: 2)
                            .padding(.leading,65)
                    } else {
                        Color.clear.frame(height: 6)
                    }
                }
                .animation(.spring(), value: self.currentTab)
                
            }
           .buttonStyle(.plain)
        }
      }
    }

I would be very happy if you could help me with this.
",76382683,"Change
var tabBarOptions: [Image] = [Image(""bim"")]

to
var tabBarOptions: [String] = [""bim""]

And change
var tabBarItemName: Image

to
var tabBarItemName: String

The issue:
Image(tabBarItemName) is expecting a String, not an Image
","The error message is indicating that you're trying to pass an `Image` type to the `Image` view's initializer, which expects a `String` argument instead. To resolve this issue, you need to convert the `Image` instance to a `String` representing the name of the image.

To fix the error, you can modify the `CategoryView` struct as follows:

```swift
struct CategoryView: View {
    @Binding var currentTab: Int
    var tab: Int
    var tabBarItemName: Image
    
    var body: some View {
        Button {
            self.currentTab = tab
        } label: {
            VStack {
                tabBarItemName
                    .renderingMode(.original)
                    .resizable()
                    .frame(width: 12, height: 12)
                    .padding(.leading, 65)
    
                if currentTab == tab {
                    Color.init(red: 0.965, green: 0.224, blue: 0.49)
                        .frame(height: 2)
                        .padding(.leading, 65)
                } else {
                    Color.clear.frame(height: 6)
                }
            }
            .animation(.spring(), value: self.currentTab)
        }
        .buttonStyle(.plain)
    }
}
```

In this updated code, we've removed the `Image` initializer around `tabBarItemName` and made some modifications to the `Image` view itself. We've added the `.renderingMode(.original)` modifier to preserve the original appearance of the image, the `.resizable()` modifier to allow it to be resized, and set the frame size accordingly.

With these changes, you should be able to use the `tabBarItemName` variable of type `Image` in the `CategoryView` without any conversion issues."
76380711,express js error handler(s) get ignored after first one,"I have an express js app with some routes and a protected route that matches all the calls to an endpoint that starts with /app. after that i add generic error handler to the app like so:

app.use(""/app"", authenticate);

// app.post(""/new_user"", validate({body: schema}), route_handler);
// more routes etc. 

app.use((error: TypeError | ValidationError, req: Request, res: Response, next: NextFunction) => {
        console.log(""CHECK FOR VALIDATION ERROR"");
        if(error instanceof ValidationError) {
            console.log(""VALIDATION ERROR FOUND"")
            res.status(403).send(error);
        } else {
            console.log(""NO VALIDATION ERROR"")
            next();
        }
    });

    app.use((error: TypeError | AuthError, req: Request, res: Response, next: NextFunction) => {
        console.log(""CHECK FOR AUTH ERROR"");
        if(error instanceof AuthError) {
            console.log(""AUTH ERROR FOUND"");
            res.status(403).send({msg: ""Authentication failed""});
        } else {
            console.log(""NO AUTH ERROR"")
            next();
        }
    });

    app.use((error: Error, req: Request, res: Response, next: NextFunction) => {
        console.log(""CHECK GENERIC ERROR"");
        if(error) {
            res.status(500).send({msg: ""Some generic error happend""});
        } else {
            next();
        }
    });

    app.use((req: Request, res: Response, next: NextFunction) => {
        console.log(""ROUTE NOT FOUND"");
        res.status(404).send({
            msg: ""this endpoint was not found"",
        });

        next();
    });

When i do a request to for example the following endpoint: https://localhost/app/ and throw a AuthError on purpose in this endpoint the only console.log()'s i ever see are:
CHECK FOR VALIDATION ERROR
NO VALIDATION ERROR
ROUTE NOT FOUND
But i think i should see the following:
CHECK FOR VALIDATION ERROR
NO VALIDATION ERROR
CHECK FOR AUTH ERROR
AUTH ERROR FOUND
Why is my Auth error middleware never called??
",76381383,"You need to pass error to the other error middleware when you call next like this:
app.use((error: TypeError | AuthError, req: Request, res: Response, next: NextFunction) => {
        console.log(""CHECK FOR AUTH ERROR"");
        if(error instanceof AuthError) {
            console.log(""AUTH ERROR FOUND"");
            res.status(403).send({msg: ""Authentication failed""});
        } else {
            console.log(""NO AUTH ERROR"")
            next(error);
        }
    });

Without it, the normal middleware will get called.
","The reason your Auth error middleware is not being called is because you are not properly chaining the middleware calls using the `next` function. In your current implementation, after the validation error middleware, you're calling `next()` even if there was an error. This causes the control to move to the next middleware in the chain, which is the ""Route not found"" middleware, skipping the Auth error middleware.

To fix this issue, you need to modify your error handling middleware to stop the middleware chain when an error is encountered. You can achieve this by removing the `next()` call when an error is handled. Here's the modified code:

```javascript
app.use(""/app"", authenticate);

app.post(""/new_user"", validate({ body: schema }), route_handler);
// more routes etc.

app.use((error: TypeError | ValidationError, req: Request, res: Response, next: NextFunction) => {
  console.log(""CHECK FOR VALIDATION ERROR"");
  if (error instanceof ValidationError) {
    console.log(""VALIDATION ERROR FOUND"");
    res.status(403).send(error);
  } else {
    console.log(""NO VALIDATION ERROR"");
    next(error); // Pass the error to the next middleware
  }
});

app.use((error: TypeError | AuthError, req: Request, res: Response, next: NextFunction) => {
  console.log(""CHECK FOR AUTH ERROR"");
  if (error instanceof AuthError) {
    console.log(""AUTH ERROR FOUND"");
    res.status(403).send({ msg: ""Authentication failed"" });
  } else {
    console.log(""NO AUTH ERROR"");
    next(error); // Pass the error to the next middleware
  }
});

app.use((error: Error, req: Request, res: Response, next: NextFunction) => {
  console.log(""CHECK GENERIC ERROR"");
  if (error) {
    res.status(500).send({ msg: ""Some generic error happened"" });
  } else {
    next();
  }
});

app.use((req: Request, res: Response, next: NextFunction) => {
  console.log(""ROUTE NOT FOUND"");
  res.status(404).send({
    msg: ""This endpoint was not found"",
  });

  next();
});
```

By calling `next(error)` instead of `next()` in the error handling middleware, you pass the error to the next middleware in the chain, ensuring that the appropriate error handling middleware is invoked."
76383987,Notification Manager getSystemService() call not working,"I am following this tutorial on creating notifications
However, I cannot seem to do the following:
NotificationManager notificationManager = getSystemService(NotificationManager.class);

Because my Android studio reports the error:
Required Type: Context
Provided: Class <android.app.NotificationManager>
reason: Class<NotificationManager> is not compatible with Context

Please let me know how to resolve this issue.
",76384806,"I found this was my solution:
NotificationManager notificationManager = (NotificationManager) context.getSystemService(Context.NOTIFICATION_SERVICE);

","The error you're encountering is because the `getSystemService()` method requires a `Context` as an argument, but you're providing `Class<NotificationManager>` instead. 

To resolve this issue, you need to call the `getSystemService()` method on a valid `Context` object. If you are trying to access the `NotificationManager` from an activity or a service, you can use the `this` keyword to refer to the current context. Here's an example:

```java
NotificationManager notificationManager = (NotificationManager) getSystemService(Context.NOTIFICATION_SERVICE);
```

Make sure you're calling this code within an activity or service where the `getSystemService()` method is available."
76382344,Can an f77 subroutine be called from C?,"In modern Fortran, we can call a subroutine from C by using C binding. E.g. The Fortran subroutine will look like this
subroutine my_routine(...) bind (C, name=""my_routine"")

However, if the fortran subroutine is an old f77 subroutine, this binding is presumably not an available solution. What would be the best alternative?
",76382710,"
However, if the fortran subroutine is an old f77 subroutine, this binding is presumably not an available solution.

It depends.  Modern Fortran is largely backwards compatible with Fortran 77.  Your F77 subroutine will not come with a bind attribute, but if you're willing to add one and compile it with a modern Fortran compiler then chances are pretty good that it will work just as you expect of code originally written for a newer Fortran version.
On the other hand, what C interop brings to Fortran is standardization, not a new capability.  People have been calling Fortran routines from C almost as long as people have been using C.
The issue here is that the specifics are system and compiler dependent.  If you don't rely on C interop, then you will need to accommodate your Fortran compiler's name-mangling and argument-passing conventions to successfully call Fortran-compiled functions from C code.  These vary.  There used to be a fair amount of tools and documentation for such activity.  I imagine it's harder to find these days, but much of it should still work.

What would be the best alternative?

That depends on your specific constraints.  If you're up for modifying the Fortran source and compiling with a modern compiler then I'd at least start with adding a bind attribute to the function, and see how that plays out.
","If you have an old Fortran 77 subroutine that does not support C binding, there are still ways to call it from C code. Here are a few alternatives:

1. Modify the Fortran subroutine: You can modify the Fortran subroutine to add C-compatible interfaces or use the ISO_C_BINDING module to define interoperability with C. This approach requires modifying the Fortran code and recompiling it, but it allows you to establish a C-compatible interface.

2. Create a wrapper subroutine: Write a new Fortran subroutine that acts as a wrapper around the old Fortran 77 subroutine. This new subroutine should have a C-compatible interface that can be called from your C code. Inside the wrapper subroutine, you can invoke the old Fortran 77 subroutine using its original calling conventions.

3. Use an interface file: Another option is to create an interface file that specifies the calling conventions of the Fortran subroutine. This file can be used by the C code to correctly interface with the Fortran subroutine. The interface file should provide the necessary information about the subroutine's name, argument types, and calling convention. This approach avoids modifying the original Fortran code but requires maintaining an additional interface file.

4. Use a wrapper in C: Write a wrapper function in C that can interface with the old Fortran 77 subroutine. This wrapper function should handle any necessary conversions between C and Fortran data types and calling conventions. You can then call the wrapper function from your C code, which will, in turn, invoke the Fortran 77 subroutine.

It's worth noting that the best alternative depends on your specific requirements and constraints. If possible, modifying the Fortran code or creating a wrapper subroutine with a C-compatible interface is generally the preferred approach, as it provides a cleaner and more maintainable solution. However, if modifying the Fortran code is not an option, using a wrapper in either Fortran or C can still allow you to call the old Fortran 77 subroutine from your C code."
76382433,How can I make an adaptive list to check against another adaptive list?,"I apologize from the start as I am not allowed to share the workbook I am working on (it has confidential information from work) but I will do my best to explain what is happening and what my issue is.
I have two sheets, ""Tracker"" and ""Reviewers"". In the tracker names are recorded in column L and their submission is recorded in column M. Everything runs on a serial code in column A so there are blank cells between names. Some people have multiple submissions so their names show multiple times in column L. In the reviewers sheet, I have:
=UNIQUE(FILTER(Tracker!L4:L4999,Tracker!L4:L4999<>0))

In cell A2 to pull all the names of people who have a submission. This works flawlessly and adapts to include any new people. Then in cell B2 I have written:
=SUMPRODUCT(IF(ISBLANK(FILTER(Tracker!$L$4:$M$4999,Tracker!$L$4:$L$4999=Reviewers!A2#))=TRUE,1,0))

The idea here was to get a count of how many ""submissions"" people have without actually writing anything. It is filtering the list of names and submissions by name in the list we just created, checking if their ""submission"" is a blank cell, then adding them up. Issue is that it works when I filter by cell A2 but not when I filter by the function that spills out of cell A2 (A2#). I need it to be adaptive so if new names are added it can make the list longer, hence why I cannot just pull the cells down the list (A2, A3, A4,...). How would you go about getting a check of how many are blank like this?
As an example, Tracker could have:




Name
Submission




Jim
Idea


Bob
Idea


Pam



Sam
Idea


Jim



Bob
Idea


Jim



Pam
Idea




And Reviewers should return:




Name
#Blank




Jim
2


Bob
0


Pam
1


Sam
0




I hope this makes sense and I hope you can help me edit the equation in cell B2 of the Reviewers sheet to be adaptive and spill the results.
",76382759,#ERROR!,
76382452,How to show the data of the list outside of the area of ListView Builder in Flutter?,"Is it technically impossible to show the data outside of the list? I searched through the internet but I couldn't get any answers at all smh -_-
I wanted to display the value of data rows of the list besides of the section ListViewBuilder.
Output:

[ ListView Builder  Screen ]


Name: You, Age: 20 Name: Him, Age: 20

An Output photo there.
enter image description here
String name = userList[index].name;
int? age = userList[index].age;

class _Passenger extends State<Passenger> {
  TextEditingController nameController = TextEditingController();
  TextEditingController ageController = TextEditingController();
  int currentIndex = 0;

  final form = GlobalKey<FormState>();
  bool update = false;
  final User user = User(name: """", age: int.tryParse(''));
  List<User> userList = [
    User(name: ""You"", age: 20),
    User(name: ""Him"", age: 20),
  ];
  String text = '';
  int? number = int.tryParse('');
  @override
  Widget build(BuildContext context) {
    return MaterialApp( debugShowCheckedModeBanner: false,
    home: Scaffold(
        body: Column(children: <Widget>[
      Column(
          children: <Widget>[
            Container(
              height: 550,
              decoration: BoxDecoration(border: Border.all(color: Colors.black)),
              child: ListView.builder(
                  itemCount: userList.length,
                  itemBuilder: (context, index) {
                    String name = userList[index].name;
                    int? age = userList[index].age;

                    return SizedBox(
                      width: 20,
                      child: Card(
                          color: Colors.grey,
                          child: Padding(
                              padding: const EdgeInsets.all(2.0),
                              child: ListTile(
                                title: Text( ""Name: $name    Age: $age""),
                              ))),
                    );
                  }),
            ),
          ],
        ),
      Container( child: Text(""Display the data here, How?"") ),
      //Add Button
      Container(
          width: 150,
          height: 50,
          margin: const EdgeInsets.all(10),
          child: ElevatedButton(
            onPressed: () {
              showDialog(
                  context: context,
                  builder: (context) => SimpleDialog(children: [
                        TextField(
                          decoration: const InputDecoration(labelText: 'Name'),
                          onChanged: (value) {
                            setState(() {
                              text = value;
                            });
                          },
                        ),
                        TextField(
                          keyboardType: TextInputType.number,
                          decoration: const InputDecoration(labelText: 'Age'),
                          onChanged: (value) {
                            setState(() {
                              number = int.parse(value);
                            });
                          },
                        ),
                        ElevatedButton(
                            onPressed: () {
                              setState(() {
                                userList.add(User(name: text, age: number));
                              });
                            },
                            child: const Text('Add'))
                      ]));
            },
            child: const Text(""Add""),
          )),
    ])));
  }
}

class User {
  String name;
  int? age;
  User({
    required this.name,
    this.age,
  });
}

",76382772,"So... if it's the same list, just add this :
Instead :
   Container( child: Text(""Display the data here, How?"") )

Do :
          SingleChildScrollView(
            scrollDirection: Axis.horizontal,
            child: Row(
              mainAxisAlignment: MainAxisAlignment.center,
              children: userList
                  .map((user) => Text(""Name: ${user.name}, Age: ${user.age} ""))
                  .toList(),
            ),
          ),

I added a SingleChildScrollView with horizontal scroll to avoid problems
To send the list of users to another page, just do :
Navigator.push(
  context,
  MaterialPageRoute(
    settings: const RouteSettings(name: ""no-route""),
    builder: (context) => OtherPage(userList: userList),
  ),
);


class OtherPage extends StatelessWidget {
  final List<User> userList;

  const OtherPage({
    required this.userList,
    Key? key,
  }) : super(key: key);

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: Column(
                mainAxisAlignment: MainAxisAlignment.center,
                children: userList
                    .map(
                        (user) => Text(""Name: ${user.name}, Age: ${user.age} ""))
                    .toList(),
              ),
    );
  }
}


","To fix this issue, you can modify your `authenticate` middleware to call `next(error)` when an error occurs. This will pass the error to the error handling middleware stack, allowing subsequent error handlers to handle it. Here's an example of how you can modify the `authenticate` middleware:"
76380806,Firebase Cloud Functions V2: The request was not authorized to invoke this service,"I'm attempting to call a callable cloud function (which is already deployed) from a client app and getting this error on the GCP logs:
{
httpRequest: {9}
insertId: ""647865c20002422d2d32b259""
labels: {1}
logName: ""projects/faker-app-flutter-firebase-dev/logs/run.googleapis.com%2Frequests""
receiveTimestamp: ""2023-06-01T09:32:50.154902339Z""
resource: {2}
severity: ""WARNING""
spanId: ""11982344486849947204""
textPayload: ""The request was not authorized to invoke this service. Read more at https://cloud.google.com/run/docs/securing/authenticating Additional troubleshooting documentation can be found at: https://cloud.google.com/run/docs/troubleshooting#401""
timestamp: ""2023-06-01T09:32:50.138090Z""
trace: ""projects/faker-app-flutter-firebase-dev/traces/ddcb5a4df500af085b7a7f6f89a72ace""
traceSampled: true
}


The same function works correctly from the Firebase Local Emulator, so I assume this is a permissions issue related to IAM and service accounts (I still don't understand too well how IAM works).
Here is my code:
import * as admin from ""firebase-admin""
import * as functions from ""firebase-functions/v2""
import * as logger from ""firebase-functions/logger"";

// https://github.com/firebase/firebase-tools/issues/1532
if (admin.apps.length === 0) {
  admin.initializeApp()
}

export const deleteAllUserJobs = functions.https.onCall(async (context: functions.https.CallableRequest) => {
  const uid = context.auth?.uid
  if (uid === undefined) {
    throw new functions.https.HttpsError(""unauthenticated"", ""You need to be authenticated to perform this action"")
  }
  const firestore = admin.firestore()
  const collectionRef = firestore.collection(`/users/${uid}/jobs`)
  const collection = await collectionRef.get()
  logger.debug(`Deleting ${collection.docs.length} docs at ""/users/${uid}/jobs""`)
  // transaction version
  await firestore.runTransaction(async (transaction) => {
      for (const doc of collection.docs) {
          transaction.delete(firestore.doc(`/users/${uid}/jobs/${doc.id}`))
      }
  })

  logger.debug(`Deleted ${collection.docs.length} docs at ""/users/${uid}/jobs""`)
  return {""success"": true}
})

The function was deployed with firebase deploy --only functions, and I made sure the client app calls this function when the user is already authorized.
According to the docs:

If you encounter permissions errors when deploying functions, make sure that the appropriate IAM roles are assigned to the user running the deployment commands.

The docs also link to this page, which says:

Cloud Functions for Firebase permissions
For a list and descriptions of Cloud Functions permissions, refer to
the IAM documentation.
Be aware that the deployment of functions requires a specific
configuration of permissions that aren't included in the standard
Firebase predefined roles. To deploy functions, use one of the
following options:
Delegate the deployment of functions to a project Owner.

If you're deploying only non-HTTP functions, then a project Editor can deploy your functions.

Delegate deployment of functions to a project member who has the following two roles:
    Cloud Functions Admin role (roles/cloudfunctions.admin)
    Service Account User role (roles/iam.serviceAccountUser)

A project Owner can assign these roles to a project member using the Google Cloud Console or gcloud CLI. For detailed steps and

security implications for this role configuration, refer to the IAM
documentation.

But like I said, I can successfully deploy the function. It's when I try to execute it that I get an error log.
In summary, what I'm trying to do is quite basic:

write a callable cloud function
deploy it
call it from the client app

When the function runs, it fails with the error above.
Any advice? Do I need to set a specific IAM role?
",76381408,"Open https://console.cloud.google.com/iam-admin/<project_name>, find the service account you are using in your firebase project and add the Rol ""Cloud Functions Invoker"".

Is like Admin, Editor or Viewer roles are about manipulating the function on GCP (don't allow you to use it) and Invoker allows that account to invoke the function.
",
76384708,How to set document quality while ingesting document into MarkLogic through PutMarkLogic NiFi processor?,"I'm using NiFi 1.21.0 and nifi-marklogic-nar-1.9.1.6.
I have been using PutMarkLogic 1.9.1.6 processor to ingest documents to MarkLogic-db for more than 2 years. Recently witnessed that the processor doesn't support adding document-quality (PFB the processor image).

So I have created a new issue against marklogic/nifi project.
",76384807,"This enhancement was fixed as part of https://github.com/marklogic/nifi/pull/121. Therefore use nifi-marklogic-nar-1.15.3.1 or later version to set document-quality. I'm currently using PutMarkLogic 1.16.3.2 and I can now see a provision to add Quality.

",```javascript
76381224,Loop through rows of a table on outlook and change (Text To Display) to an ascending number per each row,"I have used the below code to Loop through selection on outlook and convert into Hyperlinks and change Text To Display Link.
it works but it adds the the ascending number incrementally to all cells like this picture:

My need is to add the ascending number per each row like this picture:

In advance, great thanks for all your help.
Sub Hyperlink_Outlook()

  Dim wDoc As Word.Document, rngSel As Word.Selection, cel As Cell, i As Long
  
  Set wDoc = Application.ActiveInspector.WordEditor
  Set rngSel = wDoc.Windows(1).Selection
    
  If Not rngSel Is Nothing And rngSel.Information(wdWithInTable) Then
    If rngSel.Range.Cells.Count > 0 Then
       For Each cel In rngSel.Cells
         If Len(cel.Range.Text) > 10 Then
           i = i + 1
            wDoc.Hyperlinks.Add cel.Range, _
             Address:=Left(cel.Range.Text, Len(cel.Range.Text) - 1), _
             TextToDisplay:=""Attachment "" & i
         End If
       Next
    End If
  End If
  
End Sub

",76381430,"Try looping through rows first (the following is not tested):
Sub Hyperlink_Outlook()

  Dim wDoc As Word.Document, rngSel As Word.Selection, cel As Cell, i As Long
  Dim r As Variant
  
  Set wDoc = Application.ActiveInspector.WordEditor
  Set rngSel = wDoc.Windows(1).Selection
    
  If Not rngSel Is Nothing And rngSel.Information(wdWithInTable) Then
    If rngSel.Range.Cells.Count > 0 Then
      For Each r In rngSel.Rows
        i = 0 ' reset i here
        For Each cel In r.Cells
          If Len(cel.Range.Text) > 10 Then
            i = i + 1
             wDoc.Hyperlinks.Add cel.Range, _
              Address:=Left(cel.Range.Text, Len(cel.Range.Text) - 1), _
              TextToDisplay:=""Attachment "" & i
          End If
        Next cel
      Next r
    End If
  End If
  
End Sub

","const authenticate = (req, res, next) => {"
76382642,Resampling Rows minute wise not working in for Even Minutes in Python DataFrame,"I have df which has 5 columns. A column named date which has minute-wise data of a few days but the data start at 9:15 and ends at 15:29. And then there are four other columns which are named first, max, min, and last which have numerical numbers in them.
I wrote a code that uses x mins as a variable. It resamples the rows and gives rows of x minutes.
The first of resampled will be the 'first' of first row. 
The 'last' of resampled will be the 'last' of the last row. 
The max of resampled will be the highest of all the rows of the max column. 
The low of resampled will be low of all the rows for the low column.
And the date will have datetime of x minutes intervals.
My problem is for some minutes the code is working perfectly. But for other minutes I am getting the wrong time as the first row.
Instead of resampled data starting from 9:15. It starts with some other minute.
Code:
def resample_df(df, x_minutes = '15T'):
    
    df.set_index('date', inplace=True)

    resampled_df = df.resample(x_minutes).agg({
        'first': 'first',
        'max': 'max',
        'min': 'min',
        'last': 'last'
    })

    resampled_df.reset_index(inplace=True)

    return resampled_df

Input:
    date                   first    max         min         last
0   2023-06-01 09:15:00 0.014657    0.966861    0.556195    0.903073
1   2023-06-01 09:16:00 0.255174    0.607714    0.845804    0.039933
2   2023-06-01 09:17:00 0.956839    0.881803    0.876322    0.552568

Output: when x_minutes = '6T'
    date                   first    max         min         last
0   2023-06-01 09:12:00 0.014657    0.966861    0.556195    0.552568
1   2023-06-01 09:18:00 0.437867    0.988005    0.162957    0.897419
2   2023-06-01 09:24:00 0.296486    0.370957    0.013994    0.108506

The data shows 9:12 but I don't have 9:12. Why is it giving me the wrong data?
Note: It works prefectly when minutes entered are odd. e.g. x_minutes = '15T'
Code to create a dummy df:
import pandas as pd
import random
from datetime import datetime, timedelta

# Define the number of days for which data is generated
num_days = 5

# Define the start and end times for each day
start_time = datetime.strptime('09:15', '%H:%M').time()
end_time = datetime.strptime('15:30', '%H:%M').time()

# Create a list of all the timestamps for the specified days
timestamps = []
current_date = datetime.now().replace(hour=start_time.hour, minute=start_time.minute, second=0, microsecond=0)
end_date = current_date + timedelta(days=num_days)
while current_date < end_date:
    current_time = current_date.time()
    if start_time <= current_time <= end_time:
        timestamps.append(current_date)
    current_date += timedelta(minutes=1)

# Generate random data for each column
data = {
    'date': timestamps,
    'first': [random.random() for _ in range(len(timestamps))],
    'max': [random.random() for _ in range(len(timestamps))],
    'min': [random.random() for _ in range(len(timestamps))],
    'last': [random.random() for _ in range(len(timestamps))]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Display the resulting DataFrame
display(df)

",76382822,"Use:
resampled_df = df.resample(x_minutes, origin = 'start').agg({
    'first': 'first',
    'max': 'max',
    'min': 'min',
    'last': 'last'
})

",// Authentication logic
76384685,SwiftUI Toggle how to distinguish changing value by UI action vs changing programatically,"If i have a toggle, which updates its state from external async load but also by user intput, how can i differentiate those two? eg. to perform a special action on user action
    Group {
        Toggle(isOn: $on) {
            EmptyView()
        }
    }
    .onChange(of: on) { newValue in
        was ""on"" changed by user or onAppear async update?
    }
    .onAppear {
        async update on
    }

PS: this is mostly for macOS, and there the tapGesture on Toggle doesn't work
",76384818,"If you want a side effect for use the user actions, you can use a custom wrapper Binding:
struct ContentView: View {
    @State private var on: Bool = false
    
    var userManagedOn: Binding<Bool> {
        .init {
            return on
        } set: { newValue in
            print(""Side effect"")
            on = newValue
        }
    }
    
    var body: some View {
        VStack {
            Group {
                Toggle(isOn: userManagedOn) {
                    EmptyView()
                }
            }
        }
        .padding()
        .onAppear {
            Task { @MainActor in
                try? await Task.sleep(nanoseconds: NSEC_PER_SEC)
                on.toggle()
            }
        }
    }
}

",if (authenticationFails) {
76380987,How can I change the color of an icon when a checkbox is checked using Tailwind?,"<input type=""checkbox"" id=""darkmode-toggle"" class=""peer invisible h-0 w-0"" />
<label for=""darkmode-toggle"" class=""btn-toggle group"">
    <svg class=""icon absolute"">
        <use href=""../../assets/icons/spirit.svg#sun"" />
    </svg>
    <svg class=""icon absolute  group-[peer-checked]:fill-secondary-dark-300"">
        <use href=""../../assets/icons/spirit.svg#moon"" />
    </svg>
</label>

In the given HTML code (using Tailwind CSS), I want to change the color of the icon when the associated input checkbox is checked.
The attempted selector used is group-[peer-checked]:fill-secondary-dark-300. However, the desired icon color change is not happening.
How can I achieve the desired result of changing the color of the icon when the input checkbox is checked using the provided selector?
This is what I want to achieve with this selector:

group (select the parent ""label"")

[peer-checked] (when the sibling of the label ""input"" is checked)

fill-secondary-dark-300: change the icon color.


",76381434,"You could consider using group-[.peer:checked+&]::


tailwind.config = {
  theme: {
    extend: {
      colors: {
        'secondary-dark-300': 'red',
      },
    },
  },
};
<script src=""https://cdn.tailwindcss.com""></script>

<input type=""checkbox"" id=""darkmode-toggle"" class=""peer invisible h-0 w-0"" />
<label for=""darkmode-toggle"" class=""btn-toggle group"">
  <svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 24 24"" fill=""currentColor"" class=""w-6 h-6"">
    <path d=""M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0zM18.894 6.166a.75.75 0 00-1.06-1.06l-1.591 1.59a.75.75 0 101.06 1.061l1.591-1.59zM21.75 12a.75.75 0 01-.75.75h-2.25a.75.75 0 010-1.5H21a.75.75 0 01.75.75zM17.834 18.894a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 10-1.061 1.06l1.59 1.591zM12 18a.75.75 0 01.75.75V21a.75.75 0 01-1.5 0v-2.25A.75.75 0 0112 18zM7.758 17.303a.75.75 0 00-1.061-1.06l-1.591 1.59a.75.75 0 001.06 1.061l1.591-1.59zM6 12a.75.75 0 01-.75.75H3a.75.75 0 010-1.5h2.25A.75.75 0 016 12zM6.697 7.757a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 00-1.061 1.06l1.59 1.591z"" />
  </svg>
 
  <svg class=""w-6 h-6 group-[.peer:checked+&]:fill-secondary-dark-300"" xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 24 24"" fill=""currentColor"">
    <path fill-rule=""evenodd"" d=""M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z"" clip-rule=""evenodd"" />
  </svg>
</label>



","next(new AuthError(""Authentication failed""));"
76380618,Javascript - alert problem with onblur and focus-visible Firefox/Chrome,"In onblur I need to call alert(), but this doesn't work in Chrome and Firefox. Sess https://jsfiddle.net/mimomade/5sur482w/1/
In Firefox :focus-visible stays after leaving the 2nd and 4th input field and is not removed.
In Chrome I can't leave the 2nd input field. Although the 1st doesn't has any problem.
",76381442,"At the very bottom is the code with both bugs fixed. You're initial JavaScript looks like this:
// Has different bugs in Firefox and Chrome.
function blurring(el) {
  console.log(el.id + ' pre  alert');
  alert('blurring ' + el.id);
  console.log(el.id + ' post alert');
}

In Firefox, your apparent bug actually masks a bug similar to what you're encountering in Chrome. When the alert is removed, the code has the intended behavior, so alert and the event are interacting in a weird way. In this specific case, to get around this, we can just wait for the event to finish by wrapping the function in a zero-millisecond timeout.
// Has a similar bug in both browsers.
function blurring(el) {
  console.log(el.id + ' pre  alert');
  setTimeout(function () {
    alert('blurring ' + el.id);
    console.log(el.id + ' post alert');
  }, 0);
}

In Chrome, your bug appears to be caused by the blur event emitting each time the alert box is closed. Luckily, because we wait for the events to finish, the active element should be the element newly selected input instead of whatever the browser set it to. This means checking ensuring el and document.activeElement are different is sufficient to fix this bug.
// addresses both bugs.
function blurring(el) {
  console.log(el.id + ' pre  alert');
  setTimeout(function () {
    if (document.activeElement !== el) {
      alert('blurring ' + el.id);
      console.log(el.id + ' post alert');
    }
  }, 0);
}

",} else {
76382726,How to type NextJS router.query.id as string?,"I'm pretty new to TypeScript, as well as using the T3 stack (React Query / Tanstack Query).  I'm trying to type companyId as string, so that I don't have to type companyId as string every time I use it later on it in the code, but I can't figure out how to best to that or what the best practice is with this stack... I'm used to plain old JavaScript and useEffects to do API calls (and probably writing worse code).
Note:  the following code exists at /pages/companies/[id].tsx
The following is my first attempt, but I get a  ""Rendered more hooks than during the previous render"" error at ""const { data: company} ..."", which makes sense:
const CompanyPage: NextPage = () => {
  const router = useRouter()

  const companyId = router.query.id
  if (!companyId || Array.isArray(companyId)) return <div>Loading...</div> // have to check for Array.isArray because of NextJS/Typescript bug

  const { data: company } = api.companies.getSingleById.useQuery({companyId: companyId});
  if (!company ) return <div>Loading...</div>

  ...
  return (...)

I tried doing the following, but I was not allowed because the type for companyId from router.query.id is string | string[] | undefined.
const CompanyPage: NextPage = () => {
  const router = useRouter()

  const companyId: string = router.query.id // Type 'string | string[] | undefined' is not assignable to type 'string'

  const { data: company } = api.companies.getSingleById.useQuery({companyId: companyId});
  if (!company ) return <div>Loading...</div>

  ...
  return (...)

UPDATE:
I changed it to the following now, which seems to work, but it doesn't feel quite right it's the correct way to do things.  (With this method, I only have to write companyId as string once, which is fine.)
const CompanyPage: NextPage = () => {
  const router = useRouter()

  const companyId = router.query.id
  const { data: company } = api.companies.getSingleById.useQuery({companyId: companyId as string});

  if (!companyId || Array.isArray(companyId)) return <div>Loading...</div> // have to check for Array.isArray because of NextJS/Typescript bug
  if (!company ) return <div>Loading...</div>

  ...
  return (...)

ANSWER:
Thank you to Fabio for the accepted answer.
I'm destructuring router.query into multiple variables on other routes, so this is an example of doing that based on the accepted answer:
const { companyId, locationId } = useMemo(() => ({
  companyId: router.query?.companyId?.toString() ?? """",
  locationId: router.query?.locationId?.toString() ?? """",
}), [router.query?.companyId, router.query?.locationId]);

",76382845,"You can use optional chaining and nullish coalescing in combination with the query params' toString() method, heres an example based on your code snippet:
const CompanyPage: NextPage = () => {
  const router = useRouter();

  // this result will be of type string
  const companyId = useMemo(() => {
    return router.query?.id?.toString?.() ?? """";
  }, [router.query?.id]);

  const { data: company } = api.companies.getSingleById.useQuery({
    companyId: companyId,
  });

  if (!company) return <div>Loading...</div>;

  return; // ...
};

The reason why the query parameters are of type string | string[] is because if you had a catch-all segment each of the url parts will be split by / and presented to you as an array.
Example
If you had the route /some-segment/[...href] and the URL would be /some-segment/200/my-post following would apply:

The contents of the href query variable would be [""200"", ""my-post""]
Calling href?.toString?.() would return you /some-segment/200/my-post.

",next();
76384713,"Tried to use state variable inside another state variable, but it didn't work as expected it to, Why ? (in ReactJS using function component)","I was trying to use a state variables value inside another state variable, but when the other sub state variable was updated outside, its updated value didn't get reflected in parent state variable, why did this happened? can I not use state variable inside another state variable or is there any specific way to use it?
Can anyone explain this please?
const [noOfPersonstForRoom, setNoOfPersonsForRoom] = useState([2]);

const [ageOfPerson1ForRoom, setAgeOfPerson1ForRoom] = useState([0]);
const [ageOfPerson2ForRoom, setAgeOfPerson2ForRoom] = useState([0]);

    const [rooms, setRooms] = useState([
    {
        roomNo: 1,
        noOfPersons: noOfPersonsForRoom[0], 
                    ageOfPerson1: ageOfPerson1ForRoom[0],
                    ageOfPerson2: ageOfPerson2ForRoom[0]
        

        
    },
]);

This is code of what I tried, with some changes.
To change sub state variable, I used following function, and it was called on onChange() of an input field:
const changeNoOfPersonsForRoom = (e, index) => {
        let newNoOfPersonsForRoom = e.target.value;
        setNoOfPersonsForRoom([
            ...noOfPersonsForRoom.slice(0, index),
            e.target.value,
            ...noOfPersonsForRoom.slice(index + 1),
        ]);

",76384822,"Could you add how you update your state(s)?
But in general, states are not bound or connected to each other.
If you change one state, it won't update any other state.
const [rooms, setRooms] = useState([
    {
        roomNo: 1,
        noOfPersons: noOfPersonsForRoom[0], 
        ageOfPerson1: ageOfPerson1ForRoom[0],
        ageOfPerson2: ageOfPerson2ForRoom[0]
    },

Given your example, you just set the initial state of rooms with the values of your previous states. Nothing more. If you need to update several states, you have to update each of them separately.
",}
76382432,What is the best way to check for consecutive missing values in a data column in R and exclude them based on a related column value?,"I am trying to write code in R for a dataset to check if DAYS column have consecutive numbers and print out the missing DAYS number, in such a way that, if the count of missing consecutive numbers between two rows of the DAYS column equals to that count+1 in the corresponding last row of the PERIOD column, exclude it from the output. For example, consider the two rows in DAYS column 163 and 165, where the count of missing number is 1. But in this case, the last row (where DAYS is 165) has PERIOD value of 2, that is (count+1). So, exclude this missing value (164) from the output. However if you look at DAYS 170 and 172,y you can see 172 has PERIOD value of 1 (not 2 or count+1). So, show this output (171).
Here is the first 28 rows of the dataset.
DAYS PERIOD
146 1
147 1
148 1
149 1
150 1
151 1
152 1
153 1
154 1
155 1
156 1
157 1
158 1
159 1
160 1
161 1
162 1
163 1
165 2
166 1
167 1
168 1
169 1
170 1
172 1
173 1
174 1
175 1




I tried
First, created a sequence of expected DAYS values
expected_days <- seq(min(hs$DAYS), max(hs$DAYS))
Then, find the missing DAYS values
missing_days <- setdiff(expected_days, hs$DAYS)
How to do the next bit?
",76382854,"I've managed to do this using tidyverse tools:
Set up example data
I've tweaked your data slightly to show that the solution can handle longer runs of missing days.
library(vroom)
library(dplyr)
library(tidyr)

test <-
  vroom(
    I(
""days period
161 1
162 1
163 1
166 3
167 1
168 1
169 1
170 1
172 1
""),
col_types = c(""ii""))


Add 'empty' days explicitly to data frame
all_days <- min(test[[""days""]]):max(test[[""days""]])

frame <- tibble(days = all_days)

test <-
  right_join(test, frame, by = ""days"") |> 
  arrange(days)

test
#> # A tibble: 12 × 2
#>     days period
#>    <int>  <int>
#>  1   161      1
#>  2   162      1
#>  3   163      1
#>  4   164     NA
#>  5   165     NA
#>  6   166      3
#>  7   167      1
#>  8   168      1
#>  9   169      1
#> 10   170      1
#> 11   171     NA
#> 12   172      1

Find the number of consecutive missing days
test <- 
  mutate(test,
         no_na = xor(is.na(period), is.na(lag(period))),
          missingness_group = cumsum(no_na)) |> 
  select(-no_na)

test <- 
  group_by(test, missingness_group) |> 
  mutate(missing_days = 
           case_when(
             all(is.na(period)) ~ n(),
             TRUE               ~ 0)) |> 
  ungroup() |> 
  select(-missingness_group)

test
#> # A tibble: 12 × 3
#>     days period missing_days
#>    <int>  <int>        <dbl>
#>  1   161      1            0
#>  2   162      1            0
#>  3   163      1            0
#>  4   164     NA            2
#>  5   165     NA            2
#>  6   166      3            0
#>  7   167      1            0
#>  8   168      1            0
#>  9   169      1            0
#> 10   170      1            0
#> 11   171     NA            1
#> 12   172      1            0

Remove rows where days are all accounted for
test <- mutate(test, extra_days = period - 1)

test <- fill(test, extra_days, .direction = ""up"")

test <-
  filter(test, !is.na(period) | missing_days > extra_days) |> 
  select(days, period)

test
#> # A tibble: 10 × 2
#>     days period
#>    <int>  <int>
#>  1   161      1
#>  2   162      1
#>  3   163      1
#>  4   166      3
#>  5   167      1
#>  6   168      1
#>  7   169      1
#>  8   170      1
#>  9   171     NA
#> 10   172      1

Created on 2023-06-01 with reprex v2.0.2
",};
76380830,Spring Boot Wrong date returned,"I have an entity Person
@Entity
@Data
public class Person {

  @Temporal(TemporalType.DATE)
  private Calendar dob;
}

And some dao classes
@Data
public class PersonResponse {

  @JsonFormat(pattern = ""yyyy-MM-dd"")
  private Calendar dob;
}

@Data
public class PersonRequest{
  @DateTimeFormat(pattern = ""yyyy-MM-dd"")
  private Calendar dob;
}

When storing values it works perfectly. Example if I send ""2000-01-01"" it's stored as is in the database ""2000-01-01"". But When I try to return it I get ""1999-12-31"".
Now it's clear that is a Timezone Problem but I don't know how to fix it.
My explanation for the cause
The user timezone is GMT+1 so it is some how retrieved as ""2000-01-01T00:00:00.000 +01:00"", then parsed to UTC ""1999-12-31T23:00:00.000 +00:00"" to finally be returned as ""1999-12-31"".
But why? And how can I prevent this knowing that users timezones can change (so adding the time offset manually of 1 hour won't work).
I tried also changing type from Calendar to java.util.Date and java.sql.Date... but no result.
Similar questions where asked before like this one but I still couldn't understand how to fix it
",76381444,"If Applicable try to switch from class Calendar to LocalDate. LocalDate does not take time zone into consideration. This should resolve your issue (and simplify your code). Also, for formatting the LocalDate with Json see the answer to this question: Spring Data JPA - ZonedDateTime format for json serialization
",
76384705,Python: Fix missing import with a monkey patch,"I am trying to monkey patch a missing import. The old_invoke() still does not get the import.
In case it is relevant, MyClass is a gdb.Command.
(gdb) pi
>>> import mymodule
>>> old_invoke = mymodule.MyClass.invoke
>>> def new_invoke(self, *k, **kw):
...   print(""New invoke"")
...   import getopt
...   old_invoke(self, *k, **kw)
...
>>> mymodule.MyClass.invoke = new_invoke
>>>
(gdb) my_gdb_command
New invoke
Python Exception <class 'NameError'> name 'getopt' is not defined:
Error occurred in Python: name 'getopt' is not defined

Also, in case it is relevant, the initial files and sourcing looks something like this:
mymodule.py:
import gdb
class MyClass(gdb.Command):
   ...

   def invoke(self, arg, from_tty):
      options, remainder = getopt.getopt(args, 'p:s:t:o:')
      ...

MyClass()

myothermodule.py:
import mymodule
...

Sourcing the above
(gdb) source myothermodule.py

",76384834,"old_invoke is trying to reference mymodule's getopt, which doesn't exist. You need:
>>> import mymodule
>>> old_invoke = mymodule.MyClass.invoke
>>> def new_invoke(self, *k, **kw):
...   print(""New invoke"")
...   import getopt
...   
...   # here
...   mymodule.getopt = getopt
...   
...   old_invoke(self, *k, **kw)
...
>>> mymodule.MyClass.invoke = new_invoke

But, realistically, you should just have an import getopt in mymodule:
# mymodule.py
import getopt
...

Then your function is simply:
>>> import mymodule
>>> old_invoke = mymodule.MyClass.invoke
>>> def new_invoke(self, *k, **kw):
...   print(""New invoke"")
...   old_invoke(self, *k, **kw)
...
>>> mymodule.MyClass.invoke = new_invoke

Addendum
As another note, using import in a function isn't generally recommended. Unless you are only calling this function once (and even then, why?), every time you call the function you are attempting to load a module, which at best will always do a check against sys.modules when you probably don't have to.
Have the import getopt at the top of the script:
import getopt
import mymodule

mymodule.getopt = getopt

Which is where you'd probably expect this to be anyways
","app.use(""/app"", authenticate);"
76382691,Adding a volume in dockerfile,"I am trying to create an image of Windows with additional things. My question is whether it is possible to include a specific volume when creating the container. For example, I would like to do:
docker run --name container -v shared:c:\shared -it mcr.microsoft.com/windows/servercore:20H2-amd64 powershell
There I am accessing the shared volume, but I want to do this in the dockerfile as a command.
I want something like this after running the container:

Thank you for the help
I tried to use the VOLUME command but I don't know if I am doing it right or it's not for what I am trying.
",76382862,"Using VOLUME in dockerfile does not mount the volume during build, this only specifies a target where a directory can be mounted during container runtime (anonymous volume).
Because image build and container run can happen on different machines, so having VOLUME source defined in dockerfile (buid time) does not make sense.
",```
76381353,How to change cursor size in PyQt5?,"I want to implement brush cursor like in most image editors when cursor is a circle that change its size according to brush size. I've read the docs and found only setShape method, but no setSize. Is it possible in Qt to change cursor size?
",76381491,"pixmap = QPixmap(""image.png"")  # Replace with the path to your custom cursor image, brush in your case
pixmap = pixmap.scaled(30, 30)  # Set the desired size
cursor = QCursor(pixmap)
self.setCursor(cursor)

you can change the size and the ""form"" of your cursor in PyQt5 by creating a pixmap and then assigning in to your cursor
",
76384330,subprocess.run command with non-utf-8 characters (UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb),"Encoding honestly continues to confuse me, so hopefully this isn't a totally daft question.
I have a python script that calls metaflac to compare the flac fingerprints in a file to the flac fingerprints of a file. Recently I came across files with » (https://bytetool.web.app/en/ascii/code/0xbb/) in the file name. This failed with how I was dealing with the file name strings, so I'm trying to work around that. My first thought was that I needed to deal with this as bytes objects. But when I do that and then call subprocess.run, I get a UnicodeDecodeError
Here's the snippet of code that is give me errors:
def test():
    directory = b'<redacted>'
    ffp_open = open(directory + b'<redacted>.ffp','rb')
    ffp_lines = ffp_open.readlines()
    print(ffp_lines)
    for line in ffp_lines:
        if not line.startswith(b';') and b':' in line:
            txt = line.split(b':')

            ffp_cmd = b'/usr/bin/metaflac --show-md5sum \'' + directory + b'/' + txt[0]+ b'\''
            print(ffp_cmd)
            get_ffp_process = subprocess.run(ffp_cmd, stdout=PIPE, stderr=PIPE, universal_newlines=True,shell=True)

With that, I get the following output (shortened to make more sense):
[b'01 - Intro.flac:eee7ca01db887168ce8312e7a3bdf8d6\r\n', b'04 - Song title \xbb Other Song \xbb.flac:98d2d03f47790d234052c6c9a2ca5cfd\r\n']
b""/usr/bin/metaflac --show-md5sum '<redacted>/01 - Intro.flac'""
b""/usr/bin/metaflac --show-md5sum '<redacted>/04 - Song title \xbb Other Song \xbb.flac'""

    get_ffp_process = subprocess.run(ffp_cmd, stdout=PIPE, stderr=PIPE, universal_newlines=True,shell=True)
  File ""<redacted>/python/lib/python3.9/subprocess.py"", line 507, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File ""<redacted>/python/lib/python3.9/subprocess.py"", line 1134, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
  File ""<redacted>/python/lib/python3.9/subprocess.py"", line 2021, in _communicate
    stderr = self._translate_newlines(stderr,
  File ""<redacted>/python/lib/python3.9/subprocess.py"", line 1011, in _translate_newlines
    data = data.decode(encoding, errors)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 85: invalid start byte

If I run this directly on the command line it works just fine (using tabs to fill in the file name):
metaflac --show-md5sum 04\ -\ Song\ title\ »\ Other Song\ ».flac 
98d2d03f47790d234052c6c9a2ca5cfd

The FFP file, through nano, looks like this:
01 - Intro.flac:eee7ca01db887168ce8312e7a3bdf8d6
04 - Song title � Other Song �.flac:98d2d03f47790d234052c6c9a2ca5cfd

I have no control over the file names, so I'm trying to be as flexible as possible to handle them, which is why I thought a bytes object would be best. I'd appreciate any direction. Thanks!
",76384840,"I believe coding of ""latin1"" or ""cp1252"" will do decode that successfully. Also, it is easier to deal with strings than with bytes, so here is my suggestion:
import pathlib
import subprocess

directory = pathlib.Path(""/tmp"")

with open(directory / ""data.ffp"", ""r"", encoding=""latin1"") as stream:
    for line in stream:
        if line.startswith("";""):
            continue
        if "":"" not in line:
            continue

        file_name, expected_md5sum = line.strip().split("":"")
        print(f""{name=}"")
        print(f""{expected_md5sum=}"")
        command = [
            ""/usr/bin/metaflac"",
            ""--show-md5sum"",
            str(directory / file_name)
        ]
        print(f""{command=}"")

        # Now you can run the command. I assume that the command will return a MD5 sum back.
        completed_process = subprocess.run(
            command,
            encoding=""latin1"",
            capture_output=True,
        )

        # Now, completed_process.stdout will hold the output
        # as a string, not bytes.

Here is a sample output:
name='04 - Song title » Other Song ».flac'
expected_md5sum='eee7ca01db887168ce8312e7a3bdf8d6\n'
command=['/usr/bin/metaflac', '--show-md5sum', '/tmp/01 - Intro.flac']
name='04 - Song title » Other Song ».flac'
expected_md5sum='98d2d03f47790d234052c6c9a2ca5cfd\n'
command=['/usr/bin/metaflac', '--show-md5sum', '/tmp/04 - Song title » Other Song ».flac']

Since my system does not have the metaflac command, I cannot test it. Please forgive any error that come up. If an error found, please post in the comment and I will try to fix it.
","By calling `next(new AuthError(""Authentication failed""))`, the error will be propagated to the next error handler in the stack, which should trigger the `CHECK FOR AUTH ERROR` log and handle the error accordingly."
76382379,"Optimize data for t.test to avoid ""data are essentially constant"" error","There are several StackOverflow posts about situation where t.test() in R produce an error saying ""data are essentially constant"", this is due to that there is not enough difference between the groups (there is no variation) to run the t.test(). (Correct me if there is something else)
I'm in this situation, and I would like to fix this buy altering my data the way the statistical features of the data don't change drastically, so the t-test stays correct. I was wondering what if I add some very little variation to the data (e.g. change 0.301029995663981 to 0.301029995663990), or what else can I do?
For example, this is my data:
# Create the data frame
data <- data.frame(Date = c(""2021.08"",""2021.08"",""2021.09"",""2021.09"",""2021.09"",""2021.10"",""2021.10"",""2021.10"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.12"",""2021.12"",""2022.01"",""2022.01"",""2022.01"",""2022.01"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.09"",""2022.09"",""2022.10"",""2022.10"",""2022.10"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.12"",""2022.12"",""2022.12"",""2022.12"",""2023.01"",""2023.01"",""2023.01"",""2023.01"",""2021.08"",""2021.08"",""2021.09"",""2021.09"",""2021.09"",""2021.10"",""2021.10"",""2021.10"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.12"",""2021.12"",""2022.01"",""2022.01"",""2022.01"",""2022.01"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.09"",""2022.09"",""2022.09"",""2022.09"",""2022.10"",""2022.10"",""2022.10"",""2022.10"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.12"",""2022.12"",""2022.12"",""2022.12"",""2023.01"",""2023.01"",""2023.01"",""2023.01""),
Species = c(""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",
""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",
""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B""),
Site = c(""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
""Something"",""Something"",""Something"",""Something""),
Mean = c(""0.301029995663981"",""1.07918124604762"",""0.698970004336019"",""1.23044892137827"",""1.53147891704226"",""1.41497334797082"",""1.7160033436348"",
         ""0.698970004336019"",""1.39794000867204"",""1"",""0.301029995663981"",""0.301029995663981"",""0.477121254719662"",""0.301029995663981"",""0.301029995663981"",
         ""0.301029995663981"",""0.477121254719662"",""0.301029995663981"",""0.301029995663981"",""0.845098040014257"",""0.301029995663981"",""0.301029995663981"",
         ""0.477121254719662"",""0.698970004336019"",""1.23044892137827"",""1.41497334797082"",""1.95904139232109"",""1.5910646070265"",""1.53147891704226"",
         ""1.14612803567824"",""1.57978359661681"",""1.34242268082221"",""0.778151250383644"",""0.301029995663981"",""0.301029995663981"",""0.477121254719662"",
         ""0.301029995663981"",""1.20411998265592"",""0.845098040014257"",""1.17609125905568"",""1.20411998265592"",""0.698970004336019"",""0.301029995663981"",
         ""0.698970004336019"",""0.698970004336019"",""0.903089986991944"",""1.14612803567824"",""0.301029995663981"",""0.602059991327962"",""0.301029995663981"",
         ""0.845098040014257"",""0.698970004336019"",""0.698970004336019"",""0.301029995663981"",""0.698970004336019"",""0.301029995663981"",""0.301029995663981"",
         ""0.301029995663981"",""0.477121254719662"",""0.301029995663981"",""0.301029995663981"",""0.301029995663981"",""0.301029995663981"",""0.301029995663981"",
         ""0.602059991327962"",""0.301029995663981"",""0.845098040014257"",""1.92941892571429"",""1.27875360095283"",""0.698970004336019"",""1.38021124171161"",
         ""1.20411998265592"",""1.38021124171161"",""1.14612803567824"",""1"",""1.07918124604762"",""1.17609125905568"",""0.845098040014257"",""0.698970004336019"",
         ""0.778151250383644"",""0.301029995663981"",""0.845098040014257"",""1.64345267648619"",""1.46239799789896"",""1.34242268082221"",""1.34242268082221"",
         ""0.778151250383644""))

After, I set the factors:
# Set factors
str(data)
data$Date<-as.factor(data$Date)
data$Site<-as.factor(data$Site)
data$Species<-as.factor(data$Species)
data$Mean<-as.numeric(data$Mean)
str(data)

When I try t.test():
compare_means(Mean ~ Species, data = data, group.b = ""Date"", method = ""t.test"")

This is the error:
Error in `mutate()`:
ℹ In argument: `p = purrr::map(...)`.
Caused by error in `purrr::map()`:
ℹ In index: 5.
ℹ With name: Date.2021.12.
Caused by error in `t.test.default()`:
! data are essentially constant
Run `rlang::last_trace()` to see where the error occurred.

Similarly, when I use this in ggplot:
ggplot(data, aes(x = Date, y = Mean, fill=Species)) +
  geom_boxplot()+
  stat_compare_means(data=data,method=""t.test"", label = ""p.signif"") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

Warning message:
Computation failed in `stat_compare_means()`
Caused by error in `mutate()`:
ℹ In argument: `p = purrr::map(...)`.
Caused by error in `purrr::map()`:
ℹ In index: 5.
ℹ With name: x.5.
Caused by error in `t.test.default()`:
! data are essentially constant 

What is the best solution, which keeps the data still usable in t-test?
",76382881,"Finding the sd of Mean for each Date-Species combination and then filtering out any Dates where any sd is 0 will do the trick.  You could even just pipe the filtered data to compare_means():
library(dplyr)
library(ggpubr)
data <- data.frame(Date = c(""2021.08"",""2021.08"",""2021.09"",""2021.09"",""2021.09"",""2021.10"",""2021.10"",""2021.10"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.12"",""2021.12"",""2022.01"",""2022.01"",""2022.01"",""2022.01"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.09"",""2022.09"",""2022.10"",""2022.10"",""2022.10"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.12"",""2022.12"",""2022.12"",""2022.12"",""2023.01"",""2023.01"",""2023.01"",""2023.01"",""2021.08"",""2021.08"",""2021.09"",""2021.09"",""2021.09"",""2021.10"",""2021.10"",""2021.10"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.11"",""2021.12"",""2021.12"",""2022.01"",""2022.01"",""2022.01"",""2022.01"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.08"",""2022.09"",""2022.09"",""2022.09"",""2022.09"",""2022.10"",""2022.10"",""2022.10"",""2022.10"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.11"",""2022.12"",""2022.12"",""2022.12"",""2022.12"",""2023.01"",""2023.01"",""2023.01"",""2023.01""),
                   Species = c(""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",
                               ""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""A"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",
                               ""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B"",""B""),
                   Site = c(""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",""Something"",
                            ""Something"",""Something"",""Something"",""Something""),
                   Mean = c(""0.301029995663981"",""1.07918124604762"",""0.698970004336019"",""1.23044892137827"",""1.53147891704226"",""1.41497334797082"",""1.7160033436348"",
                            ""0.698970004336019"",""1.39794000867204"",""1"",""0.301029995663981"",""0.301029995663981"",""0.477121254719662"",""0.301029995663981"",""0.301029995663981"",
                            ""0.301029995663981"",""0.477121254719662"",""0.301029995663981"",""0.301029995663981"",""0.845098040014257"",""0.301029995663981"",""0.301029995663981"",
                            ""0.477121254719662"",""0.698970004336019"",""1.23044892137827"",""1.41497334797082"",""1.95904139232109"",""1.5910646070265"",""1.53147891704226"",
                            ""1.14612803567824"",""1.57978359661681"",""1.34242268082221"",""0.778151250383644"",""0.301029995663981"",""0.301029995663981"",""0.477121254719662"",
                            ""0.301029995663981"",""1.20411998265592"",""0.845098040014257"",""1.17609125905568"",""1.20411998265592"",""0.698970004336019"",""0.301029995663981"",
                            ""0.698970004336019"",""0.698970004336019"",""0.903089986991944"",""1.14612803567824"",""0.301029995663981"",""0.602059991327962"",""0.301029995663981"",
                            ""0.845098040014257"",""0.698970004336019"",""0.698970004336019"",""0.301029995663981"",""0.698970004336019"",""0.301029995663981"",""0.301029995663981"",
                            ""0.301029995663981"",""0.477121254719662"",""0.301029995663981"",""0.301029995663981"",""0.301029995663981"",""0.301029995663981"",""0.301029995663981"",
                            ""0.602059991327962"",""0.301029995663981"",""0.845098040014257"",""1.92941892571429"",""1.27875360095283"",""0.698970004336019"",""1.38021124171161"",
                            ""1.20411998265592"",""1.38021124171161"",""1.14612803567824"",""1"",""1.07918124604762"",""1.17609125905568"",""0.845098040014257"",""0.698970004336019"",
                            ""0.778151250383644"",""0.301029995663981"",""0.845098040014257"",""1.64345267648619"",""1.46239799789896"",""1.34242268082221"",""1.34242268082221"",
                            ""0.778151250383644""))

data$Date<-as.factor(data$Date)
data$Site<-as.factor(data$Site)
data$Species<-as.factor(data$Species)
data$Mean<-as.numeric(data$Mean)

data %>% 
  group_by(Date, Species) %>% 
  mutate(s = sd(Mean)) %>% 
  group_by(Date) %>%
  filter(!any(s == 0)) %>% 
  compare_means(Mean ~ Species, data = ., group.b = ""Date"", method = ""t.test"")
#> # A tibble: 11 × 9
#>    Date    .y.   group1 group2      p p.adj p.format p.signif method
#>    <fct>   <chr> <chr>  <chr>   <dbl> <dbl> <chr>    <chr>    <chr> 
#>  1 2021.08 Mean  A      B      0.718   1    0.718    ns       T-test
#>  2 2021.09 Mean  A      B      0.451   1    0.451    ns       T-test
#>  3 2021.10 Mean  A      B      0.0889  0.89 0.089    ns       T-test
#>  4 2021.11 Mean  A      B      0.850   1    0.850    ns       T-test
#>  5 2022.01 Mean  A      B      1       1    1.000    ns       T-test
#>  6 2022.08 Mean  A      B      0.234   1    0.234    ns       T-test
#>  7 2022.09 Mean  A      B      0.670   1    0.670    ns       T-test
#>  8 2022.10 Mean  A      B      0.0707  0.78 0.071    ns       T-test
#>  9 2022.11 Mean  A      B      0.783   1    0.783    ns       T-test
#> 10 2022.12 Mean  A      B      0.399   1    0.399    ns       T-test
#> 11 2023.01 Mean  A      B      0.255   1    0.255    ns       T-test

Created on 2023-06-01 with reprex v2.0.2
",
76384487,Trying to run simple code that writes a dataframe as a csv file using spark and Java. java.lang.NoClassDefFoundError: org/apache/spark/sql/Dataset,"Here is my simple code:
package org.example;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.Arrays;
import java.util.List;

public class Main {
    public static void writeOutput(Dataset<Row> df, String outputPath) {
        df.write()
                .option(""header"", ""true"")
                .option(""delimiter"", ""\t"")
                .csv(outputPath);
    }
    public static void main(String[] args) {

        // Create a SparkSession
        SparkSession spark = SparkSession.builder()
                .appName(""DataFrameWriter"")
                .getOrCreate();

        // Create a DataFrame (assuming df is already defined)
        List<Row> data = Arrays.asList(
                RowFactory.create(""John"", 25, ""New York""),
                RowFactory.create(""Alice"", 30, ""San Francisco""),
                RowFactory.create(""Bob"", 35, ""Chicago"")
        );

        StructType schema = DataTypes.createStructType(new StructField[] {
                DataTypes.createStructField(""name"", DataTypes.StringType, true),
                DataTypes.createStructField(""age"", DataTypes.IntegerType, true),
                DataTypes.createStructField(""city"", DataTypes.StringType, true)
        });

        Dataset<Row> df = spark.createDataFrame(data, schema);

        // Specify the output path
        String outputPath = ""src/main/java/output"";

        // Call the writeOutput method
        writeOutput(df, outputPath);

        // Stop the SparkSession
        spark.stop();
    }
}

Here is my build.gradle file:
plugins {
    id 'java'
}

group = 'org.example'
version = '1.0-SNAPSHOT'

repositories {
    mavenCentral()
}

dependencies {
    compileOnly 'org.apache.spark:spark-sql_2.12:3.2.0'
    implementation 'org.apache.spark:spark-core_2.12:3.2.0'


    testImplementation platform('org.junit:junit-bom:5.9.1')
    testImplementation 'org.junit.jupiter:junit-jupiter'
}

test {
    useJUnitPlatform()
}

And errors:
Task :Main.main() FAILED
Error: Unable to initialize main class org.example.Main
Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/Dataset

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':Main.main()'.
> Process 'command '/Library/Java/JavaVirtualMachines/jdk-11.0.11.jdk/Contents/Home/bin/java'' finished with non-zero exit value 1

java -version:
java version ""11.0.19"" 2023-04-18 LTS
Java(TM) SE Runtime Environment 18.9 (build 11.0.19+9-LTS-224)
Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.19+9-LTS-224, mixed mode)

scala -version:
Scala code runner version 3.3.0 -- Copyright 2002-2023, LAMP/EPFL

Spark: version 3.4.0
Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 17.0.7)
Could you tell me what could be wrong? Pretty simple code, just can't figure out what to check. I've already tried reinstalling everything.
",76384848,"Avoid using compileOnly directive for dependencies which implementation will be needed during runtime as stated on Gradle's Java library plugin user guide https://docs.gradle.org/current/userguide/java_library_plugin.html and blog https://blog.gradle.org/introducing-compile-only-dependencies
","Make sure to define the `authenticate` middleware before the error handling middleware to ensure it is executed before the error handlers. With this change, the error handling middleware should work as expected, and you should see the desired logs in your console."
76380747,Give same width to items vertically positioned,"I have two items positioned vertically, and I'd like the narrower one is as wide as the wider one.
My code looks like
<div class=""flex flex-col items-end"">
  <div>This should take all the space</div>
  <div class=""flex flex-row gap-x-4"">
    <div> this is the first element</div>
    <div> this is the second element</div>
  </div>
</div>

and produce

However, I would like the result to be

items-end is needed because the items are displayed on the right side of the page.
I have tried to mess with positioning, but I cannot achieve the final result I'm looking for.
Can anyone give me a hand on this?
",76381513,"You could shrink-wrap the container and then right align it:


<script src=""https://cdn.tailwindcss.com""></script>

<div class=""flex flex-col w-max ml-auto"">
  <div>This should take all the space</div>
  <div class=""flex flex-row gap-x-4"">
    <div> this is the first element</div>
    <div> this is the second element</div>
  </div>
</div>



You could also use a grid layout with one grid column track sized to max-content and then align the grid column track to the right:


<script src=""https://cdn.tailwindcss.com""></script>

<div class=""grid grid-cols-[max-content] justify-end"">
  <div>This should take all the space</div>
  <div class=""flex flex-row gap-x-4"">
    <div> this is the first element</div>
    <div> this is the second element</div>
  </div>
</div>



",
76382807,Is there a better way to publish messages using Pusher Channels' batch event?,"I'm trying to send messages to my users from my server using Pusher Channels. My api receives a list of users and the message needs to be sent to all the users in the list. I can't group these users into a single channel and an individual channel has to be used for each user. This makes my api slow as the list of users can have a size of anything between 1 and 10000 (possibly more in the future), and pusher batch events can only accept a Event list of size 10.

I'm using .net 6 for my api

I've tried using batch events to try and improve performance; my code looks something like this,
var events = new List<Event>();
// count can be anything between 1 and 10000
for (int i = 1; i <= count; i++)
{
    // creating a sample list of events
    events.Add(new Event
    {
        Channel = string.Format(""batch-channel-{0}"", i),
        EventName = ""batch-event"",
        Data = new
        {
            Channel = string.Format(""batch-channel-{0}"", i),
            Event = ""batch-event"",
            Message = string.Format(""{0} - sample message"", i)
        {
    });
}
var result = new List<HttpStatusCode>();
int chunkSize = 10;
int totalChunks = (int)Math.Ceiling((double)events.Length / chunkSize);
for (int i = 0; i < totalChunks; i++)
{
    var eventsChunk = events.Skip(i * chunkSize).Take(chunkSize).ToArray();
    // publishing event lists of size 10
    var chunkResult = await pusher.TriggerAsync(eventsChunk);
    result.Add(chunkResult.StatusCode);
}

I tested this code with a Event list of size 10000 and it takes around 6 minutes to complete execution. I want to know if there is anything I'm missing and if I can somehow improve performance.
Any help is appreciated. Thank you.
",76382905,"If you are sending the same event to multiple channels then you can use the standard trigger endpoint but specify a list of the channels that you are broadcasting to. For example:
using PusherServer;

var options = new PusherOptions();
options.Cluster = ""APP_CLUSTER"";
var pusher = new Pusher(""APP_ID"", ""APP_KEY"", ""APP_SECRET"", options);

ITriggerResult result = await pusher.TriggerAsync(
  new string[]{""my-channel-1"", ""my-channel-2"", ""my-channel-3""},
  ""my-event"",
  new { message: ""hello world"" });

This would trigger the event to the three specified channels. You can specify up to 100 channels in a single publish.
If you are sending a different event to each channel then the batch event endpoint you have mentioned would be the way forward. In this case you might look at multi-threading to or similar to be able to handle multiple batch triggers at the same time, rather than sequentially.
Source - https://pusher.com/docs/channels/server_api/http-api/#example-publish-an-event-on-multiple-channels
",
76384790,Pandas Webscraping Errors,"I'm currently trying to webscrape websites for tables using pandas and I get this error for one of the links.
Here's a snippet of what causes the crash:
import pandas as pd
website_df = pd.read_html(""https://ballotpedia.org/Roger_Wicker"")
print(website_df)

Below is the error I get, does anyone know how to fix this?
Traceback (most recent call last):
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\python_parser.py"", line 700, in _next_line
    line = self._check_comments([self.data[self.pos]])[0]
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\python_parser.py"", line 385, in _infer_columns
    line = self._next_line()
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\python_parser.py"", line 713, in _next_line
    raise StopIteration
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""c:\Users\legislators-current.py"", line 15, in <module>
    website_df = pd.read_html(""https://ballotpedia.org/Roger_Wicker"")
  File ""C:\Users\miniconda3\lib\site-packages\pandas\util\_decorators.py"", line 331, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\html.py"", line 1205, in read_html
    return _parse(
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\html.py"", line 1011, in _parse
    df = _data_to_frame(data=table, **kwargs)
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\html.py"", line 890, in _data_to_frame
    with TextParser(body, header=header, **kwargs) as tp:
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\readers.py"", line 1876, in TextParser
    return TextFileReader(*args, **kwds)
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\readers.py"", line 1442, in __init__
    self._engine = self._make_engine(f, self.engine)
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\readers.py"", line 1753, in _make_engine
    return mapping[engine](f, **self.options)
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\python_parser.py"", line 122, in __init__
    ) = self._infer_columns()
  File ""C:\Users\miniconda3\lib\site-packages\pandas\io\parsers\python_parser.py"", line 395, in _infer_columns
    raise ValueError(
ValueError: Passed header=[1,2], len of 2, but only 2 lines in file

",76384880,"Set header=0. You're going to get a lot of dataframes, but you can parse them to get what you need.
website_df = pd.read_html(""https://ballotpedia.org/Roger_Wicker"", header=0)

",
76381459,Adding tolerations to fluentbit pod and making it persistent,"I am using fluentbit as a pod deployment where I am creating many fluentbit pods which are attached to azure blob containers. Since multiple pods exist I tried adding tolerations as I did on daemonset deployment but it did not work and failed. Also every time I delete and start the pods reinvests all the the again. Please advise on fixing these issues.
apiVersion: v1
kind: Pod
metadata:
  name: deployment 
spec:
  volumes:
    - name: config_map_name
      configMap:
        name: config_map_name
    - name: pvc_name
      persistentVolumeClaim:
        claimName: pvc_name       
  containers:
    - name: fluentbit-logger
      image: fluent/fluent-bit:2.1.3
      env:
        - name: FLUENTBIT_USER
          valueFrom:
            secretKeyRef:
              name: fluentbit-secret
              key: user
        - name: FLUENTBIT_PWD
          valueFrom:
            secretKeyRef:
              name: fluentbit-secret
              key: pwd              
      resources:
        requests:
          memory: ""32Mi""
          cpu: ""50m""
        limits:
          memory: ""64Mi""
          cpu: ""100m""
      securityContext:
        runAsUser: 0
        privileged: true
      volumeMounts:
        - name: config_map_name
          mountPath: ""/fluent-bit/etc""
        - name: pvc_name
          mountPath: mount_path  
      tolerations: 
      - key: ""dedicated""
        operator: ""Equal""
        value: ""sgelk""
        effect: ""NoSchedule""

      - key: ""dedicated""
        operator: ""Equal""
        value: ""kafka""
        effect: ""NoSchedule""               

Getting the error as below
error: error validating ""/tmp/fluentbit-deploy.yaml"": error validating data: ValidationError(Pod.spec.containers[0]): unknown field ""tolerations"" in io.k8s.api.core.v1.Container; if you choose to ignore these errors, turn validation off with --validate=false

",76381521,"The tolerations attribute needs to be set on the pod, but you are attempting to set it on a container (that's why you see the error ""unknown field ""tolerations"" in io.k8s.api.core.v1.Container""). You would need to write:
apiVersion: v1
kind: Pod
metadata:
  name: deployment
spec:
  volumes:
    - name: config_map_name
      configMap:
        name: config_map_name
    - name: pvc_name
      persistentVolumeClaim:
        claimName: pvc_name
  containers:
    - name: fluentbit-logger
      image: fluent/fluent-bit:2.1.3
      env:
        - name: FLUENTBIT_USER
          valueFrom:
            secretKeyRef:
              name: fluentbit-secret
              key: user
        - name: FLUENTBIT_PWD
          valueFrom:
            secretKeyRef:
              name: fluentbit-secret
              key: pwd
      resources:
        requests:
          memory: ""32Mi""
          cpu: ""50m""
        limits:
          memory: ""64Mi""
          cpu: ""100m""
      securityContext:
        runAsUser: 0
        privileged: true
      volumeMounts:
        - name: config_map_name
          mountPath: ""/fluent-bit/etc""
        - name: pvc_name
          mountPath: mount_path
  tolerations:
  - key: ""dedicated""
    operator: ""Equal""
    value: ""sgelk""
    effect: ""NoSchedule""

  - key: ""dedicated""
    operator: ""Equal""
    value: ""kafka""
    effect: ""NoSchedule""

",
76382480,Ansible - starting a task after cpu load is below 2.0,"I am trying to create a playbook where I want to perform a simple debug task after cpu load is below 2.0.
I have this so far in cpu-load.yml:
---
- name: Check CPU load and wait
  hosts: localhost
  gather_facts: yes
  
  tasks:
    - name: Check cpu load
      shell: uptime | awk -F 'load average:' '{print $2}' | awk -F ', ' '{print $1}'
      register: cpu_load
      
    - name: Wait until cpu load is bellow 2.0
      wait_for:
        timeout: 300
        delay: 10
        shell: Do something here
        msg: ""cpu load is bellow 2.0""
      
    - name: Continue jobs
      debug:
        msg: ""CPU load is bellow 2.0. Continue!!!""

Now I am not sure how to make the task wait for the cpu load to go bellow 2.0. Can you guys help?
",76382917,"You need to put an until loop around your ""check cpu load"" task:
- hosts: localhost
  gather_facts: false
  tasks:
    - name: Check cpu load
      shell: uptime | awk -F 'load average:' '{print $2}' | awk -F ', ' '{print $1}'
      register: cpu_load
      until: cpu_load.stdout|float < 2.0
      retries: 300
      delay: 1

    - name: Some other task
      debug:
        msg: hello world

This will wait up to five minutes (300 retries with a 1-second delay) for the load average to drop below 2.0.

There are probably better ways to get the current 1-minute CPU load; reading from /proc/loadavg may be easiest:
- hosts: localhost
  gather_facts: false
  tasks:
    - name: Check cpu load
      command: cat /proc/loadavg
      register: cpu_load
      until: cpu_load.stdout.split()|first|float < 2.0
      retries: 300
      delay: 1

    - name: Some other task
      debug:
        msg: hello world

",
76382514,How can I use Webpack to load a separate JS file in Shopware 6 and improve web performance?,"How to load a separate JS file in Shopware 6 using webpack?
What?
I'm trying to load a separate javascript file next to the all.js file by using WebPack.
Why?
The all.js file can get really big and you're loading unnecessary javascript on a page. So by using code splitting (which should be possible since WebPack is implemented in Shopware 6) and dynamic imports you could stop loading unnecessary javascript.
What I've tried
I've added a webpack.config.js file inside the root of my theme plugin like so:
module.exports = {

    entry: {
        main: './src/main.js', 
        separateFile: './src/js/separate.js'
    },

    output: {
        filename: '[name].js'
    },

    optimization: {
        splitChunks: {
            chunks: 'all',
        },
    },
};

After running bin/build-storefront.sh there is no separate JS file added in the public folder.
I'm also trying to dynamically load this JS file in src/Resources/app/storefront/src/main.js but this results in a 404 since the separate file doesn't exist.
",76382923,"This will not work since all pre-compiled assets of plugins are collected in the ThemeCompiler and concatenated into one single script. This is done  in PHP since node is not a requirement for production environments.
You could try to add separate scripts as additional custom assets, but you would still have to extend the template to add the corresponding script tags manually.
",
76384679,Chunked xarray: load only 1 cell in memory efficiently,"Context:
I have a datacube with 3 variables (3D arrays, dims:time,y,x). The datacube is too big to fit in memory so I chunk it with xarray/dask. I want to apply a function to every cell in x,y of every variable in my datacube.
Problem:
My method takes a long time to load only one cell (1 minute) and I have to do that 112200 times. I use a for loop with dataset.variable.isel(x=i, y=j).values to load a single 1D array from my variables. Is there a better way to do that ? Also, knowing my dataset is chunked, is there a way to do that in parallel for all the chunks at once ?
Code example:
# Setup
import xarray as xr
import numpy as np

# Create the dimensions
x = np.linspace(0, 99, 100)
y = np.linspace(0, 349, 350)
time = np.linspace(0, 299, 300)

# Create the dataset
xrds= xr.Dataset()

# Add the dimensions to the dataset
xrds['time'] = time
xrds['y'] = y
xrds['x'] = x



# Create the random data variables with chunking
chunksize = (10, 100, 100)  # Chunk size for the variables
data_var1 = np.random.rand(len(time), len(y), len(x))
data_var2 = np.random.rand(len(time), len(y), len(x))
data_var3 = np.random.rand(len(time), len(y), len(x))

xrds['data_var1'] = (('time', 'y', 'x'), data_var1, {'chunks': chunksize})
xrds['data_var2'] = (('time', 'y', 'x'), data_var2, {'chunks': chunksize})
xrds['data_var3'] = (('time', 'y', 'x'), data_var3, {'chunks': chunksize})

#### ---- My Attempt ---- ####

# Iterate through all the variables in my dataset
for var_name, var_data in xrds.data_vars.items():

    # if variable is 3D
    if var_data.shape == (xrds.dims['time'], xrds.dims['y'], xrds.dims['x']):

        # Iterate through every cell of the variable along the x and y axis only
        for i in range(xrds.dims['y']):
            for j in range(xrds.dims['x']):

                # Load a single 1D cell into memory (len(cell) = len(time))
                print(xrds.v.isel(y=i,x=j).values)

",76384898,"I find that explicitly iterating over the xarray is faster than isel(), by about 10%.
Example:
    for var_name, var_data in xrds.data_vars.items():

        # if variable is 3D
        if var_data.shape == (xrds.dims['time'], xrds.dims['y'], xrds.dims['x']):

            # Iterate through every cell of the variable along the x and y axis only
            for i_array in xrds['data_var1'].transpose('x', 'y', 'time'):
                x_coordinate = i_array.x.item()
                for cell in i_array.transpose('y', 'time'):
                    y_coordinate = cell.y.item()
                    # Do something with cell

This takes 17.38s, versus 20.47s for the original.
PS: The line chunksize = (10, 100, 100) seems very suspicious to me. It seems like if you want to load an array corresponding to the entire time axis at once, the chunks should be set so that this doesn't require looking at multiple chunks. It seems like chunksize = (len(time), 100, 100) would be more efficient. However, I benchmarked this both ways and it doesn't make a difference for this data size. May make a difference on your larger problem.
",
76381460,How to change colors when using scale_fill_discrete in R?,"I have the data below:
time=c(200,218,237,237,237,237,237,246,246,246,257,257,257,272,272,272,294,294,294)
location=c(""A"",""A"",""D"",""C"",""A"",""B"",""B"",""D"",""C"",""B"",""D"",""C"",""B"",""D"",""C"",""B"",""D"",""C"",""B"")
value=c(0,774,0,0,2178,0,2178,0,1494,2644,1326,1504,4188,3558,1385,5013,12860,829,3483)
dataA=data.frame(time,location,value)

and I made a graph.
ggplot(data=dataA, aes(x=time, y=value))+
  geom_area(aes(group=location, fill=location), position=""stack"", linetype=1, size=0.5 ,colour=""black"") +
  scale_fill_discrete(breaks=c(""A"",""B"",""C"",""D""), labels=c(""Main_town"",""B"",""C"",""D""))+
  theme_classic(base_size=18, base_family=""serif"")+
  theme(legend.position=""right"",
        axis.line=element_line(linewidth=0.5, colour=""black""))+
windows(width=5.5, height=5)

I changed one of the legend label from A to main_town using scale_fill_discrete(). Then color is automatically generated.

I want to change this color according to my preference. When I add a code, scale_fill_manual(values=c(""darkblue"",""darkred"",""khaki4"",""darkgreen""))+ the below message shows up and the graph is before I changed legend label.
Scale for fill is already present.
Adding another scale for fill, which will replace the existing scale.


How can I change colors when using scale_fill_discrete()? I want to change colors to ""darkblue"",""darkred"",""khaki4"",""darkgreen""
Could you please let me know how to do that? Or do you let me know simply how to change legend labels maintaining colors I want?
Always many thanks!!!
",76381528,"I think you need scale_fill_discrete(type = c(...)).
library(ggplot2)

ggplot(data=dataA, aes(x=time, y=value))+
  geom_area(aes(group=location, fill=location), position=""stack"", linetype=1, size=0.5 ,colour=""black"") +
  scale_fill_discrete(breaks=c(""A"",""B"",""C"",""D""), labels=c(""Main_town"",""B"",""C"",""D""),
                      type=c(""darkblue"",""darkred"",""khaki4"",""darkgreen""))+
  theme_classic(base_size=18, base_family=""serif"")+
  theme(legend.position=""right"",
        axis.line=element_line(linewidth=0.5, colour=""black""))


",
76381485,C# IQueryable .Union reset sorting,"I have the following code:
var expressions = new List<IQueryable<Container>>();

var containers1 = containers
    .Where(x => EF.Functions.Like(x.ContainerReference1, $""%{message.SearchValue}%"")
             || EF.Functions.Like(x.ContainerReference2, $""%{message.SearchValue}%""))
    .OrderBy(x => x.ContainerReference1)
    .ThenBy(x => x.ContainerReference2)
    .ThenByDescending(x => x.DateUpdated);
expressions.Add(containers1);

var containers2 = containers
                  .Where(x => EF.Functions.Like(x.Description, $""%{message.SearchValue}%""))
                  .OrderBy(x => x.Description)
                  .ThenByDescending(x => x.DateUpdated);
expressions.Add(containers2);

var containers3 = containers.Where(x => x.ContactEmails
                            .OrderBy(y => y.Email)
                            .ThenBy(y => y.DisplayName)
                            .Any(y => EF.Functions.Like(y.Email, $""%{message.SearchValue}%"")
                                 || EF.Functions.Like(y.DisplayName, $""%{message.SearchValue}%"")))
                            .OrderByDescending(x => x.DateUpdated);
expressions.Add(containers3);

var containers4 = containers
                            .Where(x => EF.Functions.Like(x.Keywords, $""%{message.SearchValue}%""))
                            .OrderBy(x => x.Keywords)
                            .ThenByDescending(x => x.DateUpdated);
expressions.Add(containers4);

containers = expressions.Aggregate((acc, i) => acc.Union(i));

But after .Union operation sorting is reset.
How can I prevent resetting of sorting?
",76381539,"Union operator does not preserve the order of the elements. You need to dynamically construct the sorting logic based on the presence of data
var expressions = new List<IQueryable<Container>>();
var sortingExpressions = new List<Func<IQueryable<Container>, IOrderedQueryable<Container>>>();

var containers1 = containers
        .Where(x => EF.Functions.Like(x.ContainerReference1, $""%{message.SearchValue}%"")
                || EF.Functions.Like(x.ContainerReference2, $""%{message.SearchValue}%""));
if (containers1.Any())
{
    var containers1Sorting = new Func<IQueryable<Container>, IOrderedQueryable<Container>>(x => x
        .OrderBy(y => y.ContainerReference1)
        .ThenBy(y => y.ContainerReference2)
        .ThenByDescending(y => y.DateUpdated));

    expressions.Add(containers1);
    sortingExpressions.Add(containers1Sorting);
}

var containers2 = containers
          .Where(x => EF.Functions.Like(x.Description, $""%{message.SearchValue}%""));
if (containers2.Any())
{
    var containers2Sorting = new Func<IQueryable<Container>, IOrderedQueryable<Container>>(x => x
        .OrderBy(y => y.Description)
        .ThenByDescending(y => y.DateUpdated));

    expressions.Add(containers2);
    sortingExpressions.Add(containers2Sorting);
}

var containers3 = containers
        .Where(x => x.ContactEmails
            .Any(y => EF.Functions.Like(y.Email, $""%{message.SearchValue}%"")
                || EF.Functions.Like(y.DisplayName, $""%{message.SearchValue}%"")));
if (containers3.Any())
{
    var containers3Sorting = new Func<IQueryable<Container>, IOrderedQueryable<Container>>(x => x
        .OrderBy(y => y.ContactEmails.OrderBy(z => z.Email).ThenBy(z => z.DisplayName))
        .OrderByDescending(x => x.DateUpdated));

    expressions.Add(containers3);
    sortingExpressions.Add(containers3Sorting);
}

var containers4 = containers
      .Where(x => EF.Functions.Like(x.Keywords, $""%{message.SearchValue}%""));
if (containers4.Any())
{
    var containers4Sorting = new Func<IQueryable<Container>, IOrderedQueryable<Container>>(x => x
        .OrderBy(y => y.Keywords)
        .ThenByDescending(y => y.DateUpdated));

    expressions.Add(containers4);
    sortingExpressions.Add(containers4Sorting);
}

var mergedContainers = expressions.Aggregate((acc, i) => acc.Union(i));

if (sortingExpressions.Any())
{
    var mergedSorting = sortingExpressions
        .Aggregate((acc, next) => q => next(acc(q)));

    containers = mergedSorting(mergedContainers);
}
else
{
    containers = mergedContainers.OrderByDescending(x => x.DateUpdated);
}

",
76381526,How can I use Python to increment 'tokenId' values in a .json file?,"I have a .json file but I got the tokenId numbering wrong. I need to increase all values of ""tokenId"" by 1 number
[
    {
        ""Background"": ""Red"",
        ""Body"": ""Tunn"",
        ""Hat"": ""Bambu"",
        ""Outfit"": ""Pirate"",
        ""Expression"": ""Sad"",
        ""Accessory"": ""Rifle"",
        ""tokenId"": 0
    },
    {
        ""Background"": ""Lilac"",
        ""Body"": ""Tunn"",
        ""Hat"": ""Bicorn"",
        ""Outfit"": ""Pirate"",
        ""Expression"": ""Angry"",
        ""Accessory"": ""Balloons"",
        ""tokenId"": 1
    },

          ...
    
    {
        ""Background"": ""Green"",
        ""Body"": ""Tunn"",
        ""Hat"": ""Bicorn"",
        ""Outfit"": ""Pirate"",
        ""Expression"": ""Sad"",
        ""Accessory"": ""Balloons"",
        ""tokenId"": 3000
    },


is it possible to do this with python? i created this .json file with python.
I tried this code, but I get an error
import json

with open('traits.json') as f:
    data = json.load(f)

for item in data['tokenId']:
    item['tokenId'] = item['tokenId'].replace([int('+1')])

with open('new_data.json', 'w') as f:
    json.dump(data, f)

TypeError: list indices must be integers or slices, not str

Thank you!
",76381562,"To increase the values of the ""tokenId"" field in your JSON file by 1, you can modify your code as follows:
import json

with open('traits.json') as f:
    data = json.load(f)

for item in data:
    item['tokenId'] += 1

with open('new_data.json', 'w') as f:
    json.dump(data, f)

In your original code, you were trying to access data['tokenId'] as if it was a list, but it is actually a dictionary. Instead, you need to iterate over the list data and update the ""tokenId"" field of each item. By using item['tokenId'] += 1, you increment the value of ""tokenId"" by 1.
Finally, the modified data is saved to a new JSON file named ""new_data.json"" using json.dump(data, f).
After running this code, the ""new_data.json"" file will contain the updated ""tokenId"" values with an increment of 1.
",
76382811,Update input values into elements only JS,"I have created an index file so that the information entered here is added to the created element
this is for a review section
the index.html file is here, and includes the CSS and js


    let name = document.querySelector('.name').value;
    let message = document.querySelector('.message').value;
    let btn = document.getElementById('button');
    let div = document.querySelector('.items')

    btn.addEventListener('click', ()=>{
        let item = document.createElement('div')
        let inner = `
                <h3>${name}</h3>
                <p>${message}</p>
                `
        item.className = ""message-item""        
        item.innerHTML = inner        
        div.append(item)
    });
 html, body{
            padding: 0;
            margin: 0;
        }
        .msg{
            padding: 2em;
            margin: 2em;
            border-radius: 2vh;
            height: 70vh;
            display: flex;
            align-items: center;
            justify-content: left;
            flex-direction: column;
            background-color: #1e90ff;
        }
        .items{
            height: 65vh;
            overflow: scroll;
            color: white;
            width: 100%;
            overflow-x: hidden;
            margin: 10px;
        }
        input{
            padding: 10px;
            border: none;
            border-radius: 8px;
            outline: none;
            font-size: 1em;
        }
        #button{
            padding: 10px 20px;
            border-radius: 8px;
            border: none;
            font-size: 1em;
        }
        .button{
            padding: 10px 20px;
            border-radius: 8px;
            border: none;
            font-size: 1em;
        }
        .message-item{
            background-color: black;
            padding: 1em;
            border-radius: 8px;
            margin: 3px;
        }
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title></title>

</head>
<body>
    <div class=""msg"">
        <div class=""items""></div>
        <div class=""input"">
                <input type=""text"" class=""name"" placeholder=""Name"">
                <input type=""text"" class=""message"" placeholder=""Message"">
                <button id=""button"">Submit</button>
                <button type=""reset"">Reset</button>
        </div>
    </div>
</body>

</html>



So I am expecting it to append the elements which have different values
example once i enter the **name **""harry"" and **message **as ""this is the message""
and then i reset and enter another **name **and **message **then the newly created element should display the newly entered **name **and message
",76382942,"Your name variable should be a pointer to the element, not the value.
Also, you should clear the input after adding.


const
  name = document.querySelector('.name'),
  message = document.querySelector('.message'),
  btn = document.getElementById('button'),
  div = document.querySelector('.items');

const handleAdd = (e) => {
  div.insertAdjacentHTML('beforeend', `
    <div class=""message-item"">
      <h3>${name.value}</h3>
      <p>${message.value}</p>
    </div>
  `);
  name.value = '';    // Clear name
  message.value = ''; // Clear message
};

btn.addEventListener('click', handleAdd);
html,
body {
  padding: 0;
  margin: 0;
}

.msg {
  padding: 2em;
  margin: 2em;
  border-radius: 2vh;
  height: 70vh;
  display: flex;
  align-items: center;
  justify-content: left;
  flex-direction: column;
  background-color: #1e90ff;
}

.items {
  height: 65vh;
  overflow: scroll;
  color: white;
  width: 100%;
  overflow-x: hidden;
  margin: 10px;
}

input {
  padding: 10px;
  border: none;
  border-radius: 8px;
  outline: none;
  font-size: 1em;
}

#button {
  padding: 10px 20px;
  border-radius: 8px;
  border: none;
  font-size: 1em;
}

.button {
  padding: 10px 20px;
  border-radius: 8px;
  border: none;
  font-size: 1em;
}

.message-item {
  background-color: black;
  padding: 1em;
  border-radius: 8px;
  margin: 3px;
}
<div class=""msg"">
  <div class=""items""></div>
  <div class=""input"">
    <input type=""text"" class=""name"" placeholder=""Name"">
    <input type=""text"" class=""message"" placeholder=""Message"">
    <button id=""button"">Submit</button>
    <button type=""reset"">Reset</button>
  </div>
</div>



A better approach
A better example would be to use a form. This way you can take advantage of built-in form validation, submission, and resetting.
For example, you can call elements by their name and you have the added bonus of Enter key support.

Enter a name
Press Tab
Enter a message
Press Enter

The item is added
The form is cleared
Focus is sent to the name





const handleAdd = (e) => {
  e.preventDefault(); // Prevent page from navigating
  const
    form = e.target,
    formElements = form.elements,
    parent = form.closest('.msg'),
    items = parent.querySelector('.items');
  items.insertAdjacentHTML('beforeend', `
    <div class=""message-item"">
      <h3>${formElements.name.value}</h3>
      <p>${formElements.message.value}</p>
    </div>
  `);
  formElements.name.value = '';    // Clear name
  formElements.message.value = ''; // Clear message
  formElements.name.focus();
};

document.forms.namedItem('new-msg')
  .addEventListener('submit', handleAdd);
html,
body {
  padding: 0;
  margin: 0;
}

.msg {
  padding: 2em;
  margin: 2em;
  border-radius: 2vh;
  height: 70vh;
  display: flex;
  align-items: center;
  justify-content: left;
  flex-direction: column;
  background-color: #1e90ff;
}

.items {
  height: 65vh;
  overflow: scroll;
  color: white;
  width: 100%;
  overflow-x: hidden;
  margin: 10px;
}

input {
  padding: 10px;
  border: none;
  border-radius: 8px;
  outline: none;
  font-size: 1em;
}

.form-btn {
  padding: 10px 20px;
  border-radius: 8px;
  border: none;
  font-size: 1em;
}

.message-item {
  background-color: black;
  padding: 1em;
  border-radius: 8px;
  margin: 3px;
}
<div class=""msg"">
  <div class=""items""></div>
  <form id=""new-msg"" autocomplete=""off"">
    <input type=""text"" name=""name"" placeholder=""Name"" required>
    <input type=""text"" name=""message"" placeholder=""Message"">
    <button type=""submit"" class=""form-btn"">Submit</button>
    <button type=""reset"" class=""form-btn"">Reset</button>
  </form>
</div>




LocalStorage
Here is an example of local storage. The main idea here is how to store and restore the state of the messages.
const MESSAGES_KEY = ""messages"";

const main = () => {
  // Restore all messages
  const messageContainer = document.querySelector("".items"");
  __retrieveAllMessages().forEach((message) => {
    insertMessage(message, messageContainer);
  });
  // Add event listener
  document.forms.namedItem(""new-msg"").addEventListener(""submit"", handleAdd);
};

const saveMessage = (message) => {
  __saveAllMessages(__retrieveAllMessages().concat(message));
};

const insertMessage = (message, container) => {
  container.insertAdjacentHTML(""beforeend"", messageToHtml(message));
};

const messageToHtml = ({ name, message }) => `
  <div class=""message-item"">
    <h3>${name}</h3>
    <p>${message}</p>
  </div>
`;

const handleAdd = (e) => {
  e.preventDefault(); // Prevent page from navigating
  const form = e.target,
    message = {
      name: form.elements.name.value,
      message: form.elements.message.value,
    };
  saveMessage(message);
  insertMessage(message, form.closest("".msg"").querySelector("".items""));
  form.elements.name.value = """"; // Clear name
  form.elements.message.value = """"; // Clear message
  form.elements.name.focus();
};

const __retrieveAllMessages = () => {
  return JSON.parse(localStorage.getItem(MESSAGES_KEY) ?? ""[]"");
};

const __saveAllMessages = (messages = []) => {
  return localStorage.setItem(MESSAGES_KEY, JSON.stringify(messages));
};

main();

",
76381469,How to extract data from xml in NodeJS?,"I have the following xml:
<?xml version=""1.0"" encoding=""utf-8""?>
    <wfs:FeatureCollection xmlns:wfs=""http://www.opengis.net/wfs/2.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation="" http://www.opengis.net/wfs/2.0 http://www.wfs.nrw.de/aaa-suite/schema/ogc/wfs/2.0/wfs.xsd"" timeStamp=""2023-06-01T13:31:53.444+02:00"" numberReturned=""0"" numberMatched=""9359426""/>

How can I extract the value of numberMatched using an xml parser like fast-xml-parser in NodeJS?
",76381565,"You need to set ignoreAttributes option to false
import { XMLParser } from ""fast-xml-parser"";

const XMLdata = `<?xml version=""1.0"" encoding=""utf-8""?>
<wfs:FeatureCollection xmlns:wfs=""http://www.opengis.net/wfs/2.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation="" http://www.opengis.net/wfs/2.0 http://www.wfs.nrw.de/aaa-suite/schema/ogc/wfs/2.0/wfs.xsd"" timeStamp=""2023-06-01T13:31:53.444+02:00"" numberReturned=""0"" numberMatched=""9359426""/>`;

const parser = new XMLParser({
  ignoreAttributes: false,
  attributeNamePrefix: """"
});
let jObj = parser.parse(XMLdata);

console.log(jObj[""wfs:FeatureCollection""].numberMatched);


",
76384843,How can I update my UI after a coroutine is completed in Kotlin for Android?,"I use Retrofit and Coroutines to fetch a list of languages from an API. My ViewModel loads the list of languages whenever it is created. I want to load this list into a spinner, but whenever I update the spinner data, the data in the ViewModel hasn't completely loaded.
Here my viewmodel class to load data from an api
class TranslateViewModel : ViewModel() {
    var langToCode = HashMap<String, String>()
    var codeToLang = HashMap<String, String>()
    private val translateRepository = TranslateRepository()

    init {
        viewModelScope.launch {
            val langList = try {
                translateRepository.getLangList()
            } catch (e: IOException) {
                Log.e(TAG, ""IOException"", e)
                return@launch
            } catch (e: HttpException) {
                Log.e(TAG, ""HTTPException"", e)
                return@launch
            }
            if (langList.isSuccessful && langList.body() != null) {
                codeToLang = langList.body()!!.langList
                langToCode = codeToLang.map { (key, value) ->
                    value to key
                }.toMap() as HashMap<String, String>
            } else {
                Log.d(TAG, ""Get lang list failure"")
            }
        }
    }
}

I want to load my list of languages into my spinner but my viewmodel is taking too long to load the data, so the langList variable is alway empty.
override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
        super.onViewCreated(view, savedInstanceState)

        viewBinding.apply {
            sourceLanguage.apply {
                val langList = viewModel.langToCode.keys.toList()
                val arrayAdapter = ArrayAdapter (
                    context,
                    android.R.layout.simple_spinner_item,
                    langList
                ).also { it.setDropDownViewResource(
                     android.R.layout.simple_spinner_dropdown_item
                )
                }
                this.adapter = arrayAdapter
        }
}

I want to know how to update my UI immediately after my coroutine has finished loading data. I appreciate all of your help, and please forgive me for my poor English skills.
",76384912,"In Your case update your viewmodel as below-
class TranslateViewModel : ViewModel() {
    private val _langList = MutableLiveData<List<String>>()
    val langList: LiveData<List<String>> get() = _langList
    
    init {
        viewModelScope.launch {
            if (langList.isSuccessful && langList.body() != null) {
                codeToLang = langList.body()!!.langList
                langToCode = codeToLang.map { (key, value) ->
                    value to key
                }.toMap() as HashMap<String, String>
                _langList.value = langToCode.keys.toList() // Update the LiveData value
            } else {
                Log.d(TAG, ""Get lang list failure"")
            }
        }
    }
}

In your UI code to observe the LiveData and update the spinner as below-
viewModel.langList.observe(viewLifecycleOwner, { langList ->
            val arrayAdapter = ArrayAdapter(
                context,
                android.R.layout.simple_spinner_item,
                langList
            ).also { it.setDropDownViewResource(
                 android.R.layout.simple_spinner_dropdown_item
            )
            }
            sourceLanguage.adapter = arrayAdapter
        })

Follow this link for more detail - https://developer.android.com/topic/libraries/architecture/livedata
",
76382497,How to make a forever loop in JS not freeze,"I'm working on a block based programming language based of of Google's Blockly. I need to make a block that loops the contents forever, for making games.
I tried a while (true) loop but it froze. Is there any way to make a forever loop that won't freeze and will let other scripts run?
Thanks!
",76382988,"check setTimeout() : https://developer.mozilla.org/en-US/docs/Web/API/setTimeout
Something like that to loop indefinitely without blocking the main thread (you should probably design a way to break the loop at some point) :
function doSomeStuff() {
  // do some stuff…
  setTimeout(() => {
    doSomeStuff();
  }, 1000);
}

",
76384804,What is the correct syntax to classify ages into groups using IF statements in Google Sheets?,"[Why am i getting ""formula parse error"" when I try to classify the ages (column H) into groups using the following formula? And is there a better way? Thanks for your assistance:
=IF (H19<20, “0-19”, IF ((H19>=20 AND H19<40), “20-39”, IF ((H19>=40 AND H19<60), “40-59”, IF ((H19>=60 AND H19<70), “60-69”, IF (H19>=70, "">= 70"", “WRONG”)))))
I was expecting to output the Age column into strings based on my category definitions.
",76384938,"The portions that you have formatted as (H19>=20 AND H19<40) should be changed to AND(H19>=20, H19<40). Your final formula should then be:
=IF(H19<20, “0-19”,
  IF(AND(H19>=20, H19<40), “20-39”,
  IF(AND(H19>=40, H19<60), “40-59”,
  IF(AND(H19>=60, H19<70), “60-69”,
  IF(H19>=70, "">= 70"", “WRONG”)))))

Alternatively:
=IFS(OR(NOT(ISNUMBER(H19)),H19<0), ""WRONG"",
  H19<20, ""0-19"",
  AND(H19>=20, H19<40), ""20-39"",
  AND(H19>=40, H19<60), ""40-59"",
  AND(H19>=60, H19<70), ""60-69"",
  H19>=70, "">= 70"")

",
76381508,Masking a pandas column based on another column with slightly different values,"So what I have is two Pandas dataframes in Python with a large number of xyz-coordinates. One of them will be used to mask/remove some coordinates in the other one, but the problem is that the coordinates are very slightly different so that I cannot simply remove duplicates. As an example, let's say they look like this:
df1 = pd.DataFrame(data=None, columns=['x', 'y', 'z'])
df1.x = [104245, 252355, 547364, 135152]
df1.y = [842714, 135812, 425328, 124912]
df1.z = [125125, 547574, 364343, 346372]

df2 = pd.DataFrame(data=None, columns=['x', 'y', 'z'])
df2.x = [104230, 547298]
df2.y = [842498, 424989]
df2.z = [124976, 364001]

What I then want is for the first and second xyz-rows in df2 to remove the first and third row in df1. My idea was to create new columns with rounded values, compare those, and remove based on those. It would look something like this:
df1['id'] = np.linspace(0,len(df1)-1,len(df1))
df2['id'] = np.linspace(0,len(df2)-1,len(df2))

df3 = df1.round({'x': -3, 'y': -3, 'z': -3})
df4 = df2.round({'x': -3, 'y': -3, 'z': -3})

df5 = df3.merge(df4, on=['x', 'y', 'z'], how='inner')
df6 = df1[~df1.index.isin(df5.id_x)]

This works fine to remove some of the values, but often they round to different places. I was hoping with help if there is a better method to mask those values which are simply closest in all three coordinates. Maybe that it finds the closest xyz-pair between df1 and df2 and masks those pairs. If anyone has any ideas I would really appreciate it!
",76381592,"You can use numpy broadcasting to consider the individual distances between the coordinates:
# convert DataFrames to numpy arrays
a1 = df1.to_numpy()
a2 = df2.to_numpy()

# define a distance below which the coordinates are considered equal
thresh = 500

# compute the distances, identify matches on all coordinates
matches = (abs(a1[:,None]-a2) <= thresh).all(axis=-1)

idx1, idx2 = np.where(matches)
# (array([0, 2]), array([0, 1]))

out = df1.drop(df1.index[idx1])

To consider the euclidean distance between the points (taking into account all coordinates simultaneously), use scipy.spatial.distance.cdist:
from scipy.spatial.distance import cdist

thresh = 1000

matches = cdist(a1, a2) <= thresh

idx1, idx2 = np.where(matches)

out = df1.drop(df1.index[idx1])

Output:
        x       y       z
1  252355  135812  547574
3  135152  124912  346372

removing the single point from df1 that is closest to each row of df2 and below a threshold
from scipy.spatial.distance import cdist

thresh = 1000

dist = cdist(a1, a2)

idx = np.argmin(dist, axis=0)

out = df1.drop(df1.index[idx[dist[idx, np.arange(len(a2))] <= thresh]])

If the distance doesn't matter and you only want to remove the closest point:
from scipy.spatial.distance import cdist

dist = cdist(a1, a2)

idx = np.argmin(dist, axis=0)

out = df1.drop(df1.index[idx])

",
76382887,How to fix the line ending style (either CRLF or LF) in Python when written a text file?,"I have the following little program in Python
from pathlib import Path
filename = Path(""file.txt"")
content = ""line1\nline2\nline3\n""
with filename.open(""w+"", encoding=""utf-8"") as file:
    file.write(content)

After running it I get the following file (as expected)
line1
line2
line3

However, depending on where the program runs, line ending is different.
If I run it in Windows, I get CRLF line termination:
$ file -k file.txt
file.txt: ASCII text, with CRLF line terminators

If I run it in Linux, I get LF line termination:
$ file -k file.txt 
file.txt: ASCII text

So, I understand that Python is using the default from the system in which it runs, which is fine most of the times. However, in my case I'd like to fix the line ending style, no matter the system where I run the program.
How this could be done?
",76382994,"It is possible to explicitly specify the string used for newlines using the newline parameter. It works the same with open() and pathlib.Path.open().
The snippet below will always use Linux line endings \n:
from pathlib import Path
filename = Path(""file.txt"")
content = ""line1\nline2\nline3\n""
with filename.open(""w+"", encoding=""utf-8"", newline='\n') as file:
    file.write(content)

Setting newline='\r\n' will give Windows line endings and not setting it or setting newline=None (the default) will use the OS default.
",
76382888,Partially flatten nested JSON and pivot longer,"I have many JSON files with the following structure:


{
  ""requestId"": ""test"",
  ""executionDate"": ""2023-05-10"",
  ""executionTime"": ""12:02:22"",
  ""request"": {
    ""fields"": [{
      ""geometry"": {
        ""type"": ""Point"",
        ""coordinates"": [-90, 41]
      },
      ""colour"": ""blue"",
      ""bean"": ""blaCk"",
      ""birthday"": ""2021-01-01"",
      ""arst"": ""111"",
      ""arstg"": ""rst"",
      ""fct"": {
        ""start"": ""2011-01-10"",
        ""end"": ""2012-01-10""
      }
    }]
  },
  ""response"": {
    ""results"": [{
        ""geom"": {
          ""type"": ""geo"",
          ""coord"": [-90, 41]
        },
        ""md"": {
          ""type"": ""arstat"",
          ""mdl"": ""trstr"",
          ""vs"": ""v0"",
          ""cal"": {
            ""num"": 4,
            ""comment"": ""message""
          },
          ""bean"": [""blue"", ""green""],
          ""result_time"": 12342
        },
        ""predictions"": [{
            ""date"": ""2004-05-19"",
            ""day"": 0,
            ""count"": 0,
            ""eating_stage"": ""trt""
          }, {
            ""date"": ""2002-01-20"",
            ""day"": 1,
            ""count"": 0,
            ""eating_stage"": ""arstg""
          }, {
            ""date"": ""2004-05-21"",
            ""day"": 2,
            ""count"": 0,
            ""eating_stage"": ""strg""
          }, {
            ""date"": ""2004-05-22"",
            ""day"": 3,
            ""count"": 0,
            ""eating_stage"": ""rst""
          }
        }
      }
    }



The predictions part can be very deep. I want to convert this JSON to a CSV with the following structure:




requestId
executionDate
executionTime
colour
predictions_date
predictions_day
predictions_count
predictions_eating_stage




test
2023-05-10
12:02:22
blue
2004-05-19
0
0
trt


test
2023-05-10
12:02:22
blue
2002-01-20
1
0
astrg


test
2023-05-10
12:02:22
blue
2004-05-21
2
0
strg


test
2023-05-10
12:02:22
blue
2004-05-22
3
0
rst




I tried the following code:
flat_json = pd.DataFrame(
    flatten(json_data), index=[0]
)

The code results in every data point becoming a column, and I am not sure how to pivot longer where at the 'predictions' key using JSON functions in Python. I recognise that at this stage I could pivot longer using column names, but I feel like there is a cleaner way to achieve this.
",76383031,"I would suggest simply extracting what you need. It seems very specific for it to be solved using specific parsing. Therefore I would start by creating two dataframes:
df_prediction = pd.DataFrame(example['response']['results'][0]['predictions'])
df_data = pd.DataFrame({x:y for x,y in example.items() if type(y)==str},index=[0]) 

Renaming columns in predictions:
df_prediction.columns = ['prediction_'+x for x in df_prediction]

Joining and adding the last piece of data (colour):
output = df_data.assign(colour = example['request']['fields'][0]['colour']).join(df_prediction,how='right').ffill()

Outputting:
  requestId executionDate  ... prediction_count prediction_eating_stage
0      test    2023-05-10  ...                0                     trt
1      test    2023-05-10  ...                0                   arstg
2      test    2023-05-10  ...                0                    strg
3      test    2023-05-10  ...                0                     rst

",
76383903,Reorganize nested `dict`,"This question is connected to [-> here].
I would like to reorganize the following nested dict please:
a = {
 (0.0, 0.0): {'a': [25, 29, nan]},
 (0.0, 2.0): {'a': [25, 29, nan], 'b': [25, 35, 31.0]},
 (0.0, 4.0): {'b': [25, 35, 31.0]},
 (2.0, 0.0): {'a': [25, 29, nan], 'c': [25, 26, 29.0]},
 (2.0, 1.5): {'a': [25, 29, nan], 'c': [25, 26, 29.0]},
 (2.0, 2.0): {'a': [25, 29, nan], 'b': [25, 35, 31.0]},
 (2.0, 4.0): {'b': [25, 35, 31.0]},
 (3.0, 3.0): {'d': [25, 31, 32.0]},
 (3.0, 5.0): {'d': [25, 31, 32.0]},
 (5.0, 0.0): {'c': [25, 26, 29.0]},
 (5.0, 1.5): {'c': [25, 26, 29.0]},
 (5.0, 3.0): {'d': [25, 31, 32.0]},
 (5.0, 5.0): {'d': [25, 31, 32.0]},
 (6.0, 1.0): {'e': [25, 28, 30.0]},
 (6.0, 3.0): {'e': [25, 28, 30.0]},
 (8.0, 1.0): {'e': [25, 28, 30.0]},
 (8.0, 3.0): {'e': [25, 28, 30.0]}
}

I want to swap the inner and outer keys.
Some outer keys will duplicate and the value should become a list of lists. The result should be:
{'a': {(0.0, 0.0): [[25, 29, nan]],
       (0.0, 2.0): [[25, 29, nan], [25, 35, 31.0]],
       (2.0, 0.0): [[25, 29, nan], [25, 26, 29.0]],
       (2.0, 1.5): [[25, 29, nan], [25, 26, 29.0]],
       (2.0, 2.0): [[25, 29, nan], [25, 35, 31.0]]},
 'b': {(0.0, 2.0): [[25, 29, nan], [25, 35, 31.0]],
       (0.0, 4.0): [[25, 35, 31.0]],
       (2.0, 2.0): [[25, 29, nan], [25, 35, 31.0]],
       (2.0, 4.0): [[25, 35, 31.0]]},
 'c': {(2.0, 0.0): [[25, 29, nan], [25, 26, 29.0]],
       (2.0, 1.5): [[25, 29, nan], [25, 26, 29.0]],
       (5.0, 0.0): [[25, 26, 29.0]],
       (5.0, 1.5): [[25, 26, 29.0]]},
 'd': {(3.0, 3.0): [[25, 31, 32.0]],
       (3.0, 5.0): [[25, 31, 32.0]],
       (5.0, 3.0): [[25, 31, 32.0]],
       (5.0, 5.0): [[25, 31, 32.0]]},
 'e': {(6.0, 1.0): [[25, 28, 30.0]],
       (6.0, 3.0): [[25, 28, 30.0]],
       (8.0, 1.0): [[25, 28, 30.0]],
       (8.0, 3.0): [[25, 28, 30.0]]}
}

Intuition tells me pd.DataFrame with a .groupby() [and cull the NaN cells] would be the way to go...
df = pd.DataFrame(dict_vertices)
print(df.head(2))
             0.0               2.0                    ...  8.0       6.0
             0.0               0.0               1.5  ...  1.0  3.0  3.0
a  [25, 29, nan]     [25, 29, nan]     [25, 29, nan]  ...  NaN  NaN  NaN
c            NaN  [[25, 26, 29.0]]  [[25, 26, 29.0]]  ...  NaN  NaN  NaN

[2 rows x 17 columns]

...but I don't know.
How do I reorganize the following nested dict please; where the value follows the outer key?
",76384969,"You can use:
out = {}

for k1, d in a.items():
    for k2 in d:
        out.setdefault(k2, {})[k1] = list(d.values())

Output:
{'a': {(0.0, 0.0): [[25, 29, nan]],
       (0.0, 2.0): [[25, 29, nan], [[25, 35, 31.0]]],
       (2.0, 0.0): [[25, 29, nan], [[25, 26, 29.0]]],
       (2.0, 1.5): [[25, 29, nan], [[25, 26, 29.0]]],
       (2.0, 2.0): [[25, 29, nan], [[25, 35, 31.0]]]},
 'b': {(0.0, 2.0): [[25, 29, nan], [[25, 35, 31.0]]],
       (0.0, 4.0): [[25, 35, 31.0]],
       (2.0, 2.0): [[25, 29, nan], [[25, 35, 31.0]]],
       (2.0, 4.0): [[25, 35, 31.0]]},
 'c': {(2.0, 0.0): [[25, 29, nan], [[25, 26, 29.0]]],
       (2.0, 1.5): [[25, 29, nan], [[25, 26, 29.0]]],
       (5.0, 0.0): [[25, 26, 29.0]],
       (5.0, 1.5): [[25, 26, 29.0]]},
 'd': {(3.0, 3.0): [[25, 31, 32.0]],
       (3.0, 5.0): [[25, 31, 32.0]], 
       (5.0, 3.0): [[25, 31, 32.0]],
       (5.0, 5.0): [[25, 31, 32.0]]},
 'e': {(6.0, 1.0): [[25, 28, 30.0]],
       (6.0, 3.0): [[25, 28, 30.0]],
       (8.0, 1.0): [[25, 28, 30.0]],
       (8.0, 3.0): [[25, 28, 30.0]]},
}

",
76381414,How to exclude linebreaks from a regex match in python?,"
How can I make the bellow regex exclude matches that span across lines?
import re
reg = re.compile(r'\b(apple)(?:\W+\w+){0,4}?\W+(tree|plant|garden)')
reg.findall('my\napple tree in the garden')
reg.findall('apple\ntree in the garden')

The first one should match, the second one should not.
(Now both matches...)
",76381603,"Your \W matches newlines. To exclude them replace \W with [^\w\n]:
import re
reg = re.compile(r'\b(apple)(?:[^\n\w]+\w+){0,4}?[^\n\w]+(tree|plant|garden)')
print(reg.findall('my\napple tree in the garden'))
#  [('apple', 'tree')]
print(reg.findall('apple\ntree in the garden'))
#  []

",
76381292,How to declare a constant datatype using a class property datatype in typescript?,"I'm creating a nestjs API so I'm using classes to declare my entities for example
export class Customer {
id: number;
name: string;
}

So now I'm working in my Customer Controller and I would like to type an get query param as customer.id because I'm thinking if some day the customer id data type changes to string automatically making my controller query param as string too.
@GET()
getCustomerById(@Params('id', id: Customer.id)) {
return this.customerService.getCustomerById(id))
}

Is it possible? Thanks
",76381604,"You can use TypeScript lookup types:
getCustomerById(@Params('id') id: Customer['id']) {}

",
76384902,Different X Labels for Different Variables,"I have two data frames t1 and t2. I want a seaborn plot where it plots side by side for every variable using the for loop. I was able to achieve this but I fail when I try to set the customized x labels. How do I incorporate set_xlabel in to the for loop?
data1 = {
    'var1': [1, 2, 3, 4],
    'var2': [20, 21, 19, 18],
    'var3': [5, 6, 7, 8]
    }

data2 = {
    'var1': [5. 2. 3. 5],
    'var2': [21, 18, 3, 11]
    'var3': [1, 9, 3, 6]
    }

  
t1 = pd.DataFrame(data1)
t2 = pd.DataFrame(data2)

xlabel_list = [""new_var1"", ""new_var2"", ""new_var3""]

def fun1(df1, df2, numvar, new_label):
    plt.tight_layout()

    fig, ax = plt.subplots(1, 2)
    sns.kdeplot(data = df1[numvar], linewidth = 3, ax=ax[0])
    sns.kdeplot(data = df2[numvar], linewidth = 3, ax=ax[1])

    ax[0].set_xlabel(new_label, weight='bold', size = 10)
    ax[1].set_xlabel(new_label, weight='bold', size = 10)    

for col in t1.columns: # how to incorporate the new_label parameter in the for loop along with col?
    fun1(df1 = t1, df2 = t2, numvar = col, new_label??)

",76384970,"Use zip:
for col, new_label in zip(t1.columns, xlabels_list):

",
76382981,Add pre-defined value to DataFrame on each instance of matching index,"I am trying to add the corresponding value from df to df1 for each time the name and week match in df1
df
                 Name    Week     Value
0                Frank  Week 3       8.0
1                  Bob  Week 3       8.0
2                  Bob  Week 4       8.0
3            Elizabeth  Week 3       4.0
4                Mario  Week 2       1.5
5                Mario  Week 3       2.5
6             Michelle  Week 3       8.0
7             Michelle  Week 4       1.0
8               Darwin  Week 1       1.0
9               Darwin  Week 2       0.5
10              Darwin  Week 3      11.0
11             Collins  Week 1       8.0
12             Collins  Week 2       6.0
13             Collins  Week 3      17.0
14             Collins  Week 4       7.0
15              Alexis  Week 1       1.5
16              Daniel  Week 3       2.0

df1
                Name    Week  Total
0              Frank  Week 1     16
1              Frank  Week 1      3
2              Frank  Week 3     28
3              Frank  Week 3      1
4              Frank  Week 4      3
..               ...     ...    ...
310           Daniel  Week 2     50
311           Daniel  Week 3     56
312           Daniel  Week 4     78
313            Kevin  Week 4    162
314            Kevin  Week 4     46

Expected:
df1
                Name    Week  Total
0              Frank  Week 1     16
1              Frank  Week 1      3
2              Frank  Week 3     **36**
3              Frank  Week 3      **9**
4              Frank  Week 4      3
..               ...     ...    ...
310           Daniel  Week 2     50
311           Daniel  Week 3     **58**
312           Daniel  Week 4     78
313            Kevin  Week 4    162
314            Kevin  Week 4     46

",76383058,"Use a merge + assign:
out = (df1
  .merge(df, how='left')
  .assign(Total=lambda d: d['Total'].add(d.pop('Value'), fill_value=0))
)

Output:
     Name    Week  Total
0   Frank  Week 1   16.0
1   Frank  Week 1    3.0
2   Frank  Week 3   36.0
3   Frank  Week 3    9.0
4   Frank  Week 4    3.0
...
5  Daniel  Week 2   50.0
6  Daniel  Week 3   58.0
7  Daniel  Week 4   78.0
8   Kevin  Week 4  162.0
9   Kevin  Week 4   46.0

",
76384338,Looping through combinations of subsets of data for processing,"I am processing sales data, sub-setting across a combination of two distinct dimensions.
The first is a category as indicated by each of these three indicators ['RA','DS','TP']. There are more indicators in the data; however, those are the only ones of interest, and the others not mentioned but in the data can be ignored.
In combination with those indicators, I want to subset across varying time intervals 7 days back, 30 days back, 60 days back, 90 days back, 120 days back, and no time constraint
Without looping through this would create 18 distinct functions for those combinations of dimensions 3 categories x 6 time which was what I first started to do
for example a function that subsets on DS and 7 days back:
def seven_days_ds(df):

    subset = df[df['Status Date'] > (datetime.now() - pd.to_timedelta(""7day""))]
    subset = subset[subset['Association Label']==""DS""]
    
    grouped_subset = subset.groupby(['Status Labelled'])
    status_counts_seven_ds = (pd.DataFrame(grouped_subset['Status Labelled'].count()))
    status_counts_seven_ds.columns = ['Counts']
    status_counts_seven_ds = status_counts_seven_ds.reset_index()
    
    return status_counts_seven_ds #(the actual function is more complicated than this).

And then repeat this, but changing the subset criteria for each combination of category and time-delta for 18 combinations of the variables of interest. Obviously, this is not ideal.
Is there a way to have a single function that creates those 18 objects, or (ideally), a single object whose columns indicate the dimensions being subset on? ie counts_ds_7 etc.
Or is this not possible, and I'm stuck doing it the long way doing them all separately?
",76384983,"IIUC, you can use :
def crossubsets(df):
    labels = [""RA"", ""DS"", ""TP""]
    time_intervals = [7, 30, 60, 90, 120, None]
    group_dfs = df.loc[
        df[""Association Label""].isin(labels)
    ].groupby(""Association Label"")

    data = []
    for l, g in group_dfs:
        for ti in time_intervals:
            s = (
                g[g[""Status Date""] > (pd.Timestamp.now() - pd.Timedelta(ti, ""d""))]
                if ti is not None else g
            )
            data.append(s[""Status Labelled""].value_counts().rename(f""counts_{l}_{ti}""))

    return pd.concat(data, axis=1) #with optional .T to have 18 rows instead of cols

",
76381561,How to find columns with three or fewer distinct values,"I'm using the Boston Housing data set from the MASS package, and working with splines from the gam package in R. However, an error is returned with this code:
library(gam)
library(MASS)
library(tidyverse)

Boston.gam <- gam(medv ~ s(crim) + s(zn) + s(indus) + s(nox) + s(rm) + s(age) + s(dis) + s(rad) + s(tax) + s(ptratio) + s(black) + s(lstat), data = Boston)

The error message is:
A smoothing variable encountered with 3 or less unique values; at least 4 needed

The variable that is causing the issue is chas, it only has two values, 1 and 0.
What is a test to determine if a column has 3 or fewer unique values so it can be eliminated from the spline analysis?
",76381629,"Would this work?
You can use dplyr::n_distinct() to perform the unique check.
# Number of unique values
n_unique_vals <- map_dbl(Boston, n_distinct)

# Names of columns with >= 4 unique vals
keep <- names(n_unique_vals)[n_unique_vals >= 4]

# Model data
gam_data <- Boston %>%
  dplyr::select(all_of(keep))

",
76381619,Android - How to make the height of a TextInputEditText to be exactly of 2 lines?,"I have a layout like this:
    <com.google.android.material.textfield.TextInputLayout
        android:id=""@+id/user_description_input_layout""
        android:layout_width=""0dp""
        android:layout_height=""wrap_content""
        android:layout_marginHorizontal=""16dp""
        android:layout_marginTop=""16dp""
        android:hint=""Description""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toTopOf=""parent""
        app:startIconContentDescription=""Lalala"">

        <com.google.android.material.textfield.TextInputEditText
            android:id=""@+id/user_description_input""
            android:layout_width=""match_parent""
            android:layout_height=""wrap_content""
            android:hint=""description"" />

    </com.google.android.material.textfield.TextInputLayout>

It's height is exactly the size of ONE line, but I wish it could be the size of 2 line exactly.
I tried adding theses attributes to my TextInputEditText tag:
<com.google.android.material.textfield.TextInputEditText
    ...
    android:maxLines=""4""
    android:scrollbars=""vertical"" />

But that made it to start as a 1 line height and stop at 2 as the user types on it. I would like it to be fixed on 2 from the begining, even if it did not have enough text to need 2 lines.
I also would like it to have a fixed size and allow the user to scroll vertically in case he add some text that is larger than 2 lines.
I know I COULD do it programatically by adding enough caracters until it has a 2 lines height and fix this heigh size and then clean the TextInputEditText, but that is such an ugly solution.
",76381632,"Try this, in your TextInputEditText
android:minLines=""2""
android:gravity=""top""

",
76384389,Spring boot + JPA problem with post mapping form where is select multiple,"I have a PostMapping whith form where user can create a meeting and invite employees. My problem is that the employees are not saved to the database.
Here is my MeetingDTO:
@Data
@Builder
public class MeetingDto {
    private Long id;
    @NotEmpty(message = ""Content could not be empty"")
    private String contentOfMeeting;
    @FutureOrPresent
    private LocalDateTime startOfMeeting;
    private LocalDateTime endOfMeeting;
    private Status status;
    private List<Employee> employees;
    private Visitor visitor;
}

Here is my controller:
@GetMapping(""/visitors/new-meeting"")
    public String createMeetingForm(Model model) {
        List<Employee> employeeList = employeeRepository.findAll();
        model.addAttribute(""employeeList"", employeeList);
        model.addAttribute(""meeting"", new Meeting());
        return ""visitors-createAMeeting"";
    }

    @PostMapping(""/visitors/new-meeting"")
    public String saveMeeting(@ModelAttribute(""meeting"") MeetingDto meetingDto) {
        String nameOfVisitor;

        Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();
        if (principal instanceof UserDetails) {
            nameOfVisitor = ((UserDetails)principal).getUsername();
        } else {
            nameOfVisitor = principal.toString();
        }

        Long visitorId = visitorRepository.findByEmailAddress(nameOfVisitor).getId();
        meetingDto.setVisitor(visitorRepository.findById(visitorId).orElse(null));
        meetingService.createMeeting(visitorId, meetingDto);
        return ""redirect:/visitors/home"";
    }

ServiceImpl:
@Service
public class MeetingServiceImpl implements MeetingService {
    private MeetingRepository meetingRepository;
    private EmployeeRepository employeeRepository;
    private VisitorRepository visitorRepository;

    @Autowired
    public MeetingServiceImpl(MeetingRepository meetingRepository, EmployeeRepository employeeRepository,
                              VisitorRepository visitorRepository) {
        this.meetingRepository = meetingRepository;
        this.employeeRepository = employeeRepository;
        this.visitorRepository = visitorRepository;
    }

    private Meeting mapToMeeting(MeetingDto meetingDto) {
        return Meeting.builder()
                .id(meetingDto.getId())
                .contentOfMeeting(meetingDto.getContentOfMeeting())
                .startOfMeeting(meetingDto.getStartOfMeeting())
                .endOfMeeting(meetingDto.getEndOfMeeting())
                .status(Status.valueOf(String.valueOf(Status.REJECTED)))
                .employees(meetingDto.getEmployees())
                .build();
    }

    @Override
    public void createMeeting(Long visitorId, MeetingDto meetingDto) {
        Visitor visitor = visitorRepository.findById(visitorId).orElse(null);
        Meeting meeting = mapToMeeting(meetingDto);
        meeting.setVisitor(visitor);
        meeting.setEmployees(meetingDto.getEmployees());
        meetingRepository.save(meeting);
    }
}

And my template for GetMapping:
<!DOCTYPE html>
<html lang=""en"" xmlns:th=""http://www.thymeleaf.org"" xmlns=""http://www.w3.org/1999/html"">
<head>
    <meta charset=""UTF-8"">
    <title>Create a meeting</title>
    <link rel=""stylesheet""
          href=""https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css""
          integrity=""sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l""
          crossorigin=""anonymous"" />
</head>
<body>
<div class=""container"">
    <h3>Create a meeting</h3>
    <hr/>
    <form action=""#"" th:action=""@{/visitors/new-meeting}"" th:object=""${meeting}"" method=""post"">
        <p>Content: <input type=""text"" id=""content"" name=""content"" th:field=""*{contentOfMeeting}"" placeholder=""Content""></p>
        <p th:if=""${#fields.hasErrors('contentOfMeeting')}"" class=""text-danger"" th:errors=""*{contentOfMeeting}""></p>
        <p>Start of meeting: <input type=""datetime-local"" id=""start"" name=""start"" th:field=""*{startOfMeeting}"" placeholder=""Start of meeting""></p>
        <p th:if=""${#fields.hasErrors('startOfMeeting')}"" class=""text-danger"" th:errors=""*{startOfMeeting}""></p>
        <p>End of meeting: <input type=""datetime-local"" id=""end"" name=""email"" th:field=""*{endOfMeeting}"" placeholder=""End of meeting""></p>
        <p></pd><span th:if=""${#fields.hasErrors('endOfMeeting')}"" th:errors=""*{endOfMeeting}"" class=""text-danger"">End of meeting can not be before start of meeting</span></p>
        <label>Employees: </label>
        <p>To select more than one employee please click ctrl</p>
        <select id=""employee"" class=""form-control"" th:field=""${employeeList}"" multiple name=""employeeList"">
            <option th:each=""employee : ${employeeList}"" th:value=""${employee.id}"" th:text=""${employee.name + ' ' + employee.surname}""></option>
        </select>
        <br>
        <p><input type=""submit"" value=""Submit""/></p>
        <br>
        <h6><a th:href=""@{/logout}"">Logout</a></h6>
        <br>
    </form>
</div>
</body>
</html>

Could you be so kind to take a look and help me to solve the issue?
I tried refactor template and controller but the problem still exist.
",76384989,"While reviewing your code, I noticed a potential issue with how the relationships are handled in JPA/Hibernate. When you're dealing with related entities, in this case Meeting and Employee, it's crucial to manage both sides of the relationship correctly.
In your code, you're assigning employees to a meeting using meeting.setEmployees(meetingDto.getEmployees());. This is correct, but depending on your relationship setup, it may not be sufficient. You might also need to set the meeting to each employee. For example, you could iterate over each employee and add the meeting:
List<Employee> employees = meetingDto.getEmployees();
for(Employee employee : employees) {
    employee.getMeetings().add(meeting); // Assumes a getMeetings() method in Employee class
}

This snippet adds the current meeting to the list of meetings for each employee. When you save your meeting, the related employees should also be updated.
Of course, this suggestion is based on common practice when using JPA/Hibernate, and the specific implementation may need adjustment according to your actual entity configuration. It's important to ensure the relationship between Meeting and Employee entities is set correctly, with appropriate cascading settings. You might need to set CascadeType.PERSIST or CascadeType.MERGE to make sure the changes to the employees are stored when saving the meeting.
If the problem persists, it would be helpful to take a closer look at the parts of your Employee and Meeting entities that define their relationship. This would allow for a more precise solution to your problem.
Revised Answer
The challenge seems in correctly assigning only the selected employees to the meeting, rather than all the employees as currently happens.
From the look of your form, it seems likely that only the IDs of the selected employees are being sent to the server when the form is submitted. So, we should adjust your MeetingDto to hold a list of these IDs. Here's how:
public class MeetingDto {
    // Other fields...
    private List<Long> employeeIds; // Replaced from List<Employee> employees
    // Remaining fields...
}

Next, we can modify the createMeeting method within your MeetingService to handle these employee IDs:
@Override
public void createMeeting(Long visitorId, MeetingDto meetingDto) {
    Visitor visitor = visitorRepository.findById(visitorId).orElse(null);
    Meeting meeting = mapToMeeting(meetingDto);
    meeting.setVisitor(visitor);

    List<Employee> selectedEmployees = employeeRepository.findAllById(meetingDto.getEmployeeIds()); // Retrieve employees by their IDs
    meeting.setEmployees(selectedEmployees); 

    meetingRepository.save(meeting);
}

Lastly, we need to ensure your form is sending the IDs of the selected employees. Your select element in the form should be modified to look like this:
<select id=""employee"" class=""form-control"" th:field=""*{employees}"" multiple name=""employeeList"">
    <option th:each=""employee : ${employeeList}"" th:value=""${employee.id}"" th:text=""${employee.name + ' ' + employee.surname}""></option>
</select>

With these alterations, your form will be transmitting the IDs of the selected employees to the server. Then, your service can retrieve the relevant employees based on these IDs from the database. As a result, only the selected employees will be associated with the meeting.
",
76382806,How to group variables that falls within a range of numbers,"I a df like this
my_df <- data.frame(
    b1 = c(2, 6, 3, 6, 4, 2, 1, 9, NA), 
    b2 = c(100, 4, 106, 102, 6, 6, 1, 1, 7), 
    b3 = c(75, 79, 8, 0, 2, 3, 9, 5, 80), 
    b4 = c(NA, 6, NA, 10, 12, 8, 3, 6, 2),
    b5 = c(2, 12, 1, 7, 8, 5, 5, 6, NA),
    b6 = c(9, 2, 4, 6, 7, 6, 6, 7, 9),
    b7 = c(1, 3, 7, 7, 4, 2, 2, 9, 5),
    b8 = c(NA, 8, 4, 5, 1, 4, 1, 3, 6),
    b9 = c(4, 5, 7, 9, 5, 1, 1, 2, 12))

I wanted to create a new column (NEW) based on the following assumptions.
If b9 is <= 2 write yellow.
If b9 is between 4 and 7 write white.
If b9 is >= 9 write green
The idea is to create something like this.
my_df1 <- data.frame(
        b1 = c(2, 6, 3, 6, 4, 2, 1, 9, NA), 
        b2 = c(100, 4, 106, 102, 6, 6, 1, 1, 7), 
        b3 = c(75, 79, 8, 0, 2, 3, 9, 5, 80), 
        b4 = c(NA, 6, NA, 10, 12, 8, 3, 6, 2),
        b5 = c(2, 12, 1, 7, 8, 5, 5, 6, NA),
        b6 = c(9, 2, 4, 6, 7, 6, 6, 7, 9),
        b7 = c(1, 3, 7, 7, 4, 2, 2, 9, 5),
        b8 = c(NA, 8, 4, 5, 1, 4, 1, 3, 6),
        b9 = c(4, 5, 7, 9, 5, 1, 1, 2, 12),
        NEW = c(""white"", ""white"", ""white"", ""green"", ""white"", ""yellow"", ""yellow"", ""yellow"", ""green""))

I thought something like this will do it, but it didn't.
greater_threshold <- 2
greater_threshold1 <- 4
greater_threshold2 <- 7
greater_threshold3 <- 9

my_df1 <- my_df %>%
    mutate(NEW = case_when(b9 <= greater_threshold ~ ""yellow"", b9 >= greater_threshold1 | b9 <= greater_threshold2 ~ ""white"", b9 >= greater_threshold3 ~ ""green""))
                       

Any help will be appreciated.
",76383084,"You can use between from dplyr:
my_df %>%
  mutate(NEW = case_when(
    b9 <= 2 ~ ""Yellow"",
    between(b9, 4, 7) ~ ""white"",
    b9 >= 9 ~ ""green""
  ))

Output:
  b1  b2 b3 b4 b5 b6 b7 b8 b9    NEW
1  2 100 75 NA  2  9  1 NA  4  white
2  6   4 79  6 12  2  3  8  5  white
3  3 106  8 NA  1  4  7  4  7  white
4  6 102  0 10  7  6  7  5  9  green
5  4   6  2 12  8  7  4  1  5  white
6  2   6  3  8  5  6  2  4  1 Yellow
7  1   1  9  3  5  6  2  1  1 Yellow
8  9   1  5  6  6  7  9  3  2 Yellow
9 NA   7 80  2 NA  9  5  6 12  green

Those not falling within the conditions (ie, 8) will be NA
",
76381410,saving ccf() looped output in r,"I have a df where the first little bit looks like:
>dput(df_long_binned_sound2[1:48,])
structure(list(id = c(20230420, 20230420, 20230420, 20230420, 
20230420, 20230420, 20230420, 20230420, 20230420, 20230420, 20230420, 
20230420, 20230420, 20230420, 20230420, 20230420, 20230424, 20230424, 
20230424, 20230424, 20230424, 20230424, 20230424, 20230424, 20230424, 
20230424, 20230424, 20230424, 20230424, 20230424, 20230424, 20230424, 
20230424, 20230426, 20230426, 20230426, 20230426, 20230426, 20230426, 
20230426, 20230426, 20230426, 20230426, 20230426, 20230426, 20230426, 
20230426, 20230426), cons_id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 16L, 17L, 18L, 19L, 
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 
33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 
46L, 47L), win = c(1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 
1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 
1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1), sound = c(1, NA, 1.5, 
NA, 2, NA, 2.75, NA, 7, NA, 8, NA, 4, NA, 6.5, NA, NA, 4.5, NA, 
6, NA, 2, NA, 5.5, NA, 4.66666666666667, NA, 4.8, NA, 6, NA, 
4.5, NA, 3, NA, 2.33333333333333, NA, 6, NA, 1, NA, 1, NA, 1.66666666666667, 
NA, 4.5, NA, 5), sound2 = c(NA, 1, NA, 1.5, NA, 1.5, NA, 6, NA, 
8, NA, 1, NA, 8, NA, 7, 3, NA, 5, NA, 5, NA, 5, NA, 6.5, NA, 
8, NA, 6, NA, 5, NA, 5.66666666666667, NA, 3.5, NA, 2, NA, 2.42857142857143, 
NA, 1.5, NA, 2, NA, 8, NA, 2.33333333333333, NA)), row.names = c(NA, 
-48L), class = c(""tbl_df"", ""tbl"", ""data.frame""))

I am running some cross-correlation analysis on it and I would like to save the number outputs of ccf(). I can save all the correlograms using:
ids <- unique(df_long_binned_sound2$id)
for (i in 1:length(ids)){
  pdf(file = paste(""/Users/myname/Desktop/Current Work/CRTT study - 2022/CRTT - Full/CRTT_r_Full/Wack_A_Mole/CC_CustomBin/CC/plot_"", ids[i], "".pdf""),
      width = 10, height = 10
  )
  
  ccf(df_long_binned_sound2$sound[which(df_long_binned_sound2$id == ids[i])], df_long_binned_sound2$sound2[which(df_long_binned_sound2$id == ids[i])],
      na.action = na.pass,
      main = paste(""Corrected Correlogram \n Pair"", ids[i]),
      xlim = c(-6, 6)
  )
  
  dev.off()
}

and I can print the number outputs using:
for (i in 1:length(ids)){
  print(ccf(df_long_binned_sound2$sound[which(df_long_binned_sound2$id == ids[i])], 
                     df_long_binned_sound2$sound2[which(df_long_binned_sound2$id == ids[i])],
                     na.action = na.pass,
    )
  )
}

I would like to save the number outputs so that I end up with something like:




id
lag
lag_value




20230420
-9
-0.145


20230420
-8
-0.057




...




id
lag
lag_value




20230420
8
-0.183


20230420
9
-0.203


20230424
-9
0.234




...
I'm sure there is a simple solution but I can't seem to find it. I very optimistically tried and failed with:
df.cff <- data.frame()
for (i in 1:length(ids)){
  cff.standin <- ccf(df_long_binned_sound2$sound[which(df_long_binned_sound2$id == ids[i])], 
      df_long_binned_sound2$sound2[which(df_long_binned_sound2$id == ids[i])],
      na.action = na.pass,
    )
  df.cff <- cbind(df.cff, cff.standin)
}

Error in as.data.frame.default(x[[i]], optional = TRUE, stringsAsFactors = stringsAsFactors) : 
  cannot coerce class ‘""acf""’ to a data.frame

and:
df.cff <- data.frame()
for (i in 1:length(ids)){
  cff.standin <- ccf(df_long_binned_sound2$sound[which(df_long_binned_sound2$id == ids[i])], 
      df_long_binned_sound2$sound2[which(df_long_binned_sound2$id == ids[i])],
      na.action = na.pass,
    )
  df.cff <- rbind(df.cff, cff.standin)
}

Error in rbind(deparse.level, ...) : 
  invalid list argument: all variables should have the same length

Does anyone know a good way to save the number outputs of ccf() from a for loop? I am especially interested in a solution that formats the output like the table examples above.
TYIA :)
",76381690,"You need to inspect the ccf object with View() or checking it's help page:

Value
An object of class ""acf"", which is a list with the following
elements:
lag  A three dimensional array containing the lags at which the acf is
estimated.
acf  An array with the same dimensions as lag containing the estimated
acf.

Thus, you just want to do something like:
cbind(id = ids[i], lag = cff.standin$lag, lag_value = cff.standin$acf)

Now for the full solution:
ids <- unique(df_long_binned_sound2$id)
df_ccf <- c() #empty vector to save results
for (i in ids){ #you can pass the ids directly, instead of their index
  df1_subset <- df_long_binned_sound2[which(df_long_binned_sound2$id == i),] #saving an extra variable saves space in the call below
  
  ccf_output <- ccf(df1_subset$sound, df1_subset$sound2,
                    na.action = na.pass,
                    main = paste(""Corrected Correlogram \n Pair"", i),
                    xlim = c(-6, 6)
  )
  
  df_ccf <- rbind(df_ccf, cbind(id = i, lag = ccf_output$lag, lag_value = ccf_output$acf)) #iteratively rbind the results
}

But I prefer something using tidyverse:
df_ccf <- df_long_binned_sound2 %>%
  group_split(id) %>%
  imap_dfr(function(data, index){
    ccf(data$sound, data$sound2,
        na.action = na.pass,
        main = paste(""Corrected Correlogram \n Pair"", i),
        xlim = c(-6, 6)) %>%
      {tibble(id = ids[index],
              lag = as.numeric(.$lag),
              lag_value = as.numeric(.$acf))}
  })

",
76382508,How to make a CMake custom command depends on a target being built but without rerunning on relink?,"I have a executable used to generate a ""cache"" file. In CMake, I have something like this:
add_executable(Generator ...)
add_custom_target(OUTPUT cache
    DEPENDS Generator OtherDep1 OtherDep2
    COMMAND Generator --input OtherDep1 OtherDep2 --output cache)

However, because it takes about 10 minutes and I do not care of the cache differs when Generator changes, I do not want cache to be re-computed whenever Generator is re-linked for whatever reason. But if I remove Generator from the dependencies, it may not be available when the custom commands needs it.
I know this is a bit far from the usual Make/CMake workflow, but is there something I can do to require Generator to have been compiled before running the custom command?
",76383091,"
I do not want cache to be re-computed whenever Generator is re-linked for whatever reason.

Then you need to define target-level dependencies instead of file-level ones. Target-level dependencies are defined with add_dependencies command:
add_executable(Generator ...)

# Custom command for **file-level** dependencies.
# The output will be rebuilt whenever it will be found older than one of its dependencies.
add_custom_command(OUTPUT cache
    DEPENDS OtherDep1 OtherDep2
    COMMAND Generator --input OtherDep1 OtherDep2 --output cache)

# Custom target.
# It just makes sure, that its dependencies are up-to-date.
add_custom_target(create_cache
   DEPENDS cache
)

# **target-level** dependencies between the targets.
# They ensures only order, but do not cause rebuilding of one target
# because of another target has been rebuilt
add_dependencies(create_cache Generator)

",
76384961,Unable to connect to Apache MINA sshd server,"I'm trying to setup a sftp server with Apache MINA sshd. But I'm getting subsystem request failed on channel 0 while trying to connect to the server.
sftp -P 22 john@localhost                                                                                                                                                            
Password authentication
(john@localhost) Password:
subsystem request failed on channel 0
Connection closed

I was following this document. But I'm not sure whether I'm missing any essential parts here.
Following is the code I'm using at the moment with mina-sshd v2.10.0.

public class Main {
    public static void main(String[] args) {

        SshServer sshd = SshServer.setUpDefaultServer();
        sshd.setPort(22);
        sshd.setKeyPairProvider(new SimpleGeneratorHostKeyProvider(Paths.get(""hostkey.ser"")));

        sshd.setShellFactory(new ProcessShellFactory(""/bin/sh"", ""-i"", ""-l""));
        sshd.setCommandFactory(new ScpCommandFactory());

        sshd.setPasswordAuthenticator(new MyPasswordAuthenticator());

        try {
            System.err.println(""Starting SSHD on port 22"");
            sshd.start();
            Thread.sleep(Long.MAX_VALUE);
            System.err.println(""Exiting after a very (very very) long time"");
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

",76385001,"I think the error is caused by the server not allowing SFTP. If you check the SFTP docs for NIMA, you can see that you can enable the SFTP subsystem like this:
SftpSubsystemFactory factory = new SftpSubsystemFactory.Builder()
    //...
    .build();
sshd.setSubsystemFactories(Collections.singletonList(factory));

For further diagnosing, you could try creating a custom SftpEventListener and registering it with factory.addSftpEventListener or similar,
",
76382989,Functions intervals,"I have 3 functions, how can I plot them using differents intervals ?
This is my code:
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-5,5,100)
y = 2*x+1
k = 3*x+2
i = 2*x+2
plt.plot(x, y, '-r', label='y=2x+1')
plt.plot(x, k, '-r', label='k =3x+2')
plt.plot(x, i, '-r', label='i =2x+2')
plt.title('3 functions on 3 intervals')
plt.xlabel('x', color='#1C2833')
plt.ylabel('y', color='#1C2833')
plt.legend(loc='upper left')
plt.grid()
plt.show()

Wanted style : 3 intervals, 3 linear functions :

This is what I get :

My wish is to draw the first function 2*x+1 on the following interval [x:2->x:8].
The second function 3*x+2 has to be plotted on the interval [x:8->x:12]
The third function 2*x+2 has to be plotted on the interval [x:12->x:20]
Is it possible ?
Edit :
Ended up with this :
x = np.linspace(-5,0,100)
t = np.linspace(0,5,100)
m = np.linspace(5,10,100)
y = 2*x+1
k = 3*x-2
i = 2*x+2
plt.plot(x, y, '-r', label='y=2x+1')
plt.plot(t, k, '-r', label='k =3x-2')
plt.plot(m, i, '-r', label='i =2x+2')

Result :

",76383095,"Is it this that you want?

In [33]: import matplotlib.pyplot as plt
    ...: import numpy as np
    ...: 
    ...: xs = [np.linspace(0,4), np.linspace(-3, 5), np.linspace(4, 10)]
    ...: fs = [np.cos, np.sin, lambda x:np.sin(x)-2*np.cos(x)]
    ...: for x, f in zip(xs, fs):
    ...:     plt.plot(x, f(x), label=f.__name__)
    ...: plt.legend()
    ...: plt.show()

Plotting a linear function is no different,

import matplotlib.pyplot as plt
import numpy as np

xs = [np.linspace(0,4), np.linspace(-3, 5), np.linspace(4, 10)]
fs = [np.cos, np.sin, lambda x:(x-6)*0.5]
fs[-1].__name__ = 'x/2-3'
for x, f in zip(xs, fs):
    plt.plot(x, f(x), label=f.__name__)
plt.legend()
plt.show()

If and only if you are going to plot ONLY LINEAR FUNCTIONS,
another approach could be

import matplotlib.pyplot as plt

# plotting y = a x + b
y = lambda xmin, xmax, a, b: (a*xmin+b, a*xmax+b)
format = lambda b: (""y = %.2f x + %.2f""if b>=0 else""y = %.2f x – %.2f"")
Xmin = [0, 4, 7]
Xmax = [5, 6, 9]
A = [1, 0.5, 3]
B = [-2, 0, 3]

for xmin, xmax, a, b in zip(Xmin, Xmax, A, B):
    plt.plot((xmin, xmax), y(xmin, xmax, a, b),
             label=format(b)%(a, abs(b)))
plt.legend()
plt.show()

",
76381322,Elasticsearch query for deeply nested field,"I am trying to find all records between two dates, but can't figure out the proper query.
The mapping looks like this
GET my-books-index-1/_mapping
{
  ""my-books-index-1"": {
    ""mappings"": {
      ""properties"": {
        ""book"": {
          ""properties"": {
            ""bookInfo"": {
              ""properties"": {
                ""publisherInfo"": {
                  ""type"": ""nested"",
                  ""properties"": {
                    ""publication"": {
                      ""properties"": {
                        ""publishedOn"": {
                          ""type"": ""date""
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}

Following is a sample record for the above mapping
""_source"": {
  ""book"": {
    ""name"": ""Harry Potter"",
    ""bookInfo"": {
      ""author"": ""J.K. Rowling"",
      ""publisherInfo"": [
        {
          ""price"": ""100"",
          ""publication"": {
            ""publishedOn"": 1685268404000 // [Sunday, May 28, 2023 10:06:44 AM]
          }
        }
      ]
    }
  }
}

[NOTE]: Some additional properties are removed from the mapping sample to keep it short and precise.
I am trying to find all books published between 25th May to 31st May.
Any help is appreciated. Thanks.
",76381691,"You can use range query inside of nested path.
PUT test_my-books-index-1
{
    ""mappings"": {
      ""properties"": {
        ""book"": {
          ""properties"": {
            ""bookInfo"": {
              ""properties"": {
                ""publisherInfo"": {
                  ""type"": ""nested"",
                  ""properties"": {
                    ""publication"": {
                      ""properties"": {
                        ""publishedOn"": {
                          ""type"": ""date""
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
}


POST test_my-books-index-1/_bulk?refresh
{""index"":{""_id"":""1""}}
{""book"":{""name"":""Harry Potter"",""bookInfo"":{""author"":""J.K. Rowling"",""publisherInfo"":[{""price"":""100"",""publication"":{""publishedOn"":1685268404000}}]}}}

dynamic date bigger than 10 days ago
GET test_my-books-index-1/_search
{
  ""query"": {
    ""bool"": {
      ""must"": [
        {
          ""nested"": {
            ""path"": ""book.bookInfo.publisherInfo"",
            ""query"": {
              ""range"": {
                ""book.bookInfo.publisherInfo.publication.publishedOn"": {
                  ""gte"": ""now-10d"",
                  ""lte"": ""now""
                }
              }
            }
          }
        }
      ]
    }
  }
}

to search with exact date
GET test_my-books-index-1/_search
{
  ""query"": {
    ""bool"": {
      ""must"": [
        {
          ""nested"": {
            ""path"": ""book.bookInfo.publisherInfo"",
            ""query"": {
              ""range"": {
                ""book.bookInfo.publisherInfo.publication.publishedOn"": {
                  ""gte"": ""25/05/2023"",
                  ""lte"": ""31/05/2023"",
                  ""format"": ""dd/MM/yyyy||yyyy""
                }
              }
            }
          }
        }
      ]
    }
  }
}

another example here: elasticsearch nested range query
",
76380576,0/1 Knapsack Problem with Dynamic Programming,"This is my attempt at the problem asked in this thread. When I try to run it with input egg_weights = (1,5,10,25) and n = 99, it seems to run into an infinite loop. The code seems to give the correct answer for smaller n, albeit very slowly. What went wrong here?
def dp_make_weight(egg_weights, target_weight, memo = {}):
    if target_weight < 0:
        return float('inf')
    elif target_weight == 0:
        return 0
    elif target_weight > 0:
        try:
            return memo[target_weight]
        except:
            memo[target_weight] = float('inf')
            for weight in egg_weights:
                result = dp_make_weight(egg_weights, target_weight - weight, memo = {})
                if result < memo[target_weight]:
                    memo[target_weight] = result + 1
        return result + 1

Here's the code that was provided for testing purpose.
if __name__ == '__main__':
    egg_weights = (1, 5, 10, 25)
    n = 99
    print(""Egg weights = (1, 5, 10, 25)"")
    print(""n = 99"")
    print(""Expected ouput: 9 (3 * 25 + 2 * 10 + 4 * 1 = 99)"")
    print(""Actual output:"", dp_make_weight(egg_weights, n))
    print()

",76381711,"If you are planning to call dp_make_weight for different egg weight lists, then the default memo argument should be handled as follows. Also, read my comments in the code:
def dp_make_weight(egg_weights, target_weight, memo=None):
    if memo is None:
        memo = {}

    infinity = float('inf')

    if target_weight < 0:
        return float('inf')
    elif target_weight == 0:
        return 0
    else: # target_weight must be > 0
        if target_weight in memo:
            return memo[target_weight]
        result = infinity
        for weight in egg_weights:
            # Only update result when dp_make_weight returns a value smaller than
            # the current result value. Also note that the current value of memo
            # is what is being passed and not a new, empty dict:
            new_result = dp_make_weight(egg_weights, target_weight - weight, memo)
            if new_result < result:
                result = new_result
        result += 1
        memo[target_weight] = result
        return result

if __name__ == '__main__':
    egg_weights = (1, 5, 10, 25)
    n = 99
    print(""Egg weights ="", egg_weights)
    print(""n ="", n)
    print(""Expected ouput: 9 (3 * 25 + 2 * 10 + 4 * 1 = 99)"")
    print(""Actual output:"", dp_make_weight(egg_weights, n))

    print()

    egg_weights = (1, 6, 9, 12, 13, 15)
    n = 724
    print(""Egg weights ="", egg_weights)
    print(""n ="", n)
    print(""Expected ouput: 49"")
    print(""Actual output:"", dp_make_weight(egg_weights, n))

Prints:
Egg weights = (1, 5, 10, 25)
n = 99
Expected ouput: 9 (3 * 25 + 2 * 10 + 4 * 1 = 99)
Actual output: 9

Egg weights = (1, 6, 9, 12, 13, 15)
n = 724
Expected ouput: 49
Actual output: 49

",
76381523,"How can I get accumulating weighted-average prices in KDB+ by symbol from a table, taking into account all previous records?","I would like get accumulating weighted-average prices by sym from a table, meaning taking account of not just the previous record but all previous records.
Input
q)show t:([]sym:`a`a`a`b`b;size:(2;6;2;7;5);price:(2;10;3;4;9))
sym size price
--------------
a   2    2
a   6    10
a   2    3
b   7    4
b   5    9

Desired Output:
q)show t:([]sym:`a`a`b`b;size:(2;6;7;5);price:(2;10;4;9);avgPrice:(2;8;4;6.083))
sym size price avgPrice
-----------------------
a   2    2     2
a   6    10    8
a   2    3     7
b   7    4     4
b   5    9     6.083
so for the second row: (2*2+10*6)/(2+6)=8
so for the third row: (2*2+10*6+2*3)/(2+6+2)=7
so for the forth row: (7*4+5*9)/(7+5)=6.083

Any help would be appreciated.
Thanks in advance.
",76381725,"update avgPrice:(sums price*size)%sums size by sym from t
sym size price avgPrice
-----------------------
a   2    2     2
a   6    10    8
a   2    3     7
b   7    4     4
b   5    9     6.083333

",
76384914,Items are showing blank in DataGrid,"I have a TableData class:
public class TableData
{
    public string ID, WrestlerID;
    public string Name;
}

And some data that I then put on a list:
List<TableData> _tableData = new List<TableData>();
TableData tableData = new TableData
{
    ID = ""0"",
    WrestlerID = ""000"",
    Name = ""test1""
};
_tableData.Add(tableData);
TableData tableData2 = new TableData
{
    ID = ""1"",
    WrestlerID = ""111"",
    Name = ""test2""
};
_tableData.Add(tableData2);

I then iterate through my _tableData list and add each item on my DataGrid:
foreach (TableData data1 in _tableData)
{
    DGTable.Items.Add(data1);
}

BTW Here's my DataGrid:
<DataGrid x:Name=""DGTable"" Grid.Row=""1"">
    <DataGrid.Columns>
        <DataGridTextColumn Header=""ID"" Binding=""{Binding ID}"" Width=""100""/>
        <DataGridTextColumn Header=""Name"" Binding=""{Binding Name}"" Width=""*""/>
        <DataGridTextColumn Header=""Wrestler ID"" Binding=""{Binding WrestlerID}"" Width=""200""/>
    </DataGrid.Columns>
</DataGrid>

When I run the app, the DataGrid displays 2 rows but all fields are empty. Any thoughts? Thanks!
",76385010,"Your TableData class needs to have properties instead of fields to be able use bindings.
It should also implement the INotifyPropertyChanged interface to use observable properties, so that changes to those properties get reflected in the UI.
Change your class as follows:
public class TableData : INotifyPropertyChanged
{
    public event PropertyChangedEventHandler PropertyChanged;  
  
    private void OnPropertyChanged([CallerMemberName] String propertyName = """")  
    {  
        PropertyChanged?.Invoke(this, new PropertyChangedEventArgs(propertyName));
    }  

    private string id;
    public string ID
    {
        get => id;
        set
        {
            if(id == value) return;
            id = value;
            OnPropertyChanged();
        }
    }

    // repeat for WrestlerID and Name
    //...
}

Don't forget to add using System.ComponentModel; at the top.
",
76382920,How to doing dynamic calculation in python,"How to make it like that so odd indexes will be doing (-) and even indexes will do (+) The max iteration is 6. iteration 1 +10, iteration 2 -20, iteration 3 +30, iteration 4 -40, iteration 5 + 50, iteration 6 -60
AA = np.array([[9.27914]+10,
              [9.33246]-20,
              [9.26303]+30,
              [9.30597]-40,
              [9.6594 ]+50,
              [9.04283]-60,
              [8.88866]+10,
              [8.89956]-20])

expected results:
AA=np.array([
    [19.27914],
    [-10.66754],
    [39.26303],
    [-30.69403],
    [59.6594],
    [-50.95717],
    [18.88866],
    [-11.10044],
])

I try use this code but not working
max_iter = 6
iter = 0
for i in range(len(AA)):
    if i % 2 == 0:
        AA[i][0] = AA[i][0] + (iter % max_iter)
    else:
       AA[i][0] = AA[i][0] - (iter % max_iter)
    iter += 10

",76383096,"You were very close. Just had to make 3 small changes. I added a +1 inside the parenthesis, added *10 for each of the array operations and change iter += 10 to array += 1
max_iter = 6
iter = 0
for i in range(len(AA)):
    if i % 2 == 0:
        AA[i][0] = AA[i][0] + (iter % max_iter+1)*10
    else:
        AA[i][0] = AA[i][0] - (iter % max_iter+1)*10
    iter += 1

In fact, you can remove the if else statement and do it in a single line if you use the following:
AA[i][0] = AA[i][0] +(-1)**(i)* (iter % max_iter+1)*10

",
76384866,Set value of input control from input_group,"I am trying to set this input control using $input_group.find('input'); but it is not getting set. Is this the correct way to use find and then set the value of the input control or is there anyway to do this?
            var $container = $('#gridrow-field-container');
            var template = $('#gridrow-template-input-group').get(0);

            $(item.MeetingPollingPartsValues).each((indexPartsValues, PartsValues) => {
                var $input_group = $(template.content.cloneNode(true));
                var inputControl = $input_group.find('input');
                inputControl.val(PartsValues.QuestionValue);

                console.log(inputControl);

                console.log($input_group);
                $container.append($input_group);
                $('input_group').val(PartsValues.QuestionValue);
            });


 <template id=""gridrow-template-input-group"">
         <div class='row mb-3' id='newrowItem_1'>
            <div class=""input-group"">
               <input type='text' id='fieldrowItem_1' name='name[]' class='form-control fieldrowItem mb-3' placeholder=""Row 1"" data-value=""0"" >
               <span  id='spanrowItem_1' class=""input-group-addon"" style=""cursor:pointer;""  onclick=""RemoveRow(this)""  >
                  <i class=""fa fa-remove"" style=""color:#CDCDCD""></i>
              </span>
         </div>
         </div>
 </template>

",76385024,"I added Bootstrap 5 dependencies and fixed the template.
You can clone the contents of the template with:
const $inputGroup = $template.contents().clone();



const $container = $('#gridrow-field-container');
const $template = $('#gridrow-template-input-group');

const RemoveRow = (span) => {
  $(span).closest('.row').remove();
}

const item = {
  MeetingPollingPartsValues: [
    { QuestionValue: 'One'   },
    { QuestionValue: 'Two'   },
    { QuestionValue: 'Three' }
  ]
};

$(item.MeetingPollingPartsValues).each((index, partValue) => {
  const $inputGroup = $template.contents().clone();
  const $inputControl = $inputGroup.find('input');
  $inputControl.val(partValue.QuestionValue);
  $inputControl.attr('placeholder', `Row ${index + 1}`);
  $inputControl.attr('data-value', partValue.QuestionValue);
  $container.append($inputGroup);
});
<link href=""https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"" rel=""stylesheet"" integrity=""sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM"" crossorigin=""anonymous"">
<link href=""https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"" rel=""stylesheet""/>
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<script src=""https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"" integrity=""sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"" crossorigin=""anonymous""></script>
<template id=""gridrow-template-input-group"">
  <div class=""row"">
    <div class=""input-group mb-3"">
      <input type=""text"" name=""name[]"" class=""form-control""
          placeholder=""Row x"" data-value=""0"" >
      <div class=""input-group-text"" style=""cursor:pointer;"" onclick=""RemoveRow(this)"">
        <i class=""fa fa-remove"" style=""color:#CDCDCD""></i>
      </div>
    </div>
  </div>
</template>
<div id=""gridrow-field-container"" class=""container""></div>



",
76384474,How can I modify the inbuilt search bar of RenderDT in R Shiny to allow multiple entries separated by commas?,"I have been trying to add multiple entries on the search bar of the renderdt table function on shiny.
for example on the following code, instead of having a new search bar, i want to modify the one which is inbuilt in renderDT and allow it to take multiple entries, comma separated; for example setosa,virginica should bring rows with both setosa and virginica. I found solutions to add a new search bar but i wanted to know if i can modify this one accordingly. Any help regarding this would be highly appreciated.
if (interactive()) {
  library(shiny)
  library(DT)
  shinyApp(
    ui = fluidPage(fluidRow(column(12, DTOutput('tbl')))),
    server = function(input, output) {
      output$tbl = renderDT(
        iris, options = list(lengthChange = FALSE)
      )
    }
  )
}

i tried something like this, but this adds another search bar option and that is unnecessary
if (interactive()) {
  library(shiny)
  library(DT)
  
  shinyApp(
    ui = fluidPage(
      fluidRow(DTOutput('tbl'))
    ),
    server = function(input, output) {
      output$tbl = renderDT({
        data <- iris
        
        searchItems <- unlist(strsplit(input$search, "","")) # Split input string by commas
        searchItems <- trimws(searchItems) # Remove leading/trailing whitespace
        
        filteredData <- data[data$Species %in% searchItems, ]
        
        datatable(filteredData, options = list(lengthChange = FALSE))
      })
    }
  )
}

",76385026,"You can use this code:
library(shiny)
library(DT)

callback <- function(sep) {
  sprintf('
$(""div.search"").append($(""#mySearch""));
$(""#mySearch"").on(""keyup redraw"", function(){
  var splits = $(""#mySearch"").val().split(""%s"").filter(function(x){return x !=="""";})
  var searchString = ""("" + splits.join(""|"") + "")"";
  table.search(searchString, true).draw(true);
});
', sep)
}

ui <- fluidPage(
  tags$head(tags$style(HTML("".search {float: right;}""))),
  br(),
  tags$input(type = ""text"", id = ""mySearch"", placeholder = ""Search""),
  DTOutput(""dtable"")
)

server <- function(input, output){
  
  output[[""dtable""]] <- renderDT({
    datatable(
      iris[c(1, 2, 51, 52, 101, 102),],
      options = list(
        dom = ""l<'search'>rtip""
      ),
      callback = JS(callback("",""))
    )
  }, server = FALSE)
  
}

shinyApp(ui, server)


Personally I prefer the search builder:
datatable(
  iris[c(1, 2, 51, 52, 101, 102),],
  extensions = ""SearchBuilder"",
  options = list(
    dom = ""Qlfrtip"", 
    searchbuilder = TRUE
  )
)


",
76383038,Pytestfs write then read doesn't return expected value,"I'm trying to write a test involving the filesystem. I chose to use pyfakefs and pytest for writing these tests. When I was trying to write and then read from the fake filesystem, I couldn't seem to get any tests to work. So, I wrote a simple test to ensure that pyfakefs was reading the right value:
def test_filesystem(fs):
    with open(""fooey.txt"", ""w+"") as my_file:
        my_file.write(""Hello"")
        read = my_file.read(-1)
        assert os.path.exists(""fooey.txt"")
        assert ""Hello"" in read

The first assertion passes. The second one fails. When I debug, read has a value of ''. I'm struggling to understand what's going on here. Does file writing or reading not work within pyfakefs? Am I doing something wrong?
",76383142,"def test_filesystem(fs):
    with open(""fooey.txt"", ""w"") as my_file:
        my_file.write(""Hello"")
        
    with open(""fooey.txt"", ""r"") as my_file:
        read = my_file.read()
        assert os.path.exists(""hoklh\\fooey.txt"")
        assert ""Hello"" in read

This should do it!
",
76381054,Adding numpy array to Pandas dataframe cell results in ValueError,"I want to place a numpy array in a cell of a pandas dataframe.
For specific reasons, before assigning the array to the cell, I add another column in the same dataframe, whose values are set to NaN.
Can someone help me understand what adding the column with the nans does to my data frame, why breaks the code, and how I can fix it?
Inserting an array into a column works:
import pandas as pd
import numpy as np

#%% this works as expected
df = pd.DataFrame([0, 1, 2, 3, 4], columns=['a'])
df['a'] = df['a'].astype(object)
df.loc[4, 'a'] = np.array([5, 6, 7, 8])
df

But after inserting the column with nans, the same code breaks and I get the following error:
ValueError: Must have equal len keys and value when setting with an iterable
#%% after adding a second column, x, filled with nan, the code breaks
df = pd.DataFrame([0, 1, 2, 3, 4], columns=['a'])
df['x'] = np.nan
df['a'] = df['a'].astype(object)
df.loc[4, 'a'] = np.array([5, 6, 7, 8])
df

Finally, I want to add the array to the new column, but I get the same error.
#%% this is what I want to do, breaks, too
df = pd.DataFrame([0, 1, 2, 3, 4], columns=['a'])
df['x'] = np.nan
df['x'] = df['x'].astype(object)
df.loc[4, 'x'] = np.array([5, 6, 7, 8])
df


",76381777,"If you only need to set a single cell, use at:
df.at[4, 'a'] = np.array([5, 6, 7, 8])


",
76381509,How can I overwrite a file in a different folder using shutil.copy() and os.rename() in Python?,"I have a script which outputs an excel file '.xlsx' containing various data. It generates a file with the date in the name in one folder, and then generates a copy, using shutil.copy(), in a separate folder. I then rename the file using os.rename(), however instead of overwriting the file already there, it produces the following error:
""FileExistsError: [WinError 183] Cannot create a file when that file already exists:""

I need the file to be exactly the same name everytime the script is run, as it is subsequently used as an automatic input file for PowerBI.
Current code is as follows:
# Select file you want to copy & where to copy it to
src_file = vb.output_path
destination = vb.path_reports_cashflowcopy

# Copy the file
shutil.copy(src_file, destination)

# Define copy path
cashflow_copy_path = vb.ROOT_DIR + '\\Data_and_Reports\\Reports\\Cashflow Copies\\'

# Rename the file
os.rename(cashflow_copy_path + str(date.today()) + ' -  Lettings Report BETA - '  +       vb.academic_year_selection +  '.xlsx',
          cashflow_copy_path + 'Copy ' + vb.academic_year_selection + '.xlsx')

src_file is the original output excel file created.
vb.xxxxxx is from a connected script which describes various file paths and other inputs used to name the files.
",76381781,"To avoid the ""FileExistsError"" when renaming the file, you can check if the destination file already exists before renaming it.
import os
import shutil
from datetime import date

# Select file you want to copy & where to copy it to
src_file = vb.output_path
destination = vb.path_reports_cashflowcopy

# Copy the file
shutil.copy(src_file, destination)

# Define copy path
cashflow_copy_path = vb.ROOT_DIR + '\\Data_and_Reports\\Reports\\Cashflow Copies\\'

# Rename the file
new_file_name = cashflow_copy_path + 'Copy ' + vb.academic_year_selection + '.xlsx'
existing_file = cashflow_copy_path + str(date.today()) + ' -  Lettings Report BETA - ' + vb.academic_year_selection + '.xlsx'

if os.path.exists(existing_file):
    os.remove(existing_file) # Delete the existing file if it exists

os.rename(src_file, new_file_name)

",
76385031,How to make the index from ngFor part of an html tag value,"I have an ngFor loop set up like this:
<div *ngFor=""let record of this.RecordsProcessed; let i = index"">
   <div class=""row my-16"" data-test='test'_{{i}}>
        <div class=""col-4"">Id:</div>
        <div class=""col-8"">{{record?.Id}}</div>
    </div>
</div>

I want to put the index from ngFor on the data-text tag within the html. Is it possible to do something like this within the html?
",76385105,"Try like this:
  <div *ngFor=""let record of this.RecordsProcessed; let i = index"">
  <div class=""row my-16"" [attr.data-test]=""'test_' + i"">
       <div class=""col-4"">Id:</div>
       <div class=""col-8"">{{record?.Id}}</div>
   </div>
</div>

[] brackets let angular know that everything inside of """" is typescript code.
",
76385070,JavaScript - Code after case in switch statements,"I am trying to decipher this code (MurmurHash) and came across the following lines:
switch (remainder) {
    case 3: k1 ^= (key.charCodeAt(i + 2) & 0xff) << 16;
    case 2: k1 ^= (key.charCodeAt(i + 1) & 0xff) << 8;
    case 1: k1 ^= (key.charCodeAt(i) & 0xff);
    
    // When is this executed?

    k1 = (((k1 & 0xffff) * c1) + ((((k1 >>> 16) * c1) & 0xffff) << 16)) & 0xffffffff;
    k1 = (k1 << 15) | (k1 >>> 17);
    k1 = (((k1 & 0xffff) * c2) + ((((k1 >>> 16) * c2) & 0xffff) << 16)) & 0xffffffff;
    h1 ^= k1;
}

My question is as follows: I have never seen code inside a switch statement that is not part of either a case or a default and would greatly appreciate it if someone could explain when the part after the last case statement is supposed to get executed.
Is it an alternative way of writing a default statement?
Or will this always get executed, just as if it were written outside of the switch block?
Information on this topic seems very difficult to come by as documentation on switch statements generally deals with case and default, and it's also impossible to test without changing the code too much which might affect its behavior.
Thanks in advance!
",76385109,"The code is part of case 1.
Personally I'd re-arrange the whitespace to be more clear:
switch (remainder) {
  case 3:
    k1 ^= (key.charCodeAt(i + 2) & 0xff) << 16;
  case 2:
    k1 ^= (key.charCodeAt(i + 1) & 0xff) << 8;
  case 1:
    k1 ^= (key.charCodeAt(i) & 0xff);
    k1 = (((k1 & 0xffff) * c1) + ((((k1 >>> 16) * c1) & 0xffff) << 16)) & 0xffffffff;
    k1 = (k1 << 15) | (k1 >>> 17);
    k1 = (((k1 & 0xffff) * c2) + ((((k1 >>> 16) * c2) & 0xffff) << 16)) & 0xffffffff;
    h1 ^= k1;
}

The point is that, without a break; in any of the cases, any matched case will execute and then control will flow to the next case.
So, assuming remainder can only be 1, 2, or 3...

If it's 3, all statements are executed.
If it's 2, case 3 is skipped but the rest is executed.
If it's 1, case 3 and case 2 are skipped but the rest is executed.

The logic is, perhaps a bit unintuitively, relying on the control flow of switch to continue on to the next (not-actually-matching) case.
",
76382771,Getting the correct order of factors for parametric terms in gratia::draw(parametric_effects()),"I have a data set:
   df<- structure(list(Depth = c(6.83999999999997, 8.56, 4.64999999999998, 
8.83999999999997, 6.56, 8.64999999999998, 12.21, 11.82, 5.41000000000003, 
11.63, 9.41000000000003, 11.26, 8.95999999999998, 10.81, 10.68, 
12.74, 14.06, 8.16000000000003, 12.31, 10.76, 10.74, 1, 9.38, 
5, 4, 12, 6.70999999999998, 8.56, 14.65, 16.71, 12.56, 18.65, 
20.21, 11.82, 13.41, 13.63, 13.41, 13.26, 22.96, 14.81, 20.74, 
30.06, 30.16, 32.31, 32.21, 14.76, 14.74, 4.66000000000003, 10, 
4, 15, 8.70999999999998, 32.65, 26.21, 29.82, 29.41, 5.63, 23.41, 
29.26, 2.95999999999998, 2.81, 2.68000000000001, 2.74000000000001, 
2.06, 2.16000000000003, 2.31, 4.20999999999998, 8.75999999999999, 
2.74000000000001, 18.66, 3, 4, 20, 6.83999999999997, 1, 6.64999999999998, 
6.20999999999998, 1.81999999999999, 1.41000000000003, 3.63, 3.41000000000003, 
5.25999999999999, 2.95999999999998, 2.81, 1, 2.74000000000001, 
4.06, 4.16000000000003, 4.31, 4.20999999999998, 2.75999999999999, 
2.74000000000001, 1, 5, 3, 4.70999999999998, 2.56, 2.64999999999998, 
10.21, 7.81999999999999), NEAR_DIST = c(18.77925552, 18.30180262, 
61.36019078, 179.2770495, 10.43166516, 17.9171804, 46.20571245, 
31.99340507, 10.43166516, 26.7170903, 24.47782541, 33.08965222, 
27.27138524, 43.4212158, 46.0670014, 50.11661352, 47.39692573, 
64.4374351, 49.66872737, 12.12884673, 15.13068812, 25.02246826, 
10.46189005, 13.46373164, 16.89230952, 13.51981867, 32.50661183, 
38.24201162, 38.5502434, 82.06185032, 49.57486607, 90.64395203, 
83.61730031, 49.74483449, 397.2686612, 53.49338859, 68.02475678, 
59.6583949, 130.7528811, 67.27058895, 111.2988217, 347.3593823, 
220.5169227, 268.5649787, 194.9220113, 84.48739079, 57.1344938, 
24.35529161, 54.84148996, 18.74063124, 66.63864028, 203.7119682, 
829.3788162, 309.4190672, 395.4959263, 326.7671063, 35.65309711, 
264.2374189, 307.025746, 23.02085763, 26.3683775, 22.93486062, 
25.28307029, 15.49632807, 14.59667995, 13.36925569, 11.9476145, 
152.7517309, 11.30381957, 74.36911773, 3.773174432, 6.825998674, 
79.40020637, 38.8451901, 3.853365482, 34.8719427, 38.02805106, 
21.06138328, 20.76016614, 37.60511548, 25.71672169, 41.9543577, 
26.1675823, 26.1675823, 16.49388675, 29.12695505, 29.12695505, 
25.21064884, 27.6250245, 25.21064884, 21.06138328, 18.59893184, 
11.08799823, 19.92747995, 16.25210115, 18.52964249, 5.582718512, 
10.11944373, 56.29794875, 36.03064946), Season2 = structure(c(3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 
4L, 4L, 4L, 4L, 4L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 
3L, 3L, 3L), levels = c(""Winter"", ""Spring"", ""Summer"", ""Fall""), class = c(""ordered"", 
""factor""))), row.names = c(NA, -100L), class = c(""tbl_df"", ""tbl"", 
""data.frame""))

and am running a gam with the data:
library(mgcv)
library(gratia)
gam<-gam(Depth~s(NEAR_DIST)+Season2,data=df)

even though the Season2 variable is ordered:
unique(df$Season2)
[1] Summer Fall   Winter Spring
Levels: Winter < Spring < Summer < Fall

when I call:
draw(parametric_effects(gam))

The order of the x-axis is alphabetical.

How can I get the x-axis to match the order of my factor here? The old version of gratia used to do this. I have: version 0.8.1.34
",76383156,"As a workaround or fix for your issue you could set the order using the limits argument of scale_x_discrete:
library(mgcv)
#> Loading required package: nlme
#> This is mgcv 1.8-42. For overview type 'help(""mgcv-package"")'.
library(gratia)
library(ggplot2)

packageVersion(""gratia"")
#> [1] '0.8.1.34'

gam <- gam(Depth ~ s(NEAR_DIST) + Season2, data = df)

draw(parametric_effects(gam)) +
  ggplot2::scale_x_discrete(limits = levels(df$Season2))


",
76383137,Why I can't send data using submit button inside Form,"I want to create simple react component with form inside, every time that user click Submit button, data should be send to the server.
For testing purposes I created component with two buttons, one stand alone and one inside form.
return (
    <>
    <button onClick={submit}>Working example</button>
    <form style={cellStyle}
        onSubmit={submit}>
                
        <div>
            <input type=""submit""
                value={reactDict[""send""]}
                className=""btn btn-success""
                style={{ margin: ""15px 5px 5px 0px"" }} />
        </div>
    </form>
    </>
);

Both of them use same ""submit"" function
const submit = (event) => {
    event.preventDefault();
    const requestOptions = {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ test: 'some message' })
    };

    fetch(`neededitems`, requestOptions)
        .then(async response => {
            console.log(response);
        })
        .catch(error => {
            console.error('There was an error!', error);
        });
}

When I click button outside form everything works fine ,
When I click submit button inside form I got an error .
",76383166,"Add the attribute method=""post"" to the  element.
",
76384759,What should be the C# equivalent of Rust's Box<> type?,"I am translating Rust code into C#.
Rust has some type called Box<> which is said to be C++'s equivalent of smart pointers.
What should be the C# equivalent of Rust's Box<> type?
",76385111,"The short answer is that there isn't an exactly-equivalent type.
Box is used to put a value on the heap.  There are several reasons why you would want to do that, but that's all it does.
In C#, reference types always point to a heap allocation.  ""Boxing"" is therefore pointless on reference types, as they are already effectively boxed.
C# value types are instead allocated directly, either on the stack or directly inside an object's heap allocation if used as a non-static class field.  You can box a C# value type by casting it to a compatible reference type: either object or an interface type implemented by the value type.
However, reference types in C# are not just boxed, they also have shared ownership as well as shared mutability and so C# reference types are closer to the Rust types that implement those behaviors, such as Rc<RefCell<T>> or Arc<Mutex<T>>, though there are still very relevant semantic differences between those types and C# reference types (Rc<_> can't be sent to other threads, Arc<Mutex<_>> has to be locked to access the inner value, both Rc and Arc can create reference cycles that could cause a memory leak, etc.).  Shared mutability in particular requires some kind of synchronization/atomicity to even pass the Rust compiler, where C# has no problem letting you create data races.

In other words, you need to look at why the value is boxed.

Is it to enable polymorphism on a set of heterogeneous values (Box<dyn _>)?  Just use C# interfaces.
Is it to enable a recursive structure?  Just use C# classes, which can self-recurse without issue.

",
76383032,Stripe Customer paymentMethod applied to its subscribers,"This is a secondary question as I thought my previously answered question was resolved.
Here is my use case:

Customer (office) buys physical products.  We collect the
information on the mobile app and then the server creates a Stripe
Customer and a PaymentIntent.  This succeeds, as evidenced by
Stripe portal
When the payment is finalized, my web hook event captures the “charge.succeeded”
event and it is my understanding that now that I have a
paymentMethod I can set it up pay automatically with the confirm and
redirect-url.  However, no attempt by me has been successful.
I then create a subscriber, and I want to use the above customer payment
method to manage the subscription payment.  The payment for this
shows as incomplete, and I have to manually confirm it.

Here is how I am handling the server side:

Create payment Intent:

Stripe.apiKey = API_SECRET_KEY;
long totalCharge = calcTotalCharge(purchaseRequest.getRequestedProducts());
PaymentIntentCreateParams paymentIntentCreateParams = PaymentIntentCreateParams.builder()
                .setCustomer(customer.getId())
                .setAmount(totalCharge)
                .setCurrency(""usd"")
                .setDescription(OFFICE_PURCHASE)
                .setSetupFutureUsage(SetupFutureUsage.OFF_SESSION)
                .setAutomaticPaymentMethods(PaymentIntentCreateParams.AutomaticPaymentMethods.builder()
                                .setEnabled(true)
                                .build())
                .build();

PaymentIntent paymentIntent = PaymentIntent.create(paymentIntentCreateParams);
SetupIntentCreateParams setupIntentParms =
                SetupIntentCreateParams.builder()
                                       .setCustomer(customer.getId())
                                       .addPaymentMethodType(""card"")
                                       .build();                                     

SetupIntent setupIntent = SetupIntent.create(setupIntentParms);

This all appears to be correct.  I use the paymentIntento with the Stripe Elements to complete the order.  I I cannot set the confirm or auto payment because I don’t have the payment method at this point.

Webhook event - this throws an exception: java.lang.RuntimeException: com.stripe.exception.InvalidRequestException: Received unknown parameters: enabled, returnUrl, confirm; code: parameter_unknown; request-id: req_My6nCQVFVNbsSgtry

try {
  PaymentIntent paymentIntent = PaymentIntent.retrieve(charge.getPaymentIntent());
  Map<String, Object> automaticPaymentMethods = new HashMap<>();
  automaticPaymentMethods.put(""enabled"", true);
  automaticPaymentMethods.put(""confirm"", true);
  automaticPaymentMethods.put(""returnUrl"", ""https://cnn.com”); <== this is just for Stripe   requirement, it does nothing
  logger.info(""webhook updating paymentIntent.automatic payment method as {} "", paymentIntent);
} catch (StripeException e) {
throw new RuntimeException(e);
}

So where I appear to be stuck is how do I set the customer paymentMethod to be applied as confirm automatically since the subscriber will not have the ability to confirm the payment.  I also was uncertain about a custom URL scheme or an universal link, despite the links you provided.
Update to answer responses:
Webhook does this when customer payment is received:
(I am no longer trying to set the automaticPaymentMethods)
 PaymentIntent paymentIntent = PaymentIntent.retrieve(charge.getPaymentIntent());
paymentIntent.getAutomaticPaymentMethods().setEnabled(true);

 String paymentMethod = charge.getPaymentMethod();
 String customerId = charge.getCustomer();
 Long chargeAmount = charge.getAmountCaptured();
 // now we can update the pending order with the paymentMethod
 try {
   Customer customer = Customer.retrieve(customerId);
   customer.update(CustomerUpdateParams.builder()
   .setInvoiceSettings(InvoiceSettings.builder()
   .setDefaultPaymentMethod(paymentMethod)
   .build())
   .build());
  } catch (StripeException se) {
    logger.error(""unable to customer {} the paymentMethod {}"", customerId, paymentMethod);
 }

",76383179,"In order to create Subscriptions with customer's attached payment method, you need to set it as default payment method for the customer. Specifically on customer.invoice_settings.default_payment_method parameter
Once you do that, the subscription should charge the default payment method on creation.
For your second question, I don't fully understand what you're trying to do exactly. Automatic Payment Methods parameter on a PaymentIntent only supports enabled property. So not sure why you're trying to set confirm and returnUrl there. Are you following a guide for this?
",
76381709,How to convert a jupyter notebook to a python script with cell delimiters (#%%)?,"How to convert a jupyter notebook to a python script with cell delimiters (#%%)?
I've already checked nbconvert , but it doesn't seem to have the one. Also, the same question found, the answer doesn't satisfy the need because actual raw source codes of jupyter notebook isn't structured as such. (It'd be better to be able to convert at once, instead of converting with nbconvert first and then pattern matching)
Any tools recommended? Or could you share a script to achieve this?
",76381792,"That looks similar to the percent delimiter that Jupytext handles, see the top few commands here also. The specific commands I'm referencing:
jupytext --to py:percent notebook.ipynb         # convert notebook.ipynb to a .py file in the double percent format
jupytext --to py:percent --opt comment_magics=false notebook.ipynb   # same as above + do not comment magic commands

See the bottom of the percent format section for more about that last command and further customization options.
",
76385046,How do I use itertools in Python to generate all possible variants of a list of keywords with leet code?,"Python - How to make current script iterate through list of words instead of one string/word only?
I am very new to python, and have put together a script parsing different scripts i've looked at.
The goal is to return all possible variants of a list of keywords, replacing the characters by leet code (e.g.: 'L33T' or 'l337' instead of 'Leet')
I have been able to achieve this for one string/word only, but I wish to be able to input a list of keywords and obtain the same results.
This is my first time using Stack overflow, and I would really appreciate any help you can provide me :)
Here is my code:
import itertools

def leet(word):
    leet_matches = [['a','@','4','∆','Д','а','а','a','à'],
    ['b','8','b','ḃ','ḅ','ḇ'],
    ['c','<','{','[','(','©'],
    ['d','d','ď','ḋ','ḍ','ḏ','ḑ','ḓ'],
    ['e','3','£','₤','€','е'],
    ['f','7','ƒ','ḟ'],
    ['g','9','[','-','6','ĝ','ğ','ġ','ģ','ǧ','ǵ','ḡ'],
    ['h','4','#','ĥ','ȟ','ḣ','ḥ','ḧ','ḩ','ḫ','ẖ'],
    ['i','1','|','!','ì','í'],
    ['j','√','ĵ','ǰ'],
    ['k','ķ','ǩ','ḱ','ḳ','ḵ','ķ','ǩ','ḱ','ḳ','ḵ'],
    ['l','1','|','ĺ','ļ','ľ','ḷ','ḹ','ḻ','ḽ'],
    ['m','м','ḿ','ṁ','ṃ'],
    ['n','И','и','п','ñ','ń','ņ','ň','ǹ','ṅ','ṇ','ṉ','ṋ'],
    ['o','0','Ø','Θ','о','ө','ò','ó','ô','õ','ö','ō','ŏ','ő','ơ','ǒ','ǫ','ǭ'],
    ['p','р','ṕ','ṗ'],
    ['q','9','(',')','0'],
    ['r','Я','®','ŕ','ŗ','ř','ȑ','ȓ','ṙ','ṛ','ṝ','ṟ'],
    ['s','5','$','§','ś','ŝ','ş','š','ș','ṡ','ṣ','ṥ','ṧ','ṩ'],
    ['t','7','+','т','ţ','ť','ț','ṫ','ṭ','ṯ','ṱ','ẗ'],
    ['u','ù','ú','û','ü','ũ','ū','ŭ','ů','ű','ų','ư','ǔ','ǖ','ǘ'],
    ['v'],
    ['w','Ш','ŵ','ẁ','ẃ','ẅ','ẇ','ẉ','ẘ'],
    ['x','×','%','*','Ж','ẋ','ẍ'],
    ['y','¥','Ч','ү','у','ṽ'],
    ['z','5','ź','ż','ž','ẑ']]
    l = []
    for letter in word:
        for match in leet_matches:
            if match[0] == letter:
                l.append(match)
    return list(itertools.product(*l))

word = ""hola""
test_list = leet(word)

def remove(string):
    return string.replace("" "", """")

res = [''.join(tups) for tups in test_list]
print (str(res)+remove(str(res)))


import csv
with open ('leet_latinalphabet.csv', mode ='w') as csvfile:
    fieldnames = ['leet variants']
    writer = csv.DictWriter(csvfile,fieldnames=fieldnames)
    writer.writeheader()
    writer.writerow({""leet variants"":str(res)[1:-1].replace(""'"","""")})

",76385123,"Loop over the list of words, calling leet() on each word.
words = ['hola', 'some', 'other', 'word']

with open ('leet_latinalphabet.csv', mode ='w') as csvfile:
    fieldnames = ['word', 'leet variants']
    writer = csv.DictWriter(csvfile,fieldnames=fieldnames)
    writer.writeheader()
    for word in words:
        row = {""word"": word, ""leet variants"": "","".join(leet(word))}
        writer.writerow(row)

",
76382337,ml-kit - barcode-scanning android - Google code scanner,"I am using Google code scanner   Android MLKit for Barcode scanning. I am using below dependencies. I want the use bundled model so that initialisation time is not taken when app is launched. Is there a way can I use bundled version of model :
Please find below dependencies I used for this :
implementation 'com.google.android.gms:play-services-code-scanner:16.0.0'
AndroidManifest:

When I used the above dependencies , I see below exception during downloading the model:
Waiting for the Barcode UI module to be downloaded.
Is there a way can I use bundled version of model so that I need not wait for Barcode UI module to be downloaded. Please help me regarding this
Thanks in Adavance.
",76383224,"What about this:
dependencies {
  // ...
  // Use this dependency to bundle the model with your app
  implementation 'com.google.mlkit:barcode-scanning:17.1.0'
}

Found at: https://developers.google.com/ml-kit/vision/barcode-scanning/android
",
76381596,Spring Rest Validation ObjectNode data size limit,"I have rest controller with token creation call. Here inside ObjectNode I get big json data. The database column is varchar2(4000) nad I want limit this ObjectNode size to 4000 adding validation at controller level. Not sure how to do this?
data class TokenRequest(
    @NotEmpty(message = ""id is mandatory"")
    open val id: String,
    @NotEmpty(message = ""gameId is mandatory"")
    open val game: String,
    @NotEmpty(message = ""gameType is mandatory"")
    open val type: String,
    @NotEmpty(message = ""gameDate is mandatory"")
    open val date: String,
    @NotEmpty(message = ""coupon is mandatory"")
    open val token: ObjectNode,
)
class TokenController {
fun createToken(@Valid @RequestBody request: TokenRequest): Token {
        val now = Token.generateNowTimestamp()
        val token = Token.fromTokenRequest(request, now, now, request.teamId)
        return tokenService.create(token)
    }
}

",76381793,"It sounds like you're trying to cap the size of the JSON data contained in the 'token' field of your request. You want it to be no more than 4000 characters, right? There's actually a way to handle this in Kotlin by creating your own validation annotation. Here's how:
First, you need to create the annotation itself:
@Target(AnnotationTarget.FIELD)
@Retention(AnnotationRetention.RUNTIME)
@MustBeDocumented
@Constraint(validatedBy = [JsonNodeLengthValidator::class])
annotation class MaxJsonLength(
    val message: String = ""JSON Object is too big"",
    val groups: Array<KClass<*>> = [],
    val payload: Array<KClass<out Payload>> = [],
    val value: Int = 4000
)

Then, we'll make a custom validator for it:
import com.fasterxml.jackson.databind.node.ObjectNode
import javax.validation.ConstraintValidator
import javax.validation.ConstraintValidatorContext

class JsonNodeLengthValidator : ConstraintValidator<MaxJsonLength, ObjectNode> {

    private var maxLength: Int = 0

    override fun initialize(annotation: MaxJsonLength) {
        this.maxLength = annotation.value
    }

    override fun isValid(node: ObjectNode?, context: ConstraintValidatorContext): Boolean {
        return node?.toString()?.length ?: 0 <= maxLength
    }
}

Finally, we'll use our shiny new validator annotation in your data class:
data class TokenRequest(
    @NotEmpty(message = ""id is mandatory"")
    open val id: String,
    @NotEmpty(message = ""gameId is mandatory"")
    open val game: String,
    @NotEmpty(message = ""gameType is mandatory"")
    open val type: String,
    @NotEmpty(message = ""gameDate is mandatory"")
    open val date: String,
    @NotEmpty(message = ""coupon is mandatory"")
    @MaxJsonLength(value = 4000, message = ""Token JSON object is too big"")
    open val token: ObjectNode,
)

So there you have it! This makes sure that your TokenRequest validation will fail if the JSON string of token goes beyond 4000 characters. If it does, you'll get a validation error. Hope this helps!
",
76384241,Not able to format YAML using SnakeYaml keeping original way,"Have following YAML
image:
  repository: ""test.com/test""
  pullPolicy: IfNotPresent
  tag: ""abc""

JAVA code to modify the YAKL file
public class SnakeYaml1 {

    public static void main(String[] args) throws FileNotFoundException {
        // TODO Auto-generated method stub
        
        InputStream inputStream = new FileInputStream(new File(""C:\\yaml\\student1.yaml""));
        
        Yaml yaml = new Yaml(new Constructor(Values1.class));
    
        Values1 data = yaml.load(inputStream);
        Image image = new Image();
        image.setPullPolicy(""update"");
        data.setImage(image);
        
        DumperOptions options = new DumperOptions();
        options.setIndent(2);
        options.setDefaultFlowStyle(DumperOptions.FlowStyle.FLOW);
        options.setIndicatorIndent(2);
        options.setIndentWithIndicator(true);
        
        PrintWriter writer = new PrintWriter(new File(""C:\\yaml\\student1.yaml""));
        Yaml yaml1 = new Yaml(new Constructor(Values1.class));
        yaml1.dump(data, writer);

    }
}


public class Values1 {
    
    private Image image;

    public Image getImage() {
        return image;
    }

    public void setImage(Image image) {
        this.image = image;
    }
}

public class Image {
    
    private String repository;
    private String pullPolicy;
    private String tag;
    
    public Image()
    {   
    }
    
    public Image (String repository, String pullPolicy, String tags)
    {
        super();
        this.repository = repository;
        this.pullPolicy = pullPolicy;
        this.tag = tags;
        
    }
    
    public String getRepository() {
        return repository;
    }
    public void setRepository(String repository) {
        this.repository = repository;
    }
    public String getPullPolicy() {
        return pullPolicy;
    }
    public void setPullPolicy(String pullPolicy) {
        this.pullPolicy = pullPolicy;
    }
    public String getTag() {
        return tag;
    }
    public void setTag(String tag) {
        this.tag = tag;
    }
    

}

AFter executing the java code , the YAML format is getting changed
YAML format after executing JAVA code
!!oe.kubeapi.abc.Values1
image: {pullPolicy: update, repository: null, tag: null}

Expected YAML format after execution of java code
image:
      repository: ""test.com/test""
      pullPolicy: update
      tag: ""abc""

Not getting why the YAML format is getting changed after executing java code. Is this the bug in SnakeYaml ??
I tried putting property image in List format as well , List<Image> image still it did not work
please suggest . what should be done . Any help please ?
",76385126,"Well, you mentioned it is SnakeYaml lib, so I wonder have you ever looked through its documentation ?
Your code works as it should.
try:
 DumperOptions options = new DumperOptions();
 options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK);
 Yaml yaml  = new Yaml(options);

",
76378684,Harbor registry proxy cache vs replication,"I'm new to Harbor registry. I was asked to propose an architecture for harbor in my company. I proposed at first to use an architecture based on proxy cache. But the CISO refused to use proxy cache for the entreprise without saying why. I proposed anoter architecture based on replication. We validate some base images that are pulled from public registries and pushed into our harbor registry ( One active harbor that pulls the images from internet and another passive harbor for high avalibility + 4 other harbors that leaves in special network zones (they get the images form the master harbor)).
The question is why the ciso refused the use of proxy cache ? is there any drawbacks for using it ? what are the security risks that can appear using the harbor proxy cache vs replication ? I cant find in the internet clear informations about this question. It seems that the majority is using proxy cache.
Thank you!
",76383239,"At this stage one can only speculate, about the unprofessional behavior of not explaining the reasons and also for not asking.
Regarding Harbor proxy and replication, the main difference between both option is the difference of threat surface and its control.
Proxy

Passive, forwards requests upstream if not found locally.
No control,

Replication

Active, explicitly specify the images you want to copy from upstream
Full control

",
76383101,Reshaping a Dataframe with repeating column names,"I have data that looks like this:
    dataframe_1:                
    week    SITE        LAL SITE     LAL
0   1   BARTON CHAPEL   1.1 PENASCAL I  1
1   2   BARTON CHAPEL   1.1 PENASCAL I  1
2   3   BARTON CHAPEL   1.1 PENASCAL I  1

And, i need the final dataframe to look like this:
    dataframe_2:        
    week    SITE    LAL
0   1   BARTON CHAPEL   1.1
1   2   BARTON CHAPEL   1.1
2   3   BARTON CHAPEL   1.1
3   1   PENASCAL I  1
4   2   PENASCAL I  1
5   3   PENASCAL I  1

I've tried using 'melt' but I cannot get the desire result. Perhaps I'm using the wrong approach?
thank you,
",76383250,"Not a very generalizable solution, but will work on your example:
df.groupby('week').apply( lambda _df : pd.concat((_df.iloc[:,1:3], _df.iloc[:,3:5]))).reset_index('week')

it groups by week and then reshapes with column selection + concatenation. Removing a superfluous index column in the end.
",
76381742,How to set a timeout for a popup and close if user clicks elsewhere?,"I created a popup that appears when I click a button, but to make it disappear I have to click again. Is there a way to set a timer and make it disappear?
Function:
// When the user clicks on div, open the popup
function myFunction() {
  var popup = document.getElementById(""myPopup"");
  popup.classList.toggle(""show"");
}

Style:
.popuptext {
  display: none;
}
.popuptext.show {
  display: block;
}

The HTML:
<div class=""popup"" onclick=""myFunction()"">Click me to toggle the popup!
  <span class=""popuptext"" id=""myPopup"">A Simple Popup!</span>
</div>

I need the popup to close after 10 seconds OR when the user clicks somewhere else.
I edited the code to below and it does close after 10 seconds, how to achieve the second part (close when user clicks somewhere else):
function myFunction() {
var popup = document.getElementById(""myPopup"");
popup.classList.toggle(""show"");
if(popup.classList.contains(""show""))
    setTimeout(() => popup.classList.remove(""show""), 10000)
}

",76381799,"To do this you need to:

Define a function, hide() that hides the popup.
Add an mousedown event listener to the whole document that invokes hide

Within hide, ensure that the click event's target is not contained in the popup.


Set up the timeout to call hide
Important: Have hide clear the created timeout and remove the listener that was added.



function myFunction() {
  var popup = document.getElementById(""myPopup"");
  popup.classList.add('show')
  
  let timeout;
  
  function hide(e) {
    if (popup.contains(e.target)) return;

    popup.classList.remove(""show"");
    document.removeEventListener('mousedown', hide);
    clearTimeout(timeout)
  }
  document.addEventListener('mousedown', hide)
  timeout = setTimeout(hide, 10000)
}
.popuptext {
  display: none;
}

.popuptext.show {
  display: block;
}
<div class=""popup"" onclick=""myFunction()"">Click me to toggle the popup!
  <span class=""popuptext"" id=""myPopup"">A Simple Popup!</span>
</div>



",
76382658,3-d array with different size of row and column,"i am making a three-d array the problem i am facing is i want to create multiple 3-d array however with varying size of row and column so the first matrix size could be 0-2-2 while next matrix could be say 1-1-3 so on..
kindly do not suggest making a large matrix that could have value of all the row and columns.
i personally have tried using structure to create the code, i have defined 2-d array( for row and column) in the structure and then stored it in variable e[1].array(2-d), i have used for loop to continuously change value of row and column in array based on user input, the problem i am facing is every time the for loop changes value to next the code over writes itself hence previous values  of array can not be called so if for first matrix the size of row and column was 2-2 and next is 1-3 so e[1].x[2][2] have some value then for second loops e[1].x[1][3] the dimensions of x have been re-defined hence i could not call x[2][2].
kindly suggest ways i could store 3-d array with different size of row and column for each matrix.
 int main()
 {
 int matrix;
 printf(""ENTER NUMBER OF MATRICES\n"");
 scanf(""%d`"", &matrix);
 int row, column;


for (int m = 0; m < matrix; m++) {
    printf(""ENTER NUMBER OF ROWS IN %d MATRICE\n"", m + 1);
    scanf(""%d"", &row);
    printf(""ENTER NUMBER OF column IN %d MATICE\n"", m + 1);
    scanf(""%d"", &column);
     struct x {
  int f[row][column];
};
    struct x k[m];
    int g = 1;

    printf(""ENTER ALL ELEMENT LEFT TO RIGHT\n"");
    for (int j = 0; j < row; j++) {
        
        for (int u = 0; u < column; u++) {
            
            printf(""%d)\n"", g);
            scanf(""%d"", &k[m].f[j][u]);
            g = g + 1;
        }
    }
    for (int s = 0; s < row; s++) {
        printf(""|"");
        for (int l = 0; l < column; l++) {
             
            printf(""%d"", k[m].f[s][l]);
            if (l < column - 1) {
                printf(""  "");
                
            }
        }
        printf(""|\n"");
        
    }
    printf(""%d"", k[0].f[0][0]); // error here calling over written 
 value
 }
  return 0;

expectation:
 ENTER NUMBER OF MATRICES
 2
 ENTER NUMBER OF ROWS IN 1 MATRICE
 2
 ENTER NUMBER OF column IN 1 MATICE
 2
 ENTER ALL ELEMENT LEFT TO RIGHT  
 1)
 1
 2)
 2
 3)
 3      4
 4)
 4
 |1  2|
 |3  4|
 ENTER NUMBER OF ROWS IN 2 MATRICE
 1
 ENTER NUMBER OF column IN 2 MATICE
 3
 ENTER ALL ELEMENT LEFT TO RIGHT
 1)
 5
 2)
 6
 3)
 7
 |5  6  7|
 1 // error here in output

",76383281,"Since each matrix may be a different size, you should manage each matrix separately and record its dimensions separately. The code below shows how to use a structure type to do that.
Common C implementations support variable length arrays, so you use this to make addressing the matrix elements simpler. The program below shows how to use a pointer to a variable length array to point to the rows of a matrix. The memory address is recorded in the structure using void *, because we cannot have a variably modified type in the structure, but a separate variable is declared where it is needed, and it is set to the start of memory for the matrix.
In the absence of using variable length arrays, you would have to address the elements using manual arithmetic calculations into a linear array.
#include <stdio.h>
#include <stdlib.h>


int main(void)
{
    // Read the number of matrices.
    int NMatrices;
    printf(""Enter the number of matrices:  "");
    if (1 != scanf(""%d"", &NMatrices))
    {
        fprintf(stderr, ""Error, scanf for number of matrices failed.\n"");
        exit(EXIT_FAILURE);
    }
    if (NMatrices < 0)
    {
        fprintf(stderr, ""Error, number of matrices is negative.\n"");
        exit(EXIT_FAILURE);
    }

    // Define a type to manage a matrix.
    struct MatrixInformation
    {
        void *Memory;        // Memory for the matrix.
        int NRows, NColumns; // Number of rows and number of columns.
    };

    // Allocate memory to manage NMatrices matrices.
    struct MatrixInformation *Matrices = malloc(NMatrices * sizeof *Matrices);
    if (!Matrices)
    {
        fprintf(stderr, ""Error, failed to allocate memory.\n"");
        exit(EXIT_FAILURE);
    }

    // Read each matrix.
    for (int m = 0; m < NMatrices; ++m)
    {
        // Read the number of rows and the number of columns of this matrix.
        int NRows, NColumns;
        printf(""Enter the number of rows in matrix %d:  "", m+1);
        if (1 != scanf(""%d"", &NRows))
        {
            fprintf(stderr, ""Error, scanf for number of rows failed.\n"");
            exit(EXIT_FAILURE);
        }
        if (NRows <= 0)
        {
            fprintf(stderr, ""Error, number of rows is not positive.\n"");
            exit(EXIT_FAILURE);
        }
        printf(""Enter the number of columns in matrix %d:  "", m+1);
        if (1 != scanf(""%d"", &NColumns))
        {
            fprintf(stderr, ""Error, scanf for number of columns failed.\n"");
            exit(EXIT_FAILURE);
        }
        if (NColumns <= 0)
        {
            fprintf(stderr, ""Error, number of columns is not positive.\n"");
            exit(EXIT_FAILURE);
        }

        // Create a temporary pointer for the matrix and allocate memory.
        int (*Matrix)[NColumns] = malloc(NRows * sizeof *Matrix);
        if (!Matrix)
        {
            fprintf(stderr, ""Error, failed to allocate memory.\n"");
            exit(EXIT_FAILURE);
        }

        // Save the numbers of rows and columns and the memory address.
        Matrices[m].NRows = NRows;
        Matrices[m].NColumns = NColumns;
        Matrices[m].Memory = Matrix;

        // Get the values for the matrix elements.
        for (int r = 0; r < NRows; ++r)
            for (int c = 0; c < NColumns; ++c)
            {
                printf(""Enter the element [%d, %d]:  "", r+1, c+1);
                if (1 != scanf(""%d"", &Matrix[r][c]))
                {
                    fprintf(stderr, ""Error, scanf for element failed.\n"");
                    exit(EXIT_FAILURE);
                }
            }
    }

    // Print each matrix.
    for (int m = 0; m < NMatrices; ++m)
    {
        printf(""Matrix %d:\n"", m+1);

        // Get the numbers of rows and columns and the memory address.
        int NRows = Matrices[m].NRows;
        int NColumns = Matrices[m].NColumns;
        int (*Matrix)[NColumns] = Matrices[m].Memory;

        // Print each row.
        for (int r = 0; r < NRows; ++r)
        {
            // Start each row with a delimiter and no spaces.
            printf(""|%d"", Matrix[r][0]);

            // Print each element with two spaces for separation.
            for (int c = 1; c < NColumns; ++c)
                printf(""  %d"", Matrix[r][c]);

            // Finish each row with a delimiter and a new-line character.
            printf(""|\n"");
        }
    }

    // Free the memory of each matrix.
    for (int m = 0; m < NMatrices; ++m)
        free(Matrices[m].Memory);

    // Free the memory for the array of structures about the matrices.
    free(Matrices);
}

",
76385092,Index of .iloc API in Pandas,"<class 'pandas.core.frame.DataFrame'>
RangeIndex: 400 entries, 0 to 399
Data columns (total 11 columns):
 #   Column       Non-Null Count  Dtype 
---  ------       --------------  ----- 
 0   CompPrice    400 non-null    int64 
 1   Income       400 non-null    int64 
 2   Advertising  400 non-null    int64 
 3   Population   400 non-null    int64 
 4   Price        400 non-null    int64 
 5   ShelveLoc    400 non-null    object
 6   Age          400 non-null    int64 
 7   Education    400 non-null    int64 
 8   Urban        400 non-null    object
 9   US           400 non-null    object
 10  HighSales    400 non-null    object
dtypes: int64(7), object(4)
memory usage: 34.5+ KB

As shown in the info() result above, there are 11 columns indexed from 0 to 10 in my dataset, DF. Now, I would like to extract only the first 10 columns (that are the columns with the indices 0 to 9). However, when I try to use the code below:
DF.iloc[:, 0:9]

It returns only the first 9 columns (that is, from CompPrice to Urban).
In this case, I need to change my code to:
DF.iloc[:, 0:10]

to get what I actually want (that is, from CompPrice to US).
I'm really confused by iloc() indices. Why it requires '10' instead '9' but starts with the index '0'. The starting and ending indices are not consistent.
",76385129,"What you are observing is the standard functionality of pandas. If you look in the documentation, you can find the definition. This is intended and logical, as Python lists function the same way. As per the docs:

.iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy slice semantics).

",
76380777,Ruby IPAddr class accepting wrong IPv6 address string,"We are middle of upgrading ruby versions v2.7.3 -> v3.1.3
One of our test cases are failing related to valid ipv6 address string, check the following
# ruby 2.7.3
IPAddr.new('fe80::85e:7530:69ec:9074%en0').ipv6?
=> IPAddr::InvalidAddressError (invalid address: fe80::85e:7530:69ec:9074%en0)

# ruby 3.1.3

IPAddr.new('fe80::85e:7530:69ec:9074%en0').ipv6?
=> true

Is it really a bug or am I missing something? Please help..
",76381845,"
Is it really a bug or am I missing something?

This used to be an issue in the ipaddr default gem up to version 1.2.2 which was fixed in version 1.2.3 in order to be fully compliant with RFC 4007 and RFC 6874. Version 1.2.3 of the ipaddr default gem was shipped as part of Ruby 3.1.0.
So, you are correct. This is a bug (although opinions differ on that) which was fixed in ipaddr 1.2.3 / Ruby 3.1.0.
",
76383210,Why does pandas `date_range` rounds up to the next month?,"When using pandas.date_range with start date, frequency, and periods the date range rounds up when using the start date as the last day of a month.
It seems like a silent edge case bug. If it's not a bug, any idea why it does that?
For example
import pandas as pd

start_date = pd.Timestamp(2023, 5, 31)
date_range = pd.date_range(start=start_date, freq=""MS"", periods=6)

results in
DatetimeIndex(['2023-06-01', '2023-07-01', '2023-08-01', '2023-09-01',
               '2023-10-01', '2023-11-01'],
              dtype='datetime64[ns]', freq='MS')

From the documentation, I'd expect it to start in May and end in October:
DatetimeIndex(['2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01', '2023-09-01',
               '2023-10-01'],
              dtype='datetime64[ns]', freq='MS')

I thought it had to do with the inclusive argument but that's not the reason either.
",76383298,"pd.date_range is to generate a range of date between start and end. 2023-05-01 is less than start date 2023-05-31, it will never reach it. To do what you want, you can replace the day of pd.Timestamp by 1.
start_date = pd.Timestamp(2023, 5, 31)
date_range = pd.date_range(start=start_date.replace(day=1), freq=""MS"", periods=6)

print(date_range)

DatetimeIndex(['2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01',
               '2023-09-01', '2023-10-01'],
              dtype='datetime64[ns]', freq='MS')

",
76383232,What is the bug in my Persistent UTM Code?,"I am using the following code to persist utms across my website. However i notice that its adding a question mark to links even without the UTM parameters. Can someone help me figure out what in this code needs to change. It should only be trying to add UTM parameters to the links if there is one present in the URL.
 <script>
(function() {
  var domainsToDecorate = [
          'example.com'
      ],
      queryParams = [
          'utm_medium', //add or remove query parameters you want to transfer
          'utm_source',
          'utm_campaign',
          'utm_content',
          'utm_term'
      ]
  // do not edit anything below this line
  var links = document.querySelectorAll('a'); 

// check if links contain domain from the domainsToDecorate array and then decorates
  for (var linkIndex = 0; linkIndex < links.length; linkIndex++) {
      for (var domainIndex = 0; domainIndex < domainsToDecorate.length; domainIndex++) { 
          if (links[linkIndex].href.indexOf(domainsToDecorate[domainIndex]) > -1 && links[linkIndex].href.indexOf(""#"") === -1) {
              links[linkIndex].href = decorateUrl(links[linkIndex].href);
          }
      }
  }
// decorates the URL with query params
  function decorateUrl(urlToDecorate) {
      urlToDecorate = (urlToDecorate.indexOf('?') === -1) ? urlToDecorate + '?' : urlToDecorate + '&';
      var collectedQueryParams = [];
      for (var queryIndex = 0; queryIndex < queryParams.length; queryIndex++) {
          if (getQueryParam(queryParams[queryIndex])) {
              collectedQueryParams.push(queryParams[queryIndex] + '=' + getQueryParam(queryParams[queryIndex]))
          }
      }
      return urlToDecorate + collectedQueryParams.join('&');
  }


  // a function that retrieves the value of a query parameter
  function getQueryParam(name) {
      if (name = (new RegExp('[?&]' + encodeURIComponent(name) + '=([^&]*)')).exec(window.location.search))
          return decodeURIComponent(name[1]);
  }

})();
</script>

",76383300,"In decorateUrl you are adding the ? if there is not one
urlToDecorate = (urlToDecorate.indexOf('?') === -1) ? urlToDecorate + '?' : urlToDecorate + '&';

I would suggest you may only want to do this if collectedQueryParams contains any elements
 function decorateUrl(urlToDecorate) {
      var collectedQueryParams = [];
      for (var queryIndex = 0; queryIndex < queryParams.length; queryIndex++) {
          if (getQueryParam(queryParams[queryIndex])) {
              collectedQueryParams.push(queryParams[queryIndex] + '=' + getQueryParam(queryParams[queryIndex]))
          }
      }

      if(collectedQueryParams.length == 0){
        return urlToDecorate;
      }

      //only add the ? if we have params AND if there isn't already one
      urlToDecorate = (urlToDecorate.indexOf('?') === -1) ? urlToDecorate + '?' : urlToDecorate + '&';
      return urlToDecorate + collectedQueryParams.join('&');
  }

",
76385033,How can I convert ISCII encoding to unicode for Gujarati language in Python 3?,"I have some Gujarati string but its in ISCII encoding, so python throughing error (SyntaxError: invalid decimal literal).
string = TFH[TZDF\ I]GF.8[0 G[Xg;

line 1
    string = TFH[TZDF\ I]GF.8[0 G[Xg;
                      ^
SyntaxError: unexpected character after line continuation character

I was tried byte encoding too, but its not giving output like ISCII encoding.
I am trying this to make ISCII into unicode for Gujarati language.
I have ISCII based font and character map data also.
ISCII input string: TFH[TZDF\ I]GF.8[0 G[Xg;
Desired unicode output: તાજેતરમાં યુનાઇટેડ નેશન્સ (Typed using gujarati phonetic keyboard)
",76385142,"If you just want to write the string literal, for me, just writing print(""તાજેતરમાં યુનાઇટેડ નેશન્સ"") worked.
Or you could write:
characters = [2724, 2750, 2716, 2759, 2724, 2736, 2734, 2750, 2690, 32, 2735, 2753, 2728, 2750, 2695, 2719, 2759, 2721, 32, 2728, 2759, 2742, 2728, 2765, 2744]
string = str()
for c in characters:
        string += chr(c)

Maybe you have a look at this conversion script:
https://gist.github.com/pathumego/81672787807c23f19518c622d9e7ebb8
",
76381570,Extracting vcf columns substring with awk,"I have vcf file like this:
##bcftools_annotateVersion=1.3.1+htslib-1.3.1
##bcftools_annotateCommand=annotate 
#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  HG005
chr1    817186  rs3094315   G   A   50  PASS    platforms=2;platformnames=Illumina,CG;datasets=3;datasetnames=HiSeq250x250,CGnormal,HiSeqMatePair;callsets=5;callsetnames=HiSeq250x250Sentieon,CGnormal,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;datasetsmissingcall=IonExome,SolidSE75bp;callable=CS_HiSeq250x250Sentieon_callable,CS_CGnormal_callable,CS_HiSeq250x250freebayes_callable;AN=2;AF=1;AC=2 GT:PS:DP:ADALL:AD:GQ    1/1:.:809:0,363:78,428:237
chr1    817341  rs3131972   A   G   50  PASS    platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeq250x250,CGnormal,HiSeqMatePair,SolidSE75bp;callsets=6;callsetnames=HiSeq250x250Sentieon,CGnormal,HiSeq250x250freebayes,HiSeqMatePairSentieon,HiSeqMatePairfreebayes,SolidSE75GATKHC;datasetsmissingcall=IonExome;callable=CS_HiSeq250x250Sentieon_callable,CS_CGnormal_callable,CS_HiSeq250x250freebayes_callable;AN=2;AF=1;AC=2   GT:PS:DP:ADALL:AD:GQ    1/1:.:732:1,330:99,391:302

I need to extract ID column and  AN from INFO column to get:
ID         INFO
rs3094315   2
rs3131972   2

I'm trying something like this awk '/^[^#]/ { print $3, gsub(/^[^AN=])/,"""",$8)}' file.vcf, but still not getting the desired result.
",76381846,"You can try this awk:
awk 'BEGIN{OFS=""\t""}
/^##/{next}
/^#/{print $3,$8; next}
{
    split($8,a,"";"")
    for(i=1;i<=length(a);i++) if (a[i]~/^AN=/) {sub(/^AN=/,"""",a[i]); break}
    printf ""%s%s%s\n"", $3, OFS, a[i]
}
' file

With the example, prints:
ID  INFO
rs3094315   2
rs3131972   2

",
76384930,Load AVAsset video in the background and replace playing placeholder video once it's playable in Swift and RealityKit,"I'm using the following code to create a video player for detected reference images in AR session. Currently I display a placeholder video and after 1 second switch to real video that I want played. However, I would like to show the placeholder video until the real video is ready to be played.
I tried experimenting with AVAsset and observing the playable status based on this: Knowing when AVPlayer object is ready to play - however I didn't have any success.
        func createVideoNode(_ target: ARReferenceImage) -> ModelEntity {
        var videoPlane = ModelEntity()
        var targetName: String = """"
        
        if let name = target.name,
           let validURL = URL(string: ""https://testdomain.com/\(name).mp4"") {
            targetName = name

            // Use the preloaded placeholder asset to create an AVPlayer
            if let placeholderAsset = parent.placeholderAsset {
                let placeholderPlayer = AVPlayer(playerItem: AVPlayerItem(asset: placeholderAsset))
                let videoMaterial = VideoMaterial(avPlayer: placeholderPlayer)
                videoPlane = ModelEntity(mesh: .generatePlane(width: Float(target.physicalSize.width), depth: Float(target.physicalSize.height)), materials: [videoMaterial])
                placeholderPlayer.play()

                DispatchQueue.global(qos: .background).async {
                    let videoPlayer = AVPlayer(url: validURL)
                    NotificationCenter.default.addObserver(forName: .AVPlayerItemDidPlayToEndTime, object: videoPlayer.currentItem, queue: .main) { [weak videoPlayer] _ in
                        videoPlayer?.seek(to: CMTime.zero)
                        videoPlayer?.play()
                    }
                    DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                        let videoMaterial = VideoMaterial(avPlayer: videoPlayer)
                        videoPlane.model?.materials = [videoMaterial]
                        videoPlayer.play()
                        self.parent.videoPlayers[targetName] = videoPlayer
                        print (target.name as Any)
                    }
                }
            } else {
                fatalError(""Failed to load placeholder video asset."")
            }
        }

        return videoPlane
    }

",76385186,"The key to resolving this issue is making sure the AVPlayer's item is actually ready to play before switching the video. You can use the Key-Value Observing (KVO) on the AVPlayerItem's status property to get notified when it's ready to play.
Here is the updated createVideoNode(_:) function:
func createVideoNode(_ target: ARReferenceImage) -> ModelEntity {
    var videoPlane = ModelEntity()
    var targetName: String = """"
    
    if let name = target.name,
       let validURL = URL(string: ""https://testdomain.com/\(name).mp4"") {
        targetName = name

        // Use the preloaded placeholder asset to create an AVPlayer
        if let placeholderAsset = parent.placeholderAsset {
            let placeholderPlayer = AVPlayer(playerItem: AVPlayerItem(asset: placeholderAsset))
            let videoMaterial = VideoMaterial(avPlayer: placeholderPlayer)
            videoPlane = ModelEntity(mesh: .generatePlane(width: Float(target.physicalSize.width), depth: Float(target.physicalSize.height)), materials: [videoMaterial])
            placeholderPlayer.play()

            DispatchQueue.global(qos: .background).async {
                let asset = AVAsset(url: validURL)
                let playerItem = AVPlayerItem(asset: asset)
                let videoPlayer = AVPlayer(playerItem: playerItem)

                // Observe the status of playerItem.
                playerItem.addObserver(self, forKeyPath: ""status"", options: .new, context: nil)

                NotificationCenter.default.addObserver(forName: .AVPlayerItemDidPlayToEndTime, object: videoPlayer.currentItem, queue: .main) { [weak videoPlayer] _ in
                    videoPlayer?.seek(to: CMTime.zero)
                    videoPlayer?.play()
                }

                self.parent.videoPlayers[targetName] = videoPlayer
            }
        } else {
            fatalError(""Failed to load placeholder video asset."")
        }
    }

    return videoPlane
}

// Add this method to handle observed value change
override func observeValue(forKeyPath keyPath: String?, of object: Any?, change: [NSKeyValueChangeKey : Any]?, context: UnsafeMutableRawPointer?) {
    if keyPath == ""status"" {
        if let playerItem = object as? AVPlayerItem, playerItem.status == .readyToPlay {
            DispatchQueue.main.async { [weak self] in
                if let videoPlane = self?.videoPlane {
                    let videoMaterial = VideoMaterial(avPlayer: playerItem.player)
                    videoPlane.model?.materials = [videoMaterial]
                    playerItem.player?.play()
                }
            }
        }
    }
}

This version of the function now creates an AVPlayerItem using the AVAsset. It then adds the ViewController as an observer of the playerItem's status property. The observeValue(forKeyPath:of:change:context:) method gets called when the status changes. When the status is .readyToPlay, it switches the video on the main queue.
Please note that the observeValue method is a standard method for classes that inherit from NSObject, make sure your class does that. Also remember to remove the observer when it's no longer needed.
You will also have to hold a strong reference to your AVPlayerItem and AVPlayer in order to observe changes. This might necessitate some architectural changes (adding properties to your class).
This solution should give you a general direction, but you might need to adjust it to fit your specific project setup and requirements.
",
76383308,ruby-openai api gem in Ruby on Rails: how to implement a streaming conversation?,"Openai provides an api which allows you to implement AI services such as ChaGPT or DAL-E.
For Ruby on Rails application, and there are couple of gems available, obe of them being ruby-openai.
It works very well, but the only problem is that it doesn't come with the stream conversation feature, meaning that you can only send one question request at a time without any history tracking of the conversation. In other words, the api forgets every question you asked after having sent the reply.
So how can we fix this?
",76383309,"Basically you need to implement the whole behaviour yourself. Here are all the implementation step, including the implementation of the dal-e ai with a response with several pictures rather then just one.
You can also find my whole repository HERE and clone the app!!!
IMPLEMENTING A STREAM CONVERSATION FEATURE
Basic implementation
Check out Doug Berkley's Notion Page for basic implementation of the API
Implement a streaming conversation
By default the openai gem does not come with that feature, hence having to implement it yourself

Create your database with 3 tables (conversations, questions, answers) with thw following sctructure:

# schema.rb
ActiveRecord::Schema[7.0].define(version: 2023_05_29_194913) do
  create_table ""answers"", force: :cascade do |t|
    t.text ""content""
    t.integer ""question_id"", null: false
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
    t.index [""question_id""], name: ""index_answers_on_question_id""
  end

  create_table ""conversations"", force: :cascade do |t|
    t.text ""initial_question""
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
    t.text ""historic""
  end

  create_table ""questions"", force: :cascade do |t|
    t.text ""content""
    t.integer ""conversation_id"", null: false
    t.datetime ""created_at"", null: false
    t.datetime ""updated_at"", null: false
    t.index [""conversation_id""], name: ""index_questions_on_conversation_id""
  end

  add_foreign_key ""answers"", ""questions""
  add_foreign_key ""questions"", ""conversations""
end


Routes

Rails.application.routes.draw do
  root ""pages#home"" # supposes that you have a pages controller with a home action
  resources :conversations, only: [:create, :show]
  post ""question"", to: ""conversations#ask_question""
end


Home page view (with just a button that redirects to the create conversation action -- see bellow)

<h1>Let's talk</h1>
<%= button_to ""Create New Conversation"", conversations_path, method: :post, class: ""btn btn-primary my-3"" %>


Controller app/controllers/conversations_controller.rb

class ConversationsController < ApplicationController
  def create
    @convo = Conversation.create
    redirect_to conversation_path(@convo)
  end

  def show
    @convo = Conversation.find(params[:id])
  end

  def ask_question
    @question = Question.new(content: params[:entry])
    conversation = Conversation.find(params[:conversation])
    @question.conversation = conversation
    @question.save
    if conversation.historic.nil?
      response = OpenaiService.new(params[:entry]).call 
      conversation.historic = ""#{@question.content}\n#{response}""
    else
      response = OpenaiService.new(""#{conversation.historic}\n#{params[:entry]}"").call
      conversation.historic += ""\n#{@question.content}\n#{response}""
    end
    conversation.save
    @answer = Answer.create(content: response, question: @question)
    redirect_to conversation_path(conversation)
  end
end


Show page app/views/conversations/show.html.erb

<h1>This is your conversation</h1>
<p>Ask your question</p>
<form action=""<%= question_path %>"", method=""post"">
  <input type=""hidden"" name=""conversation"" value=""<%= @convo.id %>"">
  <textarea rows=""5"" cols=""33"" name=""entry""></textarea>
  <input type=""submit"" class=""btn btn-primary"">
</form>

<br>

<ul>
  <% @convo.questions.each do |question| %>
    <li>
      Q: <%= question.content.capitalize %> <%= ""?"" if question.content.strip.last != ""?"" %>
    </li>
    <li>
      A: <%= question.answers.first.content %>
    </li>
  <% end %>
</ul>

<%= link_to ""Back"", root_path %>



rails s and test :)

Resources:

https://github.com/OGsoundFX/ruby-open-ai
https://doug-berkley.notion.site/doug-berkley/Rails-ChatGPT-Service-Object-Setup-21748fc969514b978bf6345f897b6d3e
https://github.com/alexrudall/ruby-openai

Going Further:

https://gist.github.com/alexrudall/cb5ee1e109353ef358adb4e66631799d

",
76381726,xUnit IClassFixture reinitialized for every testcase,"I want to write integration tests with shared context (shared state) for all testcases.
From docs:

When using a class fixture, xUnit.net will ensure that the fixture instance will be created before any of the tests have run, and once all the tests have finished, it will clean up the fixture object by calling Dispose, if present.

It follows from docs that I need to use IClassFixture. Ok then.
I create sample ASP .NET Core Web API with controllers and in Program.cs add the only line:
public partial class Program { }

Nothing else is changed in a project.
Then I add xUnit test project where I add reference for my web api project and modify the default UnitTest1 class with the following code:
public class UnitTest1 : IClassFixture<WebApplicationFactory<Program>>
{
    private readonly HttpClient _client;
    private string? _val;
    public UnitTest1(WebApplicationFactory<Program> factory)
    {
        _client = factory.CreateClient();
    }

    [Fact]
    public void Test1()
    {
        Assert.Null(_val);
        _val = ""smth"";
    }

    [Fact]
    public void Test2()
    {
        Assert.NotNull(_val);
    }
}

So basically I want to set ""shared context"" (which is a string variable in this case) in Test1 and use it in Test2. I run testcases and I see that Test1 passes and Test2 fails.
I have seen xUnit IClassFixture constructor being called multiple times and tried using test explorer window or even switch to Rider but that did not help. Did someone encounter such a behavior?
",76381855,"This is working correctly, but you have implemented it wrong. xUnit runtime will create a new instance of UnitTest1 for every test execution, but it should only create a single instance of WebApplicationFactory<Program> for the lifetime of the current test batch execution context for this test class.

Your _val variable is not defined as part of the test fixture at all, so that makes sense that the value is not persisted across the different tests.

Because you are passing the factory, and not the instance, you will experience multiple calls to factory.CreateClient(); and this is expected. In this scenario you wouldn't normally use a factory as the test fixture, but your test fixture could use the factory method internally:
/// <summary>Fixture to share across many tests in the same context</summary>
public class MyTestFixture : IDisposable
{
    public HttpClient Client { get; private set; }
    public string? Value { get; set; }
    
    public MyTestFixture(WebApplicationFactory<Program> factory)
    {
        Client = factory.CreateClient();
    }

    public void Dispose()
    {
        // clean up any unmanaged references
    }
}

* if you are not using DI for your factory, then you should instantiate the factory directly in the constructor instead of expecting it as an argument.
public class UnitTest1 : IClassFixture<MyTestFixture>
{
    private readonly MyTestFixture _sharedContext;

    public UnitTest1(MyTestFixture testFixture)
    {
        _sharedContext = testFixture;
    }

    [Fact]
    public void Test1()
    {
        Assert.Null(_sharedContext.Value);
        _sharedContext.Value = ""smth"";
    }

    [Fact]
    public void Test2()
    {
        Assert.NotNull(_sharedContext.Value);
    }
}

",
76381615,ggplot2: Date range bar graph,"I am trying to build a date range bar graph usins ggplot2 (R) in the spirit of:

I have followed a thread but I am completely unable to reproduce the results with dates.
If I understood it correctly, for each ""id"", the bar length is determined by the smallest and largest ""value"" in the database.
Here is a minimally working example of my data:
# Example dataframe
DF <- data.frame(Name = as.factor(c(""1.Project1"", ""2.Project2"", ""3.Project3"", ""4.Project4"")),
                 CreationTime = as.POSIXct(c(""2019-12-10 13:22:20"", ""2019-12-17 12:25:48"", ""2020-01-02 13:02:57"", ""2020-01-14 08:37:10"")),
                 LastActivity = as.POSIXct(c(""2019-12-17 10:42:17 "", ""2020-01-02 13:27:10"", ""2021-02-11 11:32:45"", ""2023-05-03 07:41:38"")),
                 Status = as.factor(c(""Prod"", ""Prod"", ""Dev"", ""Complete"")))

# From wide to long
DFGather <- DF %>% tidyr::gather(key=""Time"", value=""Value"", 2:3)

# Generate plot
ggplot2::ggplot(DFGather, aes(x = Value, y = Name, fill = Status)) +
  ggplot2::geom_col() +
  ggplot2::coord_cartesian(xlim = c(min(DFGather$Value),max(DFGather$Value))) +
  ggplot2::scale_x_datetime(date_breaks = ""3 months"", labels = scales::label_date_short())

I have also tried converting POSIXct dates to integers but it didn't change my output:
DFGather$Value <- as.integer(format(DFGather$Value,""%Y%m%d""))

Thanks for the support,
C.
",76381868,"A quick and dirty approach using geom_segment.

ggplot2::ggplot(DF, ggplot2::aes(x = CreationTime, xend = LastActivity, y = Name, yend = Name, colour = Status)) +
  ggplot2::geom_segment(linewidth = 15) +
  ggplot2::coord_cartesian(xlim = c(min(DFGather$Value),max(DFGather$Value))) +
  ggplot2::scale_x_datetime(date_breaks = ""3 months"", labels = scales::label_date_short())


Created on 2023-06-01 with reprex v2.0.2
",
76382858,Spark is unable to handle a particular date format,"I am trying to cast multiple date formats from string type field using Pyspark. When I am using below date format it is working fine.
def custom_to_date(col):
    formats = (""MM/dd/yyyy"", ""yyyy-MM-dd"", ""dd/MM/yyyy"", ""MM/yy"",""dd/M/yyyy"")
    return coalesce(*[to_date(col, f) for f in formats])
    
from pyspark.sql.functions import coalesce, to_date
df = spark.createDataFrame([(1, ""01/22/2010""), (2, ""2018-12-01"")], (""id"", ""dt""))
df.withColumn(""pdt"", custom_to_date(""dt"")).show()

Above code gives the correct output.
But when I use the month in single digit as below, the code fails.
df = spark.createDataFrame([(1, ""01/22/2010""), (2, ""2018-12-1""),(3,""24/7/2006"")], (""id"", ""dt""))

I got the below error message.
org.apache.spark.SparkException:
  Job aborted due to stage failure:
    Task 2 in stage 2.0 failed 4 times, most recent failure: 
      Lost task 2.3 in stage 2.0 (TID 10) (10.13.82.55 executor 0): 
        org.apache.spark.SparkUpgradeException: 
        [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] 
        You may get a different result due to the upgrading to Spark >= 3.0:

",76383322,"Adding an answer since the comments and others answer doesn't cover the behaviour. The solution is not to add new formats. Since the formats itself can be better defined.
with spark 3.0 M supports 01, 1. January, Jan.
So you don't need MM
spark reference - https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
def custom_to_date(col):
    formats = (""M/d/yyyy"", ""yyyy-M-d"", ""d/M/y"", ""M/y"")
    return coalesce(*[to_date(col, f) for f in formats])
    
from pyspark.sql.functions import coalesce, to_date
df = spark.createDataFrame([(1, ""01/22/2010""), (2, ""2018-12-1""),(3,""12/2023"")], (""id"", ""dt""))
df.withColumn(""pdt"", custom_to_date(""dt"")).show()

Results -

Alternatively, if you want legacy behavior then you can use
spark.conf.set(""spark.sql.legacy.timeParserPolicy"",""LEGACY"")
or
spark.sql(""set spark.sql.legacy.timeParserPolicy=LEGACY"")
",
76384850,How to draw LineRenderer above other objects?,"My line renderer is drawing behind objects. I want it to draw on top of other game objects except for the ball.
How can I do this?
See the following image to reference the problem (the line renderer draws below the goal, and I want it to draw itself on top.

I searched for the issue but haven't found a single answer for 3D.
",76385187,"To render a material ""above"" some other materials, you must set your LineRenderer or TrailRenderer's material Rendering mode to Transparent.
Also, set the Rendering Mode of materials of objects you wish to draw LineRenderer on top to Transparent.
Now go back to the LineRenderer's material and in Advanced Options set its Render Queue to 3999. (higher than the object's materials)
Now your LineRenderer will be drawn on top.

",
76381796,How to get not duplicate rows in join?,"I got some problems with duplicate rows which I don't wanna get.
Hi!
I got two tables - tab1, tab2 and I want to join tab2 to tab1 like:
SELECT t1.column_A1, t2.column_B2 
FROM tab1 t1
JOIN
tab2 t2
ON t1.column_A1=t2.column_A2 

tab1
| Column A1 | Column B1 | Column C1 |
|  -------- |  -------- |  -------- |
|  Z1       |  Cell 2   |  Cell 3   |
|  Z2       |  Cell 5   |  Cell 6   |

tab2
| Column A2 | Column B2 | Column C2 |
|  -------- |  -------- |  -------- |
|  Z1       |  PW       |  Cell 3   |
|  Z1       |  RW       |  Cell 6   |

For some rows in tab1 there are more than 1 rows in tab2.
The result will be:
| Column A2 | Column B2 | Column C2 |
|  -------- |  -------- |  -------- |
|  Z1       |  PW       |  RE       |
|  Z1       |  RW       |  KS       |

I want to get:
if PW - show only one row with PW;
if not PW - show only one row with RW
The result should be:
| Column A2 | Column B2 | Column C2 |
|  -------- |  -------- |  -------- |
|  Z1       |  PW       |  RE       |

",76381874,"One option is to ""sort"" rows per each column_a1 by value stored in column_b2 and return rows that rank as the highest.
Sample data:
SQL> WITH
  2     tab1 (column_a1, column_b1, column_c1)
  3     AS
  4        (SELECT 'Z1', 'cell 2', 'cell 3' FROM DUAL
  5         UNION ALL
  6         SELECT 'Z2', 'cell 5', 'cell 6' FROM DUAL),
  7     tab2 (column_a2, column_b2, column_c2)
  8     AS
  9        (SELECT 'Z1', 'PW', 'cell 3' FROM DUAL
 10         UNION ALL
 11         SELECT 'Z1', 'RW', 'cell 6' FROM DUAL
 12         UNION ALL
 13         SELECT 'Z2', 'RW', 'cell 8' FROM DUAL),

Query begins here:
 14     temp
 15     AS
 16        (SELECT t1.column_A1,
 17                t2.column_B2,
 18                ROW_NUMBER () OVER (PARTITION BY t1.column_a1 ORDER BY t2.column_b2) rn
 19           FROM tab1 t1 JOIN tab2 t2 ON t1.column_A1 = t2.column_A2)
 20  SELECT column_a1, column_b2
 21    FROM temp
 22   WHERE rn = 1;

COLUMN_A1    COLUMN_B2
------------ ------------
Z1           PW
Z2           RW

SQL>

",
76378880,"Pick error ""Type 'Pick' cannot be used as an index type.""","Trying to modify my axios wrapper and can't figure out why I'm getting this typescript error...
type Headers = {
  Accept: string;
  'Content-Type': string;
  Authorization?: string;
}

export interface AxiosOptions {
  params?: any;
  data?: any;
  headers: Headers;
}

const axiosOptions: AxiosOptions = {
  headers: {
    Accept: 'application/json',
    'Content-Type': 'application/json',
  },
};

export const updateAxiosOptionsHeaders = (
    key: Pick<Headers, ""Accept"" | ""Content-Type"" | ""Authorization"">,
    value: string
  ): void => {
  axiosOptions.headers[key] = value;
}



",76378926,"This doesn't really meet the usage pattern of Pick. You just need keyof Headers for your case:
type Headers = {
  Accept: string;
  'Content-Type': string;
  Authorization?: string;
}

export interface AxiosOptions {
  params?: any;
  data?: any;
  headers: Headers;
}

const axiosOptions: AxiosOptions = {
  headers: {
    Accept: 'application/json',
    'Content-Type': 'application/json',
  },
};

export const updateAxiosOptionsHeaders = (
    key: keyof Headers,
    value: string
  ): void => {
  axiosOptions.headers[key] = value;
}

Playground link

Should you also have different value types in your object (which seems unlikely for a headers object), you can use a generic constraint:
export const updateAxiosOptionsHeaders = <K extends keyof Headers>(
    key: K,
    value: Headers[K]
  ): void => {
  axiosOptions.headers[key] = value;
}

",
76378929,x86 rep prefix with a count of zero: what happens?,"What happens for an initial count of zero for an x86 rep prefix?
Intel's manual says explicitly it’s a while count != 0 loop with the test at the top, which is the sane expected behaviour.
But most of the many vague reports I’ve seen elsewhere suggest that there’s no initial test for zero so it would be like a countdown with a test at the end and so disaster if it’s repeat {… count —=1; } until count == 0; or who knows.
",76378999,"Nothing happens with RCX=0; rep prefixes do check for zero first like the pseudocode says.  (Unlike the loop instruction which is exactly like the bottom of a do{}while(--ecx), or a dec rcx/jnz but without affecting FLAGS.)
I think I've heard of this rarely being used as an idiom for a conditional load or store with rep lodsw or rep stosw with a count of 0 or 1, especially in the bad old days before cmov.  (cmov is an unconditional load feeding an ALU select operation, so it needs a valid address, unlike rep lods with a count of zero.)  This is not efficient especially for rep stos on modern x86 with Fast Strings microcode (P6 and later), especially without anything like Fast Short Rep-Movs (Ice Lake IIRC.)
The same applies for instructions that treat the prefixes as repz / repnz (cmps/scas) instead of unconditional rep (lods/stos/movs).  Doing zero iterations means they leave FLAGS umodified.
If you want to check FLAGS after a repe/ne cmps/scas, you need to make sure the count was non-zero, or that FLAGS was already set such that you'll branch in a useful way for zero-length buffers.  (Perhaps from xor-zeroing a register that you're going to want later.)

rep movs and rep stos have fast-strings microcode on CPUs since P6, but the startup overhead makes them rarely worth it, especially when sizes can be short and/or data might be misaligned.  They're more useful in kernel code where you can't freely use XMM registers.  Some recent CPUs like Ice Lake have fast-short-rep microcode that I think is supposed to reduce startup overhead for small counts.
repe/ne scas/cmps do not have fast-strings microcode on most CPUs, only on very recent CPUs like Sapphire Rapids and maybe Alder Lake P-cores.  So they're quite slow, like one load per clock cycle (so 2 cycles per count for cmpsb/w/d/q) according to testing by https://agner.org/optimize/ and https://uops.info/.

What setup does REP do?
Why is this code using strlen heavily 6.5x slower with GCC optimizations enabled? - GCC -O1 used to use repne scasb to inline strlen.  This is a disaster for long strings.
Which processors support ""Fast Short REP CMPSB and SCASB"" (very recent feature)
Enhanced REP MOVSB for memcpy - even without ERMSB, rep movs will use no-RFO stores for large sizes, similar to NT stores but not bypassing the cache.  Good general Q&A about memory bandwidth considerations.

",
76383242,Dictionary Comprehension within pandas dataframe column,"Trying to match a dictionary item with a string value from another column.
sample data:
df =     A    B
     0  'a'  {'a': '2', 'b': '5'}
     1  'c'  {'a': '2', 'b': '16', 'c': '32'}
     2  'a'  {'a': '6', 'd': '23'} 
     3  'd'  {'b': '4', 'd': '76'}
     

I'm trying to get the following out:
Df =     A    B
     0   'a'  {'a': '2'}
     1   'c'  {'c': '32'}
     2   'a'  {'a': '6'}
     3   'd'  {'d': '76'}

I got this far not inside a dataframe:
d = {k: v for k, v in my_dict.items() if k == 'a'}

for a single line, but I couldn't get this to work and to be fair, I didn't expect it to work directly, but was hoping i was close:
Test_df['B'] = {k: v for k, v in test_df['B'].items() if k == test_df['A']}

I get the following error:
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

What do I need to do to get this to work, or is there a better more efficient way?
",76383335,"You can use a list comprehension with zip:
df['B'] = [{x: d[x]} for x, d in zip(df['A'], df['B'])]

Output:
   A            B
0  a   {'a': '2'}
1  c  {'c': '32'}
2  a   {'a': '6'}
3  d  {'d': '76'}

",
76384672,Terraform for_each over yaml file contents which is an object,"I have a yaml file which is similar to the following (FYI: ssm_secrets can be an empty array):
rabbitmq:
  repo_name: bitnami
  namespace: rabbitmq
  target_revision: 11.1.1
  path: rabbitmq
  values_file: charts/rabbitmq/values.yaml
  ssm_secrets: []
app_name_1:
  repo_name: repo_name_1
  namespace: namespace_1
  target_revision: target_revision_1
  path: charts/path
  values_file: values.yaml
  ssm_secrets:
    - name: name-dev-1
      key: .env
      ssm_path: ssm_path/dev
name-backend:
  repo_name: repo_name_2
  namespace: namespace_2
  target_revision: target_revision_2
  path: charts/name-backend
  values_file: values.yaml
  ssm_secrets:
    - name: name-backend-app-dev
      ssm_path: name-backend/app/dev
      key: app.ini
    - name: name-backend-abi-dev
      ssm_path: name-backend/abi/dev
      key: contractTokenABI.json
    - name: name-backend-widget-dev
      ssm_path: name-backend/widget/dev
      key: name.ini
    - name: name-abi-dev
      ssm_path: name-abi/dev
      key: name_1.json
    - name: name-website-dev
      ssm_path: name/website/dev
      key: website.ini
    - name: name-name-dev
      ssm_path: name/name/dev
      key: contract.ini
    - name: name-key-dev
      ssm_path: name-key/dev
      key: name.pub

And using External Secrets and EKS Blueprints, I am trying to generate the yaml file necessary to create the secrets
resource ""kubectl_manifest"" ""secret"" {
  for_each   = toset(flatten([for service in var.secrets : service.ssm_secrets[*].ssm_path]))
  yaml_body  = <<YAML
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: ${replace(each.value, ""/"", ""-"")}
  namespace: ${split(""/"", each.value)[0]}
spec:
  refreshInterval: 30m
  secretStoreRef:
    name: ${local.cluster_secretstore_name}
    kind: ClusterSecretStore
  data:
  - secretKey: .env
    remoteRef:
       key: ${each.value}
YAML
  depends_on = [kubectl_manifest.cluster_secretstore, kubernetes_namespace_v1.namespaces]
}

The above works fine, but I also need to use the key value from the yaml into secretKey: <key_value from yaml>.
If I try with for_each   = toset(flatten([for service in var.secrets : service.ssm_secrets[*]]))
resource ""kubectl_manifest"" ""secret"" {
  for_each   = toset(flatten([for service in var.secrets : service.ssm_secrets[*]]))
  yaml_body  = <<YAML
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: ${replace(each.value[""ssm_path""], ""/"", ""-"")}
  namespace: ${split(""/"", each.value[""ssm_path""])[0]}
spec:
  refreshInterval: 30m
  secretStoreRef:
    name: ${local.cluster_secretstore_name}
    kind: ClusterSecretStore
  data:
  - secretKey: .env
    remoteRef:
       key: ${each.value[""ssm_path""]}
YAML
  depends_on = [kubectl_manifest.cluster_secretstore, kubernetes_namespace_v1.namespaces]
}

It just gives me the following error:

The given ""for_each"" argument value is unsuitable: ""for_each"" supports
maps and sets of strings, but you have provided a set containing type
object.

I have tried converting the variable into a map, used lookup, but it doesn't work.
Any help would be much appreciated.
Update 1:
As per @MattSchuchard suggestion, changing the for_each into
for_each   = toset(flatten([for service in var.secrets : service.ssm_secrets]))
Gave the following error:
Error: Invalid for_each set argument
│ 
│   on ../../modules/02-plugins/external-secrets.tf line 58, in resource ""kubectl_manifest"" ""secret"":
│   58:   for_each   = toset(flatten([for service in var.secrets : service.ssm_secrets]))
│     ├────────────────
│     │ var.secrets is object with 14 attributes
│ 
│ The given ""for_each"" argument value is unsuitable: ""for_each"" supports maps and sets of strings, but you have provided a set containing type object.

Update 2:
@mariux gave the perfect solution, but here is what I came up with. It's not that cleaner, but definitely works (PS: I myself am going to use Mariux's solution):
locals {
  my_list = tolist(flatten([for service in var.secrets : service.ssm_secrets[*]]))
}


resource ""kubectl_manifest"" ""secret"" {

  count      = length(local.my_list)
  yaml_body  = <<YAML
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: ${replace(local.my_list[count.index][""ssm_path""], ""/"", ""-"")}
  namespace: ${split(""/"", local.my_list[count.index][""ssm_path""])[0]}
spec:
  refreshInterval: 30m
  secretStoreRef:
    name: ${local.cluster_secretstore_name}
    kind: ClusterSecretStore
  data:
  - secretKey: ${local.my_list[count.index][""key""]}
    remoteRef:
       key: ${local.my_list[count.index][""ssm_path""]}
YAML
  depends_on = [kubectl_manifest.cluster_secretstore, kubernetes_namespace_v1.namespaces]
}

",76385193,"Assumptions
Based on what you shared, i make the following assumptions:

the service is not actually important for you as you want to create external secrets by ssm_secrets.*.name using the given key and ssm_path attributes.
each name is globally unique for all services and never reused.

terraform hacks
Based on the assumptions you can create an array of ALL ssm_secrets using
locals {
  ssm_secrets_all = flatten(values(var.secrets)[*].ssm_secrets)
}

and convert it to a map that can be used in for_each by keying the values by .name:
locals {
  ssm_secrets_map = { for v in local.ssm_secrets_all : v.name => v }
}

Full (working) example
The example below works for me and makes some assumption where the variables should be used.

Using yamldecode to decode your original input into local.input
Using yamlencode to make reading the manifest easier and removing some string interpolcations. This also ensures that the indent is correct as we convert HCL to yaml.

A terraform init && terraform plan will plan to create the following resources:
 kubectl_manifest.secret[""name-abi-dev""] will be created
 kubectl_manifest.secret[""name-backend-abi-dev""] will be created
 kubectl_manifest.secret[""name-backend-app-dev""] will be created
 kubectl_manifest.secret[""name-backend-widget-dev""] will be created
 kubectl_manifest.secret[""name-dev-1""] will be created
 kubectl_manifest.secret[""name-key-dev""] will be created
 kubectl_manifest.secret[""name-name-dev""] will be created
 kubectl_manifest.secret[""name-website-dev""] will be created

locals {
  # input = var.secrets
  ssm_secrets_all = flatten(values(local.input)[*].ssm_secrets)
  ssm_secrets_map = { for v in local.ssm_secrets_all : v.name => v }

  cluster_secretstore_name = ""not provided secretstore name""
}

resource ""kubectl_manifest"" ""secret"" {
  for_each = local.ssm_secrets_map

  yaml_body = yamlencode({
    apiVersion = ""external-secrets.io/v1beta1""
    kind       = ""ExternalSecret""
    metadata = {
      name      = replace(each.value.ssm_path, ""/"", ""-"")
      namespace = split(""/"", each.value.ssm_path)[0]
    }
    spec = {
      refreshInterval = ""30m""
      secretStoreRef = {
        name = local.cluster_secretstore_name
        kind = ""ClusterSecretStore""
      }
      data = [
        {
          secretKey = "".env""
          remoteRef = {
            key = each.value.key
          }
        }
      ]
    }
  })

  # not included dependencies
  # depends_on = [kubectl_manifest.cluster_secretstore, kubernetes_namespace_v1.namespaces]
}

locals {
  input = yamldecode(<<-EOF
    rabbitmq:
      repo_name: bitnami
      namespace: rabbitmq
      target_revision: 11.1.1
      path: rabbitmq
      values_file: charts/rabbitmq/values.yaml
      ssm_secrets: []
    app_name_1:
      repo_name: repo_name_1
      namespace: namespace_1
      target_revision: target_revision_1
      path: charts/path
      values_file: values.yaml
      ssm_secrets:
        - name: name-dev-1
          key: .env
          ssm_path: ssm_path/dev
    name-backend:
      repo_name: repo_name_2
      namespace: namespace_2
      target_revision: target_revision_2
      path: charts/name-backend
      values_file: values.yaml
      ssm_secrets:
        - name: name-backend-app-dev
          ssm_path: name-backend/app/dev
          key: app.ini
        - name: name-backend-abi-dev
          ssm_path: name-backend/abi/dev
          key: contractTokenABI.json
        - name: name-backend-widget-dev
          ssm_path: name-backend/widget/dev
          key: name.ini
        - name: name-abi-dev
          ssm_path: name-abi/dev
          key: name_1.json
        - name: name-website-dev
          ssm_path: name/website/dev
          key: website.ini
        - name: name-name-dev
          ssm_path: name/name/dev
          key: contract.ini
        - name: name-key-dev
          ssm_path: name-key/dev
          key: name.pub
    EOF
  )
}

terraform {
  required_version = ""~> 1.0""

  required_providers {
    kubectl = {
      source  = ""gavinbunney/kubectl""
      version = ""~> 1.7""
    }
  }
}

hint: you could also try to use the kubernetes_manifest resource instead of kubectl_manifest
p.s.: We created Terramate to make complex creation of Terraform code easier. But this seems perfectly fine for pure Terraform.
",
76380957,How can I assert that the jest mocked module method was called?,"How can I assert that the jest mocked module method was called?
E.g. in my .spec.js I have the following jest mocked module:
jest.mock('../../../../services/logs.service.js', () => ({
    log: jest.fn()
}));

Now I would like to assert the log method. I.e. something like this:
//the log was called twice with the text ""foo""
expect(log).toHaveBeenCalledWith(2, ""foo"");

But I can not access the log. I tried putting the log initialization outside the jest mocked module, like so:
const log = jest.fn();
jest.mock('../../../../services/logs.service.js', () => ({
    log
}));

But I got the error:

The module factory of jest.mock() is not allowed to reference any out-of-scope variables.

",76381877,"You can do the following:
JavaScript
import { log } from '../../../../services/logs.service.js';

jest.mock('../../../../services/logs.service.js', () => ({
    log: jest.fn()
}));

expect(log).toHaveBeenCalledWith(2, ""foo""); // JavaScript

TypeScript
import { log } from '../../../../services/logs.service.js';

jest.mock('../../../../services/logs.service.js', () => ({
    log: jest.fn()
}));

const mockedLog = log as jest.MockedFunction<typeof log>;

expect(mockedLog).toHaveBeenCalledWith(2, ""foo"");

",
76382640,Ionic app error: No 'Access-Control-Allow-Origin' header is present on the requested resource,"I have a mobile app developed in ionic capacitor. The backend to the app is a .net core web api deployed on amazon elastic beanstalk. I am getting CORS error** No 'Access-Control-Allow-Origin' header is present on the requested resource** when trying to access the back end using the app.
I have attempted to allow the API to be accessible by any consumer by allowing all origins. Is there need to configure anything on AWS Elastic bean?
var app = builder.Build();
app.UseCors(builder => builder
     .AllowAnyOrigin()
     .AllowAnyMethod()
     .AllowAnyHeader()
     );

",76383361,"
Log in to the AWS Management Console and navigate to the Elastic Beanstalk service.
Select your application and environment where the .NET Core Web API is deployed.
In the navigation pane, click on ""Configuration.""
Under the ""Software"" section, click on ""Edit"" for the ""Environment properties.""
Add a new property with the following details:

Name: ACCESS_CONTROL_ALLOW_ORIGIN
Value: * (or the specific origin you want to allow if you don't want to allow all origins)


Save the changes and wait for the environment to update.

Make sure to remove the CORS configuration you mentioned from your .NET Core Web API code, as the CORS handling will now be done by Elastic Beanstalk.
",
76384683,Draw random numbers from a custom probability density function in Julia,"I would like to draw random numbers from a modified exponential distribution:
p(x) = C * a * Exp[-(a*x)^b] with C=1/Gamma[1 + 1/b] for normalization.
How can I do this in julia? Unfortunately I have only little experience with Julia and no experiences with creating custom random numbers. I would be very grateful for any help.
",76385208,"If I'm not mistaken, that is a p-Generalized Gaussian distribution, which has a rather efficient implementation in Distributions.jl:
using Distributions
mu = 0       # your location parameter
alpha = 1/a  # your scale parameter
beta = b     # your shape parameter
p = PGeneralizedGaussian(mu, alpha, beta)

Using the Distributions.jl API for univariate distributions, you can sample from this distribution by passing it to the exported rand method. Here is an example of how to sample five independent scalars from a PGeneralizedGaussian distribution with mu = 0, alpha = 1/2 and beta = 3:
julia> p = PGeneralizedGaussian(0, 1/2, 3);

julia> rand(p, 5)
5-element Vector{Float64}:
  0.2835117212764108
 -0.023160728370422268
  0.3075395764050027
 -0.19233721955795835
  0.21256694763885342

If you want to try to implement the distribution yourself, which I do not recommend unless you are doing this as an exercise in programming math in Julia, you need to define a type that holds the static parameters of the distribution (in your case, just the shape parameter and the scale parameter), then define and export the methods listed here to extend the Distributions.jl API to your custom distribution. In particular, you need to define:
struct MyDistribution <: ContinuousUnivariateDistribution
  # ... your distribution parameters here
end

rand(::AbstractRNG, d::MyDistribution) # sample a value from d
sampler(d::MyDistribution)             # return d or something that can sample from d more efficiently
logpdf(d::MyDistribution, x::Real)     # compute the log of the pdf of d at x
cdf(d::MyDistribution, x::Real)        # compute the cdf of d at x
quantile(d::MyDistribution, q::Real)   # compute the qth quantile of d
minimum(d::MyDistribution)             # return the minimum value supported by d
maximum(d::MyDistribution)             # return the maximum value supported by d
insupport(d::MyDistribution, x::Real)  # query whether x is supported by d

The documentation of the package is very good, so it's an excellent way to get your feet wet if you are trying to learn Julia.
",
76385164,Border doesn't adapt after collapsing a column,"For my main project, I'm trying to find a way to hide a column in JS. The following function :
function hide() {
    const table = document.getElementById('test');
    const cols = table.getElementsByTagName('col');
    cols[1].style.visibility = ""collapse"";
}

works great, but the borders don't move. Here's the problem :
 becomes 
How can I avoid this issue ?
EDIT : This works as intended on Chrome and Edge. Is this a bug for Firefox?
Full html is:


function hide() {
    const table = document.getElementById('test');
    const cols = table.getElementsByTagName('col');
    cols[1].style.visibility = ""collapse"";
}
table, tr, th, td {
    border: 1px solid;
    border-collapse: collapse;
}
<table id=""test"">
    <colgroup>
        <col><col><col>
    </colgroup>
    <tr>
        <th>1</th>
        <th>2</th>
        <th>3</th>
    </tr>
    <tr>
        <td>un</td>
        <td>deux</td>
        <td>trois</td>
    </tr>
    <tr>
        <td>one</td>
        <td>two</td>
        <td>three</td>
    </tr>
</table>

<button onclick=hide()>Hide</button>



",76385211,"To address this issue, you can use the border-spacing property instead of border-collapse. Modify your CSS as follows:
<style>
table {
    border-spacing: 0;
}

th, td {
    border: 1px solid;
    padding: 5px;
}
</style>

",
76381785,TypeError: data type '>' not understood using dtype from numpy,"first I saw similar questions but nothing helped me.
I'm trying to sort list of tuples, and convert the data types inside the tuple,
convert it according to a list of tuples I get.
for example, if I have a list of tuple, every tuple is built like
(ID,Grade,Height)

A = [(123,23,67),(234,67,45)]

and I have a list of type like that:
[(ID,int),(grade,'s15'),(height,float)]
now I read that 's15' is a dtype from bumpy, but I can't seem to use it.
I tried to copy from the docs:
import numpy as np
dt = np.dtype(('>14'))

but all I get is this error:
dt = np.dtype(('>14'))
TypeError: data type '>' not understood

the docs I copied from:
https://numpy.org/doc/stable/reference/arrays.dtypes.html
and is there a generic converter I can use to convert to any type I'm given?
",76381889,"I think you maybe overlooked the documentation you are referring.
You used
dt = np.dtype(('>14'))

which is >14 (fourteen)...
But in fact the documentation clearly mentions
dt = np.dtype('>i4')

which is i4 not 1 (one)
Also based on the docs > or < specifies upper/lower bound for each dtype, for example >i would be big endian integer (see Endianess)
And the number after that would indicate number of bytes given to the dtype (see docs)
Finally the S indicates Zero terminated bytes
Based on your description, your teacher wants Upper endian ~128 bit Zero terminated bytes
Furthermore,
dt = np.dtype(('>S15'))

works fine.
I hope this fixes your issue
",
76381436,Is there ST_Intersects alternative that allows two(or more) polygons to share sides,"I am using ST_Intersects to check if two polygons intersect. Relevant part of my query is:
SELECT entity_number
FROM coordinates
WHERE ST_INTERSECTS($1, location)


It works well to determine if one polygon crosses the other's surface:

I expected ST_Intersects to return false when two polygons share sides, but it does not:

I read about other methods like ST_Covers, ST_Contains, ST_ContainsProperly, ST_Within ,ST_DWithin. But i am not sure which one suits my needs.
Is there any method that allows two polygons to share sides?
",76381929,"You want ST_Overlaps:

Returns TRUE if geometry A and B ""spatially overlap"". Two geometries overlap if they have the same dimension, each has at least one point not shared by the other (or equivalently neither covers the other), and the intersection of their interiors has the same dimension. The overlaps relationship is symmetrical.

",
76385016,React Router - How can I reuse my layout for the errorElement in the root route?,"I'm using React Router v6 and following are my routes:
const router = createBrowserRouter([
  {
    path: '/',
    element: <App />,
    errorElement: <ErrorPage />,
    children: [
      {
        index: true,
        element: <HomePage />,
      },
      {
        path: '/sign-up',
        element: <SignUpPage />,
      },
      {
        path: '/log-in',
        element: <LogInPage />,
      },
    ],
  },
]);

const root = ReactDOM.createRoot(
  document.getElementById('root') as HTMLElement,
);

root.render(
  <React.StrictMode>
    <RouterProvider router={router} />
  </React.StrictMode>,
);

The App component contains my app's layout and outputs the route elements using the Outlet component. But now if there's an error that bubbles up to the root route, then the ErrorPage gets displayed as expected, but it doesn't make use of the layout from App... So, how can I reuse my layout from App when the error page gets displayed?
",76385223,"When there's an error it is kind of an either or kind of scenario. Either conditions are fine and the App component is rendered or there's an error condition and the ErrorPage component is rendered.
What you could do is to abstract the layout portion of the App component into a layout component on its own that can render either a passed children prop or the Outlet component for the nested route, and render it in App and also wrap the ErrorPage component.
Example:
const AppLayout = ({ children }) => (
  ...
  {children ?? <Outlet />}
  ...
);

const App = () => (
  ...
  <AppLayout />
  ...
);

const router = createBrowserRouter([
  {
    path: ""/"",
    element: <App />, // <-- uses Outlet
    errorElement: (
      <AppLayout>     // <-- uses children
        <ErrorPage />
      </AppLayout>
    ),
    children: [
      {
        index: true,
        element: <HomePage />
      },
      {
        path: ""/sign-up"",
        element: <SignUpPage />
      },
      {
        path: ""/log-in"",
        element: <LogInPage />
      }
    ]
  }
]);


",
76381457,How to fix a container at a particular spot on the screen,"I'm learning flutter and I have made an app that looks like this:

I'm facing a problem as to how to fix the container fixed on a particular spot on the screen like it has to be aligned to the top center. Here's the problem I'm facing:

Here's the code:
class Program7 extends StatefulWidget {
  const Program7({super.key});

  @override
  State<Program7> createState() => _Program7State();
}

class _Program7State extends State<Program7> {
  double cHeightAndWidth = 300;

  
  @override
  Widget build(BuildContext context) {
    return SafeArea(
      child: Column(
        mainAxisAlignment: MainAxisAlignment.spaceAround,
        children: [
          Container(
            height: cHeightAndWidth,
            width: cHeightAndWidth,
            decoration: BoxDecoration(
              color: Colors.purple,
            ),
          ),
          Column(
            children: [
              //A bunch of rows of buttons,
            ],
          ),
        ],
      ),
    );
  }
}


P.S.: I already tried to fix the container to the top center of another container using align but the purple color somehow bleeds out into the bigger container.
",76381967,"The issue is using MainAxisAlignment.spaceAround,. It will use the free space and put half before and another half at end of the child.
You can use fixed gap for top(Container).
return SafeArea(
  child: Column(
    children: [
      SizedBox(height: 50),
      Container(
        height: cHeightAndWidth,
        width: cHeightAndWidth,
        decoration: BoxDecoration(
          color: Colors.purple,
        ),
      ),
      Spacer(), // or other widget
      Column(
        children: [
          //A bunch of rows of buttons,
        ],
      ),
    ],
  ),
);

",
76383196,Find element by class under the same parent,"Any particular reason about this isn't matching the element with that class?
I have checked a million times and can't see what is that I'm doing wrong.


$('.lnk-folder').click(function(e) {
  e.preventDefault();
  var header = $(this).parent('thead').find('.folder-header');
  console.log($(header));
});
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<thead>
  <tr>
    <th colspan=""10"" style=""padding:0px; font-size:120%;background-color:#ff0000;"">
      <div style=""float:left;min-width:20%;"">
        <a style=""color: #000"" class=""lnk-folder folder-close"" data-hash="""" href=""#"">
          <i class='fa fa-folder-open'></i> Folder 1
        </a>
      </div>
      <div style=""float:left;height:26px;padding-left:10px;"">
        <a href=""#""><i class='fas fa-file-upload tooltip' style=""color:#fff;""><span class=""tooltiptext_m"">New</span></i></a>
      </div>
    </th>
  </tr>
  <tr class=""folder-header"">
    <th colspan=""2"" style='background-color:#0c343d;vertical-align:middle;'> Name </th>
    <th style='width:7%;background-color:#0c343d;vertical-align:middle;'> Code </th>
    <th style='width:30%;background-color:#0c343d;vertical-align:middle;'> Act</th>
    <th style='width:7%;background-color:#0c343d;vertical-align:middle;'> Version</th>
  </tr>
</thead>



",76383405,"parent() is your problem. It looks up the DOM exactly one level, to the parent element, but you need to go higher than that. To do so, use closest()


$('.lnk-folder').click(function(e) {
  e.preventDefault();
  var header = $(this).closest('thead').find('.folder-header');
  console.log($(header));
});
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<table>
  <thead>
    <tr>
      <th colspan=""10"" style=""padding:0px; font-size:120%;background-color:#ff0000;"">
        <div style=""float:left;min-width:20%;"">
          <a style=""color: #000"" class=""lnk-folder folder-close"" data-hash="""" href=""#"">
            <i class='fa fa-folder-open'></i> Folder 1
          </a>
        </div>
        <div style=""float:left;height:26px;padding-left:10px;"">
          <a href=""#""><i class='fas fa-file-upload tooltip' style=""color:#fff;""><span class=""tooltiptext_m"">New</span></i></a>
        </div>
      </th>
    </tr>
    <tr class=""folder-header"">
      <th colspan=""2"" style='background-color:#0c343d;vertical-align:middle;'> Name </th>
      <th style='width:7%;background-color:#0c343d;vertical-align:middle;'> Code </th>
      <th style='width:30%;background-color:#0c343d;vertical-align:middle;'> Act</th>
      <th style='width:7%;background-color:#0c343d;vertical-align:middle;'> Version</th>
    </tr>
  </thead>
</table>



",
76384931,How to replace a spark dataframe row with another spark dataframe's row using java,"I have 2 data frames
df1
+--------------------+---+--------------------+--------------------+
|                 ID |B  |C                   |            D       |
+--------------------+---+--------------------+--------------------+
|                   1|1.0|                 1.0|                 1.0|
|                   2|2.0|                 2.0|                 2.0|
|                   3|3.0|                 3.0|                 3.0|
|                   4|4.0|                 4.0|                 4.0|
+--------------------+---+--------------------+--------------------+

df2
+--------------------+---+--------------------+--------------------+
|                 ID |B  |C                   |            D       |
+--------------------+---+--------------------+--------------------+
|                   1|100|                 1.0|                 100|
+--------------------+---+--------------------+--------------------+

If ID in df2 matches an ID in df1, I want to replace the row in df1 with the updated values in df2. So the new df1 looks like:
df1
+--------------------+---+--------------------+--------------------+
|                 ID |B  |C                   |            D       |
+--------------------+---+--------------------+--------------------+
|                   1|100|                 1.0|                 100|
|                   2|2.0|                 2.0|                 2.0|
|                   3|3.0|                 3.0|                 3.0|
|                   4|4.0|                 4.0|                 4.0|
+--------------------+---+--------------------+--------------------+

I've been trying to figure this out with union and join and just not having any luck yet.  I first created a new dataframe based on filtering for the ID of df1 and that works and I called that dataframe matchedDF that looks like:
matchedDF (dataframe based on finding a match of ID 1 in df1)
+--------------------+---+--------------------+--------------------+
|                 ID |B  |C                   |            D       |
+--------------------+---+--------------------+--------------------+
|                   1|1.0|                 1.0|                 1.0|
+--------------------+---+--------------------+--------------------+

But I don't know if I just want to delete the original ID 1 in df1 and add the new matchedDF or do I somehow want to update the original ID 1 with the matchedDf?  Or am I approaching this all wrong?
Thanks
",76385224,"To stay computationally efficient, it's always a good idea to avoid joins/shuffles where possible.
This looks like a case where it is possible to avoid joining, have a look at the following code (it is in Scala, but the principles remain the same):
// Constructing the 2 dfs
val df = Seq(
  (1, 1.0, 1.0, 1.0),
  (2, 2.0, 2.0, 2.0),
  (3, 3.0, 3.0, 3.0),
  (4, 4.0, 4.0, 4.0)
).toDF(""ID"", ""B"", ""C"", ""D"")

val df2 = Seq(
  (1, 100, 1.0, 100),
  (2, 100, 1.0, 100)
).toDF(""ID"", ""B"", ""C"", ""D"")

// Collecting the IDs to be updated into a single Array
// IMPORTANT: we make the assumption that this array is not large (in your
// example there is only 1 row here, so the array only has 1 element which is
// totally fine)
val newIds = df2.select(""ID"").collect.map(_.getInt(0))

// Removing the original rows with the unwanted IDs and unioning the result with
// the new rows
val output = df
  .filter(not(col(""ID"").isin(newIds: _*)))
  .union(df2)

scala> output.show
+---+-----+---+-----+                                                           
| ID|    B|  C|    D|
+---+-----+---+-----+
|  3|  3.0|3.0|  3.0|
|  4|  4.0|4.0|  4.0|
|  1|100.0|1.0|100.0|
|  2|100.0|1.0|100.0|
+---+-----+---+-----+

So basically, if we can make the assumption that df2 (with the new values) is small like in your example, you can do something like the following:

collect the ID values into a single (undistributed) Array. From your example it seems like this is OK. If the amount of new rows is really large this might not be the best approach
filter the original df using the isin method of a column and negating using not (basically removing the rows with the new IDs)
union the filtered df and df2, resulting in the rows being updated WITHOUT any expensive operation like a shuffle

",
76381864,Make a loop to find the distance between stations in R,"I have made this code to find the distance between stations but in the output, there is only one value. Can you find the error?
df <- data.frame(
  station = rep(c(""A"", ""B"", ""C"", ""D""), each = 20),
  temperature = rnorm(80),
  latitude = c(40.7128, 34.0522, 41.8781, 39.9526),
  longitude = c(-74.0060, -118.2437, -87.6298, -75.1652)
)
stations <- unique(df$station)

my_points <- matrix(NA, nrow = length(unique(df$station)), ncol = length(unique(df$station)))

# Loop through each station combination
for (i in 1:length(stations)) {
  for (j in 1:length(stations)) {
    # Get temperatures for the two stations
    lat1 <- df$latitude[df$station == stations[i]]
    lon1 <- df$longitude[df$station == stations[i]]
    lat2 <- df$latitude[df$station == stations[j]]
    lon2 <- df$longitude[df$station == stations[j]]
    my_points[i, j] <- as.vector(dist(matrix(c(lon1,lon2,lat1,lat2),
                                             nrow = 2)))  
    
  }
}

distance_df <- as.data.frame(my_points)

",76382001,"There are two issues here:

Your input data frame might not look the way you expect it to - the latitude and longitude columns are recycled so you have multiple different coordinates for the same station. Try adding rep() in the lat and long columns as well as station.

In your code lat1 <- df$latitude[df$station == stations[i]] returns a vector, because there are multiple matches. I think you're expecting a single value. Use only the first matching element (since they are now all the same elements in the vector after adding rep() as above):


df <- data.frame(
  station = rep(c(""A"", ""B"", ""C"", ""D""), each = 20),
  temperature = rnorm(80),
  latitude = rep(c(40.7128, 34.0522, 41.8781, 39.9526), each = 20),
  longitude = rep(c(-74.0060, -118.2437, -87.6298, -75.1652), each = 20)
)
stations <- unique(df$station)

my_points <- matrix(NA, nrow = length(unique(df$station)), ncol = length(unique(df$station)))

# Loop through each station combination
for (i in 1:length(stations)) {
  for (j in 1:length(stations)) {
    # Get temperatures for the two stations
    lat1 <- df$latitude[df$station == stations[i]][1]
    lon1 <- df$longitude[df$station == stations[i]][1]
    lat2 <- df$latitude[df$station == stations[j]][1]
    lon2 <- df$longitude[df$station == stations[j]][1]
    my_points[i, j] <- as.vector(dist(matrix(c(lon1,lon2,lat1,lat2),
                                             nrow = 2)))  
    
  }
}

distance_df <- as.data.frame(my_points)


This gives:
         V1       V2       V3        V4
1  0.000000 44.73631 13.67355  1.386235
2 44.736313  0.00000 31.59835 43.480707
3 13.673546 31.59835  0.00000 12.612446
4  1.386235 43.48071 12.61245  0.000000

A slightly better way of finding unique stations:
unique(df[, c(""station"", ""latitude"", ""longitude"")])

You can then loop over those instead:
# Loop through each station combination
for (i in 1:length(stations)) {
  for (j in 1:length(stations)) {
    # Get temperatures for the two stations
    lat1 <- unique_df$latitude[unique_df$station == stations[i]]
    lon1 <- unique_df$longitude[unique_df$station == stations[i]]
    lat2 <- unique_df$latitude[unique_df$station == stations[j]]
    lon2 <- unique_df$longitude[unique_df$station == stations[j]]
    my_points[i, j] <- as.vector(dist(matrix(c(lon1,lon2,lat1,lat2),
                                             nrow = 2)))  
    
  }
}


",
76381890,save multiple pdf pages with different sizes in Shiny R,"Im developing a shiny app with several features. I added a button to download a single pdf file that contains many plots. I want to save those plots in individual pages but I want to choose the size of each pdf page. Is that possible?
This is he code that  have so far:
output$exportall<-downloadHandler(
filename=""Allplots.pdf"",
content=function(file){
  withProgress(message = 'Exporting', min=0,max=1, { 
    pdf(file,width=8,height=11)
    print(plot1())
    print(histogram())
    print(plots2())
    print(marrangeGrob(woodsbytimepoint(), nrow=2, ncol=1))
    print(digestion())
    print(map())
    print(marrangeGrob(allplots(), nrow=4, ncol=2, top=NULL))
    
  dev.off()
  })
 }
)

The code works fine and exports all the plots that I want. However, all pages in the pdf file are 8x11. Is there a way to speciffy the size of each page? for example I want the first plot to be 7x7 and all other 8x11.
Any ideas?
",76382014,"Perhaps the simplest is to create separate PDFs (sized appropriately) and combine them with qpdf::pdf_combine.
file <- ""file.pdf""
pdf(paste0(file, "".8x11""), width=8, height=11)
plot(disp ~ mpg, data = mtcars)
gg <- ggplot(mtcars, aes(disp, mpg)) + geom_point()
print(gg)
dev.off()
pdf(paste0(file, "".7x7""), width=7, height=7)
print(gg) # or anything else
dev.off()
qpdf::pdf_combine(paste0(file, c("".8x11"", "".7x7"")), file)
file.remove(paste0(file, c("".8x11"", "".7x7"")))

The resulting file.pdf pages:

If your sizes are not always in order (e.g., 8x11, 7x7, 8x11), you can either:

create three PDF files (would need an adjusted file name convention) and concatenate in order, or
create two PDF files (by dimensions), then also use qpdf::pdf_subset ... though since this creates new PDF files that you would then need to include in pdf_combine, it hardly seems the most efficient method.

I cannot test this, but I think this means your code should be
output$exportall<-downloadHandler(
filename=""Allplots.pdf"",
content=function(file){
  withProgress(message = 'Exporting', min=0,max=1, { 
    pdf(paste0(file, "".7x7""), width=7, height=7)
    print(plot1())
    dev.off()
    pdf(paste0(file, "".8x11""), width=8, height=11)
    print(histogram())
    print(plots2())
    print(marrangeGrob(woodsbytimepoint(), nrow=2, ncol=1))
    print(digestion())
    print(map())
    print(marrangeGrob(allplots(), nrow=4, ncol=2, top=NULL))
    dev.off()
    qpdf::pdf_combine(paste0(file, c("".7x7"", "".8x11"")), output=file)
  })
 }
)

",
76385216,Why can I only print the text of a text file once?,"I have written a little class which reads a text file and which have a method for printing the text (file.output()). For the first call it worked, but the second call of the method nothing is happening. I do not understand why, since I assume that the FOR-Loop does not change anything.
class Datei():
    def __init__(self, filename):
        self.fileobject = open(filename)
        
    def output(self): 
        for line in self.fileobject: 
            print(line.rstrip())
        
    
    def end(self): 
        self.fileobject.close()

file = Datei(""yellow_snow.txt"")
file.output()
print(""second try"")
file.output()

file.end()

I expected the text of the text file to be printed twice, but it is only printed once.
",76385238,"When you read a file, you move a pointer through it, and it's now at the end - you can .seek(0) to get back to the start (or other positions, 0 is where you started from, which is the beginning if you're not in append mode)
with open(path) as fh:
    print(fh.tell())  # start of file
    print(fh.read())  # get everything and display it
    print(fh.tell())  # end of file
    fh.seek(0)        # go back to the beginning
    print(fh.tell())  # start of file
    print(fh.read())

More detail in Python Documentation 7.2.1. Methods of File Objects
",
76381693,How to sort array elements based on the closest occurrence of a sublist with a numeric value of 1.0? And then combine this sorted matrix with another,"How do I correct my code to be able to order its elements according to which has the canonical vector with a value equal to 1.0 in the element closest to the beginning of its sublists (ignoring the first sublist, which is the one with the titles, although this will also change position according to the position of element 1.0 in the other remaining ones), thus remaining?
matrix = [['B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], [8, 2.0, 1.0, -1.0, 0, 0, 1.0, 0], [2, 1.0, 1.0, 0, 1.0, 0, 0, 0], [8, 1.0, 2.0, 0, 0, -1.0, 0, 1.0]]

matrix_aux = [['X4', 'U1', 'U2'], [0, 1.0, 0], [1.0, 0, 0], [0, 0, 1.0]]

#Extract the first title sublist
titles = matrix_aux.pop(0)

#Create a list of tuples, and sort it
tuple_list = [(sublist.index(1.0), sublist) for sublist in matrix_aux]
sorted_tuples = sorted(tuple_list, key=lambda x: x[0])

#Rebuild the sorted array
matrix_aux_ord = [[titles[i] for i in range(len(titles))]] + [sublist for _, sublist in sorted_tuples]

print(matrix_aux_ord)
for row in matrix_aux_ord: print(row) #print in matrix format

the problem now with my code is that it forgets to sort the row of titles or headers ['X4', 'U1', 'U2'], incorrectly printing this matrix
[['X4', 'U1', 'U2'], [1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]

instead of this that if it maintains consistency
[['U1', 'X4', 'U2'], [1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]

Then, having these 2 matrices, build the new matrix called new_matrix, in which I would add a column in front of the matrix, that is, I would add an element to each of the sublists that make up the rows of matrix, to the first sublist of the matrix called matrix add an 'X' before it as the first element, and to the rest of the sublists of the matrix called matrix add as the first element in an ordered manner the elements of the first sublist of the matrix called matrix_aux_ord
matrix = [['B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], [8, 2.0, 1.0, -1.0, 0, 0, 1.0, 0], [2, 1.0, 1.0, 0, 1.0, 0, 0, 0], [8, 1.0, 2.0, 0, 0, -1.0, 0, 1.0]]

#If the previous code worked, it would get this array sorted like this...
matrix_aux_ord = [['U1', 'X4', 'U2'], [1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]

#Add the column title or header 'X' to the front of the first sublist of matrix
matrix[0].insert(0, 'X')

So the resulting final matrix, the correct output, called as new_matrix, would look like this:
new_matrix = [['X', 'B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], ['U1', 8, 2.0, 1.0, -1.0, 0, 0, 1.0, 0], ['X4', 2, 1.0, 1.0, 0, 1.0, 0, 0, 0], ['U2', 8, 1.0, 2.0, 0, 0, -1.0, 0, 1.0]]

What should I do to get the matrix_aux_ord correctly, and with it to be able to get the matrix new_matrix which basically consists of a way to combine amber matrices, matrix and matrix_aux_ord?
",76382030,"You need to sort the titles together with the other lists. You can do it with zip
matrix = [['B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], [8, 2.0, 1.0, -1.0, 0, 0, 1.0, 0], [2, 1.0, 1.0, 0, 1.0, 0, 0, 0], [8, 1.0, 2.0, 0, 0, -1.0, 0, 1.0]]
matrix_aux = [['X4', 'U1', 'U2'], [0, 1.0, 0], [1.0, 0, 0], [0, 0, 1.0]]

matrix_aux_ord = list(zip(*sorted(zip(*matrix_aux), key=lambda x: x[1:].index(1.0))))
print(matrix_aux_ord)

# [('U1', 'X4', 'U2'), (1.0, 0, 0), (0, 1.0, 0), (0, 0, 1.0)]

And to add the titles to matrix use list comprehensions
titles = ['X'] + list(matrix_aux_ord[0])
new_matrix = [[titles[i]] + matrix[i] for i in range(len(titles))]
print(new_matrix)

# [['X', 'B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], ['U1', 8, 2.0, 1.0, -1.0, 0, 0, 1.0, 0], ['X4', 2, 1.0, 1.0, 0, 1.0, 0, 0, 0], ['U2', 8, 1.0, 2.0, 0, 0, -1.0, 0, 1.0]]

",
76383130,Custom Reverse Scroll view in SwiftUI,"I am building a chat window. We are currently in the migration phase from Objective-C to SwiftUI and we do support a minimum of iOS 13+.
To get behaviors of scroll view where I want to point to the bottom always as default and should be able to scroll up and down seamlessly.
Here only problem is here scroll only works when i drag from bubble of chat from other places it doesn't works.
I have debug quite long and not able to find the issue.
Reverse scroll view code which I got from here https://www.process-one.net/blog/writing-a-custom-scroll-view-with-swiftui-in-a-chat-application/
struct ReverseScrollView<Content>: View where Content: View {
    @State private var contentHeight: CGFloat = CGFloat.zero
    @State private var scrollOffset: CGFloat = CGFloat.zero
    @State private var currentOffset: CGFloat = CGFloat.zero
    
    var content: () -> Content
    
    // Calculate content offset
    func offset(outerheight: CGFloat, innerheight: CGFloat) -> CGFloat {        
        let totalOffset = currentOffset + scrollOffset
        return -((innerheight/2 - outerheight/2) - totalOffset)
    }
    
    var body: some View {
        GeometryReader { outerGeometry in
            // Render the content
            //  ... and set its sizing inside the parent
            self.content()
            .modifier(ViewHeightKey())
            .onPreferenceChange(ViewHeightKey.self) { self.contentHeight = $0 }
            .frame(height: outerGeometry.size.height)
            .offset(y: self.offset(outerheight: outerGeometry.size.height, innerheight: self.contentHeight))
            .clipped()
            .animation(.easeInOut)
            .gesture(
                 DragGesture()
                    .onChanged({ self.onDragChanged($0) })
                    .onEnded({ self.onDragEnded($0, outerHeight: outerGeometry.size.height)}))
        }
    }
    
    func onDragChanged(_ value: DragGesture.Value) {
        // Update rendered offset

        self.scrollOffset = (value.location.y - value.startLocation.y)
    }
    
    func onDragEnded(_ value: DragGesture.Value, outerHeight: CGFloat) {
        // Update view to target position based on drag position
        let scrollOffset = value.location.y - value.startLocation.y
        
        let topLimit = self.contentHeight - outerHeight
        
        // Negative topLimit => Content is smaller than screen size. We reset the scroll position on drag end:
        if topLimit < 0 {
             self.currentOffset = 0
        } else {
            // We cannot pass bottom limit (negative scroll)
            if self.currentOffset + scrollOffset < 0 {
                self.currentOffset = 0
            } else if self.currentOffset + scrollOffset > topLimit {
                self.currentOffset = topLimit
            } else {
                self.currentOffset += scrollOffset
            }
        }
        self.scrollOffset = 0
    }
}

struct ViewHeightKey: PreferenceKey {
    static var defaultValue: CGFloat { 0 }
    static func reduce(value: inout Value, nextValue: () -> Value) {
        value = value + nextValue()
    }
}

extension ViewHeightKey: ViewModifier {
    func body(content: Content) -> some View {
        return content.background(GeometryReader { proxy in
            Color.clear.preference(key: Self.self, value: proxy.size.height)
        })
    }
}

Chat window
ReverseScrollView {

    VStack{
        
        HStack {
            VStack(spacing: 5){
                Text(""message.text"")
                    .padding(.vertical, 8)
                    .padding(.horizontal)
                    .background(Color(.systemGray5))
                    .foregroundColor(.primary)
                    .clipShape(ChatBubble(isFromCurrentUser: false))
                    .frame(maxWidth: .infinity, alignment: .leading)
                    .padding(.horizontal)
                    .lineLimit(nil) // Allow unlimited lines
                    .lineSpacing(4) // Adjust line spacing as desired
                    .fixedSize(horizontal: false, vertical: true) // Allow vertical expansion

                
                Text(""ormatTime(message.timeUtc)"")
                    .font(.caption)
                    .foregroundColor(.secondary)
                    .background(Color.red)
                    .frame(maxWidth: .infinity, alignment: .leading)
                    .padding(.horizontal, 5)


            }
            .background(Color.blue)

            
            Spacer()
        }

        
        ForEach(Array(viewModel.chats.indices), id: \.self){ index in
            let message = viewModel.chats[index]
            VStack(alignment: .leading, spacing: 5) {
                // Chat bubble view for received messages
                
                if(message.isIncoming){
                    HStack {
                        VStack(spacing: 5){
                            Text(message.text)
                                .padding(.vertical, 8)
                                .padding(.horizontal)
                                .background(Color(.systemGray5))
                                .foregroundColor(.primary)
                                .clipShape(ChatBubble(isFromCurrentUser: false))
                                .frame(maxWidth: .infinity, alignment: .leading)
                                .padding(.horizontal)
                                .lineLimit(nil) // Allow unlimited lines
                                .lineSpacing(4) // Adjust line spacing as desired
                                .fixedSize(horizontal: false, vertical: true) // Allow vertical expansion
                                .frame(maxWidth: .infinity, alignment: .leading)

                            
                            Text(formatTime(message.timeUtc))
                                .font(.caption)
                                .foregroundColor(.secondary)
                                .frame(maxWidth: .infinity, alignment: .leading)
                                .padding(.horizontal, 5)
                        }

                        
                        Spacer()
                    }
                }else{
                    
                
                    HStack {
                        Spacer()
                        
                        VStack(spacing: 5){
                            Text(message.text)
                                .padding(.vertical, 8)
                                    .padding(.horizontal)
                                    .background(Color(.systemBlue))
                                    .foregroundColor(.white)
                                    .clipShape(ChatBubble(isFromCurrentUser: true))
                                    .padding(.horizontal)
                                    .lineLimit(nil) // Allow unlimited lines
                                    .lineSpacing(4) // Adjust line spacing as desired
                                    .fixedSize(horizontal: false, vertical: true) // Allow vertical expansion
                            
                            Text(formatTime(message.timeUtc))
                                .font(.caption)
                                .foregroundColor(.secondary)
                                .frame(maxWidth: .infinity, alignment: .leading)
                                .padding(.horizontal, 5)
                        }
                        .frame(maxWidth: .infinity, alignment: .trailing)

                        
                    }
                
                }
            }
          
        
        }
        if(viewModel.messageSending) {
            VStack(spacing: 5){
                HStack {
                    Spacer()
                    Text(sendingText)
                        .padding(.vertical, 8)
                        .padding(.horizontal)
                        .background(Color(.systemBlue))
                        .foregroundColor(.white)
                        .clipShape(ChatBubble(isFromCurrentUser: true))
                        .padding(.horizontal)
                }
                HStack {
                    Spacer()
                    ChatBubbleAnimationView()
                        .padding(.trailing, 8)
                }
            }
            .padding(.bottom, 20)
            .onDisappear(){
                sendingText = """"
                messageText = """"
            }
        }
    }
}

Chat bubble wrapper
struct ChatBubble: Shape {
    var isFromCurrentUser: Bool
    
    func path(in rect: CGRect) -> Path {
        let path = UIBezierPath(roundedRect: rect, byRoundingCorners: isFromCurrentUser ? [.topLeft, .bottomLeft, .bottomRight] : [.topRight, .bottomLeft, .bottomRight], cornerRadii: CGSize(width: 12, height: 12))
        
        return Path(path.cgPath)
    }
}

Please let me know something other information need. I am looking for suggestions to get the behaviours keeping in mind it should support iOS 13+ or any help to get above code fixed.
",76383421,"One option is to just flip the built-in ScrollView upside down.
import SwiftUI

struct ReverseScroll: View {
    var body: some View {
        ScrollView{
            ForEach(ChatMessage.samples) { message in
                HStack {
                    if message.isCurrent {
                        Spacer()
                    }
                    Text(message.message)
                        .padding()
                        .background {
                            RoundedRectangle(cornerRadius: 10)
                                .fill(message.isCurrent ? Color.blue : Color.gray)
                        }
                    if !message.isCurrent {
                        Spacer()
                    }
                }
            }.rotationEffect(.degrees(180)) //Flip View upside down oldest above newest below.
        }.rotationEffect(.degrees(180)) //Reverse so it works like a chat message
    }
}

struct ReverseScroll_Previews: PreviewProvider {
    static var previews: some View {
        ReverseScroll()
    }
}

struct ChatMessage: Identifiable, Equatable{
    let id: UUID = .init()
    var message: String
    var isCurrent: Bool
    
    static let samples: [ChatMessage] = (0...25).map { n in
            .init(message: n.description + UUID().uuidString, isCurrent: Bool.random())
    }
}

The scroll indicators show on the left with this but can be hidden in iOS 16+ with
.scrollIndicators(.hidden)

If you decide to support iOS 14+ you can use ScrollViewReader to scroll to the newest message.
struct ReverseScroll: View {
    @State private var messages = ChatMessage.samples
    var body: some View {
        VStack{
            ScrollViewReader { proxy in
                ScrollView{
                    ForEach(messages) { message in
                        HStack {
                            if message.isCurrent {
                                Spacer()
                            }
                            Text(message.message)
                                .padding()
                                .background {
                                    RoundedRectangle(cornerRadius: 10)
                                        .fill(message.isCurrent ? Color.blue : Color.gray)
                                }
                            if !message.isCurrent {
                                Spacer()
                            }
                        }
                        .id(message.id) //Set the ID
                        
                    }.rotationEffect(.degrees(180))
                }.rotationEffect(.degrees(180))
                    .onChange(of: messages.count) { newValue in
                        proxy.scrollTo(messages.last?.id) //When the count changes scroll to latest message
                    }
            }
            Button(""add"") {
                messages.append( ChatMessage(message: Date().description, isCurrent: Bool.random()))
                
            }
        }
    }
}

",
76384846,The first timer in a react component list is getting value NaN,"I have a React website where I have 2 toggles for different kinds of cards - one of them is Live markets (this type has a timer component).

Here is the problem- When I switch to classifieds and I switch back to live markets -
Auction timer for the first card becomes NaN. Note: this only happens to the first card, the other timers are fine.

I have a CardsLayout component, which send a request to the server for data when the above toggle is changed. And if it is in Live Markets tab, then the CardsLayout component maps each object to an AuctionCard component which has a Timer component inside it.
Here is the code for the Timer component-
import { useState, useEffect } from 'react';

export default function Timer({ id, endTime}) {
  const [remainingTime, setRemainingTime] = useState(getRemainingTime());

  useEffect(() => {
    const interval = setInterval(() => {
      setRemainingTime(getRemainingTime());
    }, 1000);

    return () => clearInterval(interval);
  }, []);

  function getRemainingTime() {
    const now = new Date();
    const end = new Date(endTime);
    const diff = end.getTime() - now.getTime();
    const days = Math.floor(diff / (1000 * 60 * 60 * 24));
    const hours = Math.floor((diff / (1000 * 60 * 60)) % 24);
    const minutes = Math.floor((diff / (1000 * 60)) % 60);
    const seconds = Math.floor((diff / 1000) % 60);
    return { days, hours, minutes, seconds };
  }

  console.log('endtime-',id,endTime)
  console.log('remtime-',id,remainingTime.seconds)

  return (
    <div className={remainingTime.seconds < 0 ? 'timer-ended' : 'timer'}>
      {remainingTime.seconds < 0 ? (
        ""Auction over""
      ) : (
        <>
          Auction time remaining: {remainingTime.days} Days {remainingTime.hours}:
          {remainingTime.minutes}:{remainingTime.seconds}
        </>
      )}
    </div>
  );
}

I also have 2 console statements. The values are getting printed every second. Here is whats getting printed for the Timer with NaN-
endtime- 4 2023-06-09T20:30:00.000Z
remtime- 4 NaN
",76385239,"Since it works after you add id and endTime as dependencies to the useEffect (as mentioned in the comments of the OP), it seems that the issue is that the first render you do of the first Time is done without/or with a wrong endTime so it end up displaying NaN.
Subsequent renders, I assume after fetching the data from somewhere, provide a valid endTime value for that property.
Initially the change in that prop would not alter the functionality of the ongoing interval, since the getRemainingTime would refer to the initial value of endTime.
There are a few solutions to this problem.

Do not render the Timer component until after you have valid data to provide to it. This need to be handled at the component using the Timer and not inside it.
Provide a key for the Timer component when using it (<Timer key={/*what ever you use for id will most likely work here too*/} id={..} endTime={..} /> that is unique and would change once you get the data from the remote location.
Use correct dependencies for the useEffect.

For 3. the more correct approach is not to add id and endTime to the useEffect but to use useCallback, with endTime for the getRemainingTime and then use that as dependency for the useEffect.
const getRemainingTime = useCallback(function() {
  const now = new Date();
  const end = new Date(endTime);
  const diff = end.getTime() - now.getTime();
  const days = Math.floor(diff / (1000 * 60 * 60 * 24));
  const hours = Math.floor((diff / (1000 * 60 * 60)) % 24);
  const minutes = Math.floor((diff / (1000 * 60)) % 60);
  const seconds = Math.floor((diff / 1000) % 60);
  return { days, hours, minutes, seconds };
}, [endTime]);

useEffect(() => {
  const interval = setInterval(() => {
    setRemainingTime(getRemainingTime());
  }, 1000);

  return () => clearInterval(interval);
}, [getRemainingTime]);

",
76381953,Reverse the content order of several columns (ideally in tidyverse),"Assuming the following data:
df <- data.frame(a = 1:3, b = c(1, 2, 6), c = c(4, 6, NA), d = c(6, NA, NA))

  a b  c  d
1 1 1  4  6
2 2 2  6 NA
3 3 6 NA NA

And what I want is:
  a b  c  d
1 1 6  4  1
2 2 6  2 NA
3 3 6 NA NA

I thought about some combination of across and rev, but my current attempts don't work.
",76382049,"You can do the following:
pivot_longer(df, -a) %>%
  filter(!is.na(value)) %>%
  mutate(value=rev(value), .by=a) %>%
  pivot_wider(names_from = name, values_from = value)

Output:
      a     b     c     d
  <int> <dbl> <dbl> <dbl>
1     1     6     4     1
2     2     6     2    NA
3     3     6    NA    NA

",
76380645,Azure Diagnostic Logs saved to another location,"I'm looking to create using Bicep, diagnostic settings on a firewall in one location and save to an Event Hub in another location. The two vnets are peered, but I am wondering if it is possibe based on this error message:
Resource '/subscriptions/123/resourceGroups/ukw-rg/providers/Microsoft.Network/azureFirewalls/ukw-fw' is in region 'ukwest' and resource '/subscriptions/123/resourcegroups/uks-rg/providers/microsoft.eventhub/namespaces/uks-evhns' is in region 'uksouth'

",76382058,"You are correct. It isn't possible. https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings?tabs=portal#destination-limitations
""The event hub namespace needs to be in the same region as the resource being monitored if the resource is regional.""
kind regards
Alistair
",
76381970,How do I get my embed tag to display videos instead of downloading them?,"My embed tag keeps downloading the video instead of displaying it.
I have tried changing the file type of the tag, but it just downloads it in a different format. I want the tag to display the video. Here's my code below.


<embed type=""video/mp4"" src=""videos/ymcaHome.mp4"" width=""400"" height=""300"">



",76382106,"You can do so by specifying the video url or the path as the src attribute value. Like this:


<embed src=""your_video_file_url.mp4"" type=""video/mp4"" with=""640"" height=""360"">



",
76382470,What causes the different NaN behavior when compiling `_mm_ucomilt_ss` intrinsic?,"Can someone explain me why the following code fails for GCC 8.5 with NaNs?
bool isfinite_sse42(float num)
{    
    return _mm_ucomilt_ss(_mm_set_ss(std::abs(num)),
                          _mm_set_ss(std::numeric_limits<float>::infinity())) == 1;
}

My expectation for GCC 8.5 would be to return false.
The Intel Intrinsics guide for _mm_ucomilt_ss says
RETURN ( a[31:0] != NaN AND b[31:0] != NaN AND a[31:0] == b[31:0] ) ? 1 : 0

i.e., if either a or b is NaN it returns 0. On assembly level (Godbolt) one can see a ucomiss abs(x), Infinity followed by a setb.
# GCC8.5 -O2  doesn't match documented intrinsic behaviour for NaN
        ucomiss xmm0, DWORD PTR .LC2[rip]
        setb    al

Interestingly newer GCCs and Clang swap the comparison from a < b to b > a and therefore use seta. But why does the code with setb returns true for NaN and why seta returns false for NaN?
",76383422,"GCC is buggy before GCC13, not implementing the documented semantics of the intrinsic for the NaN case which require either checking PF separately, or doing it as ucomiss Inf, abs so the unordered case sets CF the same way as abs < Inf.
See https://www.felixcloutier.com/x86/ucomiss#operation or the nicer table in https://www.felixcloutier.com/x86/fcomi:fcomip:fucomi:fucomip .  (All x86 scalar FP compares that set EFLAGS do it the same way, matching historical fcom / fstsw / sahf.)




Comparison Results
ZF
PF
CF




left > right
0
0
0


left < right
0
0
1


left = right
1
0
0


Unordered
1
1
1




Notice that CF is set for both the left < right and unordered cases, but not for the other two cases.
If you can arrange things such that you can check for > or >=, you don't need to setnp cl / and al, cl to rule out Unordered.  This is what clang 16 and GCC 13 do to get correct results from ucomiss inf, abs / seta.
GCC8.5 does the right thing if you write abs(x) < infinity, it's only the scalar intrinsic that it doesn't implement properly.  (With plain scalar code, it uses comiss instead of ucomiss, the only difference being that it will update the FP environment with a #I FP-exception on QNaN as well as SNaN.)
This requires a separate movss load instead of a memory source.  But this does let GCC avoid the useless SSE4.1 insertps instruction that zeros the high 3 elements of XMM0, which ucomiss doesn't read anyway.  Clang sees that and optimizes away that part of _mm_set_ss(num) but GCC doesn't. The lack of an efficient way to go from a scalar float to a __m128 with don't-care upper elements is a persistent problem in Intel's intrinsics API that only some compilers manage to optimize around.  (How to merge a scalar into a vector without the compiler wasting an instruction zeroing upper elements? Design limitation in Intel's intrinsics?)  A float is just the low element of a __m128.
",
76385202,Why won't Binary search find an element in Java?,"Why won't Binary Search find an element?
I have one array with elements: BBBB, BBBB, CCCC. I want to find elements BBBB and BBBB. I want binary search to find two elements and it finds one. The output is ""1"" and it should be ""2"".
import java.util.*;

public class Test{
    public static void main(String[] args) {
        ArrayList<String> bricks = new ArrayList<String>(List.of(""BBBB"",""BBBB"",""CCCC""));
        ArrayList<String> bricksNeeded = new ArrayList<String>(List.of(""BBBB"",""BBBB""));
        int nFound = 0;
        int index;
        for(String brickNeeded:bricksNeeded){
            index = Collections.binarySearch(bricks, brickNeeded);
            if(index >= 0){
                bricks.remove(bricks.get(index));
                nFound ++;
                break;
            }
        }
        System.out.println(nFound);
    }
}

Output:
1
Expected output:
2
",76385273,"You have statement break - loop will be stopped after first removing.
So, nFound will be incremented only once
",
76381948,How to select multiple DIVs by mousedown>mousemove>mouseup (pure JS),"I'm trying to make a selection of elements when I click on empty point and move pointer.
In this example I'm expecting to get selection of two elements:

I've tried range and selection, but not with the proper result.


const mainDiv = document.createElement(""div"");
mainDiv.style.width = ""500px"";
mainDiv.style.height = ""500px"";
document.body.appendChild(mainDiv);

const div1 = document.createElement(""div"");
div1.style.position = ""absolute"";
div1.style.top = `${50}px`;
div1.style.left = `${50}px`;
div1.style.width = ""100px"";
div1.style.height = ""100px"";
div1.style.background = ""red"";
mainDiv.appendChild(div1);

const div2 = document.createElement(""div"");
div2.style.top = `${250}px`;
div2.style.left = `${250}px`;
div2.style.width = ""100px"";
div2.style.height = ""100px"";
div2.style.background = ""green"";
div2.style.position = ""absolute"";
mainDiv.appendChild(div2);

mainDiv.onmousedown = function(event) {
  function onMouseMove(event) {
    //add divs to selection
  }
  mainDiv.addEventListener('mousemove', onMouseMove);

  mainDiv.onmouseup = function() {
    console.log(""selected divs"")
  }
}



",76382120,"You can solve this by keeping an array of selected items and pushing items to it when the mouse moves over the items if the mouse is depressed.


const mainDiv = document.createElement(""div"");
mainDiv.style.width = ""500px"";
mainDiv.style.height = ""500px"";
document.body.appendChild(mainDiv);

const div1 = document.createElement(""div"");
div1.style.position = ""absolute"";
div1.style.top = `${50}px`;
div1.style.left = `${50}px`;
div1.style.width = ""100px"";
div1.style.height = ""100px"";
div1.style.background = ""red"";
mainDiv.appendChild(div1);

const div2 = document.createElement(""div"");
div2.style.top = `${250}px`;
div2.style.left = `${250}px`;
div2.style.width = ""100px"";
div2.style.height = ""100px"";
div2.style.background = ""green"";
div2.style.position = ""absolute"";
mainDiv.appendChild(div2);


!(() => {
  let selection = [];
  let selecting = false;
  function beginSelection(e) {
    selection = [];
    selecting = true;
    checkSelection(e);
  }
  function mouseMove(e) {
    checkSelection(e);
  }
  function mouseUp(e) {
    selecting = false;
    if (selection.length) {
      console.log(""selection: "", selection); // access selection before reset
      selection = [];
    } else {
      // no selection
    }
  }
  function checkSelection(e) {
    if (!selecting) {
      return; // ignore
    }
    const selected = e.target.parentNode === mainDiv && e.target;
    if (selected && !selection.includes(selected)) {
      selection.push(selected);
    }
  }
  mainDiv.addEventListener(""mousedown"", beginSelection);
  mainDiv.addEventListener(""mousemove"", mouseMove);
  window.addEventListener(""mouseup"", mouseUp);
  
})();



",
76381936,create on off toggle Icon for flutter torch,"I tried to implement  the possibility to use the flash of the phone as a torch in my flutter app. The on/ off button is located in the appbar. This runs fine except the light on and light off Button appear both at the same time. How can I make it, that either one or the other is shown. depending on whether the lamp is on or off?
Thank you very much for your help
I used the flutter torch_light: ^0.4.0

Class TorchController extends StatelessWidget {
  const TorchController({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(

      body: FutureBuilder<bool>(
        future: _isTorchAvailable(context),
        builder: (BuildContext context, AsyncSnapshot<bool> snapshot) {
          if (snapshot.hasData && snapshot.data!) {
            return Column(
              children: [
                Expanded(
                  child: Center(
                    child: IconButton ( icon: const Icon(Icons.flashlight_on_outlined,size: 35,),


                      onPressed: () async {
                        _enableTorch(context);
                      },
                    ),
                  ),
                ),
                Expanded(
                  child: Center(
                    child: IconButton (icon: const Icon(Icons.flashlight_off_outlined,size: 35,),


                      onPressed: () async {
                        _disableTorch(context);
                      },
                    ),
                  ),
                ),
              ],
            );
          } else if (snapshot.hasData) {
            return const Center(
              child: Text('No torch available.'),
            );
          } else {
            return const Center(
              child: CircularProgressIndicator(),
            );
          }
        },
      ),
    );
  }

 Future<bool> _isTorchAvailable(BuildContext context) async {
   try {
    return await TorchLight.isTorchAvailable();
    } on Exception catch (_) {
      _showMessage(
        'Could not check if the device has an available torch',
       context,
      );
      rethrow;
    }
  }

  Future<void> _enableTorch(BuildContext context) async {
    try {
      await TorchLight.enableTorch();
    } on Exception catch (_) {
      _showMessage('Could not enable torch', context);
    }
  }

  Future<void> _disableTorch(BuildContext context) async {
    try {
      await TorchLight.disableTorch();
    } on Exception catch (_) {
      _showMessage('Could not disable torch', context);
    }
  }

  void _showMessage(String message, BuildContext context) {
    ScaffoldMessenger.of(context)
        .showSnackBar(SnackBar(content: Text(message)));
  }
}
//Ende```

",76382142,"First of all, change the widget from stateless to stateful widget. Then

define a variable to show the status of the torch

isTorchOn = false;


on _enableTorch() update the value to true

(no need to pass context as it is now a stateful widget)


Future<void> _enableTorch(BuildContext context) async {
        try {
          await TorchLight.enableTorch();
          setState(()=> isTorchOn = true);
        } on Exception catch (_) {
          _showMessage('Could not enable torch', context);
        }
      }




do the same for _disableTorch() as set isTorchOn to false



Future<void> _disableTorch(BuildContext context) async {
    try {
      await TorchLight.disableTorch();
      setState(()=> isTorchOn = false);
    } on Exception catch (_) {
      _showMessage('Could not disable torch', context);
    }
  }



",
76383358,Delete all the lines including and after nth occurance of pattern,"Basically what the title says, I have some large tsv file (approx. 20k lines) and I want to delete the rest of the files after a specific column matches a string a second time (including said line)
",76383463,"awk '{print $0} $1==""yourstring""{if(++found==2)exit}' test.tsv

Where $1 is the ""specific column"" and yourstring is the string you are searching for.
This prints each line and then checks for the occurrence of yourstring in the first column. If it finds it, it tests a variable found which we increment, to see if it hits 2. If so awk exits.
Edit: If instead you want to delete the second occurrence (as well as everything after), flipping the two blocks around will accomplish this:
 awk ' $1==""yourstring""{if(++found==2)exit}{print $0}' test.tsv

",
76384747,how can I do an empty triangle with stars in c++ to after do the british flag?,"This is the flag that I have to get at the end:
*******************
**       *       **
* *      *      * *
*  *     *     *  *
*   *    *    *   *
*    *   *   *    *
*     *  *  *     *
*      * * *      *
*       ***       *
*******************
*       ***       *
*      * * *      *
*     *  *  *     *
*    *   *   *    *
*   *    *    *   *
*  *     *     *  *
* *      *      * *
**       *       **
*******************

I know how to do full star triangles but when it's empty on the inside I have no idea about how to proceed. Can anyone help me?
I tried and I just know how to do full star triangles and a star square/rectangle empty on the inside here is the code:
int main(void)
{
    int i, j, length, width;

    cout << ""Length of rectangle? "";
    cin >> length;
    cout << endl;

    cout << ""Width of rectangle? "";
    cin >> width;
    cout << endl;

    for ( i = 0; i < length; i++ )
        cout << ""*"";

    cout << endl;

    for ( i = 1; i < width - 1; i++ )
    {
        cout << ""*"";

        for ( j = 1; j < length - 1; j++ )
        {
            cout << "" "";
        }

        cout << ""*"";
        cout << endl;
    }

    for ( i = 0; i < length; i++)
        cout << ""*"";

    cout << endl;

    return 0;
}

",76385292,"Lines 1, 10 and 19 are easy, as they each consist only of 19 *.
The problem is the lines 2 to 9 and 11 to 19.
However, do you notice a pattern in lines 2 to 9?
Line 2 consists of one *

followed by 0 spaces
followed by one *
followed by 7 spaces
followed by one *
followed by 7 spaces
followed by one *
followed by 0 spaces

followed by one *
Line 3 consists of one *

followed by 1 spaces
followed by one *
followed by 6 spaces
followed by one *
followed by 6 spaces
followed by one *
followed by 1 spaces

followed by one *.
Line 4 consists of one *

followed by 2 spaces
followed by one *
followed by 5 spaces
followed by one *
followed by 5 spaces
followed by one *
followed by 2 spaces

followed by one *.
Line 5 consists of one *

followed by 3 spaces
followed by one *
followed by 4 spaces
followed by one *
followed by 4 spaces
followed by one *
followed by 3 spaces

followed by one *.
Line 6 consists of one *

followed by 4 spaces
followed by one *
followed by 3 spaces
followed by one *
followed by 3 spaces
followed by one *
followed by 4 spaces

followed by one *.
Line 7 consists of one *

followed by 5 spaces
followed by one *
followed by 2 spaces
followed by one *
followed by 2 spaces
followed by one *
followed by 5 spaces

followed by one *.
Line 8 consists of one *

followed by 6 spaces
followed by one *
followed by 1 spaces
followed by one *
followed by 1 spaces
followed by one *
followed by 6 spaces

followed by one *.
Line 9 consists of one *

followed by 7 spaces
followed by one *
followed by 0 spaces
followed by one *
followed by 0 spaces
followed by one *
followed by 7 spaces

followed by one *.
The pattern is the following:
Assuming that size is the total size of the triangle (which is 19 in your case), then
line n consists of one *

followed by n-2 spaces
followed by one *
followed by (size/2) - n spaces
followed by one *
followed by (size/2) - n spaces
followed by one *
followed by n-2 spaces

followed by one *.
Note that in C, the result of 19 / 2 is 9, as the fractional part of the division is discarded.
Using this information about the pattern, you should be able to create a loop that in every loop iteration, prints one line as described above. That way, you should be able to solve the problem of printing the lines 2 to 9.
Printing the lines 11 to 19 should be easy afterwards, because these lines must only be printed in reverse order of the lines 2 to 9.
In accordance with the community guidelines for homework questions, I will not provide the full solution at this time. I can provide further information later, if necessary.
EDIT:
Since several other solutions have already been posted by other users, I will now also post my solution, which solves the problem as described above:
#include <iostream>

const int MAP_SIZE = 19;

static_assert( MAP_SIZE % 2 == 1, ""MAP_SIZE must be odd"" );

int main( void )
{
    //print first horizontal line
    for ( int i = 0; i < MAP_SIZE; i++ )
        std::cout << '*';
    std::cout << '\n';

    //print top half of flag
    for ( int i = 0; i < MAP_SIZE / 2 - 1; i++ )
    {
        std::cout << '*';
        for ( int j = 0; j < i; j++ )
            std::cout << ' ';
        std::cout << '*';
        for ( int j = 0; j < MAP_SIZE/2 - 2 - i; j++ )
            std::cout << ' ';
        std::cout << '*';
        for ( int j = 0; j < MAP_SIZE/2 - 2 - i; j++ )
            std::cout << ' ';
        std::cout << '*';
        for ( int j = 0; j < i; j++ )
            std::cout << ' ';
        std::cout << '*';
        std::cout << '\n';
    }

    //print second horizontal line
    for ( int i = 0; i < MAP_SIZE; i++ )
        std::cout << '*';
    std::cout << '\n';

    //print bottom half of flag
    for ( int i = 0; i < MAP_SIZE / 2 - 1; i++ )
    {
        std::cout << '*';
        for ( int j = 0; j < MAP_SIZE/2 - 2 - i; j++ )
            std::cout << ' ';
        std::cout << '*';
        for ( int j = 0; j < i; j++ )
            std::cout << ' ';
        std::cout << '*';
        for ( int j = 0; j < i; j++ )
            std::cout << ' ';
        std::cout << '*';
        for ( int j = 0; j < MAP_SIZE/2 - 2 - i; j++ )
            std::cout << ' ';
        std::cout << '*';
        std::cout << '\n';
    }

    //print third horizontal line
    for ( int i = 0; i < MAP_SIZE; i++ )
        std::cout << '*';
    std::cout << '\n';
}

However, I think that this problem is easier to solve using a 2D array (which you stated that you are not allowed to use). The 2D array is initialized to spaces and then the 3 horizontal, 3 vertical and 2 diagonal lines are drawn:
#include <iostream>

const int MAP_SIZE = 19;

static_assert( MAP_SIZE % 2 == 1, ""MAP_SIZE must be odd"" );

int main( void )
{
    char map[MAP_SIZE][MAP_SIZE];

    //initialize 2D array to spaces
    for ( int i = 0; i < MAP_SIZE; i++ )
        for ( int j = 0; j < MAP_SIZE; j++ )
            map[i][j] = ' ';

    //draw the 3 horizontal lines
    for ( int i = 0; i < MAP_SIZE; i++ )
    {
        map[         0][i] = '*';
        map[MAP_SIZE/2][i] = '*';
        map[MAP_SIZE-1][i] = '*';
    }

    //draw the 3 vertical lines
    for ( int i = 0; i < MAP_SIZE; i++ )
    {
        map[i][         0] = '*';
        map[i][MAP_SIZE/2] = '*';
        map[i][MAP_SIZE-1] = '*';
    }

    //draw the 2 diagonal lines
    for ( int i = 0; i < MAP_SIZE; i++ )
    {
        map[i][           i] = '*';
        map[i][MAP_SIZE-i-1] = '*';
    }

    //print the result
    for ( int i = 0; i < MAP_SIZE; i++ )
    {
        std::cout.write( map[i], MAP_SIZE );
        std::cout.put( '\n' );
    }
}

",
76384972,Is it possible to merge two tables in Power Query Editor (Power BI) with Python fuzzy matching?,"Merge two tables in power query editor (Power BI) based on string similarity with Python
Consider the tables bellow:
Table1
Table1




Name
...




Apple Fruit A11
...


Banana Fruit B12
...


...
...




Table2
Table2




Name
Value




Apple A11R/T
40


B4n4n4 Fruit B12_T
50


Berry A11
60


...
...




I want to get the Value from Table2 into Table1. But for some reason when I use the built-in power query editor merge with fuzzy matching. It will match Apple Fruit A11 with Berry A11 instead of Apple A11 R/T. I've read the documentation, and it says that the built-in function works best with single words. I tried to remove spaces both from Table1[Name] and Table2[Name] but it didn't improve results.
I looked around trying to find a solution, but wasn't able to figure out yet. Is there a way to do this using python? Or is there a simpler solution?
The results that I am expecting:
Table1
Expected Result




Name
...
Table2.Name
Table2.Value




Apple Fruit A11
...
Apple A11R/T
40


Banana Fruit B12
...
B4n4n4 Fruit B12_T
50


...
...
...
...




--- For some reason the tables are not showing up like the preview, that's why there are also images for each table. Disclaimer: The data present in the tables above is just an example of the pattern of the data that I am working with. And fuzzy matching will probably give the right results for the example data.
",76385313,"Fuzzy matching in Power Query works fine for me.

Set your options to the following:

",
76382116,How can I modify my C code to replace words in a file without user input?,"/**
 * C program to find and replace all occurrences of a word in file.
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define BUFFER_SIZE 1000

/* Function declaration */
void replaceAll(char *str, const char *oldWord, const char *newWord);


int main()
{
/* File pointer to hold reference of input file */
FILE * fPtr;
FILE * fTemp;
char path[100];

char buffer[BUFFER_SIZE];
char oldWord[100], newWord[100];


printf(""Enter path of source file: "");
scanf(""%s"", path);

printf(""Enter word to replace: "");
scanf(""%s"", oldWord);

printf(""Replace '%s' with: "");
scanf(""%s"", newWord);


/*  Open all required files */
fPtr  = fopen(path, ""r"");
fTemp = fopen(""replace.tmp"", ""w""); 

/* fopen() return NULL if unable to open file in given mode. */
if (fPtr == NULL || fTemp == NULL)
{
    /* Unable to open file hence exit */
    printf(""\nUnable to open file.\n"");
    printf(""Please check whether file exists and you have read/write privilege.\n"");
    exit(EXIT_SUCCESS);
}


/*
 * Read line from source file and write to destination 
 * file after replacing given word.
 */
while ((fgets(buffer, BUFFER_SIZE, fPtr)) != NULL)
{
    // Replace all occurrence of word from current line
    replaceAll(buffer, oldWord, newWord);

    // After replacing write it to temp file.
    fputs(buffer, fTemp);
}


/* Close all files to release resource */
fclose(fPtr);
fclose(fTemp);


/* Delete original source file */
remove(path);

/* Rename temp file as original file */
rename(""replace.tmp"", path);

printf(""\nSuccessfully replaced all occurrences of '%s' with '%s'."", oldWord, newWord);

return 0;
}



/**
 * Replace all occurrences of a given a word in string.
 */
void replaceAll(char *str, const char *oldWord, const char *newWord)
{
char *pos, temp[BUFFER_SIZE];
int index = 0;
int owlen;

owlen = strlen(oldWord);

// Fix: If oldWord and newWord are same it goes to infinite loop
if (!strcmp(oldWord, newWord)) {
    return;
}


/*
 * Repeat till all occurrences are replaced. 
 */
while ((pos = strstr(str, oldWord)) != NULL)
{
    // Backup current line
    strcpy(temp, str);

    // Index of current found word
    index = pos - str;

    // Terminate str after word found index
    str[index] = '\0';

    // Concatenate str with new word 
    strcat(str, newWord);
    
    // Concatenate str with remaining words after 
    // oldword found index.
    strcat(str, temp + index + owlen);
}
}

I have that code in C which can change all ""oldWords"" into ""newWords"". Works fine, but everytime I want to change the code to change the words on its own I'm completely stupid. I want that I don't have to put the words that have to change into the console, but I want to have them in the code. I just want to tell the console the path of the source file and that's it.
Would be nice if you could help me with some examples like Hello to Bye and Morning to Night.
",76382155,"If you don't want to take oldWord and newWord from user input, you can define them as constants in the code:
const char* oldWord = ""Hello"";
const char* newWord = ""Bye"";

",
76383388,I want to set new object in local storage,"function addNote() {
    const givenTitle = document.getElementById('titleInput');
    const givenNote = document.getElementById('noteInput');
    
    let notesObj = []

    let myObj = {
        title: givenTitle.value,
        note: givenNote.value,
    }

    notesObj.push(myObj)

    localStorage.setItem('userNote',JSON.stringify(notesObj));
}

This code is everyTime changing the older value I want to get a new object in local storage
",76383465,"Probably you want to add note into your existing localStorage data so here you can check if there is userNote is already set before then take it or set an empty array,
let notesObj = JSON.parse(localStorage.getItem(""userNote"")) || []

Your whole function would be like this,
function addNote() {
    const givenTitle = document.getElementById('titleInput');
    const givenNote = document.getElementById('noteInput');
    
    let notesObj = JSON.parse(localStorage.getItem(""userNote"")) || []

    let myObj = {
        title: givenTitle.value,
        note: givenNote.value,
    }

    notesObj.push(myObj)

    localStorage.setItem('userNote',JSON.stringify(notesObj));
}

",
76385055,Trying to assign a response.data value to a variable in react and the value doesnt pass well,"Im building an AuthContext in react to handle login, i connect it to a django backend where i validate the user and then i get an authorization token
import React, { createContext, useState, useEffect} from 'react';
import axios from 'axios';

export const AuthContext = createContext();


  export const AuthProvider = ({ children }) => {
    const [token, setToken] = useState(null);
  
    const login = async (username, password) => {
      try {
        const response = await axios.post('http://127.0.0.1:8000/token/', {
          username,
          password
        });
        console.log('Response data:', response.data);
        const { token: responseToken } = response.data;
        setToken(responseToken);
    
        console.log('Token loaded:', responseToken);
      } catch (error) {
        console.error('Error en el inicio de sesión:', error);
      }
    };    
  
    useEffect(() => {
      console.log('Token actual value:', token); 
    }, [token]);
  
    return (
      <AuthContext.Provider value={{ token, login }}>
        {children}
      </AuthContext.Provider>
    );
  };


From my backend i get the expected answer, an status 200 with the token that i expect, but after assign the response.data to responseToken, in the next log it shows that is undefined. Here is the output in the navigator console:
console logs
Tried to change the variable names to check if there is a conflict between those declared there, but the problem persists.
",76385319,"Your console log shows that response data has two properties refresh & access. But you are trying to get a property called token, which does not exist (or at least it's not visible in your screenshot).
const { access: responseToken } = response.data;

This should help. (if access is the one you actually need)
",
76383411,ReactJs why do i get two console logs in a simple component?,"i have this simple nav component, but it drives me crazy because it makes every console log i make in the app two times.
Im new to React btw.
export const NavBar = () => {
  const [showNav, setShowNav] = useState(false);

  const handleNavClick = () => {
    setShowNav(!showNav);
  };

  console.log(""hi"");

  return (
    <>
      <nav className=""flex items-center justify-between pl-8 pr-16 fixed w-full border h-20 top-0 bg-white/30 backdrop-blur-sm z-10"">
        {/* Logo */}
        <img
          src=""https://res.cloudinary.com/dv8nczwtj/image/upload/v1684896617/Logo_jivlnb.png""
          alt=""Logo""
          className=""logo""
        />
        {/* Nav WEB*/}
        <div className=""md:flex flex-row space-x-5 hidden"">
          <a href=""#"" className=""brand"">
            Apple
          </a>
          <a href=""#"" className=""brand"">
            Samsung
          </a>
          <a href=""#"" className=""brand"">
            Xiaomi
          </a>
          <a href=""#"" className=""brand"">
            Google
          </a>
        </div>
        {/* BTN Nav Mobil */}
        <button className=""md:hidden"" onClick={handleNavClick}>
          <img
            src=""https://res.cloudinary.com/dv8nczwtj/image/upload/v1684859901/menu_wh8ccz.png""
            alt=""Menu""
            className=""w-6""
          />
        </button>
        {/* Cart */}
        <CartWidget />
      </nav>
      {/* Nav Mobil */}
      {showNav && (
        <div
          className=""flex fixed w-full flex-col justify-center items-center space-y-4 pb-2 border-b-2 border-black md:hidden bg-white/30 top-20 pt-4 backdrop-blur-sm""
          style={{ animation: ""fadeIn .5s linear"" }}
        >
          <a href=""#"" className=""brand"">
            Apple
          </a>
          <a href=""#"" className=""brand"">
            Samsung
          </a>
          <a href=""#"" className=""brand"">
            Xiaomi
          </a>
          <a href=""#"" className=""brand"">
            Google
          </a>
        </div>
      )}
    </>
  );
};

double console.log
I have tried putting the handleClick function in a useEffect but then when i put it on the onClick it says that the function is never declared
",76383481,"Is your app using React.StrictMode (https://react.dev/reference/react/StrictMode#fixing-bugs-found-by-double-rendering-in-development) ?
",
76381116,"Error in as.vector(x, mode) : cannot coerce type 'closure' to vector of type 'any' -- when running a nested function","Problem: running get_pi_ij() gives the error:
Error in as.vector(x, mode) : cannot coerce type 'closure' to vector of type 'any'
Called from: as.vector(data)
The first thing this function does is to make the resulting alphas and beta_prelims into matrixes that match c so that they can be calculated together. This is where something goes wrong, and I have not been able to figure out what. If I use <<- to save the alphas and betas to the global environment in the prior functions for alphas and betas and replace that with those in the faulty function, it works. So I assume it has to do with how I call the functions inside the matrix creation.
get_pi_ij <- function() {
  alphas <-
    matrix(get_alpha(),
           nrow = length(get_alpha()),
           ncol = length(get_alpha()),
           byrow = FALSE)
  betas <-
    matrix(get_beta_prelim,
           nrow = length(get_beta_prelim()),
           ncol = length(get_beta_prelim()),
           byrow = TRUE) 
  pi_ij <- exp(alphas + betas + gamma * c)
  return(pi_ij)
}
get_pi_ij()

I added the full code cause it's not too long and the first parts are just definitions. Makes it easier to test it.
Everything up to the final function works as it is supposed to
size <- 18 
gamma <- -0.07 

c <- structure(c(0, 4, 8, 12, 16, 4, 4, 8, 12, 16, 8, 8, 8, 12, 16, 
            12, 12, 16, 16, 4, 0, 4, 8, 12, 8, 4, 4, 8, 12, 12, 8, 8, 12, 
            16, 12, 12, 16, 16, 8, 4, 0, 4, 8, 12, 8, 4, 8, 12, 16, 12, 8, 
            12, 16, 16, 12, 16, 16, 12, 8, 4, 0, 4, 12, 8, 4, 4, 8, 16, 12, 
            8, 8, 12, 16, 12, 12, 16, 16, 12, 8, 4, 0, 16, 12, 8, 4, 4, 16, 
            12, 8, 8, 8, 16, 12, 12, 16, 4, 8, 12, 12, 16, 0, 4, 8, 12, 16, 
            4, 4, 8, 12, 16, 8, 8, 12, 12, 4, 4, 8, 8, 12, 4, 0, 4, 8, 12, 
            8, 4, 4, 8, 12, 8, 8, 12, 12, 8, 4, 4, 4, 8, 8, 4, 0, 4, 8, 12, 
            8, 4, 8, 12, 12, 8, 12, 12, 12, 8, 8, 4, 4, 12, 8, 4, 0, 4, 12, 
            8, 4, 4, 8, 12, 8, 8, 12, 16, 12, 12, 8, 4, 16, 12, 8, 4, 0, 
            16, 12, 8, 4, 4, 12, 8, 8, 12, 8, 12, 16, 16, 16, 4, 8, 12, 12, 
            16, 0, 4, 8, 12, 16, 4, 8, 12, 8, 8, 8, 12, 12, 12, 4, 4, 8, 
            8, 12, 4, 0, 4, 8, 12, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 4, 
            8, 8, 4, 0, 4, 8, 8, 4, 8, 8, 12, 12, 12, 8, 8, 12, 8, 8, 4, 
            4, 12, 8, 4, 0, 4, 8, 4, 4, 8, 16, 16, 16, 12, 8, 16, 12, 12, 
            8, 4, 16, 12, 8, 4, 0, 12, 8, 4, 8, 12, 12, 16, 16, 16, 8, 8, 
            12, 12, 12, 4, 4, 8, 8, 12, 0, 4, 8, 4, 12, 12, 12, 12, 12, 8, 
            8, 8, 8, 8, 8, 4, 4, 4, 8, 4, 0, 4, 4, 16, 16, 16, 12, 12, 12, 
            12, 12, 8, 8, 12, 8, 8, 4, 4, 8, 4, 0, 4, 16, 16, 16, 16, 16, 
            12, 12, 12, 12, 12, 8, 8, 8, 8, 8, 4, 4, 4, 0), 
          .Dim = c(19L, 19L), 
          .Dimnames = list(NULL, c(""X1"", ""X2"", ""X3"", ""X4"", ""X5"", 
                                   ""X6"", ""X7"", ""X8"", ""X9"", ""X10"", 
                                   ""X11"", ""X12"", ""X13"", ""X14"", ""X15"", 
                                   ""X16"", ""X17"", ""X18"", ""X19"")))

set.seed(12345) 
h_share <- diff(c(0, sort(runif(size)), 1)) 
e_share <- diff(c(0, sort(runif(size)), 1)) 

alpha <- numeric(size + 1)
beta <- numeric(size + 1)

get_beta_prelim <- function() {
  a_matrix <- exp(alpha + t(c) * gamma)
  beta_prelim <- log(e_share) - log(colSums(a_matrix))
  return(beta_prelim)
}

get_beta <- function() {
  beta <- get_beta_prelim() - get_beta_prelim()[[1]]
  return(beta)
}

get_alpha_prelim <- function() {
  b_matrix <- t(exp(beta + t(c) * gamma))
  alpha_prelim <- log(h_share) - log(rowSums(b_matrix))
  return(alpha_prelim)
}

get_alpha <- function() {
  alpha <- get_alpha_prelim() - get_alpha_prelim()[[1]]
  return(alpha)
}

get_pi_ij <- function() {
  alphas <-
    matrix(get_alpha(),
           nrow = length(get_alpha()),
           ncol = length(get_alpha()),
           byrow = FALSE)
  betas <-
    matrix(get_beta_prelim,
           nrow = length(get_beta_prelim()),
           ncol = length(get_beta_prelim()),
           byrow = TRUE) 
  pi_ij <- exp(alphas + betas + gamma * c)
  return(pi_ij)
}
get_pi_ij()

",76382168,"get_pi_ij <- function() {
  alphas <-
    matrix(get_alpha(),
           nrow = length(get_alpha()),
           ncol = length(get_alpha()),
           byrow = FALSE)
  betas <-
    matrix(get_beta_prelim(), # your were missing ""()""
           nrow = length(get_beta_prelim()),
           ncol = length(get_beta_prelim()),
           byrow = TRUE) 
  pi_ij <- exp(alphas + betas + gamma * c)
  return(pi_ij)
}

",
76385032,How to Fix NoMethodError Issue with Ruby Compass (1.0.0 and 1.0.3),"I have a Fedora 38 (6.1.29-1) server with Ruby and the Compass gem installed.
When I try to execute compass -h or perform any compass compiling, I get a NoMethodError (on different lines of different .rb files, but errors nonetheless).
I've looked all around for similar errors and can't seem to find anyone else that experiences this problem.
At first I thought maybe the latest version (1.0.3) of Compass doesn't work on my server, so I also tried 1.0.0 but still get the same error.
I also tried installing the same version(s) and followed the same process on my Windows machine and had no issues when executing the same compass -h and compass compile commands.
Anyone have any idea what is causing this error on my fedora server?
When executing ""compass -h"" on the command line on the Fedora server...
Current Output:
NoMethodError on line [""144""] of /home/user1/.local/share/gem/ruby/gems/compass-1.0.0/lib/compass/installers/manifest.rb: undefined method `exists?' for File:Class

Expected Output:
Usage: compass help [command]

Description:
  The Compass Stylesheet Authoring Framework helps you
  build and maintain your stylesheets and makes it easy
  for you to use stylesheet libraries provided by others.

Donating:
  Compass is charityware. If you find it useful please make
  a tax deductable donation: http://umdf.org/compass

To get help on a particular command please specify the command.

Primary Commands:
  * clean       - Remove generated files and the sass cache
  * compile     - Compile Sass stylesheets to CSS
  * create      - Create a new compass project
  * init        - Add compass to an existing project
  * watch       - Compile Sass stylesheets to CSS when they change
Other Commands:
  * config      - Generate a configuration file for the provided command line options.
  * extension   - Manage the list of compass extensions on your system
  * frameworks  - List the available frameworks
  * help        - Get help on a compass command or extension
  * imports     - Emit an imports suitable for passing to the sass command-line.
  * install     - Install an extension's pattern into your compass project
  * interactive - Interactively evaluate SassScript
  * sprite      - Generate an import for your sprites.
  * stats       - Report statistics about your stylesheets
  * unpack      - Copy an extension into your extensions folder.
  * validate    - Validate your generated css.
  * version     - Print out version information

Available Frameworks & Patterns:

  * compass
    - compass/ellipsis  - Plugin for cross-browser ellipsis truncated text.
    - compass/extension - Generate a compass extension.
    - compass/project   - The default project layout.

Global Options:
    -r, --require LIBRARY            Require the given ruby LIBRARY before running commands.
                                       This is used to access compass plugins without having a
                                       project configuration file.
    -l, --load FRAMEWORK_DIR         Load the framework or extensions found in the FRAMEWORK directory.
    -L, --load-all FRAMEWORKS_DIR    Load all the frameworks or extensions found in the FRAMEWORKS_DIR directory.
    -I, --import-path IMPORT_PATH    Makes files under the IMPORT_PATH folder findable by Sass's @import directive.
    -q, --quiet                      Quiet mode.
        --trace                      Show a full stacktrace on error
        --force                      Allows compass to overwrite existing files.
        --boring                     Turn off colorized output.
    -?, -h, --help                   Show this message

",76385330,"File.exists? was deprecated for several minor versions and existed until Ruby 2.7. And was finally removed in Ruby 3.0.
Whereas the last version of the compass gem is more than 8 years old. That means it doesn't work with current version of Ruby anymore.
You have basically three options:

Downgrade your Ruby version to, for example, 2.7.8. That version is not terrible out-dated, but keep in mind that Ruby 2.7 reached end-of-life, it will not get any security or bug fixes anymore.
Fork the compass gem and fix the usage of File.exists? with File.exist?. This seems to be a quick fix, but given that this gem didn't get any update in the last 8 years, you might discover further compatibility issues or unfixed bugs.
Search for an alternative and replace that gem.

",
76384401,How can I obtain the folder path when right-clicking the background of a folder and invoking a context menu using a shell extension?,"How to obtain path to folder  in which user made right click in its background to invoke context menu? For example, user opened ""D:\projects"" folder and made right click in empty background area of that folder and it sees a menu item in context menu named 'Display Path'. Upon clicking it, it should invoke a simple console app to display string ""D:\projects"".
It can be done by registry by adding ""%V"" as argument to command to console app, for example, ""C:\myfolder\myapp.exe"" ""%V"". Hence, this %V gives folder path to argument list of main() of myuapp.exe. Easy huh!
How it can be done using shell extension menu handler? I wrote a simple shell context menu dll which works fine and do its job, except that I don't known how to get that folder path as string where user made right click in background.
I found that path comes as PCIDLIST_ABSOLUTE pidl argument in IShellExtInit::Initialize() method. But, I couldn't get it in simple string format. The code is below which crashes, of course.
HRESULT __stdcall Initialize(PCIDLIST_ABSOLUTE pidlFilder, IDataObject* pdtobj, HKEY hkeyProgID)
    {
        std::wstring s = L""null"";
        
        // check msg, this msgbox is shown as expected
        MessageBox(NULL, L""Before"", L""Initialize()"", MB_OK);
                
                //have problem in this line, I guess
        SHGetPathFromIDList((LPCITEMIDLIST) pidlFilder, (PWSTR) &s);
        
        // check msg, sometimes this msgbox is also shown as expected
        MessageBox(NULL, L""After"", L""Initialize()"", MB_OK);
        
        // but this msgbox is never shown. I removed it but code still crashes
        MessageBox(NULL, std::wstring(s).c_str(), L""Initialize()"", MB_OK);
        
        return S_OK;
    }

When I right click on folder background, it crashes and explorer restarts.
Does anyone know the problem and its solution? How to get folder path when right clicking background of folder to invoke context menu using shell extension?
In addition, how to get file/folder path when right clicking on it to invoke context menu using shell extension?
Thanks in advance
tried using this code too, still crashes
                IShellFolder *sf = NULL;
        STRRET pName = {};
        sf->GetDisplayNameOf(pidlFilder, SHGDN_FORPARSING, &pName);
        wchar_t *d = new wchar_t;
        lstrcpyW(d,L""nulld"");
        size_t inst = MAX_PATH, outst ;
        mbstowcs_s(&outst, d, inst, pName.cStr, MAX_PATH);
        s = std::wstring(d);
                MessageBox(NULL, std::wstring(s).c_str(), L""Initialize()"", MB_OK);

",76385345,"You are trying to make SHGetPathFromIDList() write the string data to the memory address where a std::wstring object resides, which will not work.
Use a fixed WCHAR[] array instead, eg:
HRESULT __stdcall Initialize(PCIDLIST_ABSOLUTE pidlFilder, IDataObject* pdtobj, HKEY hkeyProgID)
{
    WCHAR szPath[MAX_PATH] = {};

    SHGetPathFromIDList(pidlFilder, szPath);

    MessageBox(NULL, szPath, L""Initialize()"", MB_OK);
        
    return S_OK;
}

Alternatively, if you want to receive the string data into a std::wstring object, then you have to pre-allocate its internal character buffer and then receive into that buffer, eg:
HRESULT __stdcall Initialize(PCIDLIST_ABSOLUTE pidlFilder, IDataObject* pdtobj, HKEY hkeyProgID)
{
    std::wstring s;
    s.resize(MAX_PATH);
        
    SHGetPathFromIDList(pidlFilder, s.data() /* or &s[0] before C++17 */ );
    s.erase(s.find(L'\0'));

    MessageBox(NULL, s.c_str(), L""Initialize()"", MB_OK);
        
    return S_OK;
}

Otherwise, you can simply receive into a WCHAR[] and then assign that to your std::wstring, eg:
HRESULT __stdcall Initialize(PCIDLIST_ABSOLUTE pidlFilder, IDataObject* pdtobj, HKEY hkeyProgID)
{
    WCHAR szPath[MAX_PATH] = {};
    SHGetPathFromIDList(pidlFilder, szPath);
        
    std::wstring s = szPath;
    MessageBox(NULL, s.c_str(), L""Initialize()"", MB_OK);
        
    return S_OK;
}


Your 2nd example doesn't work for several reasons:

Your IShellFolder *sf doesn't point anywhere meaningful. Use SHGetDesktopFolder() to get the top-level IShellFolder object which you can then use to parse pidlFilder.

you are allocating only 1 wchar_t for wchar_t *d to point at, but then you are trying to copy more than 1 wchar_t into that memory.  You don't really need to allocate any memory at all, as the parsed STRRET already contains the necessary string data, so just use it as-is.  Otherwise, you can pass the STRRET to StrRetToBuf() or StrRetToStr() to get the data in a more usable format.

you are not paying attention to the STRRET::uType field to know what kind of string data it is holding.  Don't access the cStr field unless the uType field is set to STRRET_CSTR.  StrRetToBuf()/StrRetToStr() will handle this for you.


Try this instead:
HRESULT __stdcall Initialize(PCIDLIST_ABSOLUTE pidlFilder, IDataObject* pdtobj, HKEY hkeyProgID)
{
    IShellFolder *sf = NULL;
    if (SUCCEEDED(SHGetDesktopFolder(&sf))
    {
        STRRET pName = {};
        if (SUCCEEDED(sf->GetDisplayNameOf(pidlFilder, SHGDN_FORPARSING, &pName))
        {
            WCHAR szPath[MAX_PATH] = {};
            StrRetToBufW(&pName, pidlFilder, szPath, MAX_PATH);

            MessageBox(NULL, szPath, L""Initialize()"", MB_OK);
        }

        sf->Release();
    }

    return S_OK;
}

",
76383425,"Bash if statement expression evaluates to FALSE but $? is 0, why?","The example below echoes 1, as expected:
test -f /usr/bin
echo ""$?"" #1

Why does the following example echo 0?
if [[ -f /usr/bin ]]; then
    echo ""Inside if statement"" # This line is never executed    
fi
echo ""$?"" #0

I know that the $?evaluates to the returned value of the last executed command. In my understanding, the last executed command is test, that is implicitly called by the if statement, since the condition evaluates to false it should return 1, but when I execute it, it returns 0. Can anybody explain why the behavior is different than when test is executed directly (like in the first example)?
",76383491,"According to man bash:


if list; then list; [ elif list; then list; ] ... [ else list; ] fi
The if list is executed.  If its exit status is zero, the then list is  executed.   Otherwise,  each  elif list  is executed in turn, and if its exit status is zero, the corresponding then list is executed and the
command completes.  Otherwise, the else list is executed, if present.  The exit status is the exit  status
of the last command executed, or zero if no condition tested true.



",
76381858,How can I convert a left table into a summary table?,"How do I convert table left to summary table right?

I tried using get dummies function to convert values to 0 and 1.
I don't know how to proceed after that.
",76382169,"Try this:
import pandas as pd
import numpy as np
col1 = ['']+['Hampshire']*8+['']+['Hampshire']+['']+['Hampshire']+['','']+['Hampshire']*4
col2 = ['Southhampton'] + ['']*12 + ['Southhampton']*2 + ['']*4
col3 = ['']*11 + ['Isle of wight'] + ['']*7
col4 = ['Met']*5 + [''] + ['Met']*13
col5 = ['']*5 + ['Partially met'] + ['']*13
col6 = ['']*19

df = pd.DataFrame(data = dict(zip(['Hampshire', 'Southhampton', 'Isle of wight', '5met', '5partially met', '5Not met'],[col1,col2,col3,col4,col5,col6])))
df = df.replace('', np.nan)
df['Hampshire'] = df['Hampshire'].fillna(df['Southhampton'])
df['Hampshire'] = df['Hampshire'].fillna(df['Isle of wight'])
df[['Hampshire','5met','5partially met', '5Not met']].groupby(by=['Hampshire']).count()

I had to generate the data for you (since you didn't post any besides the image), but I think this get's the job done. I hope this helps.
",
76385289,Is there a way to use onSchedule and also set a custom 'timeoutSeconds' and 'memory' using Firebase functions V2?,"I have had to revert back to using Firebase functions V1 in order to schedule the running of my functions and also specify the runtime options including timeoutSeconds and memory in my code (written in TypeScript):
const runtimeOpts = {
    timeoutSeconds: 540,
    memory: ""1GB"" as const,
};
exports.cleanupEvents = functions
    .runWith(runtimeOpts)
    .pubsub.schedule(""0 0 * * *"")
    .timeZone(""Europe/Berlin"")
    .onRun(async () => {
        await cleanupOldEvents(adminDb);
        logger.log(""Event cleanup finished"");
    });


Does anyone know if it is possible with Firebase functions V2 using the onSchedule syntax to also specify these runtimeOpts in code? Without needing to go into the google cloud console and manually setting it there.
I have tried chaining'onSchedule' and 'runWith' together and seeing what other possibilities Emmet suggests, so far but had no luck.
",76385350,"The API documentation for onSchedule suggests that you can pass an object as the first parameter, which is a ScheduleOptions object, an extension of GlobalOptions:
onSchedule({
    schedule: ""your-schedule-here"",
    timeoutSeconds: your-timeout,
    memory: your-memory,
    // include other options here from SchedulerOptions or GlobalOptions
}, (event) => { ... })

",
76382755,How can I use the ASP.NET [Range] annotation for IEnumerable elements?,"I want to use the ASP.NET [Range] Annotation but for the elements IEnumerables.
I used the existing RangeAttribute like this:
public class RangeEnumerable : RangeAttribute
{
    /// <inheritdoc/>
    public RangeEnumerable(double minimum, double maximum) : base(minimum, maximum)
    {
    }

    /// <inheritdoc/>
    public RangeEnumerable(int minimum, int maximum) : base(minimum, maximum)
    {
    }

    /// <inheritdoc/>
    public RangeEnumerable([DynamicallyAccessedMembers((DynamicallyAccessedMemberTypes)(-1))] Type type, string minimum, string maximum) : base(type, minimum, maximum)
    {
    }

    /// <inheritdoc/>
    public override bool IsValid(object? value)
    {
        if (null == value) { return true; } 

        IEnumerable<object> list = ((IEnumerable)value).Cast<object>();
        
        foreach (object item in list)
        {
            if (!base.IsValid(item))
            {
                return false;
            }                
        }

        return true;
    }
}

and annotated my Parameter like this:
[RangeEnumerable(MINIMUM_ANGLE, MAXIMUM_ANGLE)]
public IEnumerable<Double> PhaseAnglesVoltage { get; set; } = new List<double>();

And wrote the following unit test:
[Test]
public void TestInvalidPhaseAngleVoltageTooLow()
{
    // Arrange       
    Loadpoint loadpoint1 = new Loadpoint();
    loadpoint1.PhaseAnglesVoltage.Append(-1);

    // Act
    var errCount = ValidateObject(loadpoint1);

    // Assert
    Assert.AreEqual(1, errCount);
}

private int ValidateObject(object obj)
{
    var validationContext = new ValidationContext(obj, null, null);
    var validationResults = new List<ValidationResult>();
    Validator.TryValidateObject(obj, validationContext, validationResults, true);
    return validationResults.Count;
}

I expected the loop to iterate over the elements of the List I used the annotation with, but in the IsValid-Function I always get an empty List instead of one with the element appended in the test.
",76383517,"Ok, I've found the error, which was in the unit test. IEnumerable.Append doesn't add the element to the original object like List.Add does (see Difference between a List's Add and Append method?).
Changing the unit test to the following does the trick.
        [Test]
        public void TestInvalidPhaseAngleVoltageTooLow()
        {
            // Arrange       
            Loadpoint loadpoint1 = new Loadpoint();
            loadpoint1.PhaseAnglesVoltage = loadpoint1.PhaseAnglesVoltage.Append(-1);

            // Act
            var errCount = ValidateObject(loadpoint1);
   
            // Assert
            Assert.AreEqual(1, errCount);
        }

",
76383286,XPath that returns true when at least one element matches,"Let's assume we have the following XML response:
<People>
    <Person>
        <Age>29</Age>
    </Person>
    <Person>
        <Age>25</Age>
    </Person>
    <Person>
        <Age>18</Age>
    </Person>
    <Person>
        <Age>45</Age>
    </Person>
</People>

I want an xpath 2.0 expression that will return true if there is at least one person with age between 18 and 22.
My current expression is:
boolean(//*:Person[xs:integer(substring(//*[local-name() = 'Age']/text(), 2)) >= 18 and 22 >= xs:integer(substring(//*[local-name() = 'Age']/text(), 2))])

But this expression is not recursive so it produces the following error:

A sequence of more than one item is not allowed as the first argument of substring() (""29"", ""25"", ...)

Any idea as to how I can achieve what I need?
",76383533,"In XPath 2.0 this is exists(//Person[Age = (18 to 22)])
",
76385245,Strange behavior of str_replace,"I have a user-settable text, where the default one is [Log in] or [register] to view the content.
What I need, is to wrap the two words in square brackets in their respective links. But first, I need to check that the user didn't change this default text, in other words that they kept the square brackets. I won't go in great lengths in checking this. Just the existence of two sets of square brackets is enough. If that's the case, then I'll assume that the first link is for the login page, and the second is for the register-an-account page...
So, the if below does the job for me:
if ( preg_match( '/\[(.*?)\].*\[(.*?)\]/', $text ) )

Then, inside the if, my plan was to perform a str_replace() with two arrays like this:
$text = str_replace( array( '[', ']', '[', ']' ), array( '%1$s', '%2$s', '%3$s', '%4$s' ), $text );

But this doesn't work the way I thought it would. I thought that since the two arrays have equal number of elements it'd do a 1-on-1 search and replace, meaning that it would turn the text to %1$sLog in%2$s or %3$sregister%4$s to view the content, whereas it turned to %1$sLog in%2$s or %1$sregister%2$s to view the content.
Why is that? If that's not the proper way to do that (which obviously isn't), what should I do instead?
Any help would be very much appreciated. TIA.
",76385381,"Try using preg_replace with your same pattern (with additional capture):
$text = preg_replace('/\[(.*?)\](.*)\[(.*?)\]/', '%1$s$1%2$s$2%3$s$3%4$s', $text); 

which produces
%1$sLog in%2$s or %3$sregister%4$s to view the content

The str_replace does not work the way you intended - the first array is an array of needles which has no sense of position so the second set of [] are duplicate needles in your case.
See tester
",
76383155,Fluid nodes list layout with JavaFX,"I use ListView to dynamically display items in my JavaFX app. Items are loaded through REST call to my backend app. Each item can be clicked and then product view is displayed instead of product list. That works (looks) fine until app window is resized. After resize, items look ugly and they use too much space.
The question: Is there a way to get some kind of fluid item order?
In HTML and CSS that would be Flexbox if I remember well.
All items would be the same width and the same height not giving a chance to calculate width or height for each item separately.
The only solution I found on the internet is here: https://github.com/onexip/FlexBoxFX -
but it uses FXML files only and there is no option to add items dynamically. The last project update is 6 years ago which tells me it's abandoned or poorly maintained. Their official website is dead: http://flexboxfx.io
EDIT: As James_D mentioned, I don't need ListView but any other solution that works.
Also I am aware of WebView but I would like to avoid HTML content in my app.
To make my case more clear, I made some screenshots and the first one is edited to represent the idea what I want. Numbers on edited screenshot represent desired order of items.
If window grows more in width, first row should have items 1, 2 and 3, next row 4, 5 and 6, next row 7, 8 and 9 and last row should have only one item (10). All rows should be centered and item 10 of last row should be positioned below item 8.

This is the final layout I want to get but I don't know how.


Everything is nice when window is not resized, but after resize, it looks ugly.


",76383536,"The primary purpose of a ListView is to provide virtualization; i.e. it provides an efficient mechanism to display a large number of items, letting the user scroll through them, without the overhead of UI components for the items that are not currently displayed. It also provides some additional functionality, such as selection (allowing the user to put one or more items in the list into a ""selected"" state which is shown visually).
If you actually need virtualization, and perhaps selection, and want a grid-like layout, then the third-party library ControlsFX provides a similarly-virtualized GridView.
However, your question appears to be only about layout and your screenshots appear to show you using a Pagination control, which would (probably) obviate the need for virtualization anyway. If you don't need the functionality of the ListView, then a standard layout pane such as FlowPane or TilePane, possibly wrapped in a ScrollPane, should provide the layout you need.
",
76382044,Factory class in Python with a mapping dictionary returns TypeError,"I made something like this dummy class:
class CreateCaseFactory:
    @classmethod
    def create(cls, user_id: uuid.UUID, type_: str) -> str:
        creator = cls.CASE_TO_METHOD_MAP.get(type_)
        if creator:
            return creator(user_id)
        else:
            raise Exception(""Invalid type"")

    @classmethod
    def _create_case_1(cls, user_id: uuid.UUID) -> str:
        result = f""Dummy Use Case 1 created for user {user_id}""
        return result

    @classmethod
    def _create_case_2(cls, user_id: uuid.UUID) -> str:
        result = f""Dummy Use Case 2 created for user {user_id}""
        return result

    CASE_TO_METHOD_MAP = {
        ""case_1"": _create_case_1,
        ""case_2"": _create_case_2,
    }

but I get an error when I try to run it:
        if creator:
>           return creator(user_id)
E           TypeError: 'classmethod' object is not callable

How can I make this factory class work.
",76382185,"As the error message says, instances of classmethod are not callable. When you call a class method with something like CreateCaseFactory.create(...), the descriptor protocol ""extracts"" the underlying function from the class method and calls it with CreateCaseFactory as the first argument.
create_case_1 and _create_case_2 should not be class methods, but regular functions.
class CreateCaseFactory:
    @classmethod
    def create(cls, user_id: uuid.UUID, type_: str) -> str:
        creator = cls.CASE_TO_METHOD_MAP.get(type_)
        if creator:
            return creator(cls, user_id)
        else:
            raise Exception(""Invalid type"")

    def _create_case_1(cls, user_id: uuid.UUID) -> str:
        result = f""Dummy Use Case 1 created for user {user_id}""
        return result

    def _create_case_2(cls, user_id: uuid.UUID) -> str:
        result = f""Dummy Use Case 2 created for user {user_id}""
        return result

    CASE_TO_METHOD_MAP = {
        ""case_1"": _create_case_1,
        ""case_2"": _create_case_2,
    }

",
76384865,PathFinding closest Player,"Why Isn't Pathfinding working I'm new to scripting and This just doesn't make sense to me I Understan my other code, and I've read the documentation but when it comes to integrating the pathfinding so he will only find/create a path when he had located the nearest player and follow that path has me stumped. To be honest after reading the documentation I would have figured I could just declare the path inside the if statement gauging if the player is close enough and he would follow the path but I looked up a tutorial and he used the GetWayPoints() but I figured that was to be to no avail so far this is my latest attempt
local runService = game:GetService(""RunService"") -- Run Service sort of like unitys Update frame by frame, SEE DOCUMENTATION SAVED FOLDER
local players = game:GetService(""Players"") -- Players will help ""get"" the players in the game

local humanoid = script.Parent -- grabs the parent of the script which is humanoid
local root = humanoid.Parent.PrimaryPart --root is the humanoids parent i.e models primary part which is typically the humanoid root part

local PathfindingService = game:GetService(""PathfindingService""); -- A path finding service



local wantedDistance = 30 --  How far he can search or should be trying to search, The value now is small testing needed
local stopDistance = 5 -- In caase we want to use this to make him stop (like if he had a kill radius instead of touching)

local damage = 50
local attackDistance = 8
local attackWait = 1
local lastAttack = tick()
function findNearestPlaya()
    local playerList = players:GetPlayers()
    
    local playerNearest = nil
    local dist = nil
    local direction = nil
    
    for _, player in pairs(playerList) do -- basically a for each loop that says for each player in the list of players
        local character = player.Character 
        if character  then -- will only eggsacute if a player/character exists
            local distanceV = player.Character.HumanoidRootPart.Position - root.Position -- Distance  ''Vector'' equals the distance from the player torso/Root position minus the distance of the models primary part which we have as root
            
        
        
            
            
            if not playerNearest then
                playerNearest = player

                dist = distanceV.Magnitude -- distance vector magnitude gives us the actual distance
                direction = distanceV.Unit -- direction of the nearest player, SEE DOCUMENTATIOIN FOR UNIT AND MAGNITUDE

            elseif distanceV.Magnitude < dist then -- If the player is closer than the set nearest player then ''replace'' the player

                playerNearest = player -- resets to new player
                dist = distanceV.Magnitude
                direction = distanceV.Unit

            end
        end
    
    end
    
    return playerNearest, dist, direction -- function so return which playa and his distance and direction
end

-- Another call from the runService class that runs every ""physics frame"", Which I think is like the updateBefore in unity
runService.Heartbeat:Connect(function() -- lua thing essentially this odd function call thing is just an anonymous function meaning it will execute every heartbeat
local path = PathfindingService:CreatePath()
local playerNearest, distance, direction = findNearestPlaya()

if the distance is within range of the wanted distance
if playerNearest  then
if distance <=wantedDistance and distance >= stopDistance then
path:ComputeAsync(humanoid.PrimaryPart.Position, playerNearest.PrimaryPart.Position)
local waypoints = path:GetWaypoints()
for _, waypoint in pairs(waypoints)  do
humanoid.MoveTo(waypoint.Position)
end

else 
humanoid:Move(Vector3.new()) -- 


end

if distance <= attackDistance and tick() - lastAttack >= attackWait then
lastAttack = tick();
playerNearest.Character.Humanoid.Health -= damage
end
end


end)
    

I was expecting him to calculate the path to the nearest player hints my code player nearest. I really dont understand why that or just telling him to move to path after calculating it wouldnt work
",76385382,"Here is the solved version comments should do a good job of explaining but essentially I was just being dumb I was harping to much on the direction as long as you ommit that you can essentially guide the NPC as long as he is in range instead of updating every player playerNearest path,  the heartbeat function is already handling that all that's required is to create a new path for each player that is nearest then guide him along that path. The script isn't perfect  but this will find the nearest player and create a path for the NPC to follow each time for a different nearest player (I'm bad at explanations and I've dragged this on for too long just read the comments)
local runService = game:GetService(""RunService"") -- Run Service sort of like unitys Update frame by frame, SEE DOCUMENTATION SAVED FOLDER
local players = game:GetService(""Players"") -- Players will help ""get"" the players in the game

local humanoid = script.Parent -- grabs the parent of the script which is humanoid
local root = humanoid.Parent.PrimaryPart --root is the humanoids parent i.e models primary part which is typically the humanoid root part

local PathfindingService = game:GetService(""PathfindingService""); -- A path finding service



local wantedDistance = 30 --  How far he can search or should be trying to search, The value now is small testing needed
local stopDistance = 5 -- In caase we want to use this to make him stop (like if he had a kill radius instead of touching)

local damage = 50
local attackDistance = 8
local attackWait = 1
local lastAttack = tick()






function findNearestPlaya()
    local playerList = players:GetPlayers()

    local playerNearest = nil
    local dist = nil
    local direction = nil   
    for _, player in pairs(playerList) do -- basically a for each loop that says for each player in the list of players
        local character = player.Character 
        if character  then -- will only eggsacute if a player/character exists
            local distanceV = player.Character.HumanoidRootPart.Position - root.Position -- Distance  ''Vector'' equals the distance from the player torso/Root position minus the distance of the models primary part which we have as root





            if not playerNearest then
                playerNearest = player

                dist = distanceV.Magnitude -- distance vector magnitude gives us the actual distance
                direction = distanceV.Unit -- direction of the nearest player, SEE DOCUMENTATIOIN FOR UNIT AND MAGNITUDE

            elseif distanceV.Magnitude < dist then -- If the player is closer than the set nearest player then ''replace'' the player

                playerNearest = player -- resets to new player
                dist = distanceV.Magnitude
                direction = distanceV.Unit

            end
        end

    end

    return playerNearest, dist, direction -- function so return which playa and his distance and direction
end

-- Another call from the runService class that runs every ""physics frame"", Which I think is like the updateBefore in unity
runService.Heartbeat:Connect(function() -- lua thing essentially this odd function call thing is just an anonymous function meaning it will execute every heartbeat


    local playerNearest, distance, direction = findNearestPlaya()   
    -- if the distance is within range of the wanted distance
    if playerNearest and distance <= wantedDistance  then
        local path = PathfindingService:CreatePath(); -- Now if hes in range of the player create a new path
        path:ComputeAsync(root.Position, playerNearest.Character.HumanoidRootPart.Position) -- compute the path with the positions of the player and Dr. Sturgeon
        local waypoints = path:GetWaypoints()  -- Assign a new waypoint and and gathers the paths 
        -- standard for each loop for the nodes SEE DOCUMENTATION/TUTORIALS TO UNDERSTAND ''NODES''
        for _, waypoint in pairs(waypoints)  do 
            humanoid:MoveTo(waypoint.Position) -- For each waypoint/node move Dr. sturgeon to it until it reaches the computed path i.e the distance from the player to the robot vice a versa
            humanoid.MoveToFinished:Wait()
        end
    end


    if distance <= attackDistance and tick() - lastAttack >= attackWait then
        lastAttack = tick();
        playerNearest.Character.Humanoid.Health -= damage
    end

end)
        

",
76383514,Pass list to Table.RemoveColumns,"I am looking for a way to pass a list to the Table.RemoveColumns() step in Power Query.
Overview of the set up, two tables as data sources, one is a config table with all the column names of the second data source with simple 'yes' 'no' selectors identifying which columns should be kept/removed. This table is used as a data source, filtered by 'no', and drilled down as a list like so:

I am looking for a way to pass that list to a step to remove columns in my 'data' source:
So the step to remove columns:
= Table.RemoveColumns(Source,{""InvoiceDate"", ""T/S Start Date"", ""TotalBreakMinutes""})
Would become:
= Table.RemoveColumns(Source,{cols})
However you can't pass a list to an argument that expects text. I tried a few work arounds like adding a prefix "" and suffix "" to each list item and using Text.Combine with a comma separator however Table.RemoveColumns step handles the string as a single column

Is there a way to pass that list as a recognisable condition for Table.RemoveColumns()?
",76383541,#ERROR!,
76382000,Swagger: how to use the generated json files?,"I've been provided with the json files generated by swashbuckle for a rest api I should be consuming and I was wondering if there are tools that can take those files as input and allow an easier navigation of exposes methods, request payloads, response payloads, headers, etc. Also when working in .NET is there a way or tool to generate payload classes as with wsdl documents?
",76382205,"You can use the swagger editor
https://editor.swagger.io/
This will allow you to view and browse the methods.  Simply paste the contents of your received JSON.
Then also, at the top of the page you have ""generate client"" options for different languages.  Which will generate C# (or other) langauge files for you.
",
76378414,Add New Polygon to Dash Leaflet Map via a Callback,"Im very new to working with GIS data (using Dash Leaflet and GeoPandas) and am currently stumped.
My goal is to create a simple app which does the following:

App starts with an empty dash_leaflet.Map() figure and a numeric input box titled ""Buffer Distance"" (with a default of 100)
User draws a polygon on the map which fires a callback
Callback takes in the GeoJSON data from the map and the ""buffer distance""
Use Geopandas to import the GeoJSON data and create a new polygon which is smaller than the user drawn polygon by ""Buffer Distance""
Pass these 2 polygons (originally drawn & post processed polygon with buffer) back to the map so that both are now displayed on the map

Im having trouble with the last step of pushing the two polygons back the map via some kind of Output
This is the app i am currently working with:
import pandas as pd
from dash import Dash, dcc, html, Input, Output, State
import dash_leaflet as dl
import geopandas as gpd

lat1, lon1 = 36.215487, -81.674006

app = Dash()

input_details = html.Div([
    html.Div([
        html.Div(['Buffer Distance'], style={'width': '37%', 'display': 'inline-block'}),
        dcc.Input(
            value=100,
            id=""buffer-distance"",
            type='number',
            placeholder='Required',
        ),
    ]),
])

default_map_children = [
    dl.TileLayer(),
    dl.FeatureGroup([
        dl.EditControl(id=""edit_control""),
    ]),
    dl.GeoJSON(id='map-geojsons')
]

map_input_results_tab = html.Div(
    [
        html.H2('Add Shapes to Map an Area of Interest'),
        dl.Map(
            id='leaflet-map',
            style={'width': '100%', 'height': '50vh'},
            center=[lat1, lon1],
            zoom=16,
            children=default_map_children
        )
    ])

app.layout = html.Div([input_details, map_input_results_tab])


@app.callback(
    Output('map-geojsons', 'data'),
    Input('edit_control', 'geojson'),
    State('buffer-distance', 'value'),
)
def update_estimates(drawn_geojson, perim_clear):
    if any([x is None for x in [drawn_geojson, perim_clear]]):
        # some value has not been provided, so do not continue with calculations
        return drawn_geojson
    elif not drawn_geojson[""features""]:
        # some value has not been provided, so do not continue with calculations
        return drawn_geojson

    gdf = gpd.GeoDataFrame.from_features(drawn_geojson[""features""])  # extract user drawn geometry data from UI
    gdf = gdf.set_crs(crs=4326)  # Set the initial CRS to specify that this is lat/lon data
    gdf = gdf.to_crs(
        crs=gdf.estimate_utm_crs())  # Let GeoPandas estimate the best CRS and use that for the area calculation

    # create a new geodataframe using buffer that incorporates the perimeter
    gdf_minus_perim_buffer = gdf['geometry'].buffer(-perim_clear)
    combine_gdf = pd.concat([gdf['geometry'], gdf_minus_perim_buffer])
    # convert back to lat, long
    combine_gdf = combine_gdf.to_crs(crs=4326)
    # convert back to GeoJSON to be rendered in the dash leaflet map
    return_geojson_data = combine_gdf.to_json()

    return return_geojson_data


if __name__ == '__main__':
    app.run_server(debug=True, port=8052)

I think I am close, but am just missing something.. Thanks in advance for any help!
",76383565,"It looks like the callback approach above is valid, I was just providing the wrong data type back to the dl.GeoJSON's data attribute .
Changing this line:
# convert back to GeoJSON to be rendered in the dash leaflet map
return_geojson_data = combine_gdf.to_json()

to
# convert back to GeoJSON to be rendered in the dash leaflet map
return_geojson_data = combine_gdf.__geo_interface__

worked perfectly!
",
76385364,"Updating ""anyarray"" or ""anyelement"" polymorphic functions when upgrading to 14.x or higher on AWS RDS aurora postgresql","When upgrading AWS RDS aurora postgresql cluster from 11.17 -> 15.2, I was met with this fatal error in the pg_upgrade logs:
fatal
Your installation contains user-defined objects that refer to internal
polymorphic functions with arguments of type ""anyarray"" or ""anyelement"".
These user-defined objects must be dropped before upgrading and restored
afterwards, changing them to refer to the new corresponding functions with
arguments of type ""anycompatiblearray"" and ""anycompatible"".

AWS does not mention this in the upgrade docs, so I thought the changed may have been introduced by a system user. After a bit of digging, it seems that the aggregate functions changed the way the types are named (in postgresql version 14 to be clear). So how do I update this?
I ran a subset the query that the upgrade failed on, on each DB in the target cluster:
--find incompatibilites on each DB:
\c <DATABASE>

SELECT 'aggregate' AS objkind,
       p.oid::regprocedure::text AS objname
FROM pg_proc AS p
JOIN pg_aggregate AS a ON a.aggfnoid=p.oid
JOIN pg_proc AS transfn ON transfn.oid=a.aggtransfn
WHERE p.oid >= 16384
  AND a.aggtransfn = ANY(ARRAY['array_append(anyarray,anyelement)', 'array_cat(anyarray,anyarray)', 'array_prepend(anyelement,anyarray)', 'array_remove(anyarray,anyelement)', 'array_replace(anyarray,anyelement,anyelement)', 'array_position(anyarray,anyelement)', 'array_position(anyarray,anyelement,integer)', 'array_positions(anyarray,anyelement)', 'width_bucket(anyelement,anyarray)']::regprocedure[]);

  objkind  |         objname         
-----------+-------------------------
 aggregate | array_accum(anyelement)
(1 row)

Okay, so now what?
",76385392,"Solution:
--drop aggregate from sub 14.x db
mygreatdatabase=> DROP AGGREGATE array_accum(anyelement);
DROP AGGREGATE

--upgrade to 14.x or higher, and then re-create using updated type:
mygreatdatabase=> CREATE AGGREGATE array_accum(anycompatible) (SFUNC = array_append,STYPE = anycompatiblearray,INITCOND = '{}');

My hope is that AWS adds this to the documentation on RDS Aurora PostgresQL Upgrade Pre-Checks, but this will be here until that is more clear.
",
76382135,How to delete rows based on columns' value?,"I am trying to delete rows based on one columns value! But the column length of the range in the worksheet is dynamic and large.
For example,
If Col C has value less than or equal to 0 that row gets deleted




A
B
C




1
SAM
100


1
SAM
0


1
BRI
-100


1
HAWK
100




It should only give me :




A
B
C




1
SAM
100


1
HAWK
100



",76382208,"as said in my comment,try this:
Sub test()
Dim LR As Long
Dim i As Long

LR = Range(""A"" & Rows.Count).End(xlUp).Row 'get last non blank row number

For i = LR To 1 Step -1 'go backwards starting at LR until row 1
    If Range(""C"" & i).Value <= 0 Then Range(""C"" & i).EntireRow.Delete
Next i


End Sub

Before code:

After code executed:

",
76383277,My bootstrap column not working on small screen,"My buttons inside bootstrap columns not appearing when the screen size is small.
I wanted the buttons to appear one below the other when screen size is small.
What changes should I make to get my buttons one below each other on a small screen.
full screen
small screen
adding the html code below:
<body>
  <div class=""top"">
    <div class=""content"">
      <h1>Welcome To <br /><span class=""fancy"">Fantasy Talk</span></h1>
      <div class=""row"">
        <div class=""col-md-4"">
          <button onClick=""window.location.href='dothraki.html';"">
            Dothraki
          </button>
        </div>

        <div class=""col-md-4"">
          <button onClick=""window.location.href='valyrian.html'; "">
            Valyrian
          </button>
        </div>

        <div class=""col-md-4"">
          <button onClick=""window.location.href='sindarin.html';"">
            Sindarin
          </button>
        </div>
      </div>
    </div>
  </div>
</body>

**Adding CSS code: **
body{
    margin: 0;
    padding: 0;
    width: 100%;
    height: 100vh;
   /* cursor: url(https://cur.cursors-4u.net/games/gam-11/gam1090.png),auto; */
   
   cursor: url(https://cur.cursors-4u.net/games/gam-13/gam1229.png),auto;
 }
 .top{
     width: 100%;
     height: 100%;
     padding: 2rem;
 
     
     position:absolute;
     background-image: url(image/img1.jpg);
     background-position: center;
     background-size: cover;
     text-align: center;
     justify-content: center;
     animation: change 13s infinite ease-in-out;
 }
 button{
     font-family: 'Almendra SC', serif;
     transition: 0.5s;
     padding: 15px 60px;
     text-decoration: none;
     font-size: 2vw;
 position: absolute;
     border-radius: 5px;
     
     top: 50%;
     transform: translate(-50%, -50%);
 border: 1px;
     transition: all 0.2s ease-in-out;
     color: rgba(255, 255, 255, 0.8);
     background: #146C94;
 }
 button:hover{
   
     margin-top: -10px;
     color: rgba(255, 255, 255, 1);
     /* box-shadow: 0 5px 15px rgba(145, 92, 182, .4); */
     box-shadow: 0 5px 15px #39B5E0;
 }

 @keyframes change{
 
     0%{
         background-image: url(image/img6.jpg);
     }
     20%{
         background-image: url(image/img2.jpg);
     }
     40%{
         background-image: url(image/img3.jpg);
     }
     60%{
         background-image: url(image/img4.jpg);
     }
     80%{
         background-image: url(image/img5.jpg);
     }
     100%{
         background-image: url(image/img6.jpg);
     }
 }
h1{
    padding: 2rem;
    color:white;
    font-family: 'Spirax', cursive;

    font-size: 5vw;
    text-transform: uppercase;
    text-align: center;
    line-height: 1;
}
.fancy{
    font-size: 8vw;
}

",76383567,"Instead of trying to position each button individually in the middle of the page, position the entire row of buttons. This will allow you to use Bootstrap columns better. You were also missing the the extra-small (col-12) and small (col-sm-12) column breakpoints.
Replace your HTML with this (Bootstrap 5):
<div class=""row gy-3 position-absolute top-50 start-50 translate-middle w-100"">
    <div class=""col-12 col-sm-12 col-md-4 text-center"">
        <button onClick=""window.location.href='dothraki.html';""> Dothraki</button>
    </div>
    <div class=""col-12 col-sm-12 col-md-4 text-center"">
        <button onClick=""window.location.href='valyrian.html';"">Valyrian</button>
    </div>
    <div class=""col-12 col-sm-12 col-md-4 text-center"">
        <button onClick=""window.location.href='sindarin.html';"">Sindarin</button>
    </div>
</div>

And then replace your button CSS with this (removing the button positioning):
button {
    font-family: 'Almendra SC', serif;
    transition: 0.5s;
    padding: 15px 60px;
    text-decoration: none;
    font-size: 2vw;
    border-radius: 5px;
    border: 1px;
    transition: all 0.2s ease-in-out;
    color: rgba(255, 255, 255, 0.8);
    background: #146C94;
}

Also, I recommend you replace your buttons with Anchor tags if they are only going to take the user to a page.
",
76383962,Drag and drop not functioning on dynamically created list,"I have a list of activities that is generated dynamically with javascript in the following manner:
const renderList = (activities) => {
  const display = document.getElementById('task-list-display');
  activities.forEach((activity) => {
    console.log(activity);
    display.insertAdjacentHTML('beforeend', ` 
    <li class=""task-item draggable"" draggable=""true"">
      <div class=""chk-descr"">
        <input data-a1=""${activity.index}"" type=""checkbox"" name=""completed""/>
        <p data-b1=""${activity.index}"" class=""description"" contenteditable=""true"">${activity.description}</p>
      </div>
    </li> 
    `);

I want to have it be responsive in a way that the items can be rearranged using drag and drop. I am not able, however, to make this work. Previously I had designed the very same app but instead of having the items of the list be inserted using insertAdjacentHTML() I was creating each element using createElement() and then appending it to the corresponding HTML element using appendChild(). The drag and drop functionality on that app was fully functioning. My question is: is there some reason why drag and drop might not work with a dynamically generated list using insertAdjacentHTML?.
Here is all the relevant code:
let activities = [];

const dragstart = (element) => {
  element.classList.add('skateover');
};

const dragover = (element, e) => {
  e.preventDefault();
  element.classList.add('dragover');
};

const dragleave = (element) => {
  element.classList.remove('dragover');
};

const drop = (element) => {
  const skateover = document.querySelector('.skateover');
  element.before(skateover);

  repopulateList();
  element.classList.remove('dragover');
};

const dragend = (element) => {
  element.classList.remove('skateover');
};

const repopulateList = () => {
  const listItems = document.querySelectorAll('.draggable');
  emptyList();

  let i = 0;
  listItems.forEach((listItem) => {
    listItem.setAttribute('activity', i);
    i += 1;
    
    const description = listItem.getElementsByClassName('description')[0].textContent;
    const completed = listItem.getElementsByClassName('completed')[0].checked;
    const index = listItem.getAttribute('activity');

    inputActivity(description, completed, index);
  });
};

const inputActivity = (description, completed, index) => {
  activities.push({ description, completed, index: parseInt(index, 10) });
};

And in the HTML file:
<ul id=""task-list-display""></ul>

",76385414,"That is not a problem. Here I add the eventlisteners to the unordered list (<ul>). So, the adding, cloning and removing of list items (<li>) is not an issue.
There is no problem in using methods like insertAdjacentHTML(). In this example I just use cloneNode() for cloning the node that is moved and then insertBefore() to insert the cloned node before the list item that is hovered/dropped on.


const aktivities = [{
    index: 1,
    description: ""Item 1""
  },
  {
    index: 2,
    description: ""Item 2""
  },
  {
    index: 3,
    description: ""Item 3""
  }
];

const display = document.getElementById('task-list-display');

const renderList = (activities) => {
  activities.forEach((activity) => {
    display.insertAdjacentHTML('beforeend', ` 
    <li class=""task-item draggable"" draggable=""true"" data-id=""${activity.index}"">
      <div class=""chk-descr"">
        <input data-a1=""${activity.index}"" type=""checkbox"" name=""completed""/>
        <p data-b1=""${activity.index}"" class=""description"" contenteditable=""true"">${activity.description}</p>
      </div>
    </li> 
    `)
  });
};

display.addEventListener(""dragstart"", e => {
  e.dataTransfer.setData(""text/plain"", e.target.dataset.id);
});

display.addEventListener(""dragover"", e => {
  e.preventDefault();
  [...display.querySelectorAll('li')].forEach(li => li.classList.remove('over'));
  e.target.closest('li.task-item').classList.add('over');
});

display.addEventListener(""drop"", e => {
  e.preventDefault();
  [...display.querySelectorAll('li')].forEach(li => li.classList.remove('over'));
  let original = document.querySelector(`li[data-id=""${e.dataTransfer.getData(""text/plain"")}""]`);
  let clone = original.cloneNode(true);
  let target = e.target.closest('li.task-item');
  display.insertBefore(clone, target);
  display.removeChild(original);
});


renderList(aktivities);
ul {
  margin: 0;
  padding: 0;
  list-style: none;
}

li div {
  display: flex;
  flex-direction: row;
}

.over {
  border-top: solid thin black;
}
<ul id=""task-list-display""></ul>



",
76383234,Typescript returns unknown for generic only on undefined input,"I'm trying to write a generic env field getter function and currently have this:
export interface Config {
  readonly PORT: number;
  readonly DATABASE_URL: string;
  // ... other fields
}
const config: Config = Object.freeze({
  ENVIRONMENT,
  PROJECT_NAME,
  PORT: parseInt(getEnvVariable('PORT', '9000'), 10),
  DATABASE_URL: getEnvVariable('DATABASE_URL'),
  AWS_REGION,
});

function getEnvVariable<T>(name: string, defaultValue?: T): T | string {
  const val = process.env[name];

  if (val) {
    return val;
  }

  if (defaultValue) {
    return defaultValue;
  }

  throw new Error(`Missing environment variable: ${name}`);
}

and I'm currently getting the error:
Type 'Readonly<{ ENVIRONMENT: string; PROJECT_NAME: string; PORT: number; DATABASE_URL: unknown; AWS_REGION: string; }>' is not assignable to type 'Config'.
  Types of property 'DATABASE_URL' are incompatible.
    Type 'unknown' is not assignable to type 'string'.


For some reason if my defaultValue input is undefined, it will give this error, even though I do a truthy check.
How would I fix this without using an '' as the defaultValue?
",76383601,"If you don't pass in a defaultValue argument to getEnvVariable, then the compiler has no inference site for the generic type parameter T.  So inference fails, and T falls back to its constraint, which is implicitly the unknown type.
If you'd like T to fall back to something else, you can use a default type argument as shown here:
function getEnvVariable<T = string>( 
// default type arg ----> ^^^^^^^^ 
  name: string, defaultValue?: T
): T | string { /* impl */ }

Then you'll get the desired behavior when defaultValue isn't supplied:
const unsuppliedDefault = getEnvVariable(""Y"");
// const unsuppliedDefault: string

without changing the behavior when it is:
const suppliedDefault = getEnvVariable(""X"", Math.random());
// const suppliedDefault: string | number

Playground link to code
",
76382016,Recursive object construction,"Task: to create a tournament bracket according to the double elimination system. For the upper bracket, there were no problems, since the teams that won in the first round meet in the second, and so on. But for the lower bracket, in addition to the games among the losers in the first round, you need to add games among the losers in the second round of the upper bracket.
For example, given an array of lower bracket games:
const games = [
  { id: 1, home_name: ""Team 1"", visitor_name: ""Team 3"", home_score: 1, visitor_score: 0, round: 2 },
  { id: 2, home_name: ""Team 6"", visitor_name: ""Team 7"", home_score: 1, visitor_score: 0, round: 2 },
  { id: 3, home_name: ""Team 1"", visitor_name: ""Team 6"", home_score: 0, visitor_score: 1, round: 3 },
  { id: 4, home_name: ""Team 4"", visitor_name: ""Team 5"", home_score: 1, visitor_score: 0, round: 3 },
  { id: 5, home_name: ""Team 6"", visitor_name: ""Team 4"", home_score: 1, visitor_score: 0, round: 4 },
];

To display the structure of the lower bracket, you need to get the following object:
{
 name: ""Team 6"",
 children: [
  {
   name: ""Team 4"",
   children: [{ name: ""Team 4"" }, { name: ""Team 5"" }],
  },
  {
   name: ""Team 6"",
   children: [
    { name: ""Team 1"", children: [{ name: ""Team 1"" }, { name: ""Team 3"" }] },
    { name: ""Team 6"", children: [{ name: ""Team 6"" }, { name: ""Team 7"" }] },
   ],
  },
 ],
}

In the parent object ""Team 6"" is the winner of the lower bracket, this can be understood from the last (4) round played between Team 4 and Team 6, in turn Team 4 is determined by the game between Team 4 and Team 5 in round 3.
What the lower bracket will look like for these games:

How to get such an object from the games array? The number of games may vary depending on the number of teams.
",76382233,"You could use a plain object to collect subtrees keyed by the winning team's name, starting out with an empty object. Then iterate the games in order of round and look up the two team's subtrees from that object (with a default {name} object). Then construct the children property from that and wrap it into a new root node. Register that node in the collection of subtrees.
Finally retain the last object that was created which will have the whole tree:


function getTree(games) {
    // Ensure the data is sorted by round -- if this is already ensured by caller, then you can drop this statement:
    games = [...games].sort((a, b) => a.round - b.round);
    
    const nodes = {}; // This will collect the subtrees, keyed by the winning team's name.
    let winner;
    for (const {home_name, home_score, visitor_name} of games) {
        const name = home_score ? home_name : visitor_name;
        winner = nodes[name] = {
            name,
            children: [
                nodes[home_name] ?? { name: home_name }, 
                nodes[visitor_name] ?? { name: visitor_name }
            ]
        };
    }
    return winner;
}

const games = [
  { id: 1, home_name: ""Team 1"", visitor_name: ""Team 3"", home_score: 1, visitor_score: 0, round: 2 },
  { id: 2, home_name: ""Team 6"", visitor_name: ""Team 7"", home_score: 1, visitor_score: 0, round: 2 },
  { id: 3, home_name: ""Team 1"", visitor_name: ""Team 6"", home_score: 0, visitor_score: 1, round: 3 },
  { id: 4, home_name: ""Team 4"", visitor_name: ""Team 5"", home_score: 1, visitor_score: 0, round: 3 },
  { id: 5, home_name: ""Team 6"", visitor_name: ""Team 4"", home_score: 1, visitor_score: 0, round: 4 },
];
console.log(getTree(games));



",
76385383,Removing Prefix from column of names in python,"I have this dataset
ID      Name     
101    DR. ADAM SMITH
102    BEN DAVIS
103    MRS. ASHELY JOHNSON
104    DR. CATHY JONES 
105    JOHN DOE SMITH

Desired Output
ID        Name 
101     ADAM SMITH
102     BEN DAVIS
103     ASHELY JOHNSON
104     CATHY JONES
105     JOHN DOE SMITH 

I need to get rid of the prefix I tried df['Name'] = df['Name'].replace(to_replace = 'DR. ', value = '')I repeated the same code for all prefixes, but I have when I do it nothing happens. Any reason for this?
Thank you in advance.
",76385418,"Use a regular expression to match the first word if it ends with ..
df['Name'] = df['Name'].str.replace(r'^[A-Z]+\.\s+', '', regex=True)

",
76383593,Closure might outlive current function even though it is joined,"fn main() {
    let foo = 5;

    std::thread::spawn(|| { // closure may outlive the current function, but it borrows `foo`, which is owned by the current function
        println!(""{}"", foo);
    })
    .join()
    .unwrap();
}

Moving the value is not an option since it have to make multiple threads
The situation in my code is a bit more complicated, but I still need threads and I ended up moving and Arc into it instead of just a reference
Here is a link to the line in the project, but you don't have to read it: https://github.com/Antosser/web-crawler/blob/5d23ffa7ed64c772080c7be08a26bda575028c7c/src/main.rs#L291
",76383613,"The compiler does not know it is joined. It does not apply any special analysis to see if threads are joined.
However, if you join your threads, you can use scoped threads to access variables:
fn main() {
    let foo = 5;

    std::thread::scope(|s| {
        s.spawn(|| {
            println!(""{}"", foo);
        });
        // Thread implicitly joined here.
    });
}

",
76383571,Perform a specific Mathematical Function on each column dynamically in R,"I wanted to perform a mathematical function on each unique item in a data frame dynamically.
Normally to perform a mathematical function, we use mutate statement and create a column and perform the mathematical function manually by writing mutate statement after mutate statement.
Which is feasible on a few columns. But what if I have 100 columns and I have to perform 2-5 mathematical function, For example: one would be 20% increase on the initial number, The other one would be to divide the initial number by 2 on each column and keep the original column as is.
Is this possible in R other than writing mutate statement for each specific item?
The data frame I am working with is:
structure(list(`Row Labels` = c(""2023-03-01"", ""2023-04-01"", ""2023-05-01"", 
""2023-06-01"", ""2023-07-01"", ""2023-08-01"", ""2023-09-01"", ""2023-10-01""
), X6 = c(14, 16, 14, 11, 9, 9, 11, 11), X7 = c(50, 50, 50, 50, 
50, 50, 50, 50), X8 = c(75, 75, 75, 75, 75, 75, 75, 75), X9 = c(100, 
100, 100, 100, 100, 100, 100, 100), X11 = c(25, 25, 50, 75, 125, 
200, 325, 525), X12 = c(50, 50, 100, 150, 250, 400, 650, 1050
)), class = c(""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, 
-8L))

For individual cases this code would suffice:
library(readxl)
library(dplyr)
Book1 <- read_excel(""C:/X/X/X- X/X/Book1.xlsx"",sheet = ""Sheet6"")

dput(Book1)

Book1 <- Book1 %>% 
  mutate(`X6 20%` = X6*1.20) %>% 
  mutate(`X6 by 2`= X6/2)

I was thinking of running this through a loop but then selection of columns to multiple becomes a problem as we have to specify the column name in mutate statement, which I believe would not be possible here right.
Can anyone let me know if this can be achieved in a simple approach?
The expected output is given below:

",76383624,"We could use across()
update: shorter:
library(dplyr)

df %>% 
  mutate(across(2:7, list(""20"" = ~. * 1.20, 
                          ""By_2"" = ~. / 2), .names = ""{col}_{fn}""))

first answer:
library(dplyr)

df %>% 
  mutate(across(2:7, ~. * 1.20, .names = ""{.col}_20%""),
         across(2:7, ~. /2, .names = ""{.col}_By 2""))

  `Row Labels`    X6    X7    X8    X9   X11   X12 `X6_20%` `X7_20%` `X8_20%` `X9_20%` `X11_20%` `X12_20%` `X6_By 2` `X7_By 2` `X8_By 2` `X9_By 2` `X11_By 2` `X12_By 2`
  <chr>        <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>    <dbl>    <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>      <dbl>      <dbl>
1 2023-03-01      14    50    75   100    25    50     16.8       60       90      120        30        60       7          25      37.5        50       12.5         25
2 2023-04-01      16    50    75   100    25    50     19.2       60       90      120        30        60       8          25      37.5        50       12.5         25
3 2023-05-01      14    50    75   100    50   100     16.8       60       90      120        60       120       7          25      37.5        50       25           50
4 2023-06-01      11    50    75   100    75   150     13.2       60       90      120        90       180       5.5        25      37.5        50       37.5         75
5 2023-07-01       9    50    75   100   125   250     10.8       60       90      120       150       300       4.5        25      37.5        50       62.5        125
6 2023-08-01       9    50    75   100   200   400     10.8       60       90      120       240       480       4.5        25      37.5        50      100          200
7 2023-09-01      11    50    75   100   325   650     13.2       60       90      120       390       780       5.5        25      37.5        50      162.         325
8 2023-10-01      11    50    75   100   525  1050     13.2       60       90      120       630      1260       5.5        25      37.5        50      262.         525

",
76382036,Python Local random seed,"I have a random seed set at the start of my run for reproducibility. But there are a few sub-functions (e.g. rando) that also use random numbers. If I used a different random number seed just for those, it affects the random seed outside of the function. Is it possible to set the random seed and use it only locally inside the function and the random state outside the function does not get affected? I believe I can always get the random state, save it and restore it. Would there be an easier option? I showed an example below.
import numpy as np

def rando():
    np.random.seed(420)
    np.random.randint(1, 100)
    np.random.randint(1, 100)
    return None


np.random.seed(69)
for n in range(3):
    np.random.randint(1,100) # outputs : 55,76,74
for n in range(3):
    np.random.randint(1,100) # outputs : 91,56,21

Is it possible to make the function below also output the same thing?
np.random.seed(69)
for n in range(3):
    np.random.randint(1,100) # outputs : 55,76,74
rando()
for n in range(3):
    np.random.randint(1,100) # would like it to output : 91,56,21

",76382266,"That's why there are numpy random generators and that is why they recommend using that. Just define one generator for each instance, e.g.:
def rando(rng):
    print('function')
    print(rng.integers(1, 100))
    print(rng.integers(1, 100))
    print('end of function')
    return None



rng1 = np.random.default_rng(69)
rng2 = np.random.default_rng(420)

for n in range(3):
    print(rng1.integers(1, 100)) # outputs : 6,58,67
rando(rng2) # outputs 62, 77
for n in range(3):
    print(rng1.integers(1, 100)) # would like it to output : 53,78,86

yielding:
6
58
67
function
62
77
end of function
53
78
86

and when you comment out the function call, you get:
6
58
67
53
78
86

",
76385267,Remix session values don't persist across pages,"I'm making a site using Remix where I'd like to persist session values across pages. I think the issue lies in the getSession request, as the values do not persist across requests to the same page. I have implemented a session cookie in sessions.ts:
const { getSession, commitSession, destroySession } =
    createCookieSessionStorage<SessionData, SessionFlashData>(
        {
            //cookie options to create a cookie
            cookie: {
                name: ""__session"",
                maxAge: 1200,
                path: ""/"",
                sameSite: ""none"",
                secure: true,
                secrets: [""surprise""]

            },
        }
    );

On one page I set a value and log it out and receive the expected value
export const loader = async ({ request }: LoaderArgs) => {
    const session = await getSession(
        request.headers.get(""Cookie"")
    );
    session.set(""token"", ""abc123"")
    var data = { ""count"": 2 }
    console.log(session.get(""token""))
    return json(data, {
        headers: {
            ""Set-Cookie"": await commitSession(session),
        },
    });
};

however when i try to access the value in a different page, the value is undefined
export const loader = async ({ request }: LoaderArgs) => {
  const session = await getSession(
    request.headers.get(""Cookie"")
  );
  var data = { ""abc"": 442 }
  console.log(session.get(""token""))
  return json(data, {
    headers: {
      ""Set-Cookie"": await commitSession(session),
    },
  });

  return null


};

I'm very new to remix and react so appreciate any help!
",76385442,"the issue was to do with the sameSite option and Secure options.
as I am working locally, Secure must be set to false which means sameSite must be either lax or strict
",
76381732,How to put 'or' into contraints in Pulp in python,"I have a problem, where I have to find the optimal cost of 3 given motor.
Motor 1 has a range of 100 - 300
Motor 2 has a range of 400 - 1000
Motor 3 has a range of 50 - 250
They have a target value of 600
Motor 1 price is 5000
Motor 2 price is 5500
Motor 3 price is 5250
The equation looks like this:
Cost = Motor1 * 5000 + Motor2 * 5500 + Motor3 * 5250.
And a very important part, NOT every motor needs to run.
I have a python code, that can calculate it, but I can give it to it that not every motors needs to be inclued.
Here is the code:
from pulp import LpProblem, LpVariable, LpMinimize

def find_lowest_cost():
    # Define the problem
    problem = LpProblem(""Motor Optimization"", LpMinimize)

    # Define the decision variables
    x = LpVariable(""Motor1"", lowBound=100, cat='Integer')  # Power of motor 1
    y = LpVariable(""Motor2"", lowBound=0, cat='Integer')  # Power of motor 2
    z = LpVariable(""Motor3"", lowBound=50, cat='Integer')  # Power of motor 3

    # Define the objective function (cost)
    problem += x * 5000 + y * 5500 + z * 5250

    # Define the constraints
    problem += x >= 100  # Motor 1 lower bound
    problem += x <= 300   # Motor 1 upper bound
    problem += y >= 350  # Motor 2 lower bound
    problem += y <= 1000  # Motor 2 upper bound
    problem += z >= 50  # Motor 3 lower bound
    problem += z <= 250  # Motor 3 upper bound
    problem += x + y + z == 500  # Total power constraint

    # Solve the problem
    problem.solve()

    # Retrieve the optimal solution
    lowest_cost = problem.objective.value()
    best_combination = (x.value(), y.value(), z.value())
    return lowest_cost, best_combination

cost, combination = find_lowest_cost()
print(""Lowest cost:"", cost)
print(""Motor combination:"", combination)

I tried to add 'or' to the ""Define the Constraints' part, but it did not help
    problem += x >= 100 or x ==0 # Motor 1 lower bound
    problem += x <= 300  # Motor 1 upper bound
    problem += y >= 350 or y == 0 # Motor 2 lower bound
    problem += y <= 1000  # Motor 2 upper bound
    problem += z >= 50 or z == 0 # Motor 3 lower bound
    problem += z <= 250  # Motor 3 upper bound
    problem += x + y + z == 500  # Total power constraint

So my Questions is, how to implement that 'OR' into my code.
Thank you in advance
",76382299,"I make some assumptions:

Continuous power of motors, not integral
Observe the minima in your variable bounds, not the redundant and inconsistent constraints added later
Use 500 as a target, not 600

You need binary selection variables, like this:
from pulp import LpProblem, LpVariable, LpMinimize, LpContinuous, lpDot, LpBinary, lpSum

powers = (
    LpVariable('Motor1', cat=LpContinuous, upBound=300),
    LpVariable('Motor2', cat=LpContinuous, upBound=1000),
    LpVariable('Motor3', cat=LpContinuous, upBound=250),
)
used = LpVariable.matrix(name='MotorUsed', cat=LpBinary, indices=range(len(powers)))

problem = LpProblem(name='Motor_Optimization', sense=LpMinimize)
problem.objective = lpDot(powers, (5000, 5500, 5250))

problem.addConstraint(name='target', constraint=lpSum(powers) == 500)

for power, power_min, use in zip(
    powers,
    (100, 0, 50),
    used,
):
    problem.addConstraint(power >= power_min*used)
    problem.addConstraint(power <= 1000*used)

problem.solve()
combination = [p.value() for p in powers]
print('Lowest cost:', problem.objective.value())
print('Motor combination:', combination)

Result - Optimal solution found

Objective value:                2550000.00000000
Enumerated nodes:               0
Total iterations:               0
Time (CPU seconds):             0.01
Time (Wallclock seconds):       0.01

Option for printingOptions changed from normal to all
Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01

Lowest cost: 2550000.0
Motor combination: [300.0, 0.0, 200.0]

",
76383521,How to fix - Overflow in int64 addition,"I am trying to calculate future dates by adding a column with number of days df['num_days'] to another column df[""sampling_date""] but getting Overflow in int64 addition.
Source code-
df['sampling_date']=pd.to_datetime(df['sampling_date'], errors='coerce')
df['future_date'] = df['sampling_date'] + pd.to_timedelta(df['num_days'], unit='D')
df['future_date'] = pd.to_datetime(df['future_date']).dt.strftime('%Y-%m-%d')
df['future_date'] = df['future_date'].astype(np.str)
df['future_date'] = np.where(df['num_days']<=0,0, df['future_date'])

for column df['num_days'], the values are as follows [0, 866, 729, 48357555, 567, 478]
I am trying to run this in unix server. Please help me resolving it.
",76383631,"The issue is this value: 48357555
You can create a simple function as shown below to return NaT if error is thrown:
import numpy as np
import pandas as pd

# Here is an example df
df = pd.DataFrame({
    'sampling_date': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01', '2022-06-01'],
    'num_days': [0, 866, 729, 48357555, 567, 478]
})

df['sampling_date'] = pd.to_datetime(df['sampling_date'], errors='coerce')

def calculate_future_date(row):
    try:
        return row['sampling_date'] + pd.to_timedelta(row['num_days'], unit='D')
    except:
        return pd.NaT

# Apply the function to each row
df['future_date'] = df.apply(calculate_future_date, axis=1)
df['future_date'] = np.where(df['num_days'] <= 0, df['sampling_date'], df['future_date'])
df['future_date'] = df['future_date'].dt.strftime('%Y-%m-%d').replace(pd.NaT, '0').astype(str)
print(df)


  sampling_date  num_days future_date
0    2022-01-01         0  2022-01-01
1    2022-02-01       866  2024-06-16
2    2022-03-01       729  2024-02-28
3    2022-04-01  48357555           0
4    2022-05-01       567  2023-11-19
5    2022-06-01       478  2023-09-22

",
76383630,Remove log files on aws s3 (rails7),"Which option is better to remove log files on aws s3 for rails7? s3 automation vs cron job
I wrote some rake tasks.
task :delete_stale_logs do
    s3 = Aws::S3::Resource.new(
      region: ENV['AWS_REGION'],
      access_key_id: ENV['AWS_ACCESS_KEY_ID'],
      secret_access_key: ENV['AWS_SECRET_ACCESS_KEY']
    )

    bucket = s3.bucket(ENV['AWS_BUCKET'])

    bucket.objects.each do |object|
      if object.key.include?('.log') && object.last_modified < Time.now - 30.days
        object.delete
        puts ""Deleted #{object.key}""
      end
    end
  end
end

",76383660,"Cron job is better in case you have some conditional logics for the number of logs, otherwise s3 automation. If you go for cron jobs, you have to handle monitoring to check if jobs fail or not. Check here for more info.
",
76385395,"What could be causing my second React JSX function to fail to return  elements, despite properly filtered data?","I am creating a simple search bar element in React JSX. I'm trying to render a list of elements that include whatever is in a search query. I basically take an array of all the elements and then I use .filter() function to find everything that includes the query. After that I use .map() function to loop through the results and render  elements for each object. I needed to create two different functions for two different datasets as one is an array deeper.
<ul>


{                    
 this.state.searchQuery &&
                                
     this.props.searchDB.projects.filter((project)=> {
         if (this.state.searchQuery === '' || this.state.searchQuery === null) {
            return project;
         } else if (project.projectName.toLowerCase().includes(this.state.searchQuery.toLowerCase())) {
            return project;
         } else {
            return null
         }
     }).map((project, index) => {
         //THIS WORKS AS EXPECTED
         return(
             <li key={'project_' + index}>
                 {index + '_' + project.projectName}
             </li>
         )
    })
                                
}

{
     this.state.searchQuery &&
                                
         this.props.searchDB.mentions.forEach((mentionYear) => {
    
               mentionYear.filter((mention) => {
    
                   if (this.state.searchQuery === '' || this.state.searchQuery === null) {
                     return mention
                   } else if (mention.mentionTitle.toLowerCase().includes(this.state.searchQuery.toLowerCase())) {
                     return mention
                   } else {
                     return null
                   }
              }).map((mention, mentionIndex) => {
                   console.log(mention.mentionTitle)
                   //THIS LOGS DATA AS IT SHOULD BUT DOESN'T RENDER ELEMENTS
                                                
                   return(
                            <li key={'project_' + mentionIndex}>
                               {mentionIndex + '_' + mention.mentionTitle}
                            </li>
                         )
                   })
               }
             ) 
}

</ul>

The first function works fine and returns a  element as it should. For some reason the second one does not and it doesn't return any element at all, even though it is basically the same code. Strange is that the data is there, I can log it from the map function and I can see that it is filtered properly. Can someone explain to me what's wrong? I tried quite a lot of possibile mistakes I could have made but I didn't find anything.
",76385443,"The second version never does anything with the result of .map().  It's invoked inside a forEach() callback, but its result is discarded.  Contrast that to the first version where the result of .map() is part of the JSX and rendered.
Don't use forEach() for this.  If the intent is that the .forEach() iteration should produce a result just like in the first version, then what you want isn't forEach().  What you want is .map().
For example:
{
  this.state.searchQuery &&
    this.props.searchDB.mentions.map((mentionYear) => {
      return mentionYear.filter((mention) => {
        // etc.
      }).map((mention, mentionIndex) => {
        // etc.
      });
    })
}

Basically, repeat the same pattern/structure you already know works when iterating over a collection in JSX to produce a rendered result.
",
76383595,Is there a way to get the key at a specified position?,"I wanted to print out a key at a specific position (like 1) in a dictionary, but the code didn't seem to work at all.
    Hobbies={
      football(american):1
      baseball:2
      basketball:3
      playing_cards:4
      swimming:5
      soccer:7
   }

I used this line :
     print (Hobbies[1])

But it got an error
How should I fix it?
",76383682,"First off, you probably shouldn't do this, because this is not how dictionaries were intended to be accessed.
But if you really need to do this, probably the most straightforward way is to get the list of keys from the dictionary, and then access the dictionary using the first key.
Something like:
list_of_keys = [key for key in Hobbies.keys()]
key_of_interest = list_of_keys[0]
value_of_interest = Hobbies[key_of_interest]

Or as a one-liner:
value_of_interest = Hobbies[[key for key in Hobbies.keys()][0]]

This may also work, but I'm not sure if the order of values is guaranteed the same way the order of keys is.  It probably is, but I can't say for sure:
value_of_interest = [value for value in Hobbies.values()][0]

",
76382170,How can I resolve TS7015 in my TypeScript/Next.js/React project when using an index expression that is not of type number?,"I Got Element implicitly has an 'any' type because index expression is not of type 'number'.ts(7015) error
I have Improted this file
import { useAmazon } from ""@context/amazon"";
I have Used hear
const { data, current_product } = useAmazon();
and Got error
const main_image = data[current_product].main_image; // error in current_product
@context/amazon file
import { JSObject } from ""@classes/JSObject"";
import React, { useState, useEffect, useContext, createContext } from ""react"";
import axios from ""axios"";
import { Product } from ""types"";

type AmazonContextType = {
  data: Product[] | null;
  current_product: string;
  setProduct: (product: string) => void;
  offers: JSObject | null;
  setOffers: (offer: JSObject | null) => void;
};

type Props = {
  children: React.ReactNode;
};

const amazonDefault: AmazonContextType = {
  data: null,
  current_product: '',
  setProduct: () => undefined,
  offers: null,
  setOffers: () => undefined,
};

const AmazonContext = createContext<AmazonContextType>(amazonDefault);

export function useAmazon() {
  return useContext(AmazonContext);
}

export function AmazonProvider({ children }: Props) {
  const [data, setData] = useState(amazonDefault.data);
  const [current_product, setProduct] = useState(amazonDefault.current_product);
  const [offers, setOffers] = useState(amazonDefault.offers);

  useEffect(() => {
    const fetchData = async () => {
      const url =
        ""https://getdata.com/abc"";
      const result = await axios(url);
      setData(result.data.payload);
      
      
    };
    fetchData();
  }, []);

  return (
    
    <AmazonContext.Provider
      value={{ data, current_product, setProduct, offers, setOffers }}
    >
      
      {children}
    </AmazonContext.Provider>
  );
}

How i can Solve this Error
const main_image = typeof current_product === 'string' ? data[current_product as keyof typeof data].main_image : null; 
I have also try chatGPT and google bard
keyword
typescript
next.js
reactjs
javascript
json
",76382315,"You can't access the item by a property of the object using the square bracket syntax.
Assuming that current_product is the name of the product (might be id, but I can't see the Product type), you need to do the following:
const main_image = data !== null ? data.find(product => product === current_product).main_image : null;

",
76384888,Change Button's color twice when it's clicked,"I'm trying to create a button whose change background color (to green) when it is cliked. But when it is cliked againg the button returns to the original background color (from orange).
var btn1 = document.getElementById(""btn-1"")

if (btn1.style.backgroundColor = ""orange"") {
        btn1.addEventListener(""click"", function () {
        btn1.style.backgroundColor = ""green""
    })
} else {btn1.addEventListener(""click"", function () {
    btn1.style.backgroundColor = ""orange""
})
    }

Could you help me? Thx!
I'm trying to create a button whose change background color (to green) when it is cliked. But when it is cliked againg the button returns to the original background color (from orange).
",76385463,"

let button = document.getElementById(""button"");
    button.style.backgroundColor = ""orange"";

button.addEventListener(""click"", function () {
        if(button.style.backgroundColor == ""orange""){
           button.style.backgroundColor = ""green"";
        } else button.style.backgroundColor = ""orange"";
})
<button id=""button"">test</button>



how i understand you: you can set starter color of button to orange;
and then add EventListener to button with this logic:
-if the color of the button is orange - change to green
or if the color is not orange - change to orange
",
76383370,Vue 3 Component losing reactivity,"I've been learning Vue 3 for the past month or so and have gotten quite far but I can't fix this no matter what I've tried. I know I'm losing reactivity but I can't figure out how and it's driving me nuts. I am using the Composition API and script setup with a simple Pinia store. I created a github repo for it here: https://github.com/thammer67/vue3-reactivity-problem
I have a view (ProjectsView.vue) of project elements that loops through a pinia store array of projects using v-for and passing the array object as a prop. ProjectsView.vue uses a hidden form component (ProjectForm.vue) that I use for adding new projects. Each project in the loop is another component (ProjectItem.vue) with a click handler to a route that loads ProjectDetail.vue. ProjectDetail.vue has a click handler that also uses ProjectForm.vue for editing the item.
Everything works great. I can add new projects, edit projects but when I edit a project the pinia store updates (I can see this in the Vue Dev tools) but the UI doesn't update untill I go back to the project list. I need to update the value in ProjectDetail.vue after saving. Here are the pertinent files.
ProjectDetail.vue:
<script setup>
import { useProjectStore } from '../stores/ProjectStore'
import { useRoute } from 'vue-router'
import { ref } from 'vue'
import ProjectForm from '@/components/Form/ProjectForm.vue'

const projectStore = useProjectStore()
const route = useRoute()
const id = route.params.id
const project = projectStore.getProjectById(id)

const showEditProject = ref(false)
const editing = ref(false)

const editProject = (id)=> {
    editing.value = id
    showEditProject.value = true
}
</script>

<template>
    <div class=""main"">
        <div v-if=""project"" :project=""project"">
            <h2>Project Details</h2>
            <div>
                <div class=""project-name"">{{ project.project }}</div> 
            </div>
            <div style=""margin-top: 1em"">
                <button type=""button"" @click=""editProject(project.id)"">Edit</button>
            </div>

            <ProjectForm
                @hideForm=""showEditProject=false"" 
                :project=""project""
                :editing=""editing""
                :showAddEntry=""showEditProject"" />
        </div>
    </div>
</template>

ProjectForm.vue:
<script setup>
import { ref, toRef, reactive } from ""vue""
import { useProjectStore } from '@/stores/ProjectStore.js'
import Input from './Input.vue'

const projectStore = useProjectStore() 
const showAddType = ref(false)

//Capture 'showAddEntry' prop from parent component
const props = defineProps(['showAddEntry', 'editing', 'project'])

//Copy prop values for the form
const projName = toRef(props.project.project)
const projId = toRef(props.project.id)

//new/edited values are stored on this reactive object
const formState = reactive({
    invalid: false,
    errMsg:  """"
})

const saveProject = () => {
    formState.invalid = false

    if(projId.value) {
        console.log(`Update existing project ${projId.value}`)

        projectStore.updateProject({
            id: projId.value,
            project: projName.value
        })
        .then(()=> {
            console.log(""save was successful!"")
            showAddType.value = false
            formState.invalid = false
            formState.errMsg = """"
            emit('hideForm')
        })
        .catch(err=>console.log(""Error: "", err))
    } else {
        console.log(`Create new project`)
        //New Project
        projectStore.createProject({
            project: projName.value,
        })
        .then(()=> {
            showAddType.value = false
            formState.invalid = false
            formState.errMsg = """"
            emit('hideForm')
        })
    }
}

const hideForm = ()=> {
    formState.invalid = false
    showAddType.value=false
    emit('hideForm')
}

//Define emit event up to the parent that hides the form
const emit = defineEmits(['hideForm'])

</script>

<template>
    <div class=""addform"" :class=""{ show: props.showAddEntry }"">
        <h1 v-if=""editing"" class=""title"">Edit Project</h1>
        <h1 v-else class=""title"">Add New Project</h1>

        <div class=""input-wrap"" :class=""{ 'input-err' : formState.invalid }"">
            <Input 
                @input=""projName = $event.target.value""
                type=""text"" 
                placeholder=""Enter project name"" 
                :value=""projName""
            />
           
            <div class=""entry-submit"">
                <button v-if=""editing"" @click=""saveProject"">Save</button>
                <button v-else @click=""saveProject"">Create Project</button>
                <button @click=""hideForm"">Cancel</button>
            </div>
        </div>
        <p v-show=""formState.invalid"" class=""err-msg"">{{ formState.errMsg }}</p>
    </div>
</template>

",76383691,"project in ProjectDetails.vue is not aware of changes being made to it in the store.  It will if you wrap it with computed()
import { computed } from 'vue'

const project = computed(() => projectStore.getProjectById(id))

",
76381667,Elastic Search Regex are not working as expected,"I have go the problem in making elastic search regex work. I have a document that looks like this:
{""content"": ""keySyAtUXpd8JxrpUH2Sd""}

I have trying the following regex key[0-9A-Za-z_]{18} which perfectly matches with the string in regexer.com but when I query the request from elastic search it doesn't show any hits.
Here's the request that i'm using:
curl -XGET 'https://localhost:9200/_search?pretty' -H 'Content-Type: application/json' -H 'Authorization: Basic redacted' -k -d '{
  ""query"": { ""regexp"": { ""content"": ""key[0-9A-Za-z_]{18}"" } }
}'

I have also tried the regex with .*key[0-9A-Za-z_]{18}.*, tried to escape - as \\- but it doesn't seems to be working as well.
",76382386,"You need to run the regexp query against the content.keyword field
curl -XGET 'https://localhost:9200/_search?pretty' -H 'Content-Type: application/json' -H 'Authorization: Basic redacted' -k -d '{
  ""query"": { ""regexp"": { ""content.keyword"": ""key[0-9A-Za-z_]{18}"" } }
}'

PS: easier to test and provide feedback with real content and real queries ;-)
",
76380607,.Net Maui Google Maps Polyline drawning route feature,"I have this situation:
I'm building a .net Maui smartphone sports app that grabs a list of latitude and longitude (new Location class) of a running activity and draws a line (polyline) in the map to display the route. I can grab the list of exercises from the database and I can draw a polyline in the map, the problem is that I can't do both together because I don't know how to databind the Map functionalitys in my ViewModel class.
Here is my xaml code for the ExercisePage.xaml:
<?xml version=""1.0"" encoding=""utf-8"" ?>
<ContentPage xmlns=""http://schemas.microsoft.com/dotnet/2021/maui""
             xmlns:x=""http://schemas.microsoft.com/winfx/2009/xaml""
             x:Class=""DoradSmartphone.Views.ExercisePage""
             xmlns:model=""clr-namespace:DoradSmartphone.Models""
             xmlns:viewmodel=""clr-namespace:DoradSmartphone.ViewModels""
             xmlns:maps=""clr-namespace:Microsoft.Maui.Controls.Maps;assembly=Microsoft.Maui.Controls.Maps""
             xmlns:sensors=""clr-namespace:Microsoft.Maui.Devices.Sensors;assembly=Microsoft.Maui.Essentials""
             x:DataType =""viewmodel:ExerciseViewModel""
             Title=""{Binding Title}"">




    <Grid Padding=""5"" Margin=""5"" RowSpacing=""5"" ColumnSpacing=""3"">
        <Grid.RowDefinitions>
            <RowDefinition Height=""2*""/>
            <RowDefinition Height=""150""/>
        </Grid.RowDefinitions>

        <maps:Map Grid.Row=""0"" x:Name=""routeMap"" VerticalOptions=""CenterAndExpand"" Grid.ColumnSpan=""3"" HeightRequest=""400"" IsZoomEnabled=""False"" IsEnabled=""False"">
            <x:Arguments>
                <MapSpan>
                    <x:Arguments>
                        <sensors:Location>
                            <x:Arguments>
                                <x:Double>38.744418137669875</x:Double>
                                <x:Double>-9.128544160596851</x:Double>
                            </x:Arguments>
                        </sensors:Location>
                        <x:Double>0.7</x:Double>
                        <x:Double>0.7</x:Double>
                    </x:Arguments>
                </MapSpan>
            </x:Arguments>
        </maps:Map>

        <CarouselView ItemsSource=""{Binding Exercises}"" Grid.Row=""1"" PeekAreaInsets=""100"">
            <CarouselView.ItemTemplate>
                <DataTemplate x:DataType=""model:Exercise"">
                    <Frame HeightRequest=""90"" Margin=""5"">
                        <Frame.GestureRecognizers>
                            <TapGestureRecognizer Command=""{Binding Source={RelativeSource AncestorType={x:Type viewmodel:ExerciseViewModel}}, Path=ExerciseDetailsCommand}
                                            "" CommandParameter=""{Binding .}""></TapGestureRecognizer>
                        </Frame.GestureRecognizers>
                        <HorizontalStackLayout Padding=""10"" Spacing=""5"" >
                            <Label Text=""{Binding Id}""></Label>
                            <Label Text=""{Binding Date}""></Label>
                        </HorizontalStackLayout>
                    </Frame>
                </DataTemplate>
            </CarouselView.ItemTemplate>
        </CarouselView>
    </Grid>
</ContentPage>

As you can see I have my map name declared as routeMap and the first location just to start in somewhere. I also has my  model and viewmodel declared for DataBinding of the exercise list in the CarouselView. The tap feature works fine and take me to a new view called ExerciseDetailsPage.
This is the code behind ExercisePage.xaml.cs
using DoradSmartphone.Models;
using DoradSmartphone.ViewModels;
using Microsoft.Maui.Controls.Maps;
using Microsoft.Maui.Maps;

namespace DoradSmartphone.Views;

public partial class ExercisePage : ContentPage
{
    public ExercisePage(ExerciseViewModel exerciseViewModel)
    {
        InitializeComponent();
        BindingContext = exerciseViewModel;
    }

    
    private void OnTapGestureRouteUpdate(object sender, EventArgs e)
    {
        var route = new Polyline
        {
            StrokeColor = Colors.Red,
            StrokeWidth = 12,
            Geopath =
            {
                new Location(38.70061856336034 , -8.957381918676203 ),
                new Location(38.70671683905933 , -8.945225024701308 ),
                new Location(38.701985630081595, -8.944503277546072 ),
                new Location(38.701872978433386, -8.940750192338834 ),
                new Location(38.71054663609023 , -8.939162348597312 ),
                new Location(38.717755109243214, -8.942193686649311 ),
                new Location(38.7435419727561  , -8.928480490699792 ),
                new Location(38.78327379379296 , -8.880556478454272 ),
                new Location(38.925473761602376, -8.881999972299806 ),
                new Location(38.93692729913667 , -8.869585920414709 ),
                new Location(38.93493556584553 , -8.86536198145887  )
            }
        };
        routeMap.MoveToRegion(
            MapSpan.FromCenterAndRadius(
                new Location(38.93479161472441, -8.865352563545757), Distance.FromMiles(1)));
        // Add the polyline to the map
        routeMap.MapElements.Add(route);
    }
}

If I change the actual tap functionality to this tap event, I can drawn any line and other stuffs with in the Map because I can read the map name defined in the xaml code. But in this codebehind class I can't reach my ViewModel, Services or Model class.
This is my ExerciseViewModel.cs class:
using CommunityToolkit.Mvvm.ComponentModel;
using CommunityToolkit.Mvvm.Input;
using DoradSmartphone;
using DoradSmartphone.Models;
using DoradSmartphone.Services;
using DoradSmartphone.Views;
using Microsoft.Maui.Controls.Maps;
using Microsoft.Maui.Maps;
using System.Collections.ObjectModel;

namespace DoradSmartphone.ViewModels
{
    public partial class ExerciseViewModel : BaseViewModel
    {
        private readonly ExerciseService exerciseService;

        public ObservableCollection<Exercise> Exercises { get; private set; } = new();
        public ExerciseViewModel(ExerciseService exerciseService)
        {
            Title = ""Training Routes"";
            this.exerciseService = exerciseService;
            _ = GetExerciseList();                                   
        }

        [ObservableProperty]
        bool isRefreshing;
        
        async Task GetExerciseList()
        {
            if (IsLoading) return;
            try
            {
                IsLoading = true;
                if (Exercises.Any()) Exercises.Clear();

                var exercices = exerciseService.GetExercises();
                foreach (var exercise in exercices) Exercises.Add(exercise);
            } catch(Exception ex) { 
                Console.WriteLine(ex.ToString());
                await Shell.Current.DisplayAlert(""Error"", ""Failed to retrieve the exercice list"", ""Ok"");
            }
            finally { 
                IsLoading = false; 
                isRefreshing= false;
             }
        }
        [RelayCommand]
        async Task ExerciseDetails(Exercise exercise)
        {
            if(exercise == null) return;

            var routes = GetLocations(exercise.Id);

            DrawRoutes(routes);
        }

        public List<Location> GetLocations(int exerciseId)
        {
            if (exerciseId == 1)
            {
                return new List<Location>
                        {
                            new Location(35.6823324582143, 139.7620853729577),
                            new Location(35.679263477092704, 139.75773939496295),
                            new Location(35.68748054650018, 139.761486207315),
                            new Location(35.690745005825136, 139.7560362984393),
                            new Location(35.68966608916097, 139.75147199952355),
                            new Location(35.68427128680411, 139.7442168083328)
                        };
            }
            else if (exerciseId == 2)
            {
                return new List<Location>
                        {
                            new Location(35.6823324582143, 139.7620853729577),
                            new Location(35.679263477092704, 139.75773939496295),
                            new Location(35.68748054650018, 139.761486207315),
                            new Location(35.690745005825136, 139.7560362984393),
                            new Location(35.68966608916097, 139.75147199952355),
                            new Location(35.68427128680411, 139.7442168083328)
                        };
            }
            else
            {
                return new List<Location>
                        {
                            new Location(35.6823324582143, 139.7620853729577),
                            new Location(35.679263477092704, 139.75773939496295),
                            new Location(35.68748054650018, 139.761486207315),
                            new Location(35.690745005825136, 139.7560362984393),
                            new Location(35.68966608916097, 139.75147199952355),
                            new Location(35.68427128680411, 139.7442168083328)
                        };
            }
        }

        private void DrawRoutes(List<Location> routes)
        {
            var polylines = new Polyline
            {
                StrokeColor = Colors.Red,
                StrokeWidth = 12,
            };

            foreach(var route in routes)
            {
                polylines.Geopath.Add(route);
            }                        
            
            routeMap.MoveToRegion(
                MapSpan.FromCenterAndRadius(
                    routes.FirstOrDefault(), Distance.FromMiles(1)));
            // Add the polyline to the map
            routeMap.MapElements.Add(polylines);
        }
    }
}


This class inherits the BaseViewModel that inherits ObservableObject and has some common properties for all others classes. In the ExerciseViewModel I have my RelayCommand related to the tap feature that grabs the exercise object and add the route, but I cant access the routeMap object. I've tried also to declare a Map class in my viewmodel class, but I get the error all the time that I can't create a instance of a static class.
This is my MauiProgram.cs just in case there's something wrong:
using DoradSmartphone.Data;
using DoradSmartphone.Services;
using DoradSmartphone.ViewModels;
using DoradSmartphone.Views;

namespace DoradSmartphone;

public static class MauiProgram
{
    public static MauiApp CreateMauiApp()
    {
        var builder = MauiApp.CreateBuilder();
        builder
            .UseMauiApp<App>()       
            .UseMauiMaps()
            .ConfigureFonts(fonts =>
            {
                fonts.AddFont(""OpenSans-Regular.ttf"", ""OpenSansRegular"");
                fonts.AddFont(""OpenSans-Semibold.ttf"", ""OpenSansSemibold"");
            });

        builder.Services.AddSingleton<DatabaseConn>();
        builder.Services.AddScoped<IRepository, DatabaseConn>();

        builder.Services.AddSingleton<MainPage>();
        builder.Services.AddSingleton<UserPage>();
        builder.Services.AddSingleton<LoginPage>();        
        builder.Services.AddSingleton<LoadingPage>();
        builder.Services.AddSingleton<ExercisePage>();
        builder.Services.AddSingleton<DashboardPage>();
        builder.Services.AddSingleton<ExerciseDetailsPage>();

        builder.Services.AddSingleton<UserService>();
        builder.Services.AddSingleton<LoginService>();        
        builder.Services.AddSingleton<ExerciseService>();
        builder.Services.AddSingleton<DashboardService>();

        builder.Services.AddSingleton<UserViewModel>();
        builder.Services.AddSingleton<LoginViewModel>();        
        builder.Services.AddSingleton<LoadingViewModel>();
        builder.Services.AddSingleton<ExerciseViewModel>();
        builder.Services.AddSingleton<DashboardViewModel>();
        builder.Services.AddTransient<ExerciseDetailsViewModel>();

        return builder.Build();
    }
}

Thank you in advance!
",76382427,"unfortunately, MapElements is not a bindable property.  However, you can work around that in a couple of ways
for example, create a public method in your VM that returns the route data
public Polyline GetRouteData()
{
    var polylines = new Polyline
        {
            StrokeColor = Colors.Red,
            StrokeWidth = 12,
        };

        foreach(var route in routes)
        {
            polylines.Geopath.Add(route);
        } 

  return polylines; 
}

then in your code behind, first create a class reference to the VM
ExerciseViewModel ViewModel;

public ExercisePage(ExerciseViewModel exerciseViewModel)
{
    InitializeComponent();
    BindingContext = ViewModel = exerciseViewModel;
}

then your code behind can get the data from the VM that it needs to update the map
routeMap.MapElements.Add(ViewModel.GetRouteData());

",
76385151,Replacing values in a string expression based on a matrix and iterating over columns,"In column X are those variables that will have values in each column j, in this case only U1, X4 and U2 have values, the rest of the variables belonging to the list ['B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'] will all have their values 0
#example matrix
new_matrix = [[ 'C',  'X', 'B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], 
              [ 0.0, 'U1',   8,  2.0,  1.0, -1.0,    0,    0,  1.0,    0], 
              ['+M', 'X4',   2,  1.0,  1.0,    0,  1.0,    0,    0,    0], 
              ['+M', 'U2',   8,  1.0,  2.0,    0,    0, -1.0,    0,  1.0]]

variables = ['X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2', 'B'] #select the first row (only variables)
variables_j_col_values = [[variables.pop(variables.index('B'))] + variables, []] # --> ['B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2']

The problem with this is that I need create the following matrix of values of the variables (without using libraries) where I would have the following:
variables_j_col_values = [['B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], 
                          [ 0 ,    0,    0,    0,    2,    0,    8,    8],    #column new_matrix[][2]
                          [ 0 ,    0,    0,    0,  1.0,    0,  2.0,  1.0],    #column new_matrix[][3]
                          [ 0 ,    0,    0,    0,  1.0,    0,  1.0,  2.0],    #column new_matrix[][4]
                          [ 0 ,    0,    0,    0,    0,    0, -1.0,    0],    #column new_matrix[][5]
                          [ 0 ,    0,    0,    0,  1.0,    0,    0,    0],    #column new_matrix[][6]
                          [ 0 ,    0,    0,    0,    0,    0,    0, -1.0],    #column new_matrix[][7]
                          [ 0 ,    0,    0,    0,    0,    0,  1.0,    0],    #column new_matrix[][8]
                          [ 0 ,    0,    0,    0,    0,    0,    0,  1.0], ]  #column new_matrix[][9]

After create the variables_j_col_values, go replacing the values of the rows (except for row 0 of the variables_j_col_values array because it is a header) in the string inside funcion_obj_z
The logic would be to use a loop that goes through the rows, and does a
.replace(new_matrix[][n], this_element)
funcion_obj_z = 'Z = 3 * X1 + 2 * X2 + 0 * X3 + 0 * X4 + 0 * X5 + M * U1 + M * U2'

In this way, using said string as an expression, it would obtain these prints in the console if it printed the value of j_func in each j iteration. These would be the desired correct output:
#for loop, print the j string replacement the values in the string

j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 2 + 0 * 0 + M * 8 + M * 8' #iteration 1
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 1.0 + 0 * 0 + M * 2.0 + M * 1.0' #iteration 2
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 1.0 + 0 * 0 + M * 1.0 + M * 2.0' #iteration 3
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 0 + 0 * 0 + M * -1.0 + M * 0' #iteration 4
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 1.0 + 0 * 0 + M * 0 + M * 0' #iteration 5
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 0 + 0 * 0 + M * 0 + M * -1.0' #iteration 6
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 0 + 0 * 0 + M * 1.0 + M * 0' #iteration 7
j_func = 'Z = 3 * 0 + 2 * 0 + 0 * 0 + 0 * 0 + 0 * 0 + M * 0 + M * 1.0' #iteration 8

",76385493,"Albeit an ugly solution, this should give you the transformation you need:
new_matrix = [[ 'C',  'X', 'B', 'X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2'], 
              [ 0.0, 'U1',   8,  2.0,  1.0, -1.0,    0,    0,  1.0,    0], 
              ['+M', 'X4',   2,  1.0,  1.0,    0,  1.0,    0,    0,    0], 
              ['+M', 'U2',   8,  1.0,  2.0,    0,    0, -1.0,    0,  1.0]]

variables = ['X1', 'X2', 'X3', 'X4', 'X5', 'U1', 'U2', 'B']

# create empty matrix
variables_j_col_values = [[0 for _ in range(len(variables))] for _ in range(len(new_matrix[0])-1)]

# replace first row with sorted variables based on new_matrix headers
variables_j_col_values[0] = sorted(variables, key=lambda x: new_matrix[0].index(x))

# loop over all value rows
for row in new_matrix[1:]
    # get correct column in variables_j_col_values based
    col = variables_j_col_values[0].index(row[1])
    # zip the values and rows and update accordingly
    for val, target in zip(row[2:], variables_j_col_values[1:]):
        target[col] = val

",
76382456,How can i create a deviations query?,"I wanted to write a query that would allow me to calculate deviations by the number of created orders.
Task: the query should look back 7 days and based on this data build a minimum allowable threshold (MAT). If the number of orders for a minimum period of time (5 minutes) is less than MAT, then an alert will be generated.
Features: The number of orders directly affects the time of day and seasonality.
Having searched the Internet, I found information about so-called Poisson distribution, and tried to apply it to the problem, but it didn't work.
In prometheus there are such functions as day_of_week(), avg_over_time() and stddev_over_time.
From what I was able to do:

The difference between the number of orders in the last 5 min.
sum(delta(my_search_counter{service_name=""car.book.v1""}[5m])
Five-minute average time variation over the last 30 minutes with a resolution of 5 minutes
avg_over_time(sum(delta(my_search_counter{service_name=""car.book.v1""}[5m]))[1w:5m])
Mean square deviation:
stddev_over_time(sum(delta(my_search_counter{service_name=""car.book.v1""}[5m]))[1w:5m])

This is where I'm stuck and can't figure out how to build a proper query. Maybe there is another way, simpler, but I haven't found it.
I tried to combine these queries with each other using addition, subtraction and division.
",76383719,"I'm not sure what statistics is this, and how adequate this is as a threshold, but here is query you described.
sum(increase(my_search_counter{service_name=""car.book.v1""}[5m]))
< sum(increase(my_search_counter{service_name=""car.book.v1""}[5m] offset 1w))
  - stddev_over_time(sum(increase(my_search_counter{service_name=""car.book.v1""}[5m] offset 1w))[1d:5m])

It returns value if number of oreder over last 5 minutes is less then number of orders over same 5 minutes 1 week ago minus standart deviation of orders number over 24 hours presiding current moment 1 week ago.
You might need to play a little with multiplier for stddev part, to get a reasonable percent of alerts.
",
76383416,Group list of objects into Map with value being true if any object in group has field set as true using Java stream API,"In a Java 17 project, I have a collection of objects with a String field propGroupName representing the group this object belongs to and a Boolean field propValActive representing whether this object is flagged as active internally.
I want to aggregate these objects by the string field into a Map<String, Boolean> with the key being the String field and the Boolean being false if all the booleans in the group are false and true if 1 or more of the booleans in the group are false. I have a working implementation with a simple for loop, but I want to know if there is a way to do this grouping through the Java Stream API, preferably in a way that short circuits? The goal is that I want to know of every group whether there are any objects in that group flagged as active.
I currently have this implementation which doesn't use the Streams API and doesn't short circuit:
public Map<String, Boolean> determineActiveGroups(
        HashMap<String, PropertyValueDefinitionGroupView> standardPvdgMap) {
    Map<String, Boolean> activeGroupsMap = new HashMap<String, Boolean>();
    for (PropertyValueDefinitionGroupView pvdgView : standardPvdgMap.values()) {
        if(pvdgView.getPropGroupOid() == null) {
            continue;
        }
        activeGroupsMap.putIfAbsent(pvdgView.getPropGroupName(), false);
        if(pvdgView.getPropValActive()) {
            activeGroupsMap.put(pvdgView.getPropGroupName(), true);
        }
    }
    return activeGroupsMap;
}

I have a different bit of code somewhere else that does something similar, but it retains the lists, and I managed to adapt something similar for what I need but I don't know what predicate I can use to finish it with. I assume it's going to use anyMatch, but I have no idea how to integrate it:
Map<String, Boolean> activeGroups = standardPvdgMap.values().stream()
    .collect(Collectors.groupingBy(PropertyValueDefinitionGroupView::getPropGroupName, ???????));

",76383740,"groupingBy is such a powerful collector :
public Map<String, Boolean> determineActiveGroups(Map<String, PropertyValueDefinitionGroupView> standardPvdgMap) {
    return standardPvdgMap.values()
            .stream()
            .filter(pvdgView -> pvdgView.getPropGroupOid() != null)
            .collect(Collectors.groupingBy(
                    PropertyValueDefinitionGroupView::getPropGroupName,
                    Collectors.mapping(
                            PropertyValueDefinitionGroupView::getPropValActive,
                            Collectors.reducing(false, (a, b) -> a || b))
            ));
}

The trick is knowing that you can apply further collectors on the downstream. In this case I map to the flag, and then reduce the flags using the logical or.
",
76381701,MS Word VBA to check for empty text form fields upon file close/exit,"I need to have a MS Word Macro check upon file exit or file close, that certain specified text fields (legacy form fields, not content control) are empty.
I have used some code that is a pretty intrusive warning box. But its also contingent on the user selecting that field then the macro pops up a warning box either upon entry or exit, as specified in the form field properties menu. I have several fields,""Text1"", ""text2"", then text7 thru 11. Trouble is, the user MUST select a field to get this code to work, on top of that, the warning box basically sends them into a death loop before they can even close the file. I also have to make a new module for each of field with the code below. Perhaps the best solution here is a macro that runs on close and/or exit of the file, which says ""Hey you forgot to fill out these fields, they are 'mandatory' so go back and do that please, thanks!"" What do you all think?
Sub MustFillIn3()
    If ActiveDocument.FormFields(""Text2"").Result = """" Then
        Do
            sInFld = InputBox(""Request date required, please fill in below."")
        Loop While sInFld = """"
        ActiveDocument.FormFields(""Text2"").Result = sInFld
    End If
End Sub


",76382431,"Yes, just write the check code in the event handler procedure Document_Close in ThisDocument object, like this
Sub Document_Close()
    Dim ff As FormField, sInFld As String, msgShown As Boolean, d As Document, i As Byte
    'Dim ffNameDict As New Scripting.Dictionary, ffNameSpecCln As New VBA.Collection
    Dim ffNameDict As Object, ffNameSpecCln As New VBA.Collection
    
    
    Dim arr(7) As String, j As Byte
    arr(0) = ""location"": arr(1) = ""request_date"": arr(2) = ""site""
    arr(3) = ""UPC"": arr(4) = ""Current_LOA"": arr(5) = ""Req_LOA""
    arr(6) = ""You Lost this One!!""
    
    For i = 1 To 11
        Select Case i
            Case 1, 2, 7, 8, 9, 10, 11 '""Text1"", ""text2"", then text7 thru 11.
        
                'to a specific name list?
                'ffNameSpecCln.Add ""Specific Name HERE "" & i, ""Text"" & i
                ffNameSpecCln.Add arr(j), ""Text"" & i
                j = j + 1
        End Select
    Next i
     
    Set ffNameDict = CreateObject(""Scripting.Dictionary"")
    Set d = ActiveDocument
    For i = 1 To 11
        Select Case i
            Case 1, 2, 7, 8, 9, 10, 11 '""Text1"", ""text2"", then text7 thru 11.
            'ffNameDict(""Text"" & i) = ""Text"" & i
            ffNameDict(""Text"" & i) = ffNameSpecCln.Item(""Text"" & i)
        End Select
    Next i
    For Each ff In d.FormFields
        If ff.Result = """" And ffNameDict.Exists(ff.Name) Then
            If Not msgShown Then
                MsgBox ""Hey you forgot to fill out these fields, they are 'mandatory' so go back and do that please, thanks!"", vbExclamation
                msgShown = True
            End If
            Do
'                sInFld = InputBox(""Request date required, please fill in below."" + vbCr + vbCr + _
                                    ""@"" + ff.Name + "" is the current text fields to fill in !"")
                sInFld = InputBox(""Request date required, please fill in below."" + vbCr + vbCr + _
                                    ""@"" + ffNameDict(ff.Name) + "" is the current text fields to fill in !"")
            Loop While sInFld = """"
            ff.Result = sInFld
        End If
    Next ff
    d.Save
End Sub


note: The Private modifier in this image should be removed in order to be called in the appWord_DocumentBeforeSave event handler (code above already set)


This check sub is triggered when the current document is closed and is not related to whether ff has focus or not (ie. the user Doesn't MUST  select a field ).
Option Explicit

Public WithEvents appWord As Word.Application
 
Private Sub appWord_DocumentBeforeSave(ByVal Doc As Document, SaveAsUI As Boolean, Cancel As Boolean)
    ThisDocument.Document_Close
End Sub



You have to run this sub to Register Event_Handler to Word Application.
Option Explicit

'https://learn.microsoft.com/en-us/office/vba/word/concepts/objects-properties-methods/using-events-with-the-application-object-word
Public X As New app 
Public Sub Register_Event_Handler() 
    Set X.appWord = Word.Application 
End Sub


""物件類別模組"" = class modules


""模組"" = modules


""表單"" = user form


""Microsof Word 物件"" = Microsof Word object


As for the details, you should adjust them yourself. Try to understand the code I have given you to simulate it. Come back to StackOverflow and ask a new question when you encounter difficulties and problems in the implementation.
I've used the text field to test:

Is this yours?
Before closing the document check if it has been modified
Option Explicit

Public WithEvents appWord As Word.Application
 
Private Sub appWord_DocumentBeforeClose(ByVal Doc As Document, Cancel As Boolean)
    If Not Doc.Saved Then
        If MsgBox(""Do you want to save?"", vbOKCancel + vbQuestion) = vbOK Then
            Doc.Save
        Else
            Doc.Close wdDoNotSaveChanges
        End If
    End If
End Sub

Private Sub appWord_DocumentBeforeSave(ByVal Doc As Document, SaveAsUI As Boolean, Cancel As Boolean)
    MS_Word_VBA_to_check_for_empty_text_form_fields_upon_file_close_exit
End Sub



Comment out the event handler Document_Close code and Registering an event handler when a document is opened:

Option Explicit

rem now can be Private, because there is no other place to call this procedure
Private Sub Document_Close()
    'MS_Word_VBA_to_check_for_empty_text_form_fields_upon_file_close_exit
End Sub


Private Sub Document_Open()
    Register_Event_Handler ' See previous code
End Sub



Extract the code to become a separate checker procedure or method:

Sub MS_Word_VBA_to_check_for_empty_text_form_fields_upon_file_close_exit()
    Dim ff As FormField, sInFld As String, msgShown As Boolean, d As Document, i As Byte
    'Dim ffNameDict As New Scripting.Dictionary, ffNameSpecCln As New VBA.Collection
    Dim ffNameDict As Object, ffNameSpecCln As New VBA.Collection
    
    
    Dim arr(7) As String, j As Byte
    arr(0) = ""location"": arr(1) = ""request_date"": arr(2) = ""site""
    arr(3) = ""UPC"": arr(4) = ""Current_LOA"": arr(5) = ""Req_LOA""
    arr(6) = ""You Lost this One!!""
    
    For i = 1 To 11
        Select Case i
            Case 1, 2, 7, 8, 9, 10, 11 '""Text1"", ""text2"", then text7 thru 11.
        
                'to a specific name list?
                'ffNameSpecCln.Add ""Specific Name HERE "" & i, ""Text"" & i
                ffNameSpecCln.Add arr(j), ""Text"" & i
                j = j + 1
        End Select
    Next i
     
    Set ffNameDict = CreateObject(""Scripting.Dictionary"")
    Set d = ActiveDocument
    For i = 1 To 11
        Select Case i
            Case 1, 2, 7, 8, 9, 10, 11 '""Text1"", ""text2"", then text7 thru 11.
            'ffNameDict(""Text"" & i) = ""Text"" & i
            ffNameDict(""Text"" & i) = ffNameSpecCln.Item(""Text"" & i)
        End Select
    Next i
    For Each ff In d.FormFields
        If ff.Result = """" And ffNameDict.Exists(ff.Name) Then
            If Not msgShown Then
                MsgBox ""Hey you forgot to fill out these fields, they are 'mandatory' so go back and do that please, thanks!"", vbExclamation
                msgShown = True
            End If
            Do
'                sInFld = InputBox(""Request date required, please fill in below."" + vbCr + vbCr + _
                                    ""@"" + ff.Name + "" is the current text fields to fill in !"")
                sInFld = InputBox(""Request date required, please fill in below."" + vbCr + vbCr + _
                                    ""@"" + ffNameDict(ff.Name) + "" is the current text fields to fill in !"")
            Loop While sInFld = """"
            ff.Result = sInFld
        End If
    Next ff
    d.Save
End Sub


",
76383035,Type inference on function parameters with nested generics?,"I am facing an issue with Typescript generics.
I have a function which must be called with a type and some attributes depending on this type.
Typescript does not manage to infer the type of attributes when inside an if guarding the type to TYPES.ME:
enum TYPES {
    ME = 'me',
    YOU = 'you'
}

type Attributes<T extends TYPES> =  T extends TYPES.ME ? {keys: true} : {hat: true}
const func = <T extends TYPES>(type: T, attr: Attributes<T>) => {
    if(type === TYPES.ME) {
        attr.keys // error Property 'keys' does not exist on type '{ keys: true; } | { hat: true; }'
    }
} 

Playground link
I would like to have your opinions on this matter and see if there is a nice workaround.
Cheers!
",76383754,"Currently, TypeScript is unable to re-constrain generic type parameters as a result of control flow analysis.  Inside the body of func(type, attr), you check that type === Types.ME.  This can narrow the type of type from T to something like T & Types.ME.  But it cannot do anything to T itself.  The type parameter T stubbornly stays the same; it is not constrained to Types.ME. And thus the compiler cannot conclude that attr is of type Attributes<Types.ME>.
And it is technically correct for the compiler to refuse to change T.  That's because, while individual values like type can only be one thing at a time, a type argument like T can be a union.  Indeed, you can call func() with a T equal to the full Types.ME | Types.YOU union, like so:
func(
    Math.random() < 0.999 ? Types.ME : Types.YOU,
    { hat: true }
) // compiles without error

If you inspect that, you'll see that T is inferred as Types (the full union of Types.ME | Types.YOU, and therefore attr is allowed to be {hat: true} even in the 99.9% likely event that type is Types.ME.
There is a longstanding open feature request at microsoft/TypeScript#27808 which asks for a way to say ""T will be exactly one of Types.ME or Types.YOU; it cannot be a union"".  And then, maybe inside the function body, checking type === Types.ME would allow T itself to be constrained to Types.ME, and things would work as expected.  And presumably the call with Math.random() < 0.999 would be rejected.
But for now it's not part of the language.

You might consider taking the approach where instead of having func be generic, you make it similar to an overloaded function, where it has one call signature per member of Types. You can write that as a function with a rest parameter whose type is a discriminated union of tuple types, and the compiler will treat it as such inside the function body.
Perhaps like this:
type FuncArg =
    [type: Types.ME, attr: { keys: true }] |
    [type: Types.YOU, attr: { hat: true }];

const func: (...args: FuncArg) => void = (type, attr) => {
    if (type === Types.ME) {
        console.log(attr.keys) // this works
    }
}

And now you can't make the invalid call:
func(
    Math.random() < 0.999 ? Types.ME : Types.YOU,
    { hat: true }
) // error, type 'Types.ME' is not assignable to type 'Types.YOU'

Everything works because now type and attr are bound together as desired in FuncArg; it's like imagining T were constrained to be just one member of Types at a time, and walking through the possibilities.  Note that FuncArg could, if necessary, be computed from the Attributes type given in the question, or from another mapping interface, but that is out of scope for the question as asked.
Playground link to code
",
76383892,SQL query for last rented car with sunroof feature in car rental system,"I'm developing a car rental automation system in SQL, and I'm seeing unwanted data in my table that I've received and constantly updated. The query I need to write is as follows: 'Write a query that retrieves information about the last rented car for customers who have rented cars with the feature of a sunroof at least once.' I've been struggling with it for hours and couldn't solve it. Can you help me?
select * from car a
where exists(
    select * from car a2
    inner join customer m on m.customer_id=a2.customer_id
    inner join rent k on k.customer_id=m.customer_id
    inner join car_rent ak on ak.rent_id=k.rent_id
    inner join package_package_option pk on pk.package_id=a2.package_id
    where pk.option_id=2 and a.rent_sell=1
    group by ak.date
    having ak.date = max(ak.date)

",76385502,"The solution has two steps:

Identify all customers who have ever rented a car with a sunroof, and
For each such customer, look up the latest rental for each such customer.

The first step is pretty straight forward - Filter the rentals for sunroof and select distinct customer IDs.
The second step can be done a couple of ways. One is to feed the customer IDs into a CROSS APPLY (SELECT TOP 1 ... ORDER BY RentalDate DESC) construct to select the latest rental for each selected customer one at a time. Another is to lookup all rentals for the selected customers, assign sequence numbers using the ROW_NUMBER() window function OVER(... ORDER BY RentalDate DESC), and then filtering for row-number = 1.
Something like (pseudocode):
SELECT *
FROM (
    SELECT DISTINCT customer_id
    FROM rentals
    WHERE has-a-sunroof
    AND is-a-rental
) C
CROSS APPLY (
    SELECT TOP 1 rental-info
    FROM rentals R
    WHERE R.customer_id = C.customer_id
    AND is-a-rental
    ORDER BY R.Rentaldate DESC
) R

or
SELECT *
FROM (
    SELECT rental-info,
        ROW_NUMBER() OVER(PARTITION BY R.customer_id ORDER BY R.Rentaldate DESC) RowNum
    FROM rentals R
    WHERE R.customer_id IN (
        SELECT DISTINCT customer_id
        FROM rentals
        WHERE has-a-sunroof
        AND is-a-rental
    )
    AND is-a-rental
) A
WHERE A.RowNum = 1

You might try both to compare performance with large data sets.  I recommend also ensuring that you have an index on the rental that includes both customer_id and rental date for best performance.
",
76382019,Read value from SPI on Raspberry Pi Pico using Rust,"I am trying to read a value from a sensor, BMP280 over SPI on a Raspberry Pi Pico. But I am getting an unexpected value.
I created a new repo based on the rp2040-project-template and modified it to add SPI functionality.
I added these imports:
use embedded_hal::prelude::_embedded_hal_spi_FullDuplex;
use rp_pico::hal::spi;
use rp_pico::hal::gpio;
use fugit::RateExtU32;

Then I setup SPI in the bottom of main function:
let _spi_sclk = pins.gpio2.into_mode::<gpio::FunctionSpi>();
let _spi_mosi = pins.gpio3.into_mode::<gpio::FunctionSpi>();
let _spi_miso = pins.gpio4.into_mode::<gpio::FunctionSpi>();
let mut spi_cs = pins.gpio5.into_push_pull_output_in_state(PinState::Low); // initial pull down, for SPI

let spi = spi::Spi::<_, _, 8>::new(pac.SPI0);

let mut spi = spi.init(
    &mut pac.RESETS,
    clocks.peripheral_clock.freq(),
    10.MHz(),                           // bmp280 has 10MHz as maximum
    &embedded_hal::spi::MODE_0,
);

spi_cs.set_high().unwrap();   // pull up, set as inactive after init
delay.delay_ms(200);                     // some delay for testing

Then I try to read the ID registry
spi_cs.set_low().unwrap();
let res_w = spi.send(0xd0 as u8); // 0xd0 is address for ID, with msb 1
let res_r = spi.read();
spi_cs.set_high().unwrap();

// check results
match res_w {
    Ok(_) => info!(""write worked""),
    Err(_) => info!(""failed to write"")
}

match res_r {
    Ok(v) => info!(""read value from SPI: {}"", v),
    Err(_) => info!(""failed to read SPI"")
}

With this code, the SPI read fails. Why is that?
Perhaps it is necessary to set a mode on the sensor, before reading the ID. I can add this code above the read, to set forced mode.
spi_cs.set_low().unwrap();
spi.send(0xf4-128 as u8).expect(""failed to send first byte"");  // registry 0xf4 with msb 0
spi.send(0x1 as u8).expect(""failed to send second byte"");
spi_cs.set_high().unwrap();

Now the read of ID registry works, but I get value 255 and not the expected 0x58.
What am I doing wrong?

I have also tried with transfer using this code:
let mut data: [u8; 2] = [0xd0, 0x0];
let transfer_success = spi.transfer(&mut data);
match transfer_success {
    Ok(v) => info!(""read data {}"", v),
    Err(_) => info!(""failed to read"")
}

But I read the values as [255, 255] with this code, not the expected 0x58.
",76382608,"read() is probably not the function you want to use here; it doesn't acutally perform any bus action but only gives you the byte that was read during the last send().
The function you actually want to use is transfer(). On a full-duplex SPI bus, a ""read"" action is always also a ""write"" action, and transfer performs both. Be aware that if you only want to read, you need to write the same amount of zeros, because only the bus master can provide the clock to do anything.
So if you want to write 0xd0, followed by reading a single byte, you need to transfer() the values [0xd0, 0x00]. The same array that you use to put your sent data into transfer() will then contain the received data; most likely [0x00, <data>] (or [0xff, <data>], not sure. Probably 0xff, as you already mentioned that you read a 255).
The implementation of transfer shows how read() is actually supposed to be used:
fn transfer<'w>(&mut self, words: &'w mut [W]) -> Result<&'w [W], S::Error> {
    for word in words.iter_mut() {
        block!(self.send(word.clone()))?;
        *word = block!(self.read())?;
    }

    Ok(words)
}

Note the block!() here - in embedded, asynchronous calls usually return an error indicating that the operation would block, until it is finished. The block!() macro converts an asynchronous call to a blocking call, which is most likely where your error comes from.
Either way, I would recommend deriving your code from the official example, those are usually pretty good at demonstrating the intended way an object should be used.
",
76383668,Font Color does not update,"I have a simple HTML snippet like this:


    <p align= ""left""> <FONT size=3><STRONG> Asset: &nbsp;    </STRONG></FONT><STRONG><FONT color=""blue"" size=2> something something here  </FONT></STRONG><br> </p>



The font color does not seem to work but the font size does change.
Also, I do understand that explicit font description is expiring and we should be using the CSS styling. If you could suggest that option,okay to use that option as well.
",76383803,"Using HTML elements and classes is the better option although I do admit some might seem overly complicated.
For my answer, first I give the paragraph a class. Inside of it, I'm aligning the text left and bolding EVERYTHING in it.
Next, spans are by default inline (meaning they show inline with the text, but you don't have control over the spacing). You could also use a div. So every span in the paragraph tag, I'm setting to inline block so I can control the spacing around it while keeping the text inline.
Next each part of the content gets wrapped in its own span.
Since earlier I already set all content to bold, I'm only going to give the title span a margin to the right to space it out and change the font size.
Next for the rest I wrapped that content in another span and changed the color to blue (hex is #Red Green Blue) and I changed the font size also.


.updated{
  text-align:left;
  font-weight:bold;
}

.updated span{
  display:inline-block;
}

.updated .title{
  margin-right:5px;
  font-size:18px;
}

.updated .content{
  color:#0000FF;  
  font-size:13px;
}
Originial: <p align= ""left""> <FONT size=3><STRONG> Asset: &nbsp;    </STRONG></FONT><STRONG><FONT color=""blue"" size=2> something something here  </FONT></STRONG><br> </p>

Updated:
<p class=""updated"">
  <span class=""title"">Asset:</span>
  <span class=""content"">something something here</span>
</p>



",
76384530,TypeError: Cannot read properties of undefined (reading 'params') Django + React,"Uncaught TypeError: Cannot read properties of undefined (reading 'params')
Unabel to navigate to id can anyone help?
i am woriking on django as backend and react as frontend
class ArticleDetail extends  React.Component{

    state={
        article:{}
    }
    componentDidMount(){
        const id = this.props.match.params.id;
      
      axios.get(`http://127.0.0.1:8000/api/${id}`)
      .then(res =>{
        this.setState({
            article:res.data
        });
        console.log(res.data)
       
      })

    }
    render(){
        return(
           
            <Card title={this.state.article.title} >
                <p>{this.state.article.content }</p>
            </Card>
        )
    }
}```


TypeError: Cannot read properties of undefined (reading 'params') Unabel to navigate to id can anyone help?
i am working on react + django. My data from server in list is showing but when i try to navigate to particular data id it shows error

",76385508,"This should be a frontend problem.
1-) Add the following line of code at the beginning of your class:
import { useParams } from 'react-router-dom';

2-) Then add this function above your class (copy it exactly):
 export function withRouter(Children){
 return(props)=>{

    const match  = {params: useParams()};
    return <Children {...props}  match = {match}/>
  }
 }

3-) Next, change your class definition to this:
class ArticleDetail extends Component

4-) Add the following line of code at the end of your class:
export default withRouter(ArticleDetail);

Ref: https://stackoverflow.com/a/75304487/11897778
If it doesn't work, please provide more details about the error, if the API request is being made or is it failing before making the API request?
",
76381276,Getting user ID instead of volunteer ID while joining tables in Laravel GraphQL search handler,"When i join two tables in search handler there is a same column in both tables, i cannot access the value of left table
for example if there are two tables user and volunteers they both have id column when I write a search handler like this
$builder->join('users', 'volunteers.user_id', ""="", ""users.id"")
  ->join('policies','volunteers.policy_id',""="",""policies.id"")
  ->where(function($q) use ($whereConditions){
      $q->where('users.first_name','like','%'.$whereConditions['OR'][0]['value'].'%');
      $q->orWhere('users.last_name','like','%'.$whereConditions['OR'][0]['value'].'%');
      $q->orWhere('policies.name','like','%'.$whereConditions['OR'][0]['value'].'%');
      $q->orWhere('volunteers.experiences','like','%'.$whereConditions['OR'][0]['value'].'%');
      $q->orWhere('volunteers.medical_facility','like','%'.$whereConditions['OR'][0]['value'].'%');
   });

and when i query, it will return user id as volunteer id
I want the volunteer id but I always get the user id. I hope the question is clear
",76382625,"I implemented your problem and the problem happened to me but I solved it by using $builder->select('your columns').


",
76383738,How can I simplify my jQuery code to avoid repeating instructions?,"Novice trying to simplify my jQuery, to avoid repetition
I am a novice with Javascript and jQuery, but have written some code to display a tooltip on a form depending on the answer selected to a dropdown. Right now I am repeating the steps twice:

Once to check the dropdown on page load, in case it has reloaded due to a submission error - in this case, if an answer has been selected to that question, it will persist, and I need the tooltip to remain.

Once to check whenever the dropdown value changes.


The value of the dropdown is a number, so I've used that in the div classes to show the appropriate div. This is the code which is working fine:
$(document).ready(function(){
    var service = """";
    var otherservice = """";

    // Check for select value on page load, in case it has refreshed due to a form error

    service = '.v' + $('select#989022_58716pi_989022_58716 option:selected').val();
    otherservice = '#form-tooltips div:not('+service+')';
    $('#form-tooltips div'+service).show();
    $(otherservice).hide();

    // Check again for select value, every time that selection changes

    $('select#989022_58716pi_989022_58716').on('change', function(){
        service = '.v' + $('select#989022_58716pi_989022_58716 option:selected').val();
        otherservice = '#form-tooltips div:not('+service+')';
        $('#form-tooltips div'+service).show();
        $(otherservice).hide();
    });
});

//The tooltips for display

$(""<div id='form-tooltips'><div class='v1381962'>Tooltip 1</div><div class='v1381965'>Tooltip 2</div></div>"").insertAfter("".add-tooltip-after"");

What I would like to do is create a function - checkTooltip - so that I do not have to repeat those tooltip instructions the second time. I have tried the following:
$(document).ready(function(){
    var service = '';
    var otherservice = '';
    function checkTooltip({
        service = '.v' + $('select#989022_58716pi_989022_58716 option:selected').val();
        otherservice = '#form-tooltips div:not(""+service+"")';
        $('#form-tooltips div'+service).show();
        $(otherservice).hide();
    });
    checkTooltip();
    $('select#989022_58716pi_989022_58716').on('change', checkTooltip());
});
$(""<div id='form-tooltips'><div class='v1381962'>Tooltip 1</div><div class='v1381965'>Tooltip 2</div></div>"").insertAfter("".add-tooltip-after"");

However this is not working. In the Chrome console, it says Uncaught SyntaxError: Unexpected token ';' on the 5th line. I have tried removing that semicolon but then it gives me Unexpected identifier 'otherservice' instead.
Am I completely misunderstanding how this works or making some kind of syntax error? Many thanks in advance to anyone who can help!
",76383823,"The syntax is definitely wrong, but in the function definition. It should look like this. No guarantees on whether the functionality is correct.:
$(document).ready(function(){
    let service = '';
    let otherservice = '';
    const checkTooltip = () => {
        service = '.v' + $('select#989022_58716pi_989022_58716 option:selected').val();
        otherservice = '#form-tooltips div:not(""+service+"")';
        $('#form-tooltips div'+service).show();
        $(otherservice).hide();
    }
    checkTooltip();
    $('select#989022_58716pi_989022_58716').on('change', checkTooltip);
});

Note the change in the on.change handler at the end too.
Also its out of scope for your question but 989022_58716pi_989022_58716 probably could be rewritten with a more human-readable id or class.
",
76383689,Azure Devops git repository permissions: how to prioritize 'Allow' over 'Deny'?,"I have a git repository created in Azure Devops. I need to restrict access to it in such a way that the repo is accessible only for TeamA. When I set Deny for all other groups and Allow only for TeamA, the permission Deny takes preference when the user belongs to both Contributors and TeamA. Would you please help me to achieve this? I cannot grant access to all contributors except for those in TeamA.
I tried security settings in repository.
",76383855,"Use the Not Set permission as an implicit Deny. As you've discovered, explicit Deny takes precedence over explicit Allow.
",
76382065,getting rid of bold characters in a filename,"mysql recently reported me the following error: [HY000][1366] Incorrect string value: '\xF0\x9D\x98\xBD\xF0\x9D...' for column 'name'
after investigation, I found that the value with weird characters comes from a filename, which apparently contains bold characters: 4 𝘽𝘼𝙉𝘿𝙀 𝘼𝙉𝙉𝙊𝙉𝘾𝙀 - TV.mp4
Instead of changing the encoding of my database to accept such characters, i'd rather sanitize the value before inserting it, in PHP. But I have no idea which operation I should run to end with the following sanitized value : 4 BANDE ANNONCE - TV.mp4
Any help would be appreciated.
",76382676,"You can use the PHP iconv function to convert the string from one character encoding to another. In this case, you can try converting the string from UTF-8 to ASCII//TRANSLIT, which will attempt to transliterate any non-ASCII characters into their closest ASCII equivalents.
Here's an example:
function sanitize_string($input_string) {
    $sanitized_string = iconv(""UTF-8"", ""ASCII//TRANSLIT"", $input_string);
    return $sanitized_string;
}

$filename = ""4 𝘽𝘼𝙉𝘿𝙀 𝘼𝙉𝙉𝙊𝙉𝘾𝙀 - TV.mp4"";
$sanitized_filename = sanitize_string($filename);
echo $sanitized_filename;

This should output 4 BANDE ANNONCE - TV.mp4, which is the sanitized value you're looking for.
",
76385320,How to get variable from component for put on the App.js,"even by searching on the internet I did not find, or in all that I did not understand.
My problem: I would like the ""inputVal"" variable found in the ""InputField.js"" component to be found where there is ""!!HERE!!"" in the ""App.js"" component(on the fetch)
please help me
thank you for reading my message!
export default function InputField() {
  
  function handleSubmit(e) {
    // Prevent the browser from reloading the page
    e.preventDefault();

    // Read the form data
    const form = e.target;
    const inputVal = form.myInput.value;

    console.log(inputVal);
  }

  return (
    <form method=""post"" onSubmit={handleSubmit}>
      <input name=""myInput"" id=""adress-field"" placeholder=""Enter adress"" autoComplete=""on"" />
      <button type=""submit"" id=""adress-button"">Send</button>
    </form>
  );
}

import './App.css';
import AccountNumber from ""./components/AccountNumber"";
import InputField from ""./components/InputField"";
import { useEffect, useState } from ""react""

function App() {

  //token fetch
  const [tockens, setTockens] = useState([])
  const [loading, setLoading] = useState(false)
  useEffect(() => {
    setLoading(true)
    fetch(""https://api.multiversx.com/accounts/!!HERE!!/tokens"")
      .then(response => response.json())
      .then(json => setTockens(json))
      .finally(() => {
        setLoading(false)
      })
      console.log(tockens); 
  }, [])


  function round(nr, ten) { // arondi un chiffre.
    return Math.round(nr * ten) / ten;
}

function numberWithSpaces(nr) { // formate un chiffre(x xxx xxx).
    return nr.toString().replace(/\B(?=(\d{3})+(?!\d))/g, "" "");
}



  return (
    <content className=""content"">
        <div className=""up-side"">
            <div className=""account-number-box"">
                <p id=""p-account-number"">Total number of accounts</p>
                <p id=""account-number""><AccountNumber/></p>
            </div>
            <div className=""adress-search"">
                {InputField()}
            </div>
            <p>{window.inputVal}</p>
        </div>
        <div className=""down-side"">
            <table className=""Token-section-output"">
            

              {loading ? (
                <div>Loading...</div>
              ) : (
                <>
                  <h1>Tockens</h1>
                  <table className='Token-section-output' border={0}>
                    <tr className='token-row-type'>
                      <th className='token-column'>Name</th>
                      <th className='center-column'>Price</th>
                      <th>Hold</th>
                    </tr>
                    <tr className=""space20""/>

                    
                    {tockens.map(tocken => (
                      <tr className='token-row' key={tocken.id}>
                        <td className='token-column'>
                        <img className=""img-Tockens"" src = {tocken?.assets?.pngUrl ?? ""img/Question.png""} /> 
                          <p>{tocken.name}</p> 
                        </td>

                        <td className='center-column'> <p>${round(tocken.price, 10000000)}</p> </td>

                        <td> 
                          <p>{round(tocken.balance / Math.pow(10, tocken.decimals), 10000000)}</p> 
                          <p className='token-hold'>${round(tocken.valueUsd, 10000000)}</p>
                        </td>
                      </tr>
                    ))}

                  </table>
                </>
              )}


            </table>
        </div>   
    </content>
  );
}

export default App;

I not very good in react and i mak search on internet
",76385519,"You want to extend your InputField component to accept a callback function, that can be passed by your app:
export default function InputField({onSubmit}) {
  
  function handleSubmit(e) {
    // Prevent the browser from reloading the page
    e.preventDefault();

    // Read the form data
    const form = e.target;
    const inputVal = form.myInput.value;

    console.log(inputVal);
    onSubmit(inputVal)
  }

  ...
}

And in your App you need to pass that callback to your component:
<div className=""adress-search"">
  <InputField onSubmit={handleSearchSubmit} />
</div>

Note: Components are not consumed by calling them like functions.
In your App logic, you'll need another state to hold your search value:
...
const [searchValue, setSearchValue] = useState(null);

const handleSearchSubmit = (val) => {
  setSearchValue(val);
}

useEffect(() => {
    setLoading(true)
    fetch(`https://api.multiversx.com/accounts/${searchValue}/tokens`)
      .then(response => response.json())
      .then(json => setTockens(json))
      .finally(() => {
        setLoading(false)
      });
    console.log(tockens); 
}, [searchValue])
...

",
76381829,Default class instantiation results in TypeError (ESM/CJS interop),"Issue Summary
Hi,
I have a TypeScript project where I am trying to instantiate a class which was the default export of a different package. I am writing my project in ESM syntax, whereas the package it's dependent upon has CJS output. The issue I am running into is that at runtime, when the flow reaches the point of class instantiation I am getting the following error -
new TestClass({ arg1: ""Hello, World!"" });
^

TypeError: TestClass is not a constructor


Code
//My package.json
{
  ""name"": ""myproject"",
  ""version"": ""1.0.0"",
  ""main"": ""dist/index.js"",
  ""scripts"": {
    ""build"": ""tsc"",
    ""start"": ""node dist/index.js""
  },
  ""type"": ""module"",
  ""dependencies"": {
    ""testpackage"": ""^1.0.0"",
    ""typescript"": ""^5.0.4""
  },
  ""devDependencies"": {
    ""@types/node"": ""^20.2.5""
  }
}


//My index.ts
import TestClass from ""testpackage"";

new TestClass({ arg1: ""Hello, World!"" });

//My tsconfig.json
{
    ""include"": [""src""],
    ""compilerOptions"": {
        ""outDir"": ""dist"",
        ""lib"": [""es2023""],
        ""target"": ""es2022"",
        ""moduleResolution"": ""node""
    }
}


//Dependency's package.json
{
    ""name"": ""testpackage"",
    ""version"": ""1.0.0"",
    ""description"": ""TestPackage"",
    ""main"": ""./dist/testFile.js"",
    ""exports"": ""./dist/testFile.js"",
    ""scripts"": {
        ""build"": ""tsc""
    },
    ""files"": [""dist""],
    ""devDependencies"": {
        ""@types/node"": ""^20.2.5"",
        ""typescript"": ""^5.0.4""
    }
}

//Dependency's testFile.ts
export default class TestClass {
    constructor({ arg1 }: { arg1: string }) {
        console.log(arg1);
    }
}

//Dependency's tsconfig.json
{
    ""include"": [""src""],
    ""compilerOptions"": {
        ""declaration"": true,
        ""lib"": [""es2023""],
        ""target"": ""es6"",
        ""module"": ""CommonJS"",
        ""outDir"": ""dist""
    }
}

//Dependency's testFile.js output
""use strict"";
Object.defineProperty(exports, ""__esModule"", { value: true });
class TestClass {
    constructor({ arg1 }) {
        console.log(arg1);
    }
}
exports.default = TestClass;


Things work fine if I remove ""type"": ""module"" from my package.json. They also work fine if the class is a named export instead of a default export in the dependency's code. Is this a known incompatibility when trying to import CJS into ESM or am I doing something incorrectly here?

Note - If I set ""moduleResolution"": ""nodenext"" in my tsconfig.json then the error is generated at compile time itself -
src/index.ts:3:5 - error TS2351: This expression is not constructable.
  Type 'typeof import(""<project_dir>/node_modules/testpackage/dist/testFile"")' has no construct signatures.

3 new TestClass({ arg1: ""Hello, World!"" });
      ~~~~~~~~~


Found 1 error in src/index.ts:3

",76382696,"There are known compatibility issues between CommonJS (CJS) and ECMAScript modules (ESM). In ESM, default exports of CJS modules are wrapped in default properties instead of being exposed directly. On the other hand, named exports are unaffected and can be imported directly.
If you specify ""type"":
Specifying ""module"" in package.json makes Node.js treat the .js file as her ESM. Therefore, you must import the module using the ESM import statement. However, if the module you are trying to import is in CJS format, you will run into compatibility issues.
There are several options to fix this.

Access the class through the default property as described above.

import Test from 'package-name';
const TestClass = Test.default;



To avoid problems caused by mixing the two module formats, convert all code to use either ESM or CJS.

Load the CJS module using the Node.js createRequire function.


import { createRequire } from 'module';
const require = createRequire(import.meta.url);
const TestClass = require('package-name');


",
76381891,Extract href from string and again bind the updated data to the element in Angular,"I am getting one string like this in innerHtml
<div class=""wrapper"" [innerHTML]=""data.testString"">

data.testString contains data like below,
data.testString=""<p>some info, email <a href=""mailto:test@test.com"">test@test.com</a>  info </p>"";

I want to add aria-label for the anchor tag.
<a aria-label=""test@test.com"" href=""mailto:test@test.com"">test@test.com</a>

so I have added below code in .ts file
ngAfterViewInit(): void {

    var myData = this.data.testString;
    var element = myData!.match!(/href=""([^""]*)/)![1];
    
   
    var ariaLabel = ""EmailId "" + element;
    
    this.data.testString=
    this.data.testString!.replace('<a', '<a aria-label = ""' + ariaLabel + '"" ');
      
  }
}


But I am getting below error
global-error-handler.ts:26 TypeError: Cannot assign to read only property 'testString' of object '[object Object]'

How to resolve this?
",76382769,"I'm not sure to understand all but below might help you :

String character escaping


var testString=""<p>some info, email <a href=""mailto:test@test.com"">test@test.com</a>  info </p>"";

There is a quote issue : a double-quoted string cannot contain double-quotes unless escaped by the \ character (cf. https://www.w3schools.com/js/js_strings.asp -> Escape Character section)
You may fix this issue by replacing some double quotes by simple quotes :
""<p>some info, email <a href='mailto:test@test.com'>test@test.com</a>  info </p>""

Regular expression


var href = datar!.match!(/href=""([^""]*)/)![1];

Not sure this is the pattern you need. Try instead :
var href = datar!.match!(/href=\'mailto:([a-z@.]+)'/)![1];

DOM element manipulation

About adding the aria-label attribute to the , you would better go with Angular nativeElement.setAttribute(key, value) native javascript querySelector
(cf. https://indepth.dev/posts/1336/how-to-do-dom-manipulation-properly-in-angular and https://angular.io/guide/property-binding)
Finally I would highly recommend the use of RegExr to test your regular expressions https://regexr.com/7ep0u
as well as jsfiddle.net to test your javascript in a safe environment : https://jsfiddle.net/nkarfgx3/
",
76383765,"One instance of Stripe Checkout works, the other gives a Preflight response code of 403","I am implementing another instance of the Checkout Session in my app.  In my donations controller, the following create action works fine:
def create
        @donation = Donation.create(create_params)

        if @donation.save
            if Rails.env.development?
                success_url = ""http://localhost:3000/donations_success?session_id={CHECKOUT_SESSION_ID}""
                cancel_url = ""http://localhost:3000/""
            elsif Rails.env.production?
                success_url = ""https://www.dbsan.org/donations_success?session_id={CHECKOUT_SESSION_ID}""
                cancel_url = ""https://www.dbsan.org/""
            end

            data = {
                line_items: [{
                    price_data: {
                        currency: 'usd',
                        product_data: {
                        name: @donation.program
                    },
                    unit_amount: @donation.amount.to_i
                    },
                    quantity: 1,
                }], 
                mode: 'payment',
                customer_email: @donation.email,
                success_url: success_url,
                cancel_url: cancel_url
            }

            session = Stripe::Checkout::Session.create(data)
            redirect_to session.url, allow_other_host: true
        end
    end

I copied the relevant Stripe part into my participant registration controller:
def create
        @registrant = @challenge.challenge_participants.build(register_params)
        @registrant.user_id = current_user.id
        unless @registrant.donations.empty?
            @registrant.donations.first.user_id = current_user.id
            @registrant.donations.first.email = current_user.email
        end

        if @registrant.save
            @challenge = @registrant.challenge
            ChallengeMailer.with(registrant: @registrant).registered.deliver_now
            if @registrant.price.price == 0
                redirect_to challenge_participant_path(@challenge, @registrant)
            else
                if Rails.env.development?
                    success_url = ""http://localhost:3000/donations_success?session_id={CHECKOUT_SESSION_ID}""
                    cancel_url = ""http://localhost:3000/""
                elsif Rails.env.production?
                    success_url = ""https://www.dbsan.org/donations_success?session_id={CHECKOUT_SESSION_ID}""
                    cancel_url = ""https://www.dbsan.org/""
                end

                data = {
                    line_items: [{
                        price_data: {
                            currency: 'usd',
                            product_data: {
                            name: ""Registration""
                        },
                        unit_amount: 100
                        },
                        quantity: 1,
                    }], 
                    mode: 'payment',
                    success_url: success_url,
                    cancel_url: cancel_url
                }

                session = Stripe::Checkout::Session.create(data)
                redirect_to session.url, allow_other_host: true
            end
        end

The Donations one will redirect to Stripe without issue; however, the registration one, if a pricing selected is greater than 0, it will then attempt to initiate a Stripe Checkout.  In my browser console I get a Preflight response was not successful error code 403 with some TypeError that it is not giving me details of.
on both of the views, the Stripe API Javascript is included just above the submit button:
= javascript_include_tag ""https://js.stripe.com/v3""

Since I copied the code over from the donations controller, I'm not seeing what my error is.
I haven't updated the success_url yet as I'm trying to first get redirected to Stripe.  The name and unit_amount are right now hard coded in case my variables aren't working.
",76383881,"The code you shared is a simple HTTP redirect server-side in Ruby and shouldn't cause a CORS error in the browser unless your client-side code is making an ajax request instead of a page/form submit.
Alternatively, it's possible your form submission is mis-configured and Rails turns this in a turbo request. Adding data-turbo=false to your form might solve that problem.
",
76385399,intersection across date ranges from multiple rows in oracle,"Intersection of date ranges across rows in oracle.
I have a table which contains following records




Item_no
item_type
active_from
active_to
rule_id




10001
SAR
2020-01-01
2023-01-01
rule1


10001
SAR.
2024-01-01
9999-12-31
rule1


10001
SAR
2020-05-01
2021-06-01
rule2


10001
SAR
2021-01-01
2021-02-01
rule2




We need to find common dates between rule ids
Output will be




Item_no
item_type
active_from
active_to




10001
SAR
2020-05-01
2021-06-01




I tried with connect by level to generate dates and then take intersection, but it is running for long time due to 9999-12-31
",76385527,"From Oracle 12, you can UNPIVOT the dates and then use analytic functions and MATCH_RECOGNIZE to process the result set row-by-row to find the consecutive rows where both rules are active:
SELECT *
FROM   (
  SELECT item_no,
         item_type,
         rule_id,
         dt,
         SUM(CASE rule_id WHEN 'rule1' THEN active END) OVER (
           PARTITION BY item_no, item_type ORDER BY dt, ACTIVE DESC
         ) AS rule1,
         SUM(CASE rule_id WHEN 'rule2' THEN active END) OVER (
           PARTITION BY item_no, item_type ORDER BY dt, ACTIVE DESC
         ) AS rule2
  FROM   table_name
         UNPIVOT (
           dt FOR active IN ( active_from AS 1, active_to AS -1 )
         )
)
MATCH_RECOGNIZE(
  PARTITION BY item_no, item_type
  ORDER BY dt, rule1 DESC, rule2 DESC
  MEASURES
    FIRST(dt) AS active_from,
    NEXT(dt) AS active_to
  PATTERN ( active_rules+ )
  DEFINE active_rules AS rule1 > 0 AND rule2 > 0
)

Which, for the sample data:
CREATE TABLE table_name (Item_no, item_type, active_from, active_to, rule_id) AS
SELECT 10001, 'SAR', DATE '2020-01-01', DATE '2023-01-01', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2024-01-01', DATE '9999-12-31', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2020-05-01', DATE '2021-06-01', 'rule2' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2021-01-01', DATE '2021-02-01', 'rule2' FROM DUAL;

Outputs:




ITEM_NO
ITEM_TYPE
ACTIVE_FROM
ACTIVE_TO




10001
SAR
2020-05-01 00:00:00
2021-06-01 00:00:00




and for:
CREATE TABLE table_name (Item_no, item_type, active_from, active_to, rule_id) AS
SELECT 10001, 'SPR', DATE '2023-01-01', DATE '2023-01-31', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SPR', DATE '2023-01-31', DATE '2023-02-27', 'rule2' FROM DUAL;

The output is:




ITEM_NO
ITEM_TYPE
ACTIVE_FROM
ACTIVE_TO




10001
SPR
2023-01-31 00:00:00
2023-01-31 00:00:00




fiddle
",
76384723,How to make an Array of images getting by input field and display the Array image as slider,"My problem is to make array of image by input field and display array images as slider in JavaScript
anyone can solve it please answer me
Please give code of JavaScript
    document.querySelector(""#a"").addEventListener(""change"", function(){
           const reader = new FileReader();
              reader.addEventListener(""load"", ()=>{
                   localStorage.setItem(""recent-image"", reader.result)
              });
           reader.readAsDataURL(this.files[0]);
    });
    document.addEventListener(""DOMContentLoaded()"", ()=> {
        
        const imageurl = localStorage.getItem(""recent-image"");

        if(imageurl){
            document.querySelector(""#b"").setAttribute(""src"", imageurl);
        }
    });

can you do this in array please answer me
it take only one image image but, I want to store multiple image in local storage by array
please answer me
",76385530,"The problem that I see is to define your localstorage and make sure that the images are stored in it to later go through each one of them, I leave an example of how to solve it
var images = localStorage.getItem('images') || [];

function saveImages() {
    localStorage.setItem('images', JSON.stringify(images));
}

function drawImages() {
    var slider = document.getElementById('slider');
    slider.innerHTML = '';
    for (var i = 0; i < images.length; i++) {
        const img = images[i];
        const html_img = document.createElement('img');
        html_img.src = img;
        html_img.alt = 'Alt img';
        html_img.width = 200;
        html_img.height = 150;
        slider.appendChild(html_img);
    }
}

document.querySelector(""#a"").addEventListener(""change"", function(){
    const reader = new FileReader();
    reader.addEventListener(""load"", ()=>{
        images.push(reader.result);
        drawImages();
    });
    reader.readAsDataURL(this.files[0]);
});

document.addEventListener(""DOMContentLoaded()"", drawImages);

",
76380801,Problem passing an async function item as parameter in Rust,"[Edit: Updated provided code and compiler error to be easely reproduced]
I'm trying to pass an async function item as parameter to an other function in Rust but it won't compile, providing a cryptic error.
Here is the code I'm trying to compile.
Structure definition (implemented in one crate)
pub struct FirstTestComponent {
    table: Vec<String>,
    counter: usize,
}

impl FirstTestComponent {
    fn render(&mut self) {
        // Some other irrelevant code
        record_callback(
            FirstTestComponent::add_component,
        );
    }
}

pub struct Services {}

impl FirstTestComponent {

    async fn add_component(&mut self, _: &mut Services) {

        let counter = self.counter;
        self.table.push(counter.to_string());
        self.counter += 1;
    }
}   


pub fn record_callback<F, Fut>(callback: F)
    where 
        F: 'static + Copy + FnOnce(&mut FirstTestComponent, &mut Services) -> Fut,
        Fut: core::future::Future<Output = ()>  + 'static
        {
        }

fn main() {

}

When compiling this code, I get the following error:
error[E0308]: mismatched types
  --> src/main.rs:9:9
   |
9  | /         record_callback(
10 | |             FirstTestComponent::add_component,
11 | |         );
   | |_________^ one type is more general than the other
   |
   = note: expected trait `for<'r, 's> <for<'r, 's> fn(&'r mut FirstTestComponent, &'s mut Services) -> impl for<'r, 's> std::future::Future<Output = ()> {FirstTestComponent::add_component} as std::ops::FnOnce<(&'r mut FirstTestComponent, &'s mut Services)>>`
              found trait `for<'r, 's> <for<'r, 's> fn(&'r mut FirstTestComponent, &'s mut Services) -> impl for<'r, 's> std::future::Future<Output = ()> {FirstTestComponent::add_component} as std::ops::FnOnce<(&'r mut FirstTestComponent, &'s mut Services)>>`
note: the lifetime requirement is introduced here
  --> src/main.rs:30:79
   |
30 |         F: 'static + Copy + FnOnce(&mut FirstTestComponent, &mut Services) -> Fut,
   |                                                                               ^^^


literally saying that something is different than itself...
I guess there is some error linked to the lifetime hidden in the error message but I can't figure it out.
What could cause this error ? What is wrong with what I've implemented ?
",76382830,"I found the answer here: How to bind lifetimes of Futures to fn arguments in Rust
The problem as I understand it is this:
From https://rust-lang.github.io/async-book/03_async_await/01_chapter.html:

Unlike traditional functions, async fns which take references or other non-'static arguments return a Future which is bounded by the lifetime of the arguments

So I had to force the Fut lifetime to be shorter than the input parameters lifetime.
The syntax won't let me simply force this by using Higher Ranked Lifetime bounds (as explained in the answer linked above) so I had to use the pattern proposed there: generate a meta trait linking lifetimes as this:
trait XFn<'a, T, S> {
    type Output: Future<Output = ()> + 'a;
    fn call(&self, this: T, services: S) -> Self::Output;
  }
  
  impl<'a, T: 'a, S: 'a, F, Fut> XFn<'a, T, S> for F
  where
    F: 'static + Copy + FnOnce(T, S) -> Fut,
    Fut: Future<Output = ()> + 'a,
  {
    type Output = Fut;
    fn call(&self, this: T, services: S) -> Fut {
        self(this, services)
    }
  }

Then I can use this trait to constraints lifetime on arguments and Future like this:
fn record_callback<F>(callback: F)
    where 
            for<'a> F: XFn<'a, &'a mut FirstTestComponent, &'a mut Services> + 'static + Copy,
    {
    }


",
76383199,Passing a prop/variable to react router 6 loader function,"My routes are working and accessing components and loader functions. I'm trying now to pass variable filmsPerPage (which is defined once) to both Home component and the loader function in
App.js:
const App = () => { 
  const filmsPerPage = 12
    
  const router = createBrowserRouter([
    {
      path: '/',
      children: [
        {
          index: true,
          element: <Home {...{filmsPerPage}} />,
          loader: () => { loaderHome(filmsPerPage) }
        },
        .................

Home.js:
const Home = (props) => {
  const { loaderData } = useLoaderData() // get loader data --> null
  console.log(props.filmsPerPage) // printing out correctly: 12
  .......
}
    
export default Home;
    
export function loaderHome(filmsPerPage) {
  console.log(filmsPerPage) --> printing out 12
  return defer({
    loaderData: loadPosts(null, 1, 12, null)
  })
}

The prop filmsPerPage is passing correctly to Home.js component and while it's passing to the loader function, the useLoaderData() in Home.js is returning null which means that although the code in the loader function is working properly, it's not returning a loader object to the component. If I do this in App.js, the useLoaderData() function (in Home.js) will get the data but now the loader function doesn't have the prop:
children: [
  {
    index: true,
    element: <Home {...{filmsPerPage}} />,
    loader: loaderHome
  },

How could I pass the filmsPerPage prop to the loader function which will then return loader data to Home.js?
",76383930,"The loader function isn't returning anything. Perhaps reformatted to a more readable format will make this more apparent:
{
  index: true,
  element: <Home {...{filmsPerPage}} />,
  loader: () => {
    loaderHome(filmsPerPage); // <-- not returned!!
  },
}

The loader should still return the result of calling loaderHome.
Examples:
{
  index: true,
  element: <Home {...{filmsPerPage}} />,
  loader: () => {
    return loaderHome(filmsPerPage); // <-- explicit return in function block
  },
}

{
  index: true,
  element: <Home {...{filmsPerPage}} />,
  loader: () => loaderHome(filmsPerPage), // <-- implicit arrow function return
}

You could even rewrite loaderHome to curry, e.g. close over in function scope, the filmsPerPage argument.
export function loaderHome(filmsPerPage) {
  console.log(filmsPerPage); --> printing out 12

  // Return loader function
  return (loaderArgs) => {
    return defer({
      loaderData:  loadPosts(null, 1, filmsPerPage, null);
    });
  };
}

{
  index: true,
  element: <Home {...{filmsPerPage}} />,
  loader: loaderHome(filmsPerPage),
}

",
76385338,How to send Messages after Spring Integration application restarted?,"I have a small Spring Integration application, I'm storing messages and messaging groups in the database. Currently, I have a case when some messages/groups are waiting to be sent after group timeout, but the application restarted. And when the application started I still have messages in DB and they won't be sent. I need some configuration to send expired message group from DB or resume timer. I tried to use reaper, but it does not work as expected. My code is:
@Configuration
public class ConsumingChannelConfig {

    @Bean
    public DirectChannel consumingChannel() {
        return new DirectChannel();
    }

    @Bean
    public KafkaMessageDrivenChannelAdapter<String, String> kafkaMessageDrivenChannelAdapter() {
        KafkaMessageDrivenChannelAdapter<String, String> kafkaMessageDrivenChannelAdapter =
                new KafkaMessageDrivenChannelAdapter<>(kafkaListenerContainer());
        kafkaMessageDrivenChannelAdapter.setOutputChannel(consumingChannel());
        MessagingMessageConverter messageConverter = new MessagingMessageConverter();
        messageConverter.setGenerateMessageId(true);
        kafkaMessageDrivenChannelAdapter.setRecordMessageConverter(messageConverter);
        return kafkaMessageDrivenChannelAdapter;
    }

    @Bean
    public DataSource getDataSource() {
        return ...;
    }

    @Bean
    public JdbcMessageStore jdbcMessageStore() {
        return new JdbcMessageStore(getDataSource());
    }

    @ServiceActivator(inputChannel = ""consumingChannel"")
    @Bean
    public MessageHandler aggregator() {
        long timeout = 10000L;
        AggregatingMessageHandler aggregator =
                new AggregatingMessageHandler(new DefaultAggregatingMessageGroupProcessor(),
                        jdbcMessageStore());
        aggregator.setOutputChannel((message, l) -> {
            System.out.println(""MESSAGE: "" + message);
            return true;
        });
        aggregator.setGroupTimeoutExpression(new ValueExpression<>(timeout));
//        aggregator.setTaskScheduler(this.taskScheduler);
        aggregator.setCorrelationStrategy(new MyCorrelationStrategy());
        aggregator.setSendPartialResultOnExpiry(true);
        aggregator.setExpireGroupsUponCompletion(true);
        aggregator.setExpireGroupsUponTimeout(true);
        aggregator.setDiscardChannel((message, timeout1) -> {
            System.out.println(""DISCARD: "" + message + "", timeout: "" + timeout1);
            return true;
        });
        aggregator.setReleaseStrategy(new ReleaseStrategy() {
            @Override
            public boolean canRelease(MessageGroup group) {
                return System.currentTimeMillis() - group.getTimestamp() >= timeout;
            }
        });
        return aggregator;
    }

    @Bean
    public MessageGroupStoreReaper reaper() {
        MessageGroupStoreReaper reaper = new MessageGroupStoreReaper(jdbcMessageStore());
        reaper.setPhase(1);
        reaper.setTimeout(2000L);
        reaper.setAutoStartup(true);
//        reaper.setExpireOnDestroy(true);
        return reaper;
    }

    @Bean
    public ConcurrentMessageListenerContainer<String, String> kafkaListenerContainer() {
        ContainerProperties containerProps = new ContainerProperties(""spring-integration-topic"");

        return new ConcurrentMessageListenerContainer<>(
                consumerFactory(), containerProps);
    }

    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        return new DefaultKafkaConsumerFactory<>(consumerConfigs());
    }

    @Bean
    public Map<String, Object> consumerConfigs() {
        Map<String, Object> properties = new HashMap<>();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, ""127.0.0.1:9092"");
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, ""spring-integration"");
        // automatically reset the offset to the earliest offset
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
//        DefaultKafkaHeaderMapper mapper = new DefaultKafkaHeaderMapper();
        return properties;
    }
}

UPD: My Solution
@EnableScheduling
@SpringBootApplication
public class SpringIntegrationExampleApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringIntegrationExampleApplication.class, args);
    }


    @Autowired
    private MessageGroupStoreReaper reaper;

    @Scheduled(initialDelay = 2000, fixedDelay = Long.MAX_VALUE)
    public void start() {
        reaper.run();
    }

}

",76385536,"The MessageGroupStoreReaper doesn't work by itself, it has to be called from a @Scheduled method: https://docs.spring.io/spring-integration/docs/current/reference/html/message-routing.html#reaper
However there is a nice option for you from an aggregator perspective:
/**
 * Perform a {@link MessageGroupStore#expireMessageGroups(long)} with the provided {@link #expireTimeout}.
 * Can be called externally at any time.
 * Internally it is called from the scheduled task with the configured {@link #expireDuration}.
 * @since 5.4
 */
public void purgeOrphanedGroups() {

You just need to set that expireTimeout > 0:
/**
 * Configure a timeout in milliseconds for purging old orphaned groups from the store.
 * Used on startup and when an {@link #expireDuration} is provided, the task for running
 * {@link #purgeOrphanedGroups()} is scheduled with that period.
 * The {@link #forceReleaseProcessor} is used to process those expired groups according
 * the ""force complete"" options. A group can be orphaned if a persistent message group
 * store is used and no new messages arrive for that group after a restart.
 * @param expireTimeout the number of milliseconds to determine old orphaned groups in the store to purge.
 * @since 5.4
 * @see #purgeOrphanedGroups()
 */
public void setExpireTimeout(long expireTimeout) {

See also docs on the matter: https://docs.spring.io/spring-integration/docs/current/reference/html/message-routing.html#aggregator-xml

Starting with version 5.4, the aggregator (and resequencer) can be configured to expire orphaned groups (groups in a persistent message store that might not otherwise be released).

",
76381843,Powershell - PSCustomObject with Calculated Property,"I'm importing a .csv file which I'm then modifying using calculated properties in a PSCustomObject.
I'm stuck on one calculation where I'm attempting to lookup a value from a datarow object using one of the .csv values.
We receive data with the supplier Part No and I need to lookup our corresponding Part No.
Would you be able to suggest how best to do this please?
The csv content looks like this:
Vendor Code,Part No,Part Description,Bonded,Quantity,PO No,Vendor Ref
TEZ,ABC1234,Dark Blue,No,50,4378923,ORD089234
TEZ,BBC1256,Orange,No,20,4378923,ORD089234
TEZ,ACD1349,Green,No,10,4378923,ORD089234

The SQL query $SKUs returns this as datarows:
ITEMNO VALUE
TYP-5063 ABC1234
TYP-5037 BBC1256
TYP-8069 ACD1349

So I'm looking to use the 'Part No' field from the .csv file to run a lookup against $SKUs.VALUE and return the matching $SKUs.ITEMNO.
The output .csv will then include a column called 'OUR_SKU' containing the $SKUs.ITEMNO value.
Here is my code so far:
$Files = Get-ChildItem -Path ""D:\Imports\Test\INVENTORY_HUB_RECEIPTS""
$ProcessingPath = ""D:\Imports\Test\INVENTORY_HUB_RECEIPTS\Processing\""
$UKEntity = ""TESTTRG""
$HUB_ID = ""TEST""
$SKUs = Invoke-Sqlcmd -ServerInstance ""localhost"" -Database ""XXXX"" -Query ""SELECT RTRIM(ITEMNO) AS ITEMNO, RTRIM(VALUE) AS VALUE FROM [XXXX].[dbo].[ICITEMO] WHERE OPTFIELD = 'CUSTITMNO' AND VALUE <>''""

foreach ($file in $Files) {

    
    $Content = (Import-Csv -path ($ProcessingPath + $file.Name)) |
    Select-Object @{n='HUB_ID'; e={ $HUB_ID }},
    @{e={$_.'Part No'}; l='PART_NO'},
    @{e={$_.Quantity}; l='QTY_RECEIVED'},
    DATE,
    @{n='ENTITY'; e={ $UKEntity }},
    @{e={$_.'Vendor Ref'.Substring($_.'Vendor Ref'.Length -8)}; l='ORDER_ID'},

    @{n='OUR_SKU'; e={ $SKUs | Where-Object {$($_.VALUE) -eq '123ABC'} | Select-Object -ExpandProperty ITEMNO}},
    @{n='OUR_SKU_X'; e={ $SKUs | Where-Object {$($_.VALUE) -eq $_.'PART_NO'} | Select-Object -ExpandProperty ITEMNO}}

    if ($Content.Count -eq 0) {Remove-Item ($ProcessingPath + $file.Name)} else {$Content | Export-Csv -Path ($ProcessingPath + $file.Name) -Not -Force}
    }

I've tried two examples for the new property 'OUR_SKU' this works but is obviously a static value.
The property 'OUR_SKU_X' is my attempt to use the supplied $.'PART_NO' and this currently returns a blank field.
The variable $SKUs does contain data and so does $.'PART_NO'.
I'm thinking it's either a simple syntax error or it's not possible to use $_.'PART_NO' in the script block?
Thanks
Colin
",76382973,"Per comments, inside the where-object scriptblock on the line:
$SKUs |  Where-Object {$($_.VALUE) -eq $_.'PART_NO'}
the automatic variable $_ relates to the individual items piped in from $SKUs, which hides the outer $_ from the $Content = (Import-Csv ...) | Select-Object ...
If you want to be able to access the outer $_ inside the where-object you'll need to capture it into a temporary variable like this:
e={ $tmp = $_; $SKUs | Where-Object { $_.VALUE -eq $tmp.PARTNO } | Select-Object -ExpandProperty ITEMNO}}
Here's a cut-down example:
$parts = @""
Vendor Code,Part No,Part Description,Bonded,Quantity,PO No,Vendor Ref
TEZ,ABC1234,Dark Blue,No,50,4378923,ORD089234
TEZ,BBC1256,Orange,No,20,4378923,ORD089234
TEZ,ACD1349,Green,No,10,4378923,ORD089234
""@ | ConvertFrom-Csv

$skus = @""
ITEMNO,VALUE
TYP-5063,ABC1234
TYP-5037,BBC1256
TYP-8069,ACD1349
""@ | ConvertFrom-Csv

$results = $parts | select-object @(
    @{l=""PART_NO"";  e={ $_.""Part No"" } },
    @{l=""DESC"";     e={ $_.""Part Description"" } },
    @{n='OUR_SKU';  e={ $part = $_; $skus | where-object { $_.VALUE -eq $part.""Part No"" } | Select-Object -ExpandProperty ITEMNO} }
)

Note the $part = $_; and $_.VALUE -eq $part.""Part No"" inside the definition of the third calculated property.
The output from the above is:
$results

PART_NO DESC      OUR_SKU
------- ----      -------
ABC1234 Dark Blue TYP-5063
BBC1256 Orange    TYP-5037
ACD1349 Green     TYP-8069

",
76383838,Hover is not working on navbar items and the cursor is changing to hand when we hover beside text and not when we hover on text,"The hover effect is not working in my code. Can someone help?When I run this code there is navbar present but is not clickable whereas the empty space on it's left side is clickable nor are my css hover effect working on it.


* {
    padding: 0;
    margin: 0;
    box-sizing: border-box;
    scroll-behavior: smooth;
    font-family: 'Poppins', sans-serif;
    list-style: none;
    text-decoration: none;
}

:root {
    /* global variables */
    --main-color: #ff702a;
    --text-color: #fff;
    --background-color: #1e1c2a;
    --big-font: 5rem;
    --h2-font: 2.25rem;
    --p-font: 0.9rem;
}

*::selection {
    background: var(--main-color);
    color: #fff;
}

body {
    color: var(--text-color);
    background: var(--background-color);
}

header {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    z-index: 1000;
    /*z-index defines stack order of element*/
    display: flex;
    align-items: center;
    /*controls space around cross axis*/
    justify-content: space-between;
    /*controls space around main axis*/
    padding: 30px 170px;
    background: var(--background-color);
}

.logo {
    color: var(--main-color);
    font-weight: 600;
    font-size: 2.4rem;
}

.navbar {
    display: flex;
}

.navbar li a {
    color: var(--text-color);
    font-size: 1.1rem;
    padding: 10px 20px;
    font-weight: 500;
}

.navbar li a:hover {
    color: var(--main-color);
    transition: .4s;
}
<!DOCTYPE html>
<html>

<head>
    <meta charset='utf-8'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <title>Website for Foodies!</title>
    <meta name='viewport' content='width=device-width, initial-scale=1'>
    <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
    <link rel=""stylesheet"" href=""https://cdn.jsdelivr.net/npm/boxicons@latest/css/boxicons.min.css"">
    <link rel=""preconnect"" href=""https://fonts.googleapis.com"">
    <link rel=""preconnect"" href=""https://fonts.gstatic.com"" crossorigin>
    <link
        href=""https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;800&family=Poppins:wght@100;200;300;400;500;600;700;800;900&display=swap""
        rel=""stylesheet"">
</head>

<body>
    <header>
        <a href=""#"" class=""logo"">Foods</a>
        <div class=""bx bx-menu"" id=""menu-icon""></div>
        <!--class="" bx bx-menu"" is responsible for icon from boxicon-->
        <ul class=""navbar"">
            <li><a href=""#Home""></a>Home</li>
            <li><a href=""#About""></a>About</li>
            <li><a href=""#Menu""></a>Menu</li>
            <li><a href=""#Service""></a>Service</li>
            <li><a href=""#Contact""></a>Contact</li>
        </ul>
    </header>
</body>
</html>



I was trying to make a responsive website and I was expecting the text in my navbar to change color when i hover on it
",76383944,"According to your code :
.navbar li a:hover {
    color: var(--main-color);
    transition: .4s;
}

you tried to give hover effect on anchor tag as  class navbar > li > a.
Look at this your html code now:
<ul class=""navbar"">
            <li><a href=""#Home""></a>Home</li>
            <li><a href=""#About""></a>About</li>
            <li><a href=""#Menu""></a>Menu</li>
            <li><a href=""#Service""></a>Service</li>
            <li><a href=""#Contact""></a>Contact</li>
        </ul>

<a href=""#Home""></a>

There is nothing to show inside anchor tag. Hope you understand this problem. Put all the menu texts Home about menu service contact inside anchor tag. like this:
<a href=""#Home"">Home</a>

",
76383381,Adding new column for different rows based on the values present in the same row for a different column,"I have a dataframe with different combination of factors. each factor is presented in its column (see below)
F1   F2   F3   F4
1     1
1          1  
1                1

I want to add a new column at the end like below
F1   F2   F3   F4  trt
1    1             F1_F2
1          1       F1_F3
1               1  F1_F4

How do I create this column with conditional merging in R. Any advice would be appreciated!
",76383955,"aggregate(ind~row, na.omit(cbind(row = c(row(df)), stack(df))), paste, collapse = ""_"")
  row   ind
1   1 F1_F2
2   2 F1_F3
3   3 F1_F4


df <- structure(list(F1 = c(1L, 1L, 1L), F2 = c(1L, NA, NA), F3 = c(NA, 
1L, NA), F4 = c(NA, NA, 1L)), class = ""data.frame"", row.names = c(NA, 
-3L))

",
76385226,why my basic terraform fails to run terraform plan?,"I have this very basic terraform file: main.tf
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
    profile = ""default""
}

resource ""aws_s3_bucket"" ""test-bucket-terraform-regergjegreg"" {
  bucket = ""test-bucket-terraform-regergjegreg""

  tags = {
    Name        = ""My bucket""
    Environment = ""Dev""
  }
}

when doing terraform validate, I have no error:
Success! The configuration is valid.
Now when I do terraform plan, I got this error:
Planning failed. Terraform encountered an error while generating this plan.

╷
│ Error: configuring Terraform AWS Provider: credential type source_profile requires role_arn, profile default
│ 
│   with provider[""registry.terraform.io/hashicorp/aws""],
│   on main.tf line 12, in provider ""aws"":
│   12: provider ""aws"" {
│ 
╵

Here is my terraform version:
Terraform v1.4.6
on darwin_amd64
+ provider registry.terraform.io/hashicorp/aws v3.76.1

the default profile exists in my /.aws/credentials and /.aws/config
Not sure what could be wrong really. Any help is appreciated. thanks
",76385541,"Ok, I resolved the issue.
My problem was not in credentials file but in config.
For some reason I had this:
[default]
output = json
region = eu-west-1
source_profile = default

removing source_profile makes it work.
",
76381780,How to visualize formatted text without removing empty space?,"I want to visualize a pre-formatted text (with YAML format and indent). It seems that the <|{text}|> markdown pattern and the state representation removes intents from the text, i.e. all becomes a long mashed text. Here is an example output.
version: '3.1' stories: - story: 06a6e2c5e8bd4058b304b4f23d57aa80 steps: - intent: bot_capabilities user: What can you do

Correct is this:
version: '3.1'
stories:
-   story: 06a6e2c5e8bd4058b304b4f23d57aa80
    steps:
    -   intent: bot_capabilities
        user: What can you do

Is there a way to keep preformatted text especially with indents?
I could not yet find a fitting property for the ""text"" control. Raw does not seem to solve the issue. If I print the string before assigning it to a state variable, the output format is correct. Therefore, I assume the stripping of empty space happens automatically afterwards.
",76383057,"The most straightforward way is to use an input visual element with multiline property turned on.
main.py:
from taipy.gui import Gui 

#with open(""file.yaml"", ""r"") as f:
#    yaml_text = f.read()

yaml_text = """"""
version: '3.1'
stories:
-   story: loferum ipsi
    steps:
    -   intent: bot_capabilities
        user: What can you do
""""""

page = """"""
<|{yaml_text}|input|multiline|label=Input|>

<|{yaml_text}|input|multiline|not active|label=Inactive input|>

<|{yaml_text}|input|multiline|not active|label=Inactive white input|id=yaml|>
""""""

Gui(page).run()

Here is the result:

The first input is editable
The second is not editable and grey
The third is not editable and white


For the last element, I added a bit of CSS to make the inactive input white:
main.css:
#yaml.Mui-disabled{
    color: white !important;
    -webkit-text-fill-color: white !important;
}

An issue has been created on GitHub to improve text visual elements directly.
",
76383189,How can I limit os.walk results for a single file?,"I am trying to search a given directory for a specific file, and if that file does not exist I would want the code to say ""File does not exist"". Currently with os.walk I can get this to work, however this will hit on every single file that isn't the specified file and print ""File dos not exist"". I know that this is how os.walk functions, but I was not sure if there is a way to make it only print out once if it is found or not found.
Folder structure:
root folder|
|Project Folder
|file.xml
|other files/subfolders
How I would want the code to work is to go inside of ""Project Folder"", do a recursive search for ""file.xml"", and once it is found print out once ""Found"", otherwise prints out once ""Not found"".
The code is:
def check_file(x): #x = root folder dir
   for d in next(os.walk(x))[1]: #if I understand correctly, [1] will be Project Folder
        for root, directories, files in os.walk(x):
            for name in files:
                if ""file.xml"" not in name:
                    print(""found"")
                else:
                    print(""File Missing"")

If I change the code to
            for name in files:
                if ""file.xml"" in name:
                    print(""found"")
                else:
                    pass

The code technically works as intended, but it doesn't really do much to help point out if it isn't there, so this isn't a good solution. It would be easier if I was able to give the code a specific path to look in, however as the user is able to place the 'root folder' anywhere on their machine as well as the 'project folder' would have different names depending on the project, I don't think I would be able to give the code a specific location.
Is there a way to get this to work with os.walk, or would another method work best?
",76383961,"The glob module is very convenient for this kind of wildcard-based recursive search. Particularly, the ** wildcard matches a directory tree of arbitrary depth, so you can find a file anywhere in the descendants of your root directory.
For example:
import glob

def check_file(x):  # where x is the root directory for the search
    files = glob.glob('**/file.xml', root_dir=x, recursive=True)
    if files:
        print(f""Found {len(files)} matching files"")
    else:
        print(""Did not find a matching file"")

",
76385139,ABAP ODATA Service implement custom GET or POST request,"How can I implement a custom method to check if the user is exists by the given parameter in the url? So I want to make a GET request.
",76385542,"Assumption: you are talking about ABAP implmentation of OData v2 with SEGW approach.
You are looking for a so called Function Import.
This allows you to define custom functions next to the predefined CRUDQ (Create, Read, Update, Delete, Query) functions. This custom function can have custom input parameters and return values as simple types or Complex Types (=Structures/Entities)
As stated also in SAP Help this should be only used if it does not fit into the CRUDQ methods for your entities.
Find an example implementation in this SAP Blog
For other approaches, e.g. RESTful ABAP Programming Model (RAP), it would be Actions and Validations but cannot generally be answered without details of the scenario.
",
76380562,Have incremental local perforce scls similar to git for a single file,"Consider i have a file called developers.txt
What i want to do is (and i could do it in git neatly)
have 3-4 commits, each adding 1 line per commit
commit 2ad54bfe954006bafcb209f06ac0c12091d297c8 (HEAD -> main)
Author: Anuraag <anuraag@something.com>
Date:   Thu Jun 1 15:04:57 2023 +0530

    author #3 added

developers.txt

commit 26cabf7fa07154b19deff41c5cbb07bf0782bde7
Author: Anuraag <anuraag@something.com>
Date:   Thu Jun 1 15:04:43 2023 +0530

    author #2 added

developers.txt

commit d9e57385b505e2e02a8dc6064466c3974f22e66d
Author: Anuraag <anuraag@something.com>
Date:   Thu Jun 1 15:04:23 2023 +0530

    author #1 added

developers.txt

commit 03e7f4abddfa26b4b0518994c81e0a724ba1a778
Author: Anuraag <anuraag@something.com>
Date:   Thu Jun 1 15:03:56 2023 +0530

    adds heading

developers.txt

% cat developers.txt 
Authors:
1. Arthur
2. Canon
3. Doyle

What i want to know is, is there such an incremental local workspace development method when using perforce?
I want to have SCL#3 to be on top of SCL#2, SCL#1 etc.. but each SCL will be for the same file, in this case developers.txt
",76383104,"Yes, this is just basic versioning and should behave similarly across any version control system.  Every version builds on the one before it.
C:\Perforce\test>echo Authors:>developers.txt

C:\Perforce\test>p4 add developers.txt
//stream/main/developers.txt#1 - opened for add

C:\Perforce\test>p4 submit -d ""adds heading""
Submitting change 460.
Locking 1 files ...
add //stream/main/developers.txt#1
Change 460 submitted.

C:\Perforce\test>p4 edit developers.txt
//stream/main/developers.txt#1 - opened for edit

C:\Perforce\test>echo 1. Arthur>>developers.txt

C:\Perforce\test>p4 submit -d ""author #1 added""
Submitting change 461.
Locking 1 files ...
edit //stream/main/developers.txt#2
Change 461 submitted.

C:\Perforce\test>p4 edit developers.txt
//stream/main/developers.txt#2 - opened for edit

C:\Perforce\test>echo 2. Conan>>developers.txt

C:\Perforce\test>p4 submit -d ""author #2 added""
Submitting change 462.
Locking 1 files ...
edit //stream/main/developers.txt#3
Change 462 submitted.

C:\Perforce\test>p4 edit developers.txt
//stream/main/developers.txt#3 - opened for edit

C:\Perforce\test>echo 3. Doyle>>developers.txt

C:\Perforce\test>p4 submit -d ""author #3 added""
Submitting change 463.
Locking 1 files ...
edit //stream/main/developers.txt#4
Change 463 submitted.

Now we have our developers.txt with 4 versions.  We can see that the head revision contains all 4 changes:
C:\Perforce\test>cat developers.txt
Authors:
1. Arthur
2. Conan
3. Doyle

We can see its history as a list of the changes made to it:
C:\Perforce\test>p4 filelog developers.txt
//stream/main/developers.txt
... #4 change 463 edit on 2023/06/01 by Samwise@Samwise-dvcs-1509687817 (text) 'author #3 added'
... #3 change 462 edit on 2023/06/01 by Samwise@Samwise-dvcs-1509687817 (text) 'author #2 added'
... #2 change 461 edit on 2023/06/01 by Samwise@Samwise-dvcs-1509687817 (text) 'author #1 added'
... #1 change 460 add on 2023/06/01 by Samwise@Samwise-dvcs-1509687817 (text) 'adds heading'

And we can annotate the file to see the content of the file in context of the history, i.e. which revision/changelist added each line of content:
C:\Perforce\test>p4 annotate developers.txt
//stream/main/developers.txt#4 - edit change 463 (text)
1: Authors:
2: 1. Arthur
3: 2. Conan
4: 3. Doyle

C:\Perforce\test>p4 annotate -c developers.txt
//stream/main/shelves/developers.txt#4 - edit change 463 (text)
460: Authors:
461: 1. Arthur
462: 2. Conan
463: 3. Doyle

",
76383470,"Why by calling new File(""."").getAbsolutePath() I get completely different path from my project's location?","My project is located here:
D:/WorkSpace/PuzzleApp

but when I am calling
new File(""."").getAbsolutePath();

I get:
D:\!Documents\Desktop\.

Why? And how to fix this?
I'm using Eclipse.
",76383965,"When you open File with relative path ""."" this path is relative to the current process' working directory.
Usually, the process working directory is inherited from the parent process (e.g. if you run your app with Terminal - the current terminal's working directory will be your process' working dir).
Using a relative path inside your application is considered a bad idea because you can not control this process' working directory.
But using an absolute hardcoded path also goes with some problems and can't be called a good practice.
There are several ways to solve the issue:

Use a path relative to some NOT hardcoded absolute path. This absolute path should be externalized to environment variables, properties files, etc.

//Getting the directory path from the external system variable
String myAbsoultePath = System.getenv(""LOCAL_STORAGE_DIR"");


Use a path relative to your application root. It allows you to READ files even located inside .jar or .war archives.

//This path looks absolute, but the path's root is your project's root
InputStream stream = MyClass.class.getResourceAsStream(""/dir/another/file.txt"");

But you should understand, that your application won't run inside your IDE project folder. It will be deployed in a production server/user's desktop/android device environment and usually, it is packed in a jar/war/ear/... archive. It means that you won't have access to your src folder or something like this.
",
76381260,ClassCastException for configuration CGLIB proxy and org.springframework.cglib.proxy.Factory after upgrade Spring to 6.0.9 and Spring Boot to 3.0.6,"In our project, after upgrade the SpringBoot from 3.0.4 to 3.0.9, several of our tests started to fail on
Caused by: org.springframework.aop.framework.AopConfigException: Unexpected AOP exception
    at app//org.springframework.aop.framework.CglibAopProxy.buildProxy(CglibAopProxy.java:222)
    at app//org.springframework.aop.framework.CglibAopProxy.getProxy(CglibAopProxy.java:158)
    at app//org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:110)
    at app//org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.buildProxy(AbstractAutoProxyCreator.java:517)
    at app//org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.createProxy(AbstractAutoProxyCreator.java:464)
    at app//org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.wrapIfNecessary(AbstractAutoProxyCreator.java:369)
    at app//org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator.postProcessAfterInitialization(AbstractAutoProxyCreator.java:318)
    at app//org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:434)
    at app//org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1773)
    at app//org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:598)
    ... 80 more
Caused by: java.lang.ClassCastException: class our.project.RepositoryConfiguration$LiquibaseConfiguration$$SpringCGLIB$$0 cannot be cast to class org.springframework.cglib.proxy.Factory (our.project.RepositoryConfiguration$LiquibaseConfiguration$$SpringCGLIB$$0 and org.springframework.cglib.proxy.Factory are in unnamed module of loader 'app')
    at org.springframework.aop.framework.ObjenesisCglibAopProxy.createProxyClassAndInstance(ObjenesisCglibAopProxy.java:91)
    at org.springframework.aop.framework.CglibAopProxy.buildProxy(CglibAopProxy.java:213)
    ... 89 more

It's not bounded just to the our.project.RepositoryConfiguration$LiquibaseConfiguration. When I disable this configuration, then similar exception occurs on next configuration.
Another weird thing is that the test passes if only the single test class is called or if this class is executed first. Otherwise the ClassCastException ocurs.
We use TestNG for testing.
I tried to upgrade Spring to version 3.0.6 from 3.0.4 and Spring Boot to version 3.0.6 from 3.0.4 and I expect that all our current tests will pass. But for 34 tests of our ~ 2000 there is an exception java.lang.ClassCastException: class our.project.RepositoryConfiguration$LiquibaseConfiguration$$SpringCGLIB$$0 cannot be cast to class org.springframework.cglib.proxy.Factory (our.project.RepositoryConfiguration$LiquibaseConfiguration$$SpringCGLIB$$0 and org.springframework.cglib.proxy.Factory are in unnamed module of loader 'app')
",76383270,"Finally. after almost of week of investigations,  I managed to isolate the sinner. Solr 8.2.1 causes this issue. With Solr 8.2.0 it works with  Solr 8.2.1 there is class cast exception for spring proxies.
Hard to believe that this is related.
",
76380844,How can I create a frequency plot/histogram in R using ggplot2 while normalizing to the total of a factor?,"My problem is the following: I can make a figure in which the data is weighed relative to the entire population, but not relative to their own subpopulation. To illustrate with an example:
Suppose I have a dataset DS, with two columns: X and type.
X is a continues value ranging from -5 to 5, and type is either A, B or C.
How would I create a frequency plot of X in which each tuple is weighed by the total of its type, not the total of all tuples in the dataset?
This is my closest attempt, yet it weighs to the total population:
figure1 <- ggplot(data = DS, aes(x = X))+ geom_freqpoly(aes(colour = type, y= after_stat(count / sum(count)))) + ...
Its not surprising that this normalizes to the entire dataset, but I wouldnt know how to get it such that it only normalizes to a subset.
Using dput(), I generate the following example dataframe:
DS <- structure(list(X = c(0, -0.01, 0.042944432215413, 0.0431301011419889, 0.042944432215413, 0.0424042102083902, 0.2100000012 , 0.13513333335333), TimePoint = c(""early"", ""early"", ""late"", ""mid"", ""mid"", ""early"", ""late"", ""early"")), row.names = c(NA,8L), class = ""data.frame"")

In which 'X' is the continous value and 'TimePoint' is the factor which can be either 'early', 'mid' or late.
",76383334,"One option would be to use e.g. ave() to compute the count per group or Timepoint:
library(ggplot2)

ggplot(data = DS, aes(x = X)) +
  geom_freqpoly(
    aes(
      colour = TimePoint,
      y = after_stat(count / ave(count, group, FUN = sum))
    )
  )
#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.


",
76385118,Powershell/CMD issue,"So I was trying to write a quick Batch file but it only did half the code, so I switched it to PowerShell because I remembered a while back I was able to get it to work and turns out this did work.
My issue is essentially I have a handful of users I want to have access to this ""it just closes a program and reopens"". They're familiar with Batch files (which I couldn't get to reopen the program) but would not be used to using a PowerShell script and having to right click to run as PowerShell which I'm expecting will cause issues and many of the users to not use the script in the first place.
Is there either something I did wrong on the batch file for it not to reopen or is there a way to change the left click option on the PowerShell(currently left click opens the script in notepad; ideally it would just run as PowerShell on left click)
The code is the same for both PowerShell and CMD copied and pasted directly to.
taskkill /IM ADM.TrayApp.exe /F
Start-Process  ""C:\Program Files (x86)\athenahealth, Inc\aNetDeviceManager\3.1.4.0\TrayApp\CoreModule\ADM.TrayApp.EXE""

",76385549,"
You cannot use PowerShell's Start-Process directly in a batch file - you'd have to call via powershell.exe, the Windows PowerShell CLI or pwsh, the PowerShell (Core) CLI).

However, as Stephan points out, cmd.exe's internal start command provides similar functionality, so its use should be sufficient in your case (as Stephan notes, some window title enclosed in ""..."" is needed as the first argument if the executable to launch is enclosed in ""..."" too; """" will do):
start """" ""C:\Program Files (x86)\athenahealth, Inc\aNetDeviceManager\3.1.4.0\TrayApp\CoreModule\ADM.TrayApp.EXE""


It is possible to make PowerShell scripts execute by default when (double-)left-clicked from File Explorer or the desktop, but requires nontrivial setup on each machine:

See this answer.
The linked answer also describes an alternative technique of providing simple companion batch files whose sole purpose is to execute an associated PowerShell script.



",
76380912,What is the best way to find text content under relatively indistinguishable tags by Selenium-webdriver?,"As newbie I prefer use Abs XPath to get find WebElemnts where text is positioned.
I tried:
List<WebElement> elements = web.findElements(By.xpath(""/html[1]/body[1]/div[2]/div[2]/dl[1]/dd[2]/div[2]/div/div[1]/ol[5]/li[1]/div[2]/div/p""));

But i failed to catch text under tags with minor changes
Target xpaths:
/html[1]/body[1]/div[2]/div[2]/dl[1]/dd[2]/div[2]/div[6]/div[1]/ol[5]/li[1]/div[2]/div[4]/div[1]/div[1]/p[1]
/html[1]/body[1]/div[2]/div[2]/dl[1]/dd[2]/div[2]/div[6]/div[1]/ol[5]/li[1]/div[2]/div[8]/p
/html[1]/body[1]/div[2]/div[2]/dl[1]/dd[2]/div[2]/div[2]/div[1]/ol[5]/li[1]/div[2]/div[3]/h3[1]
/html[1]/body[1]/div[2]/div[2]/dl[1]/dd[2]/div[2]/div[2]/div[1]/ol[5]/li[1]/div[2]/div[2]/div[1]/p
/html[1]/body[1]/div[2]/div[2]/dl[1]/dd[2]/div[2]/div[5]/div[1]/ol[5]/li[1]/div[2]/div[1]/div[1]/p[1]/strong[1]

What is correct formula or way to get all text content in the above mentioned xpaths ?
",76383339,"Not very clear what you want: If you want all elements that contain direct text you could use:
/html/body[1]//*[text()[normalize-space()]]

this will return all elements with direct text()-nodes that after filtering unnecessary whitespace, have character-data.
meaning XPath-parts:
// = any descendant; see this info on axes
* = any element
[some filter] = predicate to filter on direct previous node.
[#number] = the position within its siblings. body[1] maybe seems redundant, but can help the XPath-engine not to look any further for other body elements
text() = node of type text
normalize-space() = strips white-space according this rules
",
76384278,How to do a lookup for the four primary keys columns yet output 6 columns to OLE DB Command,"I'm moving from a SQL Server database to a DB2 database. I'm trying to compare the primary key of the SQL Server database with the DB2 database and if its the same primary key values then do an update if they are not the same do an insert.
My problem is when I use a lookup for the primary key (first four columns) it only returns the first four columns, I need someway of getting the other two columns so I can run a OLE DB command and do the update.
I want to compare the four primary keys columns (FISCAL_YR, LOC_CODE, SYSTEM_ID, SYSTEM_CODE) and see if they exist in the destination table if they do I want to update two other columns (LOC_NAME, ALIAS_NAME) and if they don't match I want to do an insert.
This is what I have for the lookup currently but it matches all the columns I just want to do the first four (FISCAL_YR, LOC_CODE, SYSTEM_ID, SYSTEM_CODE) but output 6 columns (FISCAL_YR, LOC_CODE, SYSTEM_ID, SYSTEM_CODE, LOC_NAME, ALIAS_NAME) to OLE DB Command so I can do an update:
SELECT 
    FISCAL_YR, LOC_CODE, SYSTEM_ID,  
    SYSTEM_CODE, LOC_NAME, ALIAS_NAME 
FROM 
    LC1U1.Location_Supertbl1;

This is the update command I want to execute:
UPDATE LC1U1.Location_Supertbl1 
SET ALIAS_NAME = UPPER('test') 
WHERE FISCAL_YR = ? 
  AND LOC_CODE = ? 
  AND SYSTEM_ID = ? 
  AND SYSTEM_CODE = ?

How do I do this with a Lookup transformation in SSIS?
Thank you.
",76385594,"I think the issue you're not asking for the columns from the lookup component.
In the UI, you drag lines between the left (Source) and right (Lookup) side. This defines the equality match for the lookup.
What you want to do is check the 2 additional columns in that menu, something like the following image. This will add LOC_NAME and ALIAS_NAME as new columns, after the Lookup Component, into my data flow

",
76383537,How can I validate a MongoDB document using a validation schema from a file?,"I want to validate document before it is inserted into the database. I know that I can set static validator, but I would prefer to have a file with the validation schemas that I could modify at any time.
//example schema
const userCreateValidationSchema = {
        bsonType: 'object',
        required: ['username', 'password'],
        properties: {
            username: {
                bsonType: 'string',
                maxLength: 16,
            },
            password: {
                bsonType: 'string',
                maxLength: 64,
            },
        },
        additionalProperties: false,
};

//example document
const document = {
        username: ""user"",
        password: ""passwd"",
};

Then I would do something like validate(document, userCreateValidationSchema).
Thanks for any thoughts.
I have tried looking for the answer in the documentation but unfortunatelly didn't find the solution.
",76383985,"To perform login validation using the npm package Joi :-
npm install joi

Import Joi and Define Validation Schema :-
const Joi = require('joi');

const loginSchema = Joi.object({
  email: Joi.string().email().required(),
  password: Joi.string().min(6).required(),
});

In the above example, the validation schema requires the email field to be a valid email address and the password field to have a minimum length of 6 characters.
Perform Validation:
Now you can use the validate() method provided by Joi for the validation.
function validateLogin(loginData) {
  const { error, value } = loginSchema.validate(loginData);
  return error ? error.details[0].message : null;
}

Usage Example:
Here's an example of how you can use the validateLogin() function to validate the login data:
const loginData = {
  email: 'test@example.com',
  password: 'password123',
};

const validationError = validateLogin(loginData);

if (validationError) {
  console.log('Login validation failed:', validationError);
} else {
  console.log('Login data is valid.');
}

",
76385098,"R-Shiny, Use action button into a leaflet popup inside a Shiny module","I am trying to use an actionbutton in a leaflet popup into a shiny module
When trying to use an action button into a leaflet popup in a Shiny module, button do not work.
See the exemple below :
library(shiny)
library(leaflet)
library(DT)

map_ui <- function(id) {
  ns <- NS(id)
  tagList(
    
    leafletOutput(ns(""mymap""))
    
  )
}

map_Server <- function(id) {
  moduleServer(
    id,
    function(input, output, session) {
      
      mapdata <- datasets::quakes
      mapdata$latitude <- as.numeric(mapdata$lat)
      mapdata$longitude <- as.numeric(mapdata$long)
      mapdata$id <- 1:nrow(mapdata)
      
      output$mymap <- renderLeaflet({
        leaflet(options = leafletOptions(maxZoom = 18)) %>% addTiles() %>%
          addMarkers(lat = ~ latitude, lng = ~ longitude,
                     data = mapdata,
                     layerId = mapdata$id,
                     popup= ~paste(""<b>"", mag, ""</b></br>"", actionLink(inputId = ""modal"", label = ""Modal"", onclick = 'Shiny.setInputValue(\""button_click\"", this.id, {priority: \""event\""})')))
      })

      observeEvent(input$button_click, {
        showModal(modalDialog(
          title = ""TEST MODAL""
        ))      })
      

      
    }
  )
}

ui <- fluidPage(
  
  map_ui('ex1')
  
)

server <- function(input, output){
  map_Server('ex1')
  
}

shinyApp(ui, server)

Is there any way to make work that button into the module ? I think that it comes that the button is not ns() but i don't find a way to make it works.
Thanks
",76385628,"Yes, you have to add the ns:
function(input, output, session) {
  
  ns <- session$ns  
  
  ...... 
  
  output$mymap <- renderLeaflet({
    leaflet(options = leafletOptions(maxZoom = 18)) %>% addTiles() %>%
      addMarkers(
        lat = ~ latitude, lng = ~ longitude,
        data = mapdata,
        layerId = mapdata$id,
        popup = 
          ~paste(
            ""<b>"", mag, ""</b></br>"", 
            actionLink(
              inputId = ""modal"", label = ""Modal"", 
              onclick = sprintf(
                'Shiny.setInputValue(\""%s\"", this.id, {priority: \""event\""})',
                ns(""button_click"")
              )
            )
          )
      )
  })
  
  ......
  
}

",
76384287,Adding a legend to a ggplot map,"I am using ggplot2 to visualise map-related data. I have coloured regions according to a continuous value, and I would like to add a legend with colors and region names. My own data is a bit cumbersome to share, but I have recreated the scenario with public data (Mapping in ggplot2). The following code creates the included map:
library(ggplot2)
library(sf)

# Import a geojson or shapefile
map <- read_sf(""https://raw.githubusercontent.com/R-CoderDotCom/data/main/shapefile_spain/spain.geojson"")

ggplot(map) +
  geom_sf(color = ""white"", aes(fill = unemp_rate)) +
  geom_sf_text(aes(label = name), size = 2)


Instead of the continuous default legend, I would like to have a legend with names, numbers and colors. Basically, a legend that shows the name and unemp_rate columns of the data with colors matching the map (eg. unemp_rate). Somewhat like the legend of the second included picture (but the colors are not right).




name
unemp_rate




""Andalucía""
18.68


""Aragón""
8.96


""Principado de Asturias""
11.36


""Islas Baleares""
9.29


""Islas Canarias""
17.76


""Cantabria""
8.17


""Castilla y León""
10.19


""Castilla-La Mancha""
14.11


""Cataluña""
9.29


""Comunidad Valenciana""
12.81


""Extremadura""
16.73


""Galicia""
11.20


""Comunidad de Madrid""
10.18


""Región de Murcia""
12.18


""Comunidad Foral de Navarra""
8.76


""País Vasco""
8.75


""La Rioja""
10.19


""Ceuta y Melilla""
23.71





My actual code looks like so:
ggplot(map, aes(geometry = geometry, fill = Y1)) +
  theme_bw() + geom_sf(show.legend = FALSE) +
  scale_fill_gradient2(low = ""brown"", high = ""green"") +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank())

",76385632,"Perhaps an inset bar chart instead:
library(ggplot2)
library(sf)
library(dplyr)
library(patchwork)
# Import a geojson or shapefile
map_ <- read_sf(""https://raw.githubusercontent.com/R-CoderDotCom/data/main/shapefile_spain/spain.geojson"") %>% 
  mutate(name = forcats::fct_reorder(name, desc(unemp_rate)))

g1 <- map_ %>% 
  ggplot() +
  geom_sf(color = ""white"", aes(fill = unemp_rate)) +
  geom_sf_text(aes(label = name), size = 2) +
  scale_fill_gradient2(low = ""brown"", high = ""green"", midpoint = 16) +
  theme_minimal() +
  theme(legend.position = ""none"", axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank())

g2 <- map_ %>% 
  ggplot(aes(x = unemp_rate, y = name, fill = unemp_rate)) +
  geom_col() + 
  scale_fill_gradient2(low = ""brown"", high = ""green"", midpoint = 16) +
  geom_text(aes(label = name, x = .5, hjust = 0)) +
  geom_text(aes(label = unemp_rate), nudge_x = - .5, hjust = 1) +
  theme_void() +
  theme(legend.position = ""none"")

g1 + inset_element(g2, 0, .2, .9, 1)


",
76382104,android app LocalDate alternative for older API level,"I am building a calendar app and mostly in my code I use LocalDate to set my RecyclerView and some more functions.
I have a tablet that it's android version is 7.1.1, so it cant run anything in my code related to LocalDate.
I want to contest my self and make my app run to min SDK 16. Are there any alternatives for LocalDate for my code? I also use Calendar in my code but for my RecyclerView is almost the triple code to do same task. Any other suggestion would be apreciated.
",76383359,"If you want to have Java 8+ features available in lower Android version, you can use one of the following options:

import the ThreeTen Android Backport (ABP)
make use of Android API Desugaring

With (one of) these two, you can use nearly all of the classes and functions of java.time in Android API Levels < 26, including LocalDate.
",
76383806,I cant build my spring-boot multi module project with SAM,"When running SAM build, I get a dependency error on the module that depends on another local module within my project. From mvn or IntelliJ I have no problems, but when i execute SAM build, i got an error of notFound symbols and classses.

    Build Failed
    Error: JavaMavenWorkflow:MavenBuild - Maven Failed: [INFO] Scanning for projects...
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Build Order:
    [INFO]
    [INFO] backend                                                            [pom]
    [INFO] Api                                                                [jar]
    [INFO] Register                                                           [jar]
    [INFO]
    [INFO] ------------------------< com.backend:backend >-------------------------
    [INFO] Building backend 0.0.1-SNAPSHOT                                    [1/3]
    [INFO] --------------------------------[ pom ]---------------------------------
    [INFO]
    [INFO] --- maven-clean-plugin:3.2.0:clean (default-clean) @ backend ---
    [INFO]
    [INFO] --- spring-boot-maven-plugin:3.0.6:repackage (repackage) @ backend ---
    [INFO]
    [INFO] --- maven-install-plugin:3.0.1:install (default-install) @ backend ---
    [INFO] Installing /tmp/tmptflbmqgd/pom.xml to /home/laingard/.m2/repository/com/backend/backend/0.0.1-SNAPSHOT/backend-0.0.1-SNAPSHOT.pom
    [INFO]
    [INFO] --------------------------< com.backend:Api >---------------------------
    [INFO] Building Api 0.0.1-SNAPSHOT                                        [2/3]
    [INFO] --------------------------------[ jar ]---------------------------------
    [INFO]
    [INFO] --- maven-clean-plugin:3.2.0:clean (default-clean) @ Api ---
    [INFO] Deleting /tmp/tmptflbmqgd/Api/target
    [INFO]
    [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ Api ---
    [INFO] Copying 0 resource from src/main/resources to target/classes
    [INFO] skip non existing resourceDirectory /tmp/tmptflbmqgd/Api/config
    [INFO]
    [INFO] --- maven-compiler-plugin:3.10.1:compile (default-compile) @ Api ---
    [INFO] Changes detected - recompiling the module!
    [INFO] Compiling 93 source files to /tmp/tmptflbmqgd/Api/target/classes
    [INFO]
    [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ Api ---
    [INFO] skip non existing resourceDirectory /tmp/tmptflbmqgd/Api/src/test/resources
    [INFO]
    [INFO] --- maven-compiler-plugin:3.10.1:testCompile (default-testCompile) @ Api ---
    [INFO] Changes detected - recompiling the module!
    [INFO]
    [INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ Api ---
    [INFO] Tests are skipped.
    [INFO]
    [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ Api ---
    [INFO] Building jar: /tmp/tmptflbmqgd/Api/target/Api-0.0.1-SNAPSHOT.jar
    [INFO]
    [INFO] --- spring-boot-maven-plugin:3.0.6:repackage (repackage) @ Api ---
    [INFO] Replacing main artifact with repackaged archive
    [INFO]
    [INFO] --- maven-install-plugin:3.0.1:install (default-install) @ Api ---
    [INFO] Installing /tmp/tmptflbmqgd/Api/pom.xml to /home/laingard/.m2/repository/com/backend/Api/0.0.1-SNAPSHOT/Api-0.0.1-SNAPSHOT.pom
    [INFO] Installing /tmp/tmptflbmqgd/Api/target/Api-0.0.1-SNAPSHOT.jar to /home/laingard/.m2/repository/com/backend/Api/0.0.1-SNAPSHOT/Api-0.0.1-SNAPSHOT.jar
    [INFO]
    [INFO] ------------------------< com.backend:Register >------------------------
    [INFO] Building Register 0.0.1-SNAPSHOT                                   [3/3]
    [INFO] --------------------------------[ jar ]---------------------------------
    [INFO]
    [INFO] --- maven-clean-plugin:3.2.0:clean (default-clean) @ Register ---
    [INFO] Deleting /tmp/tmptflbmqgd/Register/target
    [INFO]
    [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ Register ---
    [INFO] Copying 0 resource from src/main/resources to target/classes
    [INFO] skip non existing resourceDirectory /tmp/tmptflbmqgd/Register/config
    [INFO]
    [INFO] --- maven-compiler-plugin:3.10.1:compile (default-compile) @ Register ---
    [INFO] Changes detected - recompiling the module!
    [INFO] Compiling 9 source files to /tmp/tmptflbmqgd/Register/target/classes
    [INFO] -------------------------------------------------------------
    [ERROR] COMPILATION ERROR :
    [INFO] -------------------------------------------------------------
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[3,34] package com.careerwatch.Api.entity does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[4,37] package com.careerwatch.Api.exception does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[5,38] package com.careerwatch.Api.repository does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[18,19] cannot find symbol
      symbol:   class UserRepository
      location: class com.careerwatch.register.service.RegisterServiceImpl
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[3,34] package com.careerwatch.Api.entity does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[4,38] package com.careerwatch.Api.repository does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/mapper/RegisterDtoMapper.java:[3,34] package com.careerwatch.Api.entity does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[15,1] cannot find symbol
      symbol:   class UserRepository
      location: class com.careerwatch.register.service.RegisterServiceImpl
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/mapper/RegisterDtoMapper.java:[13,12] cannot find symbol
      symbol:   class User
      location: class com.careerwatch.register.mapper.RegisterDtoMapper
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[26,5] cannot find symbol
      symbol:   class UserRepository
      location: class com.careerwatch.register.jwt.JwtService
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[34,33] cannot find symbol
      symbol:   class User
      location: class com.careerwatch.register.jwt.JwtService
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[66,47] cannot find symbol
      symbol:   class User
      location: class com.careerwatch.register.jwt.JwtService
    [INFO] 12 errors
    [INFO] -------------------------------------------------------------
    [INFO] ------------------------------------------------------------------------
    [INFO] Reactor Summary for backend 0.0.1-SNAPSHOT:
    [INFO]
    [INFO] backend ............................................ SUCCESS [  0.565 s]
    [INFO] Api ................................................ SUCCESS [  3.314 s]
    [INFO] Register ........................................... FAILURE [  0.534 s]
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD FAILURE
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time:  4.575 s
    [INFO] Finished at: 2023-06-01T12:51:53-03:00
    [INFO] ------------------------------------------------------------------------
    [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.10.1:compile (default-compile) on project Register: Compilation failure: Compilation failure:
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[3,34] package com.careerwatch.Api.entity does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[4,37] package com.careerwatch.Api.exception does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[5,38] package com.careerwatch.Api.repository does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[18,19] cannot find symbol
    [ERROR]   symbol:   class UserRepository
    [ERROR]   location: class com.careerwatch.register.service.RegisterServiceImpl
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[3,34] package com.careerwatch.Api.entity does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[4,38] package com.careerwatch.Api.repository does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/mapper/RegisterDtoMapper.java:[3,34] package com.careerwatch.Api.entity does not exist
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/service/RegisterServiceImpl.java:[15,1] cannot find symbol
    [ERROR]   symbol:   class UserRepository
    [ERROR]   location: class com.careerwatch.register.service.RegisterServiceImpl
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/mapper/RegisterDtoMapper.java:[13,12] cannot find symbol
    [ERROR]   symbol:   class User
    [ERROR]   location: class com.careerwatch.register.mapper.RegisterDtoMapper
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[26,5] cannot find symbol
    [ERROR]   symbol:   class UserRepository
    [ERROR]   location: class com.careerwatch.register.jwt.JwtService
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[34,33] cannot find symbol
    [ERROR]   symbol:   class User
    [ERROR]   location: class com.careerwatch.register.jwt.JwtService
    [ERROR] /tmp/tmptflbmqgd/Register/src/main/java/com/careerwatch/register/jwt/JwtService.java:[66,47] cannot find symbol
    [ERROR]   symbol:   class User
    [ERROR]   location: class com.careerwatch.register.jwt.JwtService
    [ERROR] -> [Help 1]
    [ERROR]
    [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
    [ERROR] Re-run Maven using the -X switch to enable full debug logging.
    [ERROR]
    [ERROR] For more information about the errors and possible solutions, please read the following articles:
    [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
    [ERROR]
    [ERROR] After correcting the problems, you can resume the build with the command
    [ERROR]   mvn <args> -rf :Register


This is my template.yml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31

Globals:
  Function:
    Timeout: 30

Resources:
  CareerWatchFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: .
      Handler: com.careerwatch.Api.StreamLambdaHandler::handleRequest
      Runtime: java17
      AutoPublishAlias: production
      SnapStart:
        ApplyOn: PublishedVersions
      Architectures:
        - x86_64
      MemorySize: 1024
      Environment:
        Variables:
          POWERTOOLS_SERVICE_NAME: CareerWatchApi
          DB_HOST: !Ref DBhost
          DB_PORT: !Ref DBport
          DB_NAME: !Ref DBname
          DB_USERNAME: !Ref DBusername
          DB_PASSWORD: !Ref DBpassword
      Events:
        HelloWorld:
          Type: Api
          Properties:
            Path: /{proxy+}
            Method: ANY
  RegisterFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: .
      Handler: com.careerwatch.register.RegisterLambdaHandler::handleRequest
      Runtime: java17
      AutoPublishAlias: production
      SnapStart:
        ApplyOn: PublishedVersions
      Architectures:
        - x86_64
      MemorySize: 1024
      Environment:
        Variables:
          POWERTOOLS_SERVICE_NAME: CareerWatchApi
          DB_HOST: !Ref DBhost
          DB_PORT: !Ref DBport
          DB_NAME: !Ref DBname
          DB_USERNAME: !Ref DBusername
          DB_PASSWORD: !Ref DBpassword
          SECRET_KEY: !Ref SecretKey
      Events:
        RegisterEndpoint:
          Type: Api
          Properties:
            Path: /api/v1/register
            Method: POST
Parameters:
  DBhost:
    Type: String
    Default: ''
    Description: Enter the DB host name or IP address
  DBport:
    Type: String
    Default: ''
    Description: Enter the DB port
  DBname:
    Type: String
    Default: ''
    Description: Enter the DB name
  DBusername:
    Type: String
    Default: ''
    Description: Enter the DB username
  DBpassword:
    Type: String
    Default: ''
    Description: Enter the DB password
  SecretKey:
      Type: String
      Default: ''
      Description: Enter the secret jwt key password

I did mvn install and I checked the whole project structure, however with maven it works, but with sam build it doesn't.
",76384014,"You stated in your comment:
""I am building a serverless application, deploying lambdas functions in api gateway.""
If you are intereted in builidng a serverless app with Java, look at the PAM example. This example builds a complete serverless example that uses API Gateway, Lambda functions, Java SDK, a client app that uses Cognito to log in users, etc.
Here is the overview illustration:

As well, this example uses the AWS CDK to standup various resources.
This does not use SAM.
See:
Create a photo asset management application that lets users manage photos using labels
",
76384235,Variable not updated in compose,"I have a class in Kotlin (Jetpack compose) with a variable title and an exoplayer that updates the title.
class Player{
var title by mutableStatOf(""value"")
....

title= ""new value""
...
}

@Composable
fun Display(){
val player = Player()

player.title?.let{
Text(it)
} 
}

In the user interface an instance of the class is created and the title displayed but it remains unchanged after being updated in the class. Can someone help?
",76385656,"You forgot to remember the player instance. Here, when you change title, your composable is recomposed because it reads the title. But when it is recomposed, you create new instance of Player with the default title.
val player = remember { Player() }

",
76384397,"No loader is configured for "".html"" files: index.html Vitejs","Greetings I have problem. I am using Visual studio 2022 and created two projects there for one solution. One for back-end (ASP.NET) and the second one for fron-end (vuejs and vite). So here starts the problem. I used npm create vue@3 command to create vue project. And  its launched fine , but when I did same thing in folder of front-end in my sln project vite throws error what it can not find index.html file
Error:   Failed to scan for dependencies from entries:
  D:/Projects/C#/DAINIS/vueapp/index.html

  X [ERROR] No loader is configured for "".html"" files: index.html

    <stdin>:1:7:
      1 │ import ""D:/Projects/C#/DAINIS/vueapp/index.html""
        ╵        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


    at failureErrorWithLog (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:1638:15)
    at D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:1050:25
    at runOnEndCallbacks (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:1473:45)
    at buildResponseToResult (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:1048:7)
    at D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:1060:9
    at new Promise (<anonymous>)
    at requestCallbacks.on-end (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:1059:54)
    at handleRequest (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:725:19)
    at handleIncomingPacket (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:747:7)
    at Socket.readFromStdout (D:\Projects\C#\DAINIS\vueapp\node_modules\esbuild\lib\main.js:675:7)

Just in case I did not changed project it is as is.
Project structure
Error dump example
I tried solutions found here and here in stack overflow but still no luck
",76385670,"The issue is the # symbol in your file path
D:/Projects/C#/DAINIS/vueapp/

I don't know the technical reason why this causes it to fail, but if you remove it, the project should run.
",
76382075,Saving an extremely long figure as pdf in R will cause all text invisible,"R 4.2.1
package usage: netmeta 2.8-1
Issue:
In netmeta package, the function forest() will create a forest plot. One need to know that forest() can generate a forest plot presenting the network inconsistency between direct and indirect comparisons.
In my case, I have an extremely large network, which will lead to more than 2000 comparisons. The forest plot describing network inconsistency will be VERY long in height.
The following script can only create ""part"" of the forest because the forest is too long.
pdf(""filename.pdf"",width = 12, height = 200)
forest(out, show = ""all"")
dev.off()

I tried to increase the height to make all the content in this document. But any value above 200 in height will generate a ""blank"" pdf (but I think the text should be there because the file size is 240KB rather than a NULL file which only takes 4KB in my disk).
I have no idea how to make the text visible in pdf and keep all content in this file at the same time. Any suggestion will be appreciated. Thank you.
",76383557,"PDF 1.x has an implementation limit for the page size:

The minimum page size should be 3 by 3 units in default user space; the maximum should be 14,400 by 14,400 units. In versions of PDF earlier than 1.6, the size of the default user space unit was fixed at 1⁄72 inch, yielding a minimum of approximately 0.04 by 0.04 inch and a maximum of 200 by 200 inches. Beginning with PDF 1.6, the size of the unit may be set on a page-by-page basis; the default remains at 1/72 inch.

(ISO 32000-1, Annex C.2 Architectural limits)
If R doesn't adjust the default user space unit size in case of large width or height arguments, it generates PDFs that go beyond these limits whenever you set either argument to a value greater than 200.
As those limits originate from the Adobe implementation of the PDF spec, i.e. Adobe Acrobat, tests with Adobe software may indeed show issues...
",
76382178,Complex Cocatenation in gsheets,"I have a table to entries where I am trying to concatenate few columns and the concatenation should not happen when any one of the required values are empty
Here is the spreadsheet. https://docs.google.com/spreadsheets/d/1lQUG4TmFTKghV8r6Gg3EilLx6zyuTkrNfnCatRVWp_U/edit#gid=189998773
I did try to use the formula =MAP(scan(,D5:D,I5:I,lambda(a,b,d,if(or(a="""",b="""",d="""",and(e="""",f="""")),,if(f<>"""",f,e)&"" ""&a&"" ""&b&"" ""&c))),D5:D,H5:H,I5:I,J5:J,K5:K,lambda(a,b,c,e,f,if(or(a="""",b="""",c="""",and(e="""",f="""")),,if(f<>"""",f,e)&"" ""&a&"" ""&b&"" ""&c)))
But not able to get the the desired output.
Please help!
",76383650,"You can simplify your formula by omitting SCAN
=MAP(D5:D,H5:H,I5:I,J5:J,K5:K,
      LAMBDA(dd,hh,ii,jj,kk,
       IF(OR(dd="""",hh="""",ii="""",AND(jj="""",kk="""")),,dd&""-""&hh&""-""&ii&""-""&if(kk<>"""",kk,jj))))

(for future reference: try naming your LAMBDAs accordingly)

",
76385291,Using glm in R for linear regression on a large dataframe - issues with column subsetting,"I am trying to use glm in R using a dataframe containing ~ 1000 columns, where I want to select a specific independent variable and run as a loop for each of the 1000 columns representing the dependent variables.
As a test, the glm equation works perfectly fine when I specify a single column using df$col1 for both my dependent and independent variables.
I can't seem to correctly subset a range of columns (below) and I keep getting this error, no matter how many ways I try to format the df:
'data' must be a data.frame, environment, or list

What I tried:
df = my df
cols <- df[, 20:1112]

for (i in cols{
    glm <- glm(df$col1 ~ ., data=df, family=gaussian)
}

",76385673,"It would be more idiomatic to do:
predvars <- names(df)[20:1112]
glm_list <- list()  ## presumably you want to save the results??
for (pv in predvars) {
    glm_list[[pv]] <- glm(reformulate(pv, response = ""col1""), 
       data=df, family=gaussian)
}

In fact, if you really just want to do a Gaussian GLM then it will be slightly faster to use
lm(reformulate(pv, response = ""col1""), data = df)

in the loop instead.
If you want to get fancy:
formlist <- lapply(predvars, reformulate, response = ""col1"")
lm_list <- lapply(formlist, lm, data = df)
names(lm_list) <- predvars

",
76381069,Firebase Hosting Can't Add Domain,"When I try to add a custom domain to Firebase Hosting, I encounter this error. Nothing happens when I click Continue.
Upon clicking continue, it loads for a few seconds, and then the same screen persists.

When I checked the console, HTTP requests are responded with 503 Service Unable.

",76383663,"firebaser here
While we did make some changes to our custom domain provisioning this week, it seems that you're hitting another problem.
Is any part of this on a Google Workspace account by any chance? If that is the case, your domain may not allow the search console. You'll want to reach out to their GSuite organization admin/owner to enable the Search Console. See Turn Google Search Console on or off for users for complete instructions.
",
76383791,Google Sheets vlookup to prioritize a column,"Wonder if anyone can help.
On the attached on tab 2 I'm trying to do a vlookup to tab 1 to use column 1 'code' to bring back column 3 'email'. But if there is more than 1 match in column 1 for 'code', prioritize the row that has Level 1 in column 2 in tab 1. For example, there are two code 20, on tab 1 but the formula should bring back row 5 - test4@live.co.uk rather than row 2 - test1@live.co.uk as row 5 has Level 1 in column 2. If there is no duplicate code in column 1 just bring back the first result in column 1.
I've got a formula already in column 3 in tab 2 but it isn't working as it doesn't bring back anything if there isn't level 1 in column 2, see column 4. On tab 3, I've got an example of what the right result should be. This can be either a vlookup, a match & index or what ever works as long as its in google sheets.
Addition to the above, it needs to be an array based on something being in column 1 and have an iferror, just in case. Thanks.
Link to sheet
",76384034,"You may try:
=index(ifna(vlookup(A2:A,sort(LookTable!A:C,2,),3,)))


the above code prioritizes Level 1 over blank level. Not sure if there's goin' to be 10s of levels and I have to choose say level 15 over level 8 or so; then use this alternate variant
=index(ifna(vlookup(A2:A,sort(LookTable!A2:C,--ifna(regexextract(LookTable!B2:B,""\d+"")),),3,)))

",
76384189,Sort Rows Based on Tuple Index,"Overview:
Pandas dataframe with a tuple index and corresponding 'Num' column:
Index                               Num

('Total', 'A')                      23

('Total', 'A', 'Pandas')            3

('Total', 'A', 'Row')               7

('Total', 'A', 'Tuple')             13

('Total', 'B')                      35

('Total', 'B', 'Rows')              12

('Total', 'B', 'Two')               23

('Total', 'C')                      54

('Total', 'C', 'Row')               54

Total                               112

The index and 'Num' column are already sorted with a lambda function by Alphabetical Order and based on the length of tuple elements:
dataTable = dataTable.reindex(sorted(dataTable.index, key=lambda x: (not isinstance(x, tuple), x)))

Problem:
Now, I want to sort only the 3rd tuple index element based on it's corresponding 'Num' value. Here would be an updated example of the dataframe:
Index                               Num

('Total', 'A')                      23

('Total', 'A', 'Tuple')             13

('Total', 'A', 'Row')               7

('Total', 'A', 'Pandas')            3

('Total', 'B')                      35

('Total', 'B', 'Two')               23

('Total', 'B', 'Rows')              12

('Total', 'C')                      54

('Total', 'C', 'Row')               54

Total                               112

Question:
What Lambda function can achieve this?
",76385755,"You can try:
def fn(x):
    vals = x.sort_values(by='Num', ascending=False)
    df.loc[x.index] = vals.values

m = df['Index'].apply(len).eq(3)
df[m].groupby(df.loc[m, 'Index'].str[1], group_keys=False).apply(fn)

print(df)

Prints:
                Index  Num
0          (Total, A)   23
1   (Total, A, Tuple)   13
2     (Total, A, Row)    7
3  (Total, A, Pandas)    3
4          (Total, B)   35
5     (Total, B, Two)   23
6    (Total, B, Rows)   12
7          (Total, C)   54
8     (Total, C, Row)   54
9               Total  112


Initial df:
                Index  Num
0          (Total, A)   23
1  (Total, A, Pandas)    3
2     (Total, A, Row)    7
3   (Total, A, Tuple)   13
4          (Total, B)   35
5    (Total, B, Rows)   12
6     (Total, B, Two)   23
7          (Total, C)   54
8     (Total, C, Row)   54
9               Total  112

",
76382968,RouteChildrenProps is not an exported member,"I am getting the below error pointing to 'RouteChildrenProps':

I am just trying to get through a tutorial but I got stuck here.  Here is the full code:
import React from 'react';
import { Route, RouteChildrenProps, Routes } from 'react-router';
import routes from './config/routes';

export interface IApplicationProps {}

const Application: React.FunctionComponent<IApplicationProps> = (props) => {
  return (
    <Routes>
      {routes.map((route, index) => {
        return <Route 
          key={index} 
          exact={route.exact} 
          path={route.path} 
          render={(routeProps: RouteChildrenProps<any>) =>
            <route.component {...routeProps} />} />;
          }
      )}
    </Routes>
  );
};

export default Application;

The tutorial itself did not encounter this issue, so I'm hoping someone here can me solve this.
",76384045,"React-Router v6 removed route props, these are a v4/5 export. The Route component API changed significantly from v4/5 to v6. There are no route props, no exact prop since routes are now always exactly matched, and all routed content is rendered on a single element prop taking a React.ReactNode, e.g. JSX, value.
import React from 'react';
import { Route, Routes } from 'react-router';
import routes from './config/routes';

export interface IApplicationProps {}

const Application: React.FunctionComponent<IApplicationProps> = (props) => {
  return (
    <Routes>
      {routes.map((route) => {
        const Component = route.component;
        return (
          <Route 
            key={route.path} 
            path={route.path} 
            element={<Component />}
          />
        );
      })}
    </Routes>
  );
};

If any of the routed component need to access what was previously passed via props, they should use the provided React hooks: useNavigate for navigate function that replaced useHistory, useParams for route path parameters, useLocation for the location object, etc.
",
76381810,Call Rust DLL function to spawn new thread from C wrapper and return the main thread back to C,"I have a use case where C wrapper is loading the Rust DLL. Both C and Rust have infinate loop.
C code
#include ""main.h""
#include <stdio.h>
#include ""addition.h""
#include <time.h>
#include <unistd.h>

extern void spawn_thread_and_get_back(); // Rust function :
extern void keep_calling_rust_fn(); // Rust function :

int main()
{
    spawn_thread_and_get_back(); // This should spawn thread with infinite loop in rust

    int sleep_seconds = 7;
    
    
    while(1){
      
        printf(""Calling rust function);

        keep_calling_rust_fn(); // Call rust function
    }
}

and here is the rust lib code

async fn counter() {
    loop {
        println!(""I am getting called by Tokio every 2 seconds"");
        // Sleep for 1 second
        sleep(Duration::from_secs(2)).await;
    }
}

#[tokio::main]
async fn runForever() {
    let counterTask = tokio::spawn(
        counter()
    );
    tokio::try_join!(counterTask);

}

use std::thread;

#[no_mangle]
pub unsafe extern ""C"" fn spawn_thread_and_get_back() {
    let handle = thread::spawn(move || {
        // some work here
        println!(""Trying to create new thread for Tokio runtime"");
        runForever();

    });
    handle.join();
}

#[no_mangle]
pub unsafe extern ""C"" fn keep_calling_rust_fn() {
    println!(""I am getting called by C wrapper every 7 second"");
    someRandomPrintTask();
}


async fn printTask(task_number: u32) {
    println!(""Print task {} -"", task_number);
}

async fn someRandomPrintTask() {
    let printTask = tokio::spawn(
        printTask(10)
    );
    tokio::try_join!(printTask);
}


The issue I am facing is once I call the spawn_thread_and_get_back() from C and never get the thread back to execute while loop in C
I would like to call the rust DLL from C and spawn seperate thread for rust. And the idea is the caller thread from C will get free once it initializes the rust forever loop thread.
",76383698,"Thanks everyone, After discussing in the comments above, here is the working answer. We just need to change Rust code

async fn counter() {
    loop {
        println!(""I am getting called by Tokio every 2 seconds"");
        // Sleep for 1 second
        sleep(Duration::from_secs(2)).await;
    }
}

#[tokio::main]
async fn runForever() {
    let counterTask = tokio::spawn(
        counter()
    );
    tokio::try_join!(counterTask);

}

use std::thread;

#[no_mangle]
pub unsafe extern ""C"" fn spawn_thread_and_get_back() {
    let handle = thread::spawn(move || {
        // some work here
        println!(""Trying to create new thread for Tokio runtime"");
        runForever();

    });
    

   `handle.join();` *** Update - Removed this as it was blocking the main thread as it was waiting for handle to finish ***

}

#[no_mangle]
pub unsafe extern ""C"" fn keep_calling_rust_fn() {
    println!(""I am getting called by C wrapper every 7 second"");
    someRandomPrintTask();
}


async fn printTask(task_number: u32) {
    println!(""Print task {} -"", task_number);
}

#[tokio::main]  *** Update - Added the decorater here ***
async fn someRandomPrintTask() {
    let printTask = tokio::spawn(
        printTask(10)
    );
    tokio::try_join!(printTask);
}


",
76383253,How to show result of static model (=plain number) in Xcos?,"I add two numbers in Xcos and would like to show the result in the diagram. I managed to do so using a CSCOPE element and adding an extra CLOCK_c element:

However, I would prefer a display element that simply shows the number:

=> What component could I use for that?
If there is no existing display component for plain numbers, how can I create one?
Related questions:
How to show results of a static model in Modeling view with OpenModelica?
https://softwarerecs.stackexchange.com/questions/87166/python-framework-for-block-simulations-with-graphical-user-interface-like-openm
xcos example file:
xcos_demo.xcos
<?xml version=""1.0"" ?>
<XcosDiagram debugLevel=""0"" finalIntegrationTime=""30.0"" integratorAbsoluteTolerance=""1.0E-6"" integratorRelativeTolerance=""1.0E-6"" toleranceOnTime=""1.0E-10"" maxIntegrationTimeInterval=""100001.0"" maximumStepSize=""0.0"" realTimeScaling=""0.0"" solver=""1.0"" background=""-1"" gridEnabled=""1"" title=""Untitled""><!--Xcos - 2.0 - scilab-2023.1.0 - 20230523 0919-->
  <Array as=""context"" scilabClass=""String[]""></Array>
  <mxGraphModel as=""model"">
    <root>
      <mxCell id=""0:1:0""/>
      <mxCell id=""0:2:0"" parent=""0:1:0""/>
      <BasicBlock id=""7ca5d227:1887764bffb:-7ff9"" parent=""0:2:0"" interfaceFunctionName=""CONST_m"" blockType=""d"" dependsOnU=""0"" dependsOnT=""0"" simulationFunctionName=""cstblk4_m"" simulationFunctionType=""C_OR_FORTRAN"" style=""CONST_m"">
        <ScilabString as=""exprs"" height=""1"" width=""1"">
          <data line=""0"" column=""0"" value=""1""/>
        </ScilabString>
        <ScilabDouble as=""realParameters"" height=""0"" width=""0""/>
        <ScilabDouble as=""integerParameters"" height=""0"" width=""0""/>
        <Array as=""objectsParameters"" scilabClass=""ScilabList"">
          <ScilabDouble height=""1"" width=""1"">
            <data line=""0"" column=""0"" realPart=""1.0""/>
          </ScilabDouble>
        </Array>
        <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabDouble as=""state"" height=""0"" width=""0""/>
        <ScilabDouble as=""dState"" height=""0"" width=""0""/>
        <Array as=""oDState"" scilabClass=""ScilabList""/>
        <Array as=""equations"" scilabClass=""ScilabList""/>
        <mxGeometry as=""geometry"" x=""170.0"" y=""270.0"" width=""40.0"" height=""40.0""/>
      </BasicBlock>
      <ExplicitOutputPort id=""7ca5d227:1887764bffb:-7ff8"" parent=""7ca5d227:1887764bffb:-7ff9"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.0"" style=""ExplicitOutputPort;align=right;verticalAlign=middle;spacing=10.0;rotation=0"" value=""""/>
      <BigSom id=""7ca5d227:1887764bffb:-7ff1"" parent=""0:2:0"" interfaceFunctionName=""BIGSOM_f"" blockType=""c"" dependsOnU=""1"" dependsOnT=""0"" simulationFunctionName=""sum"" simulationFunctionType=""TYPE_2"" style=""BIGSOM_f"">
        <ScilabString as=""exprs"" height=""1"" width=""1"">
          <data line=""0"" column=""0"" value=""[1;1]""/>
        </ScilabString>
        <ScilabDouble as=""realParameters"" height=""1"" width=""2"">
          <data line=""0"" column=""0"" realPart=""1.0""/>
          <data line=""0"" column=""1"" realPart=""1.0""/>
        </ScilabDouble>
        <ScilabDouble as=""integerParameters"" height=""0"" width=""0""/>
        <Array as=""objectsParameters"" scilabClass=""ScilabList""/>
        <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabDouble as=""state"" height=""0"" width=""0""/>
        <ScilabDouble as=""dState"" height=""0"" width=""0""/>
        <Array as=""oDState"" scilabClass=""ScilabList""/>
        <Array as=""equations"" scilabClass=""ScilabList""/>
        <mxGeometry as=""geometry"" x=""430.0"" y=""310.0"" width=""40.0"" height=""60.0""/>
      </BigSom>
      <ExplicitInputPort id=""7ca5d227:1887764bffb:-7ff0"" parent=""7ca5d227:1887764bffb:-7ff1"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""-1"" initialState=""0.0"" style=""ExplicitInputPort;align=left;verticalAlign=middle;spacing=10.0;rotation=0"" value=""""/>
      <ExplicitInputPort id=""7ca5d227:1887764bffb:-7fef"" parent=""7ca5d227:1887764bffb:-7ff1"" ordering=""2"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""-1"" initialState=""0.0"" style=""ExplicitInputPort;align=left;verticalAlign=middle;spacing=10.0;rotation=0"" value=""""/>
      <ExplicitOutputPort id=""7ca5d227:1887764bffb:-7fee"" parent=""7ca5d227:1887764bffb:-7ff1"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""-1"" initialState=""0.0"" style=""ExplicitOutputPort;align=right;verticalAlign=middle;spacing=10.0;rotation=0"" value=""""/>
      <BasicBlock id=""7ca5d227:1887764bffb:-7fec"" parent=""0:2:0"" interfaceFunctionName=""CONST_m"" blockType=""d"" dependsOnU=""0"" dependsOnT=""0"" simulationFunctionName=""cstblk4_m"" simulationFunctionType=""C_OR_FORTRAN"" style=""CONST_m"">
        <ScilabString as=""exprs"" height=""1"" width=""1"">
          <data line=""0"" column=""0"" value=""1""/>
        </ScilabString>
        <ScilabDouble as=""realParameters"" height=""0"" width=""0""/>
        <ScilabDouble as=""integerParameters"" height=""0"" width=""0""/>
        <Array as=""objectsParameters"" scilabClass=""ScilabList"">
          <ScilabDouble height=""1"" width=""1"">
            <data line=""0"" column=""0"" realPart=""1.0""/>
          </ScilabDouble>
        </Array>
        <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabDouble as=""state"" height=""0"" width=""0""/>
        <ScilabDouble as=""dState"" height=""0"" width=""0""/>
        <Array as=""oDState"" scilabClass=""ScilabList""/>
        <Array as=""equations"" scilabClass=""ScilabList""/>
        <mxGeometry as=""geometry"" x=""170.0"" y=""360.0"" width=""40.0"" height=""40.0""/>
      </BasicBlock>
      <ExplicitOutputPort id=""7ca5d227:1887764bffb:-7feb"" parent=""7ca5d227:1887764bffb:-7fec"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.0"" style=""ExplicitOutputPort;align=right;verticalAlign=middle;spacing=10.0;rotation=0"" value=""""/>
      <BasicBlock id=""7ca5d227:1887764bffb:-7fd6"" parent=""0:2:0"" interfaceFunctionName=""CSCOPE"" blockType=""c"" dependsOnU=""1"" dependsOnT=""0"" simulationFunctionName=""cscope"" simulationFunctionType=""C_OR_FORTRAN"" style=""CSCOPE;verticalLabelPosition=bottom;verticalAlign=top;spacing=2;displayedLabel="">
        <ScilabString as=""exprs"" height=""10"" width=""1"">
          <data line=""0"" column=""0"" value=""1 3 5 7 9 11 13 15""/>
          <data line=""1"" column=""0"" value=""-1""/>
          <data line=""2"" column=""0"" value=""[]""/>
          <data line=""3"" column=""0"" value=""[600;400]""/>
          <data line=""4"" column=""0"" value=""-15""/>
          <data line=""5"" column=""0"" value=""15""/>
          <data line=""6"" column=""0"" value=""30""/>
          <data line=""7"" column=""0"" value=""20""/>
          <data line=""8"" column=""0"" value=""0""/>
          <data line=""9"" column=""0"" value=""""/>
        </ScilabString>
        <ScilabDouble as=""realParameters"" height=""1"" width=""4"">
          <data line=""0"" column=""0"" realPart=""0.0""/>
          <data line=""0"" column=""1"" realPart=""-15.0""/>
          <data line=""0"" column=""2"" realPart=""15.0""/>
          <data line=""0"" column=""3"" realPart=""30.0""/>
        </ScilabDouble>
        <ScilabInteger as=""integerParameters"" height=""1"" width=""15"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""-1""/>
          <data line=""0"" column=""1"" value=""1""/>
          <data line=""0"" column=""2"" value=""20""/>
          <data line=""0"" column=""3"" value=""1""/>
          <data line=""0"" column=""4"" value=""3""/>
          <data line=""0"" column=""5"" value=""5""/>
          <data line=""0"" column=""6"" value=""7""/>
          <data line=""0"" column=""7"" value=""9""/>
          <data line=""0"" column=""8"" value=""11""/>
          <data line=""0"" column=""9"" value=""13""/>
          <data line=""0"" column=""10"" value=""15""/>
          <data line=""0"" column=""11"" value=""-1""/>
          <data line=""0"" column=""12"" value=""-1""/>
          <data line=""0"" column=""13"" value=""600""/>
          <data line=""0"" column=""14"" value=""400""/>
        </ScilabInteger>
        <Array as=""objectsParameters"" scilabClass=""ScilabList""/>
        <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabDouble as=""state"" height=""0"" width=""0""/>
        <ScilabDouble as=""dState"" height=""0"" width=""0""/>
        <Array as=""oDState"" scilabClass=""ScilabList""/>
        <Array as=""equations"" scilabClass=""ScilabList""/>
        <mxGeometry as=""geometry"" x=""610.0"" y=""320.0"" width=""40.0"" height=""40.0""/>
      </BasicBlock>
      <ExplicitInputPort id=""7ca5d227:1887764bffb:-7fd5"" parent=""7ca5d227:1887764bffb:-7fd6"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""-1"" initialState=""0.0"" style=""ExplicitInputPort;align=left;verticalAlign=middle;spacing=10.0;rotation=0"" value=""""/>
      <ControlPort id=""7ca5d227:1887764bffb:-7fd4"" parent=""7ca5d227:1887764bffb:-7fd6"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.0"" style=""ControlPort;align=center;verticalAlign=top;spacing=10.0;rotation=90"" value=""""/>
      <BasicBlock id=""7ca5d227:1887764bffb:-7fd1"" parent=""0:2:0"" interfaceFunctionName=""CLOCK_c"" blockType=""h"" dependsOnU=""0"" dependsOnT=""0"" simulationFunctionName=""csuper"" simulationFunctionType=""DEFAULT"" style=""CLOCK_c"">
        <ScilabDouble as=""exprs"" height=""0"" width=""0""/>
        <ScilabDouble as=""realParameters"" height=""0"" width=""0""/>
        <ScilabDouble as=""integerParameters"" height=""0"" width=""0""/>
        <Array as=""objectsParameters"" scilabClass=""ScilabList""/>
        <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
          <data line=""0"" column=""0"" value=""0""/>
        </ScilabInteger>
        <ScilabDouble as=""state"" height=""0"" width=""0""/>
        <ScilabDouble as=""dState"" height=""0"" width=""0""/>
        <Array as=""oDState"" scilabClass=""ScilabList""/>
        <Array as=""equations"" scilabClass=""ScilabList""/>
        <mxGeometry as=""geometry"" x=""610.0"" y=""180.0"" width=""40.0"" height=""40.0""/>
        <SuperBlockDiagram as=""child"" background=""-1"" gridEnabled=""1"" title="""">
          <Array as=""context"" scilabClass=""String[]""></Array>
          <mxGraphModel as=""model"">
            <root>
              <mxCell id=""7ca5d227:1887764bffc:-7fd1""/>
              <mxCell id=""7ca5d227:1887764bffd:-7fd1"" parent=""7ca5d227:1887764bffc:-7fd1""/>
              <EventOutBlock id=""7ca5d227:1887764bffb:-7fbc"" parent=""7ca5d227:1887764bffd:-7fd1"" interfaceFunctionName=""CLKOUT_f"" blockType=""d"" dependsOnU=""0"" dependsOnT=""0"" simulationFunctionName=""output"" simulationFunctionType=""DEFAULT"" style="""">
                <ScilabString as=""exprs"" height=""1"" width=""1"">
                  <data line=""0"" column=""0"" value=""1""/>
                </ScilabString>
                <ScilabDouble as=""realParameters"" height=""0"" width=""0""/>
                <ScilabInteger as=""integerParameters"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""1""/>
                </ScilabInteger>
                <Array as=""objectsParameters"" scilabClass=""ScilabList""/>
                <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""0""/>
                </ScilabInteger>
                <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""0""/>
                </ScilabInteger>
                <ScilabDouble as=""state"" height=""0"" width=""0""/>
                <ScilabDouble as=""dState"" height=""0"" width=""0""/>
                <Array as=""oDState"" scilabClass=""ScilabList""/>
                <Array as=""equations"" scilabClass=""ScilabList""/>
                <mxGeometry as=""geometry"" x=""399.0"" y=""162.0"" width=""20.0"" height=""20.0""/>
              </EventOutBlock>
              <ControlPort id=""7ca5d227:1887764bffb:-7fbb"" parent=""7ca5d227:1887764bffb:-7fbc"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.0"" style="""" value=""""/>
              <BasicBlock id=""7ca5d227:1887764bffb:-7fba"" parent=""7ca5d227:1887764bffd:-7fd1"" interfaceFunctionName=""EVTDLY_c"" blockType=""d"" dependsOnU=""0"" dependsOnT=""0"" simulationFunctionName=""evtdly4"" simulationFunctionType=""C_OR_FORTRAN"" style="""">
                <ScilabString as=""exprs"" height=""2"" width=""1"">
                  <data line=""0"" column=""0"" value=""0.1""/>
                  <data line=""1"" column=""0"" value=""0.1""/>
                </ScilabString>
                <ScilabDouble as=""realParameters"" height=""1"" width=""2"">
                  <data line=""0"" column=""0"" realPart=""0.1""/>
                  <data line=""0"" column=""1"" realPart=""0.1""/>
                </ScilabDouble>
                <ScilabDouble as=""integerParameters"" height=""0"" width=""0""/>
                <Array as=""objectsParameters"" scilabClass=""ScilabList""/>
                <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""0""/>
                </ScilabInteger>
                <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""0""/>
                </ScilabInteger>
                <ScilabDouble as=""state"" height=""0"" width=""0""/>
                <ScilabDouble as=""dState"" height=""0"" width=""0""/>
                <Array as=""oDState"" scilabClass=""ScilabList""/>
                <Array as=""equations"" scilabClass=""ScilabList""/>
                <mxGeometry as=""geometry"" x=""320.0"" y=""232.0"" width=""40.0"" height=""40.0""/>
              </BasicBlock>
              <ControlPort id=""7ca5d227:1887764bffb:-7fb9"" parent=""7ca5d227:1887764bffb:-7fba"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.0"" style="""" value=""""/>
              <CommandPort id=""7ca5d227:1887764bffb:-7fb8"" parent=""7ca5d227:1887764bffb:-7fba"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.1"" style="""" value=""""/>
              <SplitBlock id=""7ca5d227:1887764bffb:-7fb7"" parent=""7ca5d227:1887764bffd:-7fd1"" interfaceFunctionName=""CLKSPLIT_f"" blockType=""d"" dependsOnU=""0"" dependsOnT=""0"" simulationFunctionName=""split"" simulationFunctionType=""DEFAULT"" style="""">
                <ScilabDouble as=""exprs"" height=""0"" width=""0""/>
                <ScilabDouble as=""realParameters"" height=""0"" width=""0""/>
                <ScilabDouble as=""integerParameters"" height=""0"" width=""0""/>
                <Array as=""objectsParameters"" scilabClass=""ScilabList""/>
                <ScilabInteger as=""nbZerosCrossing"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""0""/>
                </ScilabInteger>
                <ScilabInteger as=""nmode"" height=""1"" width=""1"" intPrecision=""sci_int32"">
                  <data line=""0"" column=""0"" value=""0""/>
                </ScilabInteger>
                <ScilabDouble as=""state"" height=""0"" width=""0""/>
                <ScilabDouble as=""dState"" height=""0"" width=""0""/>
                <Array as=""oDState"" scilabClass=""ScilabList""/>
                <Array as=""equations"" scilabClass=""ScilabList""/>
                <mxGeometry as=""geometry"" x=""380.71066"" y=""172.0"" width=""0.3333333333333333"" height=""0.3333333333333333""/>
              </SplitBlock>
              <ControlPort id=""7ca5d227:1887764bffb:-7fb6"" parent=""7ca5d227:1887764bffb:-7fb7"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""0.0"" style="""" value=""""/>
              <CommandPort id=""7ca5d227:1887764bffb:-7fb5"" parent=""7ca5d227:1887764bffb:-7fb7"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""-1.0"" style="""" value=""""/>
              <CommandPort id=""7ca5d227:1887764bffb:-7fb4"" parent=""7ca5d227:1887764bffb:-7fb7"" ordering=""2"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""-1.0"" style="""" value=""""/>
              <CommandControlLink id=""7ca5d227:1887764bffb:-7fb3"" parent=""7ca5d227:1887764bffd:-7fd1"" source=""7ca5d227:1887764bffb:-7fb8"" target=""7ca5d227:1887764bffb:-7fb6"" style="""" value="""">
                <mxGeometry as=""geometry"">
                  <mxPoint as=""sourcePoint"" x=""340.0"" y=""226.29""/>
                  <Array as=""points"">
                    <mxPoint x=""340.0"" y=""172.0""/>
                  </Array>
                  <mxPoint as=""targetPoint"" x=""380.71"" y=""172.0""/>
                </mxGeometry>
              </CommandControlLink>
              <CommandControlLink id=""7ca5d227:1887764bffb:-7fb2"" parent=""7ca5d227:1887764bffd:-7fd1"" source=""7ca5d227:1887764bffb:-7fb5"" target=""7ca5d227:1887764bffb:-7fbb"" style="""" value="""">
                <mxGeometry as=""geometry"">
                  <mxPoint as=""sourcePoint"" x=""380.71"" y=""172.0""/>
                  <Array as=""points""></Array>
                  <mxPoint as=""targetPoint"" x=""399.0"" y=""172.0""/>
                </mxGeometry>
              </CommandControlLink>
              <CommandControlLink id=""7ca5d227:1887764bffb:-7fb1"" parent=""7ca5d227:1887764bffd:-7fd1"" source=""7ca5d227:1887764bffb:-7fb4"" target=""7ca5d227:1887764bffb:-7fb9"" style="""" value="""">
                <mxGeometry as=""geometry"">
                  <mxPoint as=""sourcePoint"" x=""380.71"" y=""172.0""/>
                  <Array as=""points"">
                    <mxPoint x=""380.71"" y=""302.0""/>
                    <mxPoint x=""340.0"" y=""302.0""/>
                  </Array>
                  <mxPoint as=""targetPoint"" x=""340.0"" y=""277.71""/>
                </mxGeometry>
              </CommandControlLink>
            </root>
          </mxGraphModel>
          <mxCell as=""defaultParent"" id=""7ca5d227:1887764bffd:-7fd1"" parent=""7ca5d227:1887764bffc:-7fd1""/>
        </SuperBlockDiagram>
      </BasicBlock>
      <CommandPort id=""7ca5d227:1887764bffb:-7fd0"" parent=""7ca5d227:1887764bffb:-7fd1"" ordering=""1"" dataType=""REAL_MATRIX"" dataColumns=""1"" dataLines=""1"" initialState=""-1.0"" style=""CommandPort;align=center;verticalAlign=bottom;spacing=10.0;rotation=90"" value=""""/>
      <ExplicitLink id=""7ca5d227:1887764bffb:-7fed"" parent=""0:2:0"" source=""7ca5d227:1887764bffb:-7ff8"" target=""7ca5d227:1887764bffb:-7ff0"" style=""ExplicitLink"" value="""">
        <mxGeometry as=""geometry"">
          <mxPoint as=""sourcePoint"" x=""44.0"" y=""20.0""/>
          <Array as=""points""></Array>
          <mxPoint as=""targetPoint"" x=""-4.0"" y=""20.0""/>
        </mxGeometry>
      </ExplicitLink>
      <ExplicitLink id=""7ca5d227:1887764bffb:-7fea"" parent=""0:2:0"" source=""7ca5d227:1887764bffb:-7feb"" target=""7ca5d227:1887764bffb:-7fef"" style=""ExplicitLink"" value="""">
        <mxGeometry as=""geometry"">
          <mxPoint as=""sourcePoint"" x=""44.0"" y=""20.0""/>
          <Array as=""points""></Array>
          <mxPoint as=""targetPoint"" x=""-4.0"" y=""40.0""/>
        </mxGeometry>
      </ExplicitLink>
      <ExplicitLink id=""7ca5d227:1887764bffb:-7fd2"" parent=""0:2:0"" source=""7ca5d227:1887764bffb:-7fee"" target=""7ca5d227:1887764bffb:-7fd5"" style=""ExplicitLink"" value="""">
        <mxGeometry as=""geometry"">
          <mxPoint as=""sourcePoint"" x=""44.0"" y=""30.0""/>
          <Array as=""points""></Array>
          <mxPoint as=""targetPoint"" x=""-4.0"" y=""20.0""/>
        </mxGeometry>
      </ExplicitLink>
      <CommandControlLink id=""7ca5d227:1887764bffb:-7fce"" parent=""0:2:0"" source=""7ca5d227:1887764bffb:-7fd0"" target=""7ca5d227:1887764bffb:-7fd4"" style=""CommandControlLink"" value="""">
        <mxGeometry as=""geometry"">
          <mxPoint as=""sourcePoint"" x=""20.0"" y=""44.0""/>
          <Array as=""points""></Array>
          <mxPoint as=""targetPoint"" x=""20.0"" y=""-4.0""/>
        </mxGeometry>
      </CommandControlLink>
    </root>
  </mxGraphModel>
  <mxCell as=""defaultParent"" id=""0:2:0"" parent=""0:1:0""/>
</XcosDiagram>

",76384048,"Use the AFFICH_m block (https://help.scilab.org/AFFICH_m). However, be warned that you still have to run the simulation to see the value:

",
76385130,Ms Access running sum with dates and order inside dates,"I am using MS Access to create a DB. I have different physical containers and I want to obtain the running sum of the liquid that has been added and taken out of the container to give the balance of the liquid inside the container.
The biggest problem is ordering liquid transactions. I have read lots of resources, and I found out one way of doing this is adding time to the dates so they can be ordered, but I am supposed to not use time, so I have decided to add a manual ordering number within a date which I am not sure is the best way of achieving this, but at least I can do the ordering.
The fields I have are:
-ContainerId
-DateTransaction
-OrderInDate
-Quantity

Quantity is ""-"" for withdrawal and ""+"" for additions, so if I can properly get a running total, I will get the balance.
I thought of adding OrderInDate as seconds to the Date to correctly order the data and I have written a query like this (qryInventoryTransactionsOrder):
SELECT ContainerId, Quantity, DateTransaction, OrderInDate,
DateAdd(""s"",OrderInDate,DateTransaction) AS Expr1,
DSum(""Quantity"",""qryInventoryTransactionsOrder"",
""[ContainerId]="" & [ContainerId] & "" AND
[Expr1] <= #"" & [Expr1] & ""#"") AS Balance
FROM InventoryTransactions
ORDER BY ContainerId, DateAdd(""s"",OrderInDate,DateTransaction);

This returns very interesting result like this :




ContainerId
DateTransaction
OrderInDate
Quantity
Expr1
Balance




1
29/05/2023
1
-50
29/05/2023 00:00:01
-50


1
31/05/2023
1
100
31/05/2023 00:00:01
50


1
31/05/2023
2
255
31/05/2023 00:00:02
305


1
01/06/2023
1
-155
01/06/2023 00:00:01



1
01/06/2023
2
-155
01/06/2023 00:00:02



1
01/06/2023
3
2500
01/06/2023 00:00:03



1
08/06/2023
1
-500
08/06/2023 00:00:01
1995




As you will see ""Balance"" is correct for the first 3 lines, then it returns 3 empty results, and then 1995.
What am I doing wrong here or is there a better way to achieve this result?
",76385764,"I can't say anything about recursive queries in MS Access.It is interesting.
Try this query, where DSum counts sum from main table.
SELECT ContainerId, Quantity, DateTransaction, OrderInDate
    ,DateAdd(""s"",OrderInDate,DateTransaction) AS Expr1
    ,DSum(""Quantity"",""InventoryTransactions""
       ,""[ContainerId]="" & [ContainerId] 
        & "" AND Format(DateAdd(""""s"""",OrderInDate,DateTransaction),""""yyyyMMddhhmmss"""") <= "" 
      & format(DateAdd(""s"",OrderInDate,DateTransaction),""yyyyMMddhhmmss"") & """") 
    AS Balance
FROM InventoryTransactions
ORDER BY ContainerId, DateAdd(""s"",OrderInDate,DateTransaction);

",
76383683,"IN DRF, how to create a POST serializer where I can add multiple values of a Foreign Key field","These are 2 models I have:

class Skill(models.Model):
    name = models.CharField(max_length=100)

    def __str__(self):
        return self.name + "" - ID: "" + str(self.id)

class Experience(models.Model):
    consultant = models.ForeignKey(""Consultant"", related_name=""experience"", on_delete=models.CASCADE)
    project_name = models.CharField(max_length=100)
    company = models.CharField(max_length=100)
    company_description = models.TextField(null=True, blank=True)
    from_date = models.DateField()
    to_date = models.DateField()
    project_description = models.CharField(max_length=100)
    contribution = models.TextField()
    summary = models.TextField()
    is_pinned = models.BooleanField(default=False)
    role = models.CharField(max_length=100, null=True)
    skill = models.ForeignKey(""Skill"", related_name=""experience"", on_delete=models.CASCADE)


I want to do something that is quite common but apparently not possible out of the box with DRF: I want to have an endpoint /experience/ with a POST method where I can send a LIST of skill ids (skill field, ForeignKey). For example:
{
  ""project_name"": ""Project AVC"",
  ""company"": ""XYZ Company"",
  ""company_description"": ""Description of XYZ Company"",
  ""from_date"": ""2022-01-01"",
  ""to_date"": ""2022-12-31"",
  ""project_description"": ""Description of Project ABC"",
  ""contribution"": ""Contributions to Project ABC"",
  ""summary"": ""Summary of Experience"",
  ""is_pinned"": false,
  ""role"": ""Consultant"",
  ""skills_ids"": [1,2,3],
  ""consultant"": 1
}

If there are Skill records in the DB with ids 1,2,3 then it will create 3 records in the experience table (one for each skill ofc) . If there's no skill with such id, then during validation it should return an error to the user informing so.
The name of the field can be either skill , skills, skill_ids... it does not matter.
This is the ExperienceSerializer I created:
class ExperienceSerializer(serializers.ModelSerializer):
    skills = serializers.PrimaryKeyRelatedField(
        many=True,
        queryset=Skill.objects.all(),
        write_only=True
    )

    class Meta:
        model = Experience
        exclude = ['skill']

    def create(self, validated_data):
        skills_data = validated_data.pop('skills', [])
        experience = Experience.objects.create(**validated_data)

        for skill in skills_data:
            experience.skill.add(skill)

        return experience

but that gives me the error:
django.db.utils.IntegrityError: null value in column ""skill_id"" of relation ""coody_portfolio_experience"" violates not-null constraint
DETAIL:  Failing row contains (21, BOOM, XYZ Company, 2022-01-01, 2022-12-31, Description of Project ABC, Contributions to Project ABC, Summary of Experience, 1, null, f, Consultant, Description of XYZ Company).
I also tried using serializers.ListField but it doesn't seem to be quite the serializer for this.
Tried the approach from this answer as well, so then I had my serializer like this:
class ExperienceSerializer(serializers.ModelSerializer):
    skill_ids = serializers.ListField(
        child=SkillSerializer(),
        write_only=True
    )

    class Meta:
        model = Experience
        fields = (
            'consultant',
            'project_name',
            'company',
            'company_description',
            'from_date',
            'to_date',
            'project_description',
            'contribution',
            'summary',
            'is_pinned',
            'role',
            'skill',
            'skill_ids'
        )

    def create(self, validated_data):
        skill_ids = validated_data.pop('skill_ids')
        experience = Experience.objects.create(**validated_data)
        experience.set(skill_ids)

        return experience


I modified the answer a bit from child = serializers.IntegerField, to child=SkillSerializer(), as it was giving me an error of child not being instantiated. Noticed also the use of ListField now as well.
And here is my payload in this version:
{
 ""project_name"": ""BOOM"",
 ""company"": ""XYZ Company"",
 ""company_description"": ""Description of XYZ Company"",
 ""from_date"": ""2022-01-01"",
 ""to_date"": ""2022-12-31"",
 ""project_description"": ""Description of Project ABC"",
 ""contribution"": ""Contributions to Project ABC"",
 ""summary"": ""Summary of Experience"",
 ""is_pinned"": false,
 ""role"": ""Consultant"",
 ""skill_ids"": [3, 4,2,1],
   ""consultant"": 1
}

which gives error 400:
{
    ""skill"": [
        ""This field is required.""
    ],
    ""skill_ids"": {
        ""0"": {
            ""non_field_errors"": [
                ""Invalid data. Expected a dictionary, but got int.""
            ]
        },
        ""1"": {
            ""non_field_errors"": [
                ""Invalid data. Expected a dictionary, but got int.""
            ]
        },
        ""2"": {
            ""non_field_errors"": [
                ""Invalid data. Expected a dictionary, but got int.""
            ]
        },
        ""3"": {
            ""non_field_errors"": [
                ""Invalid data. Expected a dictionary, but got int.""
            ]
        }
    }
}

Tried also this example here to no avail.
Spend some time reading this entire post explaining the issue of nested serialization, but I don't think it's quite related to my issue. All I want is a list to be sent in POST
I'm honestly going into a rabbit hole now of just trying different pieces together, but I have no idea how DRF wants me to do these stuff and their documentation is awful and lacking simple examples.
If someone could post example but also with explanations and not just the solution that would be much appreciated
",76384081,"With the current relation, if your payload contains ""skills_ids"": [1,2,3], then you would create three differrent instances of Experience each one containing a skill, which is NOT what you want, that is bad practice.
Instead, a many-to-many relationship is more adequate, associating multiple skills to an Experience and the other way around, thus avoiding duplicate values in your database.
Which is also the syntax that you are using at experience.skill.add(skill) that is how you would attach a Skill to an Experience using such relation. But, in reality you do not need to do anything other than letting the framework work for you!
models.py
class Skill(models.Model):
    ...


class Experience(models.Model):
    ...
    skills = models.ManyToManyField(Skill)

serializers.py
class ExperienceSerializer(serializers.ModelSerializer):
    class Meta:
        model = Experience
        fields = '__all__'

payload
{
  ""project_name"": ""Project AVC"",
  ""company"": ""XYZ Company"",
  ""company_description"": ""Description of XYZ Company"",
  ""from_date"": ""2022-01-01"",
  ""to_date"": ""2022-12-31"",
  ""project_description"": ""Description of Project ABC"",
  ""contribution"": ""Contributions to Project ABC"",
  ""summary"": ""Summary of Experience"",
  ""is_pinned"": false,
  ""role"": ""Consultant"",
  ""skills"": [1,2,3],
  ""consultant"": 1
}

",
76381541,Generate combinations by combining adjacent characters,"I have a program that should generate combinations of concatenation of all possible adjacent characters.
For example
Input = [a,b,c]
Output = [a,b,c], [ab,c], [a,bc]

Input = [a,b,c,d]
Output = [a,b,c,d], [ab,c,d], [a,bc,d], [a,b,cd], [ab,cd]

Input = [a,b,c,d,e]
Output = [a,b,c,d,e], [ab,c,d,e], [a,bc,d,e], [a,b,cd,e], [a,b,c,de], [ab,cd,e], [a,bc,de], **[ab,c,de]**
Last one missing from my program output

Basically we are only allowed to combine two adjacent characters.
I have written the program below.
public class Program
{
    public static void Main(string[] args)
    {
        List<string> cases = new List<string> {""a b c"", ""a b c d"", ""a b c d e""};
        for (int c = 0; c < cases.Count; c++)
        {
            var result = F(cases[c]);
            Console.WriteLine(cases[c]);
            result.ForEach(Console.WriteLine);
            Console.WriteLine(""---------------------------"");
        }
    }

    public static List<string> F(string searchTerm)
    {
        List<string> result = new List<string>();
        var terms = searchTerm.Split(new[] { ' ' }, StringSplitOptions.RemoveEmptyEntries).ToList();
        if (terms.Count == 1)
            return new List<string> { searchTerm };

        for (int x = 1; x <= 2; x++)
        {
            for (int i = 0; i < terms.Count - 1; i++)
            {
                if (x == 1)
                {
                    int j = i;
                    var joinedWord = terms[j] + terms[j + 1];
                    result.Add(searchTerm.Replace($""{terms[j]} {terms[j + 1]}"", joinedWord));
                }

                if (x == 2)
                {
                    int j = i;
                    if (j + 3 < terms.Count)
                    {
                        var firstJoinedWord = terms[j] + terms[j + 1];
                        var secondJoinedWord = terms[j + 2] + terms[j + 3];
                        result.Add(searchTerm.Replace($""{terms[j]} {terms[j + 1]} {terms[j + 2]} {terms[j + 3]}"", firstJoinedWord + "" "" + secondJoinedWord));
                    }
                }
            }
        }
        

        return result;
    }
}

And here is the output.

I don't know if we need to use Recursion/Dynamic Programming to solve this? because there can be N number of combinations. Any help will be appreciated. Thanks.
",76383769,"Here's a quick JavaScript version that you can run in your browser to verify the result -


function *mates(t) {
  if (t.length == 0) return yield []
  if (t.length == 1) return yield t
  for (const m of mates(t.slice(1)))
    yield [t[0], ...m]
  for (const m of mates(t.slice(2)))
    yield [mate(t[0], t[1]), ...m]
}

function mate(a,b) {
  return a + b
}

for (const m of mates([""a"", ""b"", ""c"", ""d"", ""e""]))
  console.log(m.join("",""))
.as-console-wrapper { min-height: 100%; top: 0; }



a,b,c,d,e
a,b,c,de
a,b,cd,e
a,bc,d,e
a,bc,de
ab,c,d,e
ab,c,de
ab,cd,e

We can easily convert that to a C# program -
using System;
using System.Linq;
using System.Collections.Generic;

class Program
{
    public static IEnumerable<List<string>> Mates(List<string> t)
    {
        if (t.Count == 0)
        {
            yield return new List<string> {};
            yield break;
        }
    
        if (t.Count == 1)
        {
            yield return t;
            yield break;
        }
        
        foreach (var m in Mates(t.GetRange(1, t.Count - 1)))
            yield return new List<string> { t[0] }
                .Concat(m)
                .ToList();
        
        foreach (var m in Mates(t.GetRange(2, t.Count - 2)))
            yield return new List<string> { Mate(t[0], t[1]) }
                .Concat(m)
                .ToList();
    }
    
    public static string Mate(string a, string b)
    {
        return a + b;
    }
    
    public static void Main(string[] args)
    {
        var input = new List<string> { ""a"", ""b"", ""c"", ""d"", ""e"" };
        
        foreach (var m in Mates(input))
            Console.WriteLine(string.Join("","", m));
    }
}

a,b,c,d,e
a,b,c,de
a,b,cd,e
a,bc,d,e
a,bc,de
ab,c,d,e
ab,c,de
ab,cd,e

",
76383755,Why is my FParsec parser failing to recognize a block comment?,"I'm trying to parse C style comments using FParsec.  Not sure why this is failing:
My parser code:
let openComment : Parser<_,unit>  = pstring ""/*""
let closeComment : Parser<_,unit> = pstring ""*/""
let comment = pstring ""//"" >>. restOfLine true
                <|> openComment >>. (charsTillString ""*/"" true System.Int32.MaxValue) |>> Comment
                //<|> openComment >>. manyCharsTill anyChar closeComment |>> Comment
let spaceComments = many ((spaces1 |>> IgnoreU) <|> comment)
let str s  = spaceComments >>. pstring s .>> spaceComments

Test Harness:
let testStr = @""
// test comment
/* a block comment
   */
   x  // another comment
   ""
match run (str ""x"") testStr with
| Success(result, _, _)   -> printfn ""Success: %A"" result
| Failure(errorMsg, _, _) -> assert false
()


Error messager.  It is the same for both charsTillString and manyCharsTill
Error in Ln: 6 Col: 4
   
   ^
Note: The error occurred at the end of the input stream.
Could not find the string '*/'.

Comment and IgnoreU are both a discrimated type of string
",76384101,"The problem is that the combinators in your comment parser don't have the precedence/associativity that you want. You can fix this by grouping with parens:
let comment = (pstring ""//"" >>. restOfLine true)
                <|> (openComment >>. (charsTillString ""*/"" true System.Int32.MaxValue)) |>> Comment

I find that choice is often easier to read than <|> for complex parsers:
let comment =
    choice [
        pstring ""//"" >>. restOfLine true
        openComment >>. (charsTillString ""*/"" true System.Int32.MaxValue)
    ] |>> Comment

",
76385372,How can I create a code template in Android Studio to replicate the val template in Kotlin,"Kotlin and Java code templates in Android Studio.
I tried to create a code template for the truth library that would work like the val template works i.e,

When you want to create a variable from the result of calling a function? you just type,
functionName().val.
When you press enter a variable is created ie, val f = functionName().

How would I replicate this behaviour on the truth library such that when I type f.assert for instance, I get Truth.assertThat(f)?
",76385796,"This feature is named ""Postfix completion""
You can find details here https://www.jetbrains.com/help/idea/settings-postfix-completion.html
I have tried now and it's allowed me to add and edit java and groovy templates but kotlin is not supported.
The other perfect feature of Idea is Live templates you can use them to implement required templates.
You can find details here https://www.jetbrains.com/help/idea/using-live-templates.html
Update: I have checked the current version of Idea, kotlin is not supported yet.
",
76381363,Serialize property access exception to json,"I have a simple class that one of properties is:
public TValue Value => IsSuccess
    ? _value
    : throw new InvalidOperationException(""The value of a failure result can not be accessed."");

So it works like this, when some operation is a success assign value from this operation to property easy. But now I when I do sutch a thig:
var result = new Result (someValue) { IsSuccess = false }
var serialized =  JsonConvert.SerializeObject(result); // of course result type is marked as seriazable

So here I am setting such a result to false, and next I want to serilize it, problem is that exception is not serializing and I am getting just thrown exception. What am I missing here?
",76383926,"I'm wondering whether there isn't a better design for what you're trying to achieve. Namely, it might be better to move the IsSuccess check logic into a method so that it doesn't get hit during serialization. But if you really decide to do it this way, you could use a JsonConverter to catch and serialize the exception for you:
public class ValueOrExceptionConverter : JsonConverter
{
    public override void WriteJson(JsonWriter writer, object? value, JsonSerializer serializer)
    {
        if (value == null)
        {
            return;
        }

        try
        {
            serializer.Serialize(writer, ((dynamic)value).Value);
        }
        catch (Exception ex)
        {
            serializer.Serialize(writer, new { Value = new { ex.Message } });
        }
    }

    public override object? ReadJson(JsonReader reader, Type objectType, object? existingValue, JsonSerializer serializer)
    {
        throw new NotImplementedException();
    }

    public override bool CanConvert(Type objectType) => true;
}

I'm assuming your Result class is generic so using this converter would look like this:
[JsonConverter(typeof(ValueOrExceptionConverter))]
public class Result<TValue>

This is all assuming that you're using Newtonsoft.Json. The result of using that converter is something like this:
{""Value"":""Message"":""The value of a failure result can not be accessed.""}}

A few notes about the converter code:

Instead of casting to dynamic, you might be better of using an interface or abstract class if you have it. Or if you know exactly which generics you might use for TValue then perhaps you can cast even more specifically.
I've assumed that you don't want to serialize the entire exception. So if you need more than just the Message then you can add those properties like so for example: new { ex.Message, InnerMessage = ex.InnerException.Message }.
I've assumed that the Result class only has one property, named Value. You can add more properties in the same way you can add additional Exception properties.
I haven't implemented the ReadJson because deserializing the exception instead of the value makes that quite tricky. But I'm sure it's not impossible if you need to also deserialize the Result.

",
76383757,React Router cancel loader,"I'm throwing an error in my loaders when response.status.ok of fetch is false. Error component is then loaded.
But my server sometimes returns 429 status code (too many requests) upon which I don't want to load an error component but instead simply do nothing, or maybe display some message but certainly without reloading the already loaded component, or redirecting, etc.
How can that be implemented?
https://codesandbox.io/s/hungry-hooks-yuucsp?file=/src/App.js
",76384134,"You can check the response status code specifically for a 429 status and return any defined value back to the UI. I think the important detail here that you should return something instead of undefined. null appears to work and not throw any extraneous errors
function loader() {
  let response = imitateFetch();
  console.log(response);

  if (response.status === 429) {
    console.log(429);
    return null; // <-- return null
  }

  if (!response.ok) {
    throw ""loader error"";
  }
  return {};
}

but you can return anything you like, for example, change the response status to 200.
function loader() {
  let response = imitateFetch();
  console.log(response);

  if (response.status === 429) {
    console.log(429);
    response.status = 200;
    response.ok = true;
    return response;
  }

  if (!response.ok) {
    throw ""loader error"";
  }
  return {};
}


It's really up to your app's, or code's, specific use case what it returns and how the UI handles it.
",
76380714,How can I center an ImageView and TextView in a ConstraintLayout while adjusting ImageView size to maintain TextView visibility?,"Question
How can I center an ImageView and a TextView within a ConstraintLayout, ensuring that they remain centered regardless of the ConstraintLayout's height, while also ensuring that the ImageView shrinks if necessary to maintain visibility of the TextView?
Expected results:
In case of a large ConstraintLayout (centered, but not larger than the original image):

In case of a small ConstraintLayout (centered, shrinked image, text still visible):

I build the following reproducible example:
<androidx.constraintlayout.widget.ConstraintLayout
        android:layout_width=""match_parent""
        android:layout_height=""500dp"">

        <ImageView
            android:id=""@+id/imageView""
            android:layout_width=""wrap_content""
            android:layout_height=""wrap_content""
            tools:srcCompat=""@tools:sample/avatars""
            app:layout_constraintStart_toStartOf=""parent""
            app:layout_constraintEnd_toEndOf=""parent""
            app:layout_constraintTop_toTopOf=""parent""
            app:layout_constraintBottom_toTopOf=""@id/textView"" />

        <TextView
            android:id=""@+id/textView""
            android:layout_width=""wrap_content""
            android:layout_height=""wrap_content""
            android:paddingHorizontal=""@dimen/primary_margin""
            android:text=""Just a text""
            app:layout_constraintStart_toStartOf=""parent""
            app:layout_constraintEnd_toEndOf=""parent""
            app:layout_constraintTop_toBottomOf=""@+id/imageView""
            app:layout_constraintBottom_toBottomOf=""parent"" />

    </androidx.constraintlayout.widget.ConstraintLayout>

Edit
The closest solution I have found so far is as follows. However, the only remaining drawback is that I need to manually specify the maximum width and height, whereas my ideal scenario would be to dynamically adjust them according to the image's actual width and height.
<androidx.constraintlayout.widget.ConstraintLayout
        android:layout_width=""match_parent""
        android:layout_height=""match_parent"">

        <ImageView
            android:id=""@+id/imageView""
            android:layout_width=""0dp""
            android:layout_height=""0dp""
            android:layout_marginBottom=""16dp""
            app:layout_constraintHeight_max=""200dp""
            app:layout_constraintWidth_max=""200dp""
            tools:srcCompat=""@tools:sample/avatars""
            app:layout_constraintVertical_chainStyle=""packed""
            app:layout_constraintTop_toTopOf=""parent""
            app:layout_constraintBottom_toTopOf=""@id/textView""
            app:layout_constraintStart_toStartOf=""parent""
            app:layout_constraintEnd_toEndOf=""parent"" />

        <TextView
            android:id=""@+id/textView""
            android:layout_width=""wrap_content""
            android:layout_height=""wrap_content""
            android:paddingHorizontal=""@dimen/primary_margin""
            android:text=""Just a text""
            app:layout_constraintTop_toBottomOf=""@id/imageView""
            app:layout_constraintStart_toStartOf=""parent""
            app:layout_constraintEnd_toEndOf=""parent""
            app:layout_constraintBottom_toBottomOf=""parent"" />

    </androidx.constraintlayout.widget.ConstraintLayout>

",76384044,"If you take your closest solution, try setting the ImageView height and width to wrap_content again (so it displays at its native size) but add app:layout_constrainedWidth=""true"" and app:layout_constrainedHeight=""true"". This basically lets you define the height and width you want (i.e. whatever the image size actually is), but also enforces the constraints, so it acts like 0dp (i.e. MATCH_CONSTRAINT) when necessary:
<?xml version=""1.0"" encoding=""utf-8""?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:padding=""16dp""
    xmlns:app=""http://schemas.android.com/apk/res-auto"">

    <ImageView
        android:id=""@+id/image""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        app:layout_constrainedWidth=""true""
        app:layout_constrainedHeight=""true""
        android:src=""@mipmap/ic_launcher_round""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintTop_toTopOf=""parent""
        app:layout_constraintBottom_toTopOf=""@id/text""
        app:layout_constraintVertical_chainStyle=""packed""
        />

    <TextView
        android:id=""@+id/text""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""Hi""
        android:textSize=""85sp""
        android:layout_marginTop=""16dp""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintTop_toBottomOf=""@id/image""
        app:layout_constraintBottom_toBottomOf=""parent""
        />

</androidx.constraintlayout.widget.ConstraintLayout>

Here's the same layout in the Phone and Wear OS Square reference device templates:


The actual ImageView is distorted (because both dimensions are trying to size to wrap_content) but the image maintains its aspect ratio to fit in both dimensions (if you're using the default scaleType).

",
76385243,How to mock string as content of file for pytest?,"How can I provide string to function in pytest, so that function under test treat it as content of file ?
I have function that is parsing file. File is custom text format.
I want to test multiple different input files.
Initial idea was to use pytest is this way:
import pytest
import mymodule as t

# this is all valid one line config file, starting simple
@pytest.mark.parametrize(""input_file"", [
    ""#"",
    ""##"",
    """",
    ""   "",
    ""  "",
])

def test_is_input_file_valid(input_file):
    assert t.myclass(input_file)

Problem that I have is each line need to be content of input file, because t.myclass(input_file) is expecting file not string. So, I need somehow to mock it.
I am assuming pytest have this functionality by default or plugin, but was not able to find it.
",76385822,"I wrote a pytest plugin called pytest_tmp_files to solve this exact problem.  Here's how it would look for your example:
@pytest.mark.parametrize(
    'tmp_files', [
        {'f': '#'},
        {'f': '##'},
        {'f': ''},
        {'f': '   '},
        {'f': '  '},
    ],
    indirect=['tmp_files'],
)
def test_is_input_file_valid(tmp_files):
    assert t.myclass(tmp_files / 'f')

Some things worth noting:

The parameters are dictionaries where the keys are file names and the values are files contents.  This is a bit more verbose that just specifying the file contents, but much more flexible.
I'm assuming that your parser accepts pathlib.Path objects, which is what tmp_files / 'f' is.  If not, you might have to do some sort of conversion here, but it should be straight forward.
The specified files are actually created in a real temporary directory (unique for each test case), so you can treat them exactly like real files.
I wrote another pytest plugin called parametrize_from_file that allows you to specify test parameters in structured data files (e.g. YAML), separate from the test code.  I'd recommend checking it out as well.  File contents often end up being long, multi-line strings that mess up the indentation of the parameter list and make the whole test script hard to read.  Moving these parameters to a separate file can really help readability.

See also this discussion on GitHub, where I tried to get this feature added to pytest proper.
",
76383483,defaultValue for input type 'date' is not working in Nextjs,"I am using Nextjs and I would like to use the html tag, input type of 'date'. I also would like to set a defaultValue and I did something like this and it is not working:
const [dDate, setdDate] = useState('17/11/2022');


const ReactComponent = ()=>{
  
    return(
       
          <input type=""date"" className="" border-2 py-1 px-2 rounded-md"" 
         onChange={(text)=>handleDepositeDate(text.target.value, index)} defaultValue={dDate} 
      />
     )

 };

It does not display on the screen. I also tried using value instead of defaultValue, still didn't work. I also tried other date input on the application and defaultValue didn't work.
",76384221,"Try using a different date format with the value attribute, like so:
<input type=""date"" value=""2022-11-17""/>

",
76381366,Can't use assume role to accesss aws eks which is created by assume role api,"I created a aws eks using assume role api. Role A assume role B to performe create EKS api. I create the eks and specify that the EKS's cluster role is role C. As I know,the role C's arn will be stored in eks aws-auth configMap.
When A assume role C to access the created EKS, ""Failed to get namespaces: Unauthorized"" returned.
I always use assume role to invoke API. Does anyone know, whether aws-auth store role C's arn like 'arn:aws:iam::C:role/k8s-cluster-role' or eks store the role arn in aws-auth in another way.
",76384056,"You have some misconception; The role that is stored in aws-auth configmap for system:masters group in your cluster is not the cluster role, but the iam principal that creates the cluster itself, as per official doc.

When you create an Amazon EKS cluster, the IAM principal that creates the cluster is automatically granted system:masters permissions in the cluster's role-based access control (RBAC) configuration in the Amazon EKS control plane.

From what you have written, if the sequence is right, and that assume-role approach you are following works properly, you should be able to query your cluster api resources with role-b not role-c, since b is the one you used to create the cluster. In your current setup, you are expecting role C to be able to access cluster resources, though you created with role b.
",
76380876,Description bug in DEB file created by jpackage when opened with QApt installer,"Adding description with --description while creating deb package on linux creates deb file, which when opened with QApt package installer (double click installation) shows defined text in description section, which seems like a bold header and underneath there is that description again with first character stripped:

Command:
jpackage -t deb \
--app-version 1.0.0 \
--icon books256.png \
--name hello \
--description 'my testing text' \
--dest target/jpackage_outputdir/ \
--temp target/jpackage_tempdir \
--input target/jpackage_inputdir \
--main-class core.Main \
--main-jar jarfile.jar \
--linux-menu-group Office \
--linux-shortcut

Is there a way to prevent this unprofessional look by finely setting bold header and text underneath separately or at least by removing this duplication in description section?
Using Kubuntu 23.0.4, JDK - openjdk 17.0.7 2023-04-18 LTS
",76384084,"A solution to this problem might be to use multi-line description. First line is then bold text and second line is below it, altough bug still remains when using single-line description.
jpackage -t deb \
--app-version 1.0.0 \
--icon books256.png \
--name hello \
--description 'first line
second line' \
--dest target/jpackage_outputdir/ \
--temp target/jpackage_tempdir \
--input target/jpackage_inputdir \
--main-class core.Main \
--main-jar jarfile.jar \
--linux-menu-group Office \
--linux-shortcut


",
76384941,How to type Material UI Icon's fontSize prop?,"I've composed the component below, and I need to apply a type for the custom iconFontSize prop. How can I do this?
import { SvgIconComponent } from '@mui/icons-material'
import { Typography, TypographyProps} from '@mui/material'

type Props = TypographyProps & {
  Icon: SvgIconComponent
  iconFontSize: /* insert type here! */
}

export const IconTypography = ({
  Icon,
  iconFontSize = 'inherit',
  columnGap = 1,
  children,
  ...props
}: Props) => {
  return (
    <Typography display=""flex"" alignItems=""center"" columnGap={columnGap} {...props}>
      <Icon fontSize={iconFontSize} />
      {children}
    </Typography>
  )
}

Thanks in advance!
",76385914,"You can type iconFontSize as a union type of  'inherit' | 'large' | 'medium' | 'small':
type Props = TypographyProps & {
  Icon: SvgIconComponent
  iconFontSize: ""inherit"" | ""small"" | ""medium"" | ""large"" 
}

If you want to use the exact type from the MUI type definition file, you can alternatively use:

import { OverridableStringUnion } from '@mui/types';

fontSize?: OverridableStringUnion<
  'inherit' | 'large' | 'medium' | 'small',
  SvgIconPropsSizeOverrides
>;

The OverridableStringUnion type is in turn defined as such:
export type OverridableStringUnion<T extends string | number, U = {}> = GenerateStringUnion<
  Overwrite<Record<T, true>, U>
>;

See the type definition in the MUI docs here: https://mui.com/material-ui/api/svg-icon/
",
76381138,Cloning a texture created with THREE.WebGLRenderTarget,"I'm using three.js and I need to create a few clones of a texture made with THREE.WebGLRenderTarget().
I can use the original texture, e.g.:
scene.background = renderTarget.texture;
But if I try to use a clone of it:
const tex = renderTarget.texture.clone();
scene.background = tex;
I get the following error:
THREE.WebGLState: TypeError: Failed to execute 'texSubImage2D' on 'WebGL2RenderingContext': Overload resolution failed.
If I add the line:
tex.isRenderTargetTexture = true;
Now I don't get any error, but the texture is all black.
I have also tried to clone the render target (instead of its texture) but it didn't work either. Can you please help me?
Thank you in advance.
",76384119,"Problem solved: I created a framebuffer texture, and I copied the texture by using method renderer.copyFramebufferToTexture() of class THREE.WebGLRenderer.
",
76382996,algorithm to calculate speeds to move in order to arrive in x turns,"given a distance, turns calculate the number of turns to be on each speed - 1 2 4, and 8. to complete the distance on the last turn.
you start on speed 1, and in each turn you can accelerate to the next speed or do nothing (1 -> 2, 2 -> 4, 4 -> 8), once you accelerate you can't slow back down.
each turn you are moving speed steps (distance -= speed).
also, it's ok to go more than distance steps but only if it happens on the last turn.
for example: distance = 25, turns = 10 -> speed 1: 1 turn, speed 2: 5 turns, speed 4: 4 turns, the total distance is 1 * 1 + 2 * 5 + 4 * 4 = 27 steps, but we got to 25 steps on the last turn which is what we need.
I need help writing a function that will calculate that.
def calc_speeds(distance: int, arrive_in_x_turns: int) -> dict[int, int]:


so far i've used this turns_till_arrival = ((turns_till_arrival - (speed // 2)) // speed) + (speed // 2) + 1 formula, in a for loop, for each speed and if turns_till_arrival is equal to turns I will accelerate until I get to speed without spending extra turns in other speeds (only the 1 necessary turn, because I can only accelerate once per turn) but then there are a lot of times that it doesn't work because in order for it to work I must spend more than 1 turn at other speeds but I can't figure out a way to calculate that.
",76384240,"This is a fairly exhaustive approach, but it does provide the correct answer to the problem above about how to cover your distance in a specified number of steps.
For my method, I first created the output dictionary in the form of {1: distance, 2: 0, 4: 0, 8: 0}, where, for each key-value pair, the key represents your speed and the value represents the total number of turns spent at that speed. This dictionary above represents the way to cover your distance in the maximum number of turns possible, since each turn you are just running at speed=1.
The important realization here is that if you subtract two turns from output[1] and add a turn to output[2], you are covering the same distance as before, but you have lost a turn. Therefore, you can make a loop that stops when the number of turns in your output equals arrive_in_x_turns, and during each iteration, you lose a turn by doing the subprocess I have described above. This also works for the other speeds (i.e. subtracting two turns from output[2] and adding a turn into output[4] maintains the distance you want to travel while losing a turn).
Here is my implementation below. When running your example of calc_speeds(distance=25, arrive_in_x_turns=10), my output is {1: 1, 2: 6, 4: 3, 8: 0}. The number of turns are correct (1 + 6 + 3 = 10), as is the distance covered (1*1 + 2*6 + 4*3 + 8*0 = 1 + 12 + 12 + 0 = 25).
def calc_speeds(distance: int, arrive_in_x_turns: int) -> dict[int, int]:
    output = {1: distance, 2: 0, 4: 0, 8: 0}

    # Loop until the total number of turns is equal to the number of turns requested
    while (sum(output.values()) > arrive_in_x_turns):
        # Subtract two turns off of '1' and add a turn to '2' until not possible
        # This method ensures that during each iteration, number of turns decreases
        # by one while the distance traveled remains the same
        if output[1] // 2 > 0:
            output[1] -= 2
            output[2] += 1
        # Do a similar method for the ones above
        elif output[2] // 2 > 0:
            output[2] -= 2
            output[4] += 1
        elif output[4] // 2 > 0:
            output[4] -= 2
            output[8] += 1
    
    return output

Again, this is an exhaustive solution that will have a long runtime with harder examples, but hopefully this will help get you started on finding a more optimal way to solve it!
EDIT:
This answer above can be optimized! Instead of subtracting two turns from a current speed and adding one turn to the next, we can instead find the minimum between half the number of turns of the current speed and the remaining number of turns that need to be gotten rid of. Here's a newer version.
def calc_speeds(distance: int, arrive_in_x_turns: int) -> dict[int, int]:
    output = {1: distance, 2: 0, 4: 0, 8: 0}

    # Loop until the total number of turns is equal to the number of turns requested
    while sum(output.values()) > arrive_in_x_turns:
        if output[1] // 2 > 0:
            # Calculate turns to subtract from current speed as the minimum between half the 
            # current turns under this speed and the number of turns remaining
            turns_to_subtract = min(output[1] // 2, sum(output.values()) - arrive_in_x_turns)
            # Use similar logic to the previous version of this algorithm
            output[1] -= turns_to_subtract * 2
            output[2] += turns_to_subtract
        elif output[2] // 2 > 0:
            turns_to_subtract = min(output[2] // 2, sum(output.values()) - arrive_in_x_turns)
            output[2] -= turns_to_subtract * 2
            output[4] += turns_to_subtract
        elif output[4] // 2 > 0:
            turns_to_subtract = min(output[4] // 2, sum(output.values()) - arrive_in_x_turns)
            output[4] -= turns_to_subtract * 2
            output[8] += turns_to_subtract
    
    return output

",
76385015,Jolt not printing anything,"I am writing jolt for transforming this data but not getting desired result
If practice_loc,prac_num and topId are same for two or more data then they will be combined together with separate S1 and S2 within subList. Else they would pass as it is with addition of subList only.
Data
[
  {
    ""practice_loc"": ""120"",
    ""prac_num"": ""oswal"",
    ""topId"": ""t1"",
    ""S1"": ""A1"",
    ""S2"": ""B1""
  },
  {
    ""practice_loc"": ""120"",
    ""prac_num"": ""oswal"",
    ""topId"": ""t1"",
    ""S1"": ""A2"",
    ""S2"": """"
  },
  {
    ""practice_loc"": ""334"",
    ""prac_num"": ""L3"",
    ""topId"": ""plumcherry"",
    ""S1"": ""A3"",
    ""S2"": """"
  },
  {
    ""practice_loc"": ""987"",
    ""prac_num"": ""L3"",
    ""topId"": ""artica"",
    ""S1"": ""A5"",
    ""S2"": ""B7""
  }
]

Expected Output:
[
  {
    ""practice_loc"": ""120"",
    ""prac_num"": ""oswal"",
    ""topId"": ""t1""
    ""subList"": [
      {
        ""S1"": ""A1"",
        ""S2"": ""B1""
      },
      {
        ""S1"": ""A2"",
        ""S2"": """"
      }
    ]
  },
  {
    ""practice_loc"": ""334"",
    ""prac_num"": ""L3"",
    ""topId"": ""plumcherry""
    ""subList"": [
      {
        ""SubID1"": ""A3"",
        ""SubID2"": """"
      }
    ]
  },
  {
    ""practice_loc"": ""987"",
    ""prac_num"": ""L3"",
    ""topId"": ""artica"",
    ""subList"": [
      {
        ""SubID1"": ""A5"",
        ""SubID2"": ""B7""
      }
    ]
  }
]

Here is what I tried but didnt get desired result Its not printing anything
[
  {
    ""operation"": ""shift"",
    ""spec"": {
      ""*"": {
        ""@"": ""@(1,practice_loc).@(1,prac_num).@(1,topId)""
      }
    }
  },
  {
    ""operation"": ""cardinality"",
    ""spec"": {
      ""*"": {
        ""*"": ""MANY""
      }
    }
  },
  {
    ""operation"": ""shift"",
    ""spec"": {
      ""*"": {
        ""*"": {
          ""*"": {
            ""practice_loc"": ""[#4].&"",
            ""prac_num"": ""[#4].&"",
            ""topId"": ""[#4].&"",
            ""S*"": ""[#4].subList[&1].&""
          }
        }
      }
    }
  },
  {
    ""operation"": ""cardinality"",
    ""spec"": {
      ""*"": {
        ""practice_loc"": ""ONE"",
        ""prac_num"": ""ONE"",
        ""topId"": ""ONE""
      }
    }
  }
]

",76385939,"Your current spec is pretty good. Would be suitable to rearrange it like that
[
  { // group by those three attributes
    ""operation"": ""shift"",
    ""spec"": {
      ""*"": {
        ""*"": ""@1,practice_loc.@1,prac_num.@1,topId.&"",
        ""S*"": ""@1,practice_loc.@1,prac_num.@1,topId.subList[&1].&""
      }
    }
  },
  { // get rid of wrappers
    ""operation"": ""shift"",
    ""spec"": {
      ""*"": {
        ""*"": {
          ""*"": {
            ""@"": """"
          }
        }
      }
    }
  },
  {
    ""operation"": ""cardinality"",
    ""spec"": {
      ""*"": {
        ""*"": ""ONE"", // pick only single one from repeating components 
        ""subList"": ""MANY""
      }
    }
  },
  { // get rid of generated nulls within subList arrays 
    ""operation"": ""modify-overwrite-beta"",
    ""spec"": {
      ""*"": ""=recursivelySquashNulls""
    }
  }
]

Edit for illustration : Below, I have pasted the image what I get after toggling ADVANCED tab of Configure section for the JoltTransformJSON processor which has the version 1.21.0 as NiFi does. Btw, yours is a recent version as well.

",
76381511,Different Checkbox groups should write data in textfields,"My problem is that I have found a solution for one group of checkboxes and shows the selected data in a text field in my dynamic formular.
But I think the line $('input:checkbox').change((e) does not make sense if I want to use a different, or new, group of checkboxes.
My idea is that the two different groups of checkboxes get a unique id to handle with them.
The data i want to include in an MariaDB Database.
                        <tr>
                            <td>
                                Entsperrcode:
                            </td>
                            <td>
                                    <script src=""inc/jquery-3.7.0.min.js""></script>
                                    <br>
                                    <table border=""1""cellspacing=""0"" cellpadding=""0"">
                                        <tr>
                                            <td><center>1</center></td>
                                            <td><center>2</center></td>
                                            <td><center>3</center></td>
                                        </tr>
                                        <tr>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch1"" value=""1""></td>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch2"" value=""2""></td>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch3"" value=""3""></td>
                                        </tr>
                                            <td><center>4</center></td>
                                            <td><center>5</center></td>
                                            <td><center>6</center></td>
                                        </tr>
                                        <tr>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch4"" value=""4""></td>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch5"" value=""5""></td>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch6"" value=""6""></td>
                                        </tr>
                                        <tr>
                                            <td><center>7</center></td>
                                            <td><center>8</center></td>
                                            <td><center>9</center></td>
                                        </tr>
                                        <tr>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch7"" value=""7""></td>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch8"" value=""8""></td>
                                            <td><input type=""checkbox"" id=""entsperrcodewisch9"" value=""9""></td>
                                        </tr>
                                </table>
                                    <input type=""text"" id=""selected"" name=""entsperrcode""/><br><br>


                                    <script>
                                    (function() {
                                    $('input:checkbox').change((e) => {
                                        if ($(e.currentTarget).is(':checked')) {
                                        var curVal = $('#selected').val();
                                        if (curVal) {
                                            $('#selected').val(curVal + '-' + e.currentTarget.value);
                                        } else {

                                            $('#selected').val(e.currentTarget.value);
                                        }
                                        } else {
                                        var curVal = $('#selected').val().split('-');
                                        var filteredVal = curVal.filter(el => el.trim() !== e.currentTarget.value)
                                        $('#selected').val(filteredVal.join('-'));
                                        }
                                    });
                                    })();
                                    </script>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                Beschädigungen:
                            </td>
                            <td>
                                    <script src=""inc/jquery-3.7.0.min.js""></script>
                                    <br>
                                    <input type=""checkbox"" id=""beschaedingung1"" value=""Display""><br>
                                    <input type=""checkbox"" id=""beschaedingung2"" value=""Rückseite""><br>
                                    <input type=""checkbox"" id=""beschaedingung3"" value=""Rand""><br>

                                    <input type=""text"" id=""beschaedig"" name=""beschaedig""/><br><br>


                                    <script>
                                    (function() {
                                    $('input:checkbox').change((e) => {
                                        if ($(e.currentTarget).is(':checked')) {
                                        var curVal = $('#beschaedig').val();
                                        if (curVal) {
                                            $('#beschaedig').val(curVal + '-' + e.currentTarget.value);
                                        } else {

                                            $('#beschaedig').val(e.currentTarget.value);
                                        }
                                        } else {
                                        var curVal = $('#beschaedig').val().split('-');
                                        var filteredVal = curVal.filter(el => el.trim() !== e.currentTarget.value)
                                        $('#beschaedig').val(filteredVal.join('-'));
                                        }
                                    });
                                    })();
                                    </script>
                            </td>
                        </tr>

",76384131,"The way I'd approach this is as below, with explanatory comments in the code:


// simple utility variable and functions to reduce some of the repetitive typing:
const D = document,
  // here we have means of creating an element, and passing various properties
  // to that new element (className, textContent, borderColor...):
  create = (tag, props) => Object.assign(D.createElement(tag), props),
  // an alias for document, and element, querySelector(), which is used
  // depends on the context which is the document by default:
  get = (selector, context = D) => context.querySelector(selector),
  // as above, but an alias for querySelectorAll(), this explicitly returns
  // an Array instead of a NodeList in order to allow for Array methods to be
  // used (map(), filter()...):
  getAll = (selector, context = D) => [...context.querySelectorAll(selector)];

// named function to handle the events on the <input type=""checkbox"">
// elements, this is bound later using EventTarget.addEventListener();
// this function takes one argument - 'evt', a reference to the Event Object -
// passed from EventTarget.addEventListener():
const checkboxHandler = (evt) => {

    // this is the element to which the event-handling function is bound:
    let changed = evt.currentTarget,
      // the 'output' is the element in which I'll be showing the results,
      // and uses Element.querySelector() to find the first (if any) element
      // matching the selector which is found within the closest ancestor
      // <fieldset> element:
      output = get('.result', changed.closest('fieldset')),
      // this retrieves the delimiter custom property defined in the CSS,
      // whether via the stylesheet or the inline ""style"" attribute:
      delimiter = window.getComputedStyle(output,null).getPropertyValue(""--delimiter""),
      // we retrieve the value of the changed element (removing leading/trailing
      // white-space; this may or may not be necessary depending on your use-case):
      result = changed.value.trim(),
      // here we use a template-literal string to concatenate the various
      // variables together to create an identifier for the created element
      // (an id could be used, given that this is likely to be unique, but I
      // chose to assume that conflicts may happen across the document):
      resultClass = `${changed.name}${delimiter}${result}`,
      // we create a <span> element with the given textContent and className:
      resultWrapper = create('span', {
          textContent: result,
          className: resultClass,
      }),
      // creating another element - an <em> - to wrap the delimiter character:
      delimiterWrapper = create('em', {
      textContent: delimiter,
      className: ""delimiter""
    });
    
    // a checkbox may fire the change event both when it's checked or
    // unchecked by the user; therefore we first test to see whether it
    // was checked (this returns a Boolean true or false):
    if (changed.checked) {
      // if it was checked we append both the created <em> element, along
      // with the created <span> element to the output element:
        output.append(delimiterWrapper, resultWrapper);
    } else {
      // or if it was unchecked, we retrieve the existing element via
      // the class we created created earlier and looking within the
      // output element, using the alias (above) of element.querySelector()
      // by passing a context:
    let toRemove = get(`.${resultClass}`, output);
    
    // we then use an Array literal, passing in both the previous element
    // sibling of the element we wish to remove and the element itself:
    [toRemove.previousElementSibling, toRemove]
      // we then use Array.prototype.forEach() to iterate over the Array:
      .forEach(
        // passing in a reference to the current Array element 'el',
        // and removing that element:
        (el) => el.remove()
      );
    }
};
  
// here we selector all checkbox <input> elements in the document, and
// iterate over that Array:
getAll('input[type=checkbox]').forEach(
  // passing in a reference - 'el' - to the current Node of the
  // Array of Nodes, and using EventTargetTarget.addEventListener()
  // to bind the named function checkboxHandler() (note the deliberately
  // missing parentheses) as the event-handler for the 'change' event:
    (el) => el.addEventListener('change', checkboxHandler)
);
/* setting a custom property to use later as a basic
   demonstration of how custom properties might be
   used: */
form {
  --labelSize: 3rem;
}

fieldset {
  /* allows us to easily style the <input> elements
     into a grid, tabular-style, format: */
  display: inline grid;
  /* defining the space between adjacent elements: */
  gap: 0.5rem;
  /* defining the size of the various rows: */
  grid-auto-rows: var(--labelSize);
  /* defining the size of the various columns, using
     repeat() to create a number of columns stored in
     the --columnCount custom variable, with a default
     value of 3: */
  grid-template-columns: repeat(var(--columnCount, 3), var(--labelSize));
}

label {
  border: 1px solid currentColor;
  display: grid;
  padding: 0.25rem;
  text-align: center;
}

/* this moves the <input> children of the <label>
   element after their next sibling element in order
   that the DOM allows for a checked <input> to style
   the following <span>, despite that <span> visually
   appearing before the <input>: */
label > input {
  order: 1;
}

/* styling the adjacent <span> of the checked <input>: */
input:checked + span {
  /* this isn't particularly great example, but serves
     only to demonstrate how the <span> may be styled
     based on the state of the check-box: */
  background-image:
    radial-gradient(
      at 0 0,
      lightskyblue,
      palegreen
    );
  font-weight: bold;
}

.result {
  border: 1px solid currentColor;
  /* using flex layout: */
  display: flex;
  /* shorthand for:
      flex-direction: row;
      flex-wrap: wrap;
     adjust to your preferences: */
  flex-flow: row wrap;
  gap: 0.25rem;
  /* positioning the element so that it
     starts in grid-column: 1 (the first)
     and finishes in grid-column: -1
     (the last) */
  grid-column: 1 / -1;
  padding-block: 0.25rem;
  padding-inline: 0.5rem;
}

/* within the JavaScript we add the .result
   elements along with a previous .delimiter
   element, so here we style a .delimiter
   when it's the first child so that it's not
   visible in the document: */
.result .delimiter:first-child {
  display: none;
}
<form action=""#"">
  <!-- I opted to wrap each ""group"" of <input> elements within
       a <fieldset> element within a <form>, as your demo didn't
       look tabular (the element choices aren't really important
       changes require only minor adjustments to the JavaScript: -->
  <fieldset>
    <!-- provides a ""title"" to the grouped <input> elements: -->
    <legend>Group 1</legend>
    <!-- wrapping the <input> within the <label> so that clicking
         either the <input> or the text will update the checked
         state of the <input> and trigger the change event: -->
    <label><input type=""checkbox"" value=""1"" name=""group-1"">
      <span class=""labelText"">1</span>
      <!-- to associate the <input> elements together in their
           ""groups"" I've taken advantage of the ""name""
           attribute in place of the (invalid duplication of
           ""id"" attributes) -->
      
    </label>
    <label><input type=""checkbox"" value=""2"" name=""group-1"">
      <span class=""labelText"">2</span>
      
    </label>
    <label><input type=""checkbox"" value=""3"" name=""group-1"">
      <span class=""labelText"">3</span>
      
    </label>
    <label><input type=""checkbox"" value=""4"" name=""group-1"">
      <span class=""labelText"">4</span>
      
    </label>
    <label><input type=""checkbox"" value=""5"" name=""group-1"">
      <span class=""labelText"">5</span>
      
    </label>
    <label><input type=""checkbox"" value=""6"" name=""group-1"">
      <span class=""labelText"">6</span>
      
    </label>
    <label><input type=""checkbox"" value=""7"" name=""group-1"">
      <span class=""labelText"">7</span>
      
    </label>
    <label><input type=""checkbox"" value=""8"" name=""group-1"">
      <span class=""labelText"">8</span>
      
    </label>
    <label><input type=""checkbox"" value=""9"" name=""group-1"">
      <span class=""labelText"">9</span>
      
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>
  <fieldset>
    <legend>Group 2</legend>
    <label><input type=""checkbox"" value=""1"" name=""group-2"">
      <span class=""labelText"">1</span>
      </label>
    <label><input type=""checkbox"" value=""2"" name=""group-2"">
      <span class=""labelText"">2</span>
      
    </label>
    <label><input type=""checkbox"" value=""3"" name=""group-2"">
      <span class=""labelText"">3</span>
      
    </label>
    <label><input type=""checkbox"" value=""4"" name=""group-2"">
      <span class=""labelText"">4</span>
      
    </label>
    <label><input type=""checkbox"" value=""5"" name=""group-2"">
      <span class=""labelText"">5</span>
      
    </label>
    <label><input type=""checkbox"" value=""6"" name=""group-2"">
      <span class=""labelText"">6</span>
      
    </label>
    <label><input type=""checkbox"" value=""7"" name=""group-2"">
      <span class=""labelText"">7</span>
      
    </label>
    <label><input type=""checkbox"" value=""8"" name=""group-2"">
      <span class=""labelText"">8</span>
      
    </label>
    <label><input type=""checkbox"" value=""9"" name=""group-2"">
      <span class=""labelText"">9</span>
      
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>
  <fieldset>
    <legend>Group 3</legend>
    <label><input type=""checkbox"" value=""1"" name=""group-3"">
      <span class=""labelText"">1</span>
      </label>
    <label><input type=""checkbox"" value=""2"" name=""group-3"">
      <span class=""labelText"">2</span>
      
    </label>
    <label><input type=""checkbox"" value=""3"" name=""group-3"">
      <span class=""labelText"">3</span>
      
    </label>
    <label><input type=""checkbox"" value=""4"" name=""group-3"">
      <span class=""labelText"">4</span>
      
    </label>
    <label><input type=""checkbox"" value=""5"" name=""group-3"">
      <span class=""labelText"">5</span>
      
    </label>
    <label><input type=""checkbox"" value=""6"" name=""group-3"">
      <span class=""labelText"">6</span>
      
    </label>
    <label><input type=""checkbox"" value=""7"" name=""group-3"">
      <span class=""labelText"">7</span>
      
    </label>
    <label><input type=""checkbox"" value=""8"" name=""group-3"">
      <span class=""labelText"">8</span>
      
    </label>
    <label><input type=""checkbox"" value=""9"" name=""group-3"">
      <span class=""labelText"">9</span>
      
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>
  <fieldset>
    <legend>Group 4</legend>
    <label><input type=""checkbox"" value=""1"" name=""group-4"">
      <span class=""labelText"">1</span>
      </label>
    <label><input type=""checkbox"" value=""2"" name=""group-4"">
      <span class=""labelText"">2</span>
      
    </label>
    <label><input type=""checkbox"" value=""3"" name=""group-4"">
      <span class=""labelText"">3</span>
      
    </label>
    <label><input type=""checkbox"" value=""4"" name=""group-4"">
      <span class=""labelText"">4</span>
      
    </label>
    <label><input type=""checkbox"" value=""5"" name=""group-4"">
      <span class=""labelText"">5</span>
      
    </label>
    <label><input type=""checkbox"" value=""6"" name=""group-4"">
      <span class=""labelText"">6</span>
      
    </label>
    <label><input type=""checkbox"" value=""7"" name=""group-4"">
      <span class=""labelText"">7</span>
      
    </label>
    <label><input type=""checkbox"" value=""8"" name=""group-4"">
      <span class=""labelText"">8</span>
      
    </label>
    <label><input type=""checkbox"" value=""9"" name=""group-4"">
      <span class=""labelText"">9</span>
      
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>
</form>



JS Fiddle demo.
In response to OP's comment:

... but if I change the name ""group-1"" to ""group1[]"" to build an array the code does not reset the numbers.

I've updated the name attribute to reflect those requirements, and added a custom data-* attribute to from which the selector is derived (purely for simplicity's sakes, as [ and ] would have to be escaped in order to be used as part of the selector, which is needlessly complex though still possible of course).
This leads to the following, adjusted code:


console.clear();
const D = document,
  create = (tag, props) => Object.assign(D.createElement(tag), props),
  get = (selector, context = D) => context.querySelector(selector),
  getAll = (selector, context = D) => [...context.querySelectorAll(selector)];

const checkboxHandler = (evt) => {
  let changed = evt.currentTarget,
    output = get('.result', changed.closest('fieldset')),
    delimiter = window.getComputedStyle(output, null).getPropertyValue(""--delimiter""),
    result = changed.value.trim(),
    // here we use the Element.dataset API to form the resultClass selector to
    // enable easy removal of the value from the <output>
    resultClass = `${changed.dataset.name}${delimiter}${result}`,
    resultWrapper = create('span', {
      textContent: result,
      className: resultClass,
    }),
    delimiterWrapper = create('em', {
      textContent: delimiter,
      className: ""delimiter""
    });

  if (changed.checked) {
    output.append(delimiterWrapper, resultWrapper);
  } else {
    let toRemove = get(`.${resultClass}`, output);
    [toRemove.previousElementSibling, toRemove].forEach((el) => el.remove());
  }
};

getAll('input[type=checkbox]').forEach(
  (el) => el.addEventListener('change', checkboxHandler)
);
form {
  --labelSize: 3rem;
}

fieldset {
  --accent: palegreen;
  display: inline grid;
  gap: 0.5rem;
  grid-auto-rows: var(--labelSize);
  grid-template-columns: repeat(var(--columnCount, 3), var(--labelSize));
}

label {
  border: 1px solid currentColor;
  display: grid;
  padding: 0.25rem;
  text-align: center;
}

label input {
  accent-color: var(--accent, unset);
  order: 1;
}

input:checked+span {
  background-image: linear-gradient( 90deg, aqua, var(--accent, transparent));
}

.result {
  border: 1px solid currentColor;
  display: flex;
  flex-flow: row wrap;
  gap: 0.25rem;
  grid-column: span 3;
  padding-block: 0.25rem;
  padding-inline: 0.5rem;
}

.result .delimiter:first-child {
  display: none;
}
<form action=""#"">
  <fieldset>
    <legend>Group 1</legend>
    <label>
      <!-- adding the ""array-indicator"" to the ""name"" attribute,
           and adding an additional data-name custom attribute: -->
      <input type=""checkbox"" value=""1"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">1</span>
    </label>
    <label>
      <input type=""checkbox"" value=""2"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">2</span>
    </label>
    <label>
      <input type=""checkbox"" value=""3"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">3</span>
    </label>
    <label>
      <input type=""checkbox"" value=""4"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">4</span>
    </label>
    <label>
      <input type=""checkbox"" value=""5"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">5</span>
    </label>
    <label>
      <input type=""checkbox"" value=""6"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">6</span>
    </label>
    <label>
      <input type=""checkbox"" value=""7"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">7</span>
    </label>
    <label>
      <input type=""checkbox"" value=""8"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">8</span>
    </label>
    <label>
      <input type=""checkbox"" value=""9"" name=""group-1[]"" data-name=""group-1"">
      <span class=""labelText"">9</span>
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>

  <fieldset>
    <legend>Group 2</legend>
    <label>
      <input type=""checkbox"" value=""1"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">1</span>
    </label>
    <label>
      <input type=""checkbox"" value=""2"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">2</span>
    </label>
    <label>
      <input type=""checkbox"" value=""3"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">3</span>
    </label>
    <label>
      <input type=""checkbox"" value=""4"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">4</span>
    </label>
    <label>
      <input type=""checkbox"" value=""5"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">5</span>
    </label>
    <label>
      <input type=""checkbox"" value=""6"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">6</span>
    </label>
    <label>
      <input type=""checkbox"" value=""7"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">7</span>
    </label>
    <label>
      <input type=""checkbox"" value=""8"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">8</span>
    </label>
    <label>
      <input type=""checkbox"" value=""9"" name=""group-2[]"" data-name=""group-2"">
      <span class=""labelText"">9</span>
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>

  <fieldset>
    <legend>Group 3</legend>
    <label>
      <input type=""checkbox"" value=""1"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">1</span>
    </label>
    <label>
      <input type=""checkbox"" value=""2"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">2</span>
    </label>
    <label>
      <input type=""checkbox"" value=""3"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">3</span>
    </label>
    <label>
      <input type=""checkbox"" value=""4"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">4</span>
    </label>
    <label>
      <input type=""checkbox"" value=""5"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">5</span>
    </label>
    <label>
      <input type=""checkbox"" value=""6"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">6</span>
    </label>
    <label>
      <input type=""checkbox"" value=""7"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">7</span>
    </label>
    <label>
      <input type=""checkbox"" value=""8"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">8</span>
    </label>
    <label>
      <input type=""checkbox"" value=""9"" name=""group-3[]"" data-name=""group-3"">
      <span class=""labelText"">9</span>
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>

  <fieldset>
    <legend>Group 4</legend>
    <label>
      <input type=""checkbox"" value=""1"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">1</span>
    </label>
    <label>
      <input type=""checkbox"" value=""2"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">2</span>
    </label>
    <label>
      <input type=""checkbox"" value=""3"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">3</span>
    </label>
    <label>
      <input type=""checkbox"" value=""4"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">4</span>
    </label>
    <label>
      <input type=""checkbox"" value=""5"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">5</span>
    </label>
    <label>
      <input type=""checkbox"" value=""6"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">6</span>
    </label>
    <label>
      <input type=""checkbox"" value=""7"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">7</span>
    </label>
    <label>
      <input type=""checkbox"" value=""8"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">8</span>
    </label>
    <label>
      <input type=""checkbox"" value=""9"" name=""group-4[]"" data-name=""group-4"">
      <span class=""labelText"">9</span>
    </label>
    <output class=""result"" style=""--delimiter: -;""></output>
  </fieldset>
</form>



JS Fiddle demo.
References:

CSS:

background-image.
CSS custom properties.
CSS logical properties.
display.
:first-child.
gap.
grid-auto-rows.
grid-template-columns.
linear-gradient().
order.
radial-gradient().
padding.
padding-block.
padding-inline.
repeat().
text-align.
var().


JavaScript:

Array literals.
Array.prototype.filter().
Array.prototype.forEach().
Array.prototype.map().
document.createElement().
document.querySelector().
document.querySelectorAll().
Element.append().
Element.previousElementSibling().
Element.remove().
Element.querySelector().
Element.querySelectorAll().
EventTarget.addEventListener().
HTMLElement.dataset API.
Object.assign().
Template literals.



",
76384666,Can't find services package.json when running docker-compose,"I am building a docker-compose.yml file inside a workspace, but when I try to run docker-compose, I can't start my services because of the following error ENOENT: no such file or directory, open '/entity-service/package.json' the same error happens when trying to start the agent-portal-service container as well but the error happens in this directory /agent-portal-service/package.json. But my projects indeed have a package.json file, I think that this is related to the context that docker-compose is running.
Here is my workspace structure:
├── agent-portal-service
├── data
├── docker-compose.yml
└── entity-service

My docker-compose.yml:
version: ""3""

services:
  agent_portal_service:
    working_dir: /agent-portal-service
    container_name: agent_portal_service
    image: node:16
    volumes:
      - /agent-workspace:/agent-portal-service
    ports:
      - 3100:3100
    command: npm run start:debug
    tty: true

  entity_service:
    working_dir: /entity-service
    container_name: entity_service
    image: node:16
    volumes:
      - /agent-workspace:/entity-service
    ports:
      - 3101:3101
    command: npm run start:debug
    tty: true
    depends_on:
      - mongodb

  mongodb:
    container_name: mongodb
    image: mongo:4.4.6
    ports:
      - 27017:27017
    volumes:
      - ./data/db:/data/db
    command: mongod --port 27017
    restart: always

I am expecting to be able to run the agent_portal_service and entity_service containers successfully
",76385966,"The important parts of your entity-service definition look like this
  entity_service:
    working_dir: /entity-service
    volumes:
      - /agent-workspace:/entity-service
    command: npm run start:debug

You map /agent-workspace to /entity-service in the volumes section. You also set your working directory to /entity-service. In effect, your working directory is /agent-workspace on your host machine. There's no package.json file in the /agent-workspace directory, so your npm command fails.
To fix it, I'd map /agent-workspace/entity-service to /entity-service instead. That way, you'll have a package.json file in your /entity-service directory inside the container.
Like this
volumes:
  - /agent-workspace/entity-service:/entity-service

",
76382916,Specify function parameter types in TypeScript mapped trype,"I have two functions that take a single options argument with differing properties, except for type, which is used to identify the function.
type FuncAOptions = {
    type: 'A'
    opt1: string
    opt2: boolean
}

function funcA(options: FuncAOptions): number {
    if (!options.opt1) throw new Error('Missing required option')
    return 1
}

type FuncBOptions = {
    type: 'B'
    opt3: number
    opt4: (a: number) => number
}

function funcB(options: FuncBOptions): string {
    if (!options.opt3) throw new Error('Missing required option')
    return 'B'
}

I then have a map of these functions, with an associated mapped type, so that I can call the functions conditionally with variable runtime data.
type AllFunctions = FuncAOptions | FuncBOptions

type FunctionMap = { [K in AllFunctions['type']]: (options: any) => any; }

const functionMap: FunctionMap = {
    A: funcA,
    B: funcB
}

function callFunction(type: keyof typeof functionMap, options: any) {
    return functionMap[type](options)
}

When I call the functions directly, I get the proper type-checking to know if I'm passing an incorrect set of options. I want to be able to do the same when calling the function through the intermediary method.
callFunction('A', { type: 'B', opt3: 'Hello' }) // NO TS ERROR
funcA({ type: 'B', opt3: 'Hello' }) // TS ERROR: The expected type comes from property 'type' which is declared here on type 'FuncAOptions'

I like having the map typed with K in AllFunctions['type'] because when I add a function to AllFunctions, I am reminded that I need to add the key-value pair to functionMap.
Full example here
",76384251,"If you want functionMap[type](options) to type check without loosening things up with type assertions or the any type, then you'll need to write it terms of generic indexes into a base key-value type or mapped types of that type.  This is as described in microsoft/TypeScript#47109.
Essentially you want type to be seen as some generic type K, and functionMap to be seen as some mapped type like {[P in keyof FuncOptionMap]: (arg: FuncOptionMap[P]) => FuncRetMap[P]} and for options to be seen as type FuncOptionMap[K].  Then the compiler can conclude that the function functionMap[type] is of type (arg: FuncOptionMap[K]) => FuncRetMap[K], and is therefore callable with options as an argument, and will return the corresponding return type FuncRetMap[K].  So we'll need to define FuncOptionMap and FuncRetMap in terms of the values you have.
You might have hoped that you could have done this with type merely being the union type equivalent to keyof FuncOptionMap without needing generics.  But TypeScript can't follow that sort of logic, as described in microsoft/TypeScript#30581.  The recommended approach is to use generic indexing into mapped types instead; indeed microsoft/TypeScript#47109 is the solution to microsoft/TypeScript#30581 (or at least the closest we have to a solution).
It could look like this:
const _functionMap = {
    A: funcA,
    B: funcB
}

type FuncOptionMap =
    { [K in keyof typeof _functionMap]: Parameters<typeof _functionMap[K]>[0] }
type FuncRetMap =
    { [K in keyof typeof _functionMap]: ReturnType<typeof _functionMap[K]> }

const functionMap: { [K in keyof FuncOptionMap]: (options: FuncOptionMap[K]) => FuncRetMap[K] }
    = _functionMap

Essentially we rename your functionMap to _functionMap and then later assign it back with the FuncOptionMap and FuncRetMap types computed from it.  It might look like a no-op, since the type of _functionMap and the type of functionMap appear to be identical.  But this is actually very important; the compiler can only follow the logic when things are written as this mapping. If you try to use _functionMap instead of functionMap in what follows, the compiler will lose the thread and output an error.
Continuing:
function callFunction<K extends keyof FuncOptionMap>(
    type: K, options: FuncOptionMap[K]
) {
    return functionMap[type](options); // okay
}

That type checks, and the return type is FuncRetMap[K].  Now you get the error you expected:
callFunction('A', { type: 'B', opt3: 'Hello' }) // error!
// ---------------------> ~~~ B is not A

and when you call it the compiler knows how the output type as depends on the input type:
console.log(callFunction(
    ""B"",
    { type: ""B"", opt3: 1, opt4: x => x + 1 }
).toLowerCase()); // okay, the compiler knows it's string and not number

Playground link to code
",
76381320,Spring Security Access Denied with Spring Boot 3.0,"I'm doing a pet project of a social network and I'm having problems with authorization.I getting Access Denied when I send a request with a jwt token.
I am doing the following chain of actions:

Registration where I specify the login email and password (Works well)
Authorization where by login and password I get a token (Works well)
I'm trying to send a request in the postman with the received token where I get an error(Access Denied)

Whats wrong with my code?
My Security Config:
@Configuration
@EnableWebSecurity
@EnableMethodSecurity( proxyTargetClass = true)
public class WebSecurityConfig  {  
    @Autowired
    UserDetailsServiceImpl userDetailsService;

    @Autowired
    private AuthEntryPointJwt unauthorizedHandler;

    @Bean
    public AuthTokenFilter authenticationJwtTokenFilter() {
        return new AuthTokenFilter();
    }
    
    @Bean
    public DaoAuthenticationProvider authenticationProvider() {
        DaoAuthenticationProvider authProvider = new DaoAuthenticationProvider();

        authProvider.setUserDetailsService(userDetailsService);
        authProvider.setPasswordEncoder(passwordEncoder());

        return authProvider;
    }
    
    @Bean
    public AuthenticationManager authenticationManager(AuthenticationConfiguration authConfig) throws Exception {
        return authConfig.getAuthenticationManager();
    }

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }
    
    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http.cors().and().csrf().disable()
                .exceptionHandling()
                .authenticationEntryPoint(unauthorizedHandler)
                .and()
                .sessionManagement()
                .sessionCreationPolicy(SessionCreationPolicy.STATELESS)
                .and()
                .authorizeHttpRequests()
                .requestMatchers(""/auth/**"").permitAll()
                .requestMatchers(""/swagger/**"").permitAll()
                .requestMatchers(""/swagger-ui/**"").permitAll()
                .requestMatchers(""/v3/api-docs/**"").permitAll()
                .requestMatchers(""/auth/test/**"").permitAll()
                .requestMatchers(""/h2/**"").permitAll()
                .anyRequest().authenticated();
        
        http.authenticationProvider(authenticationProvider());
        http.addFilterBefore(authenticationJwtTokenFilter(), UsernamePasswordAuthenticationFilter.class);

        return http.build();
    }
}

AythTokenFilter:
@Slf4j
public class AuthTokenFilter extends OncePerRequestFilter {
    @Autowired
    private JwtUtils jwtUtils;

    @Autowired
    private UserDetailsServiceImpl userDetailsService;


    @Override
    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain)
            throws ServletException, IOException {
        try {
            String accessToken = parseJwt(request);
            if (accessToken != null && jwtUtils.validateJwtToken(accessToken)) {
                String username = jwtUtils.getUserNameFromJwtToken(accessToken);

                UserDetails userDetails = userDetailsService.loadUserByUsername(username);
                UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(userDetails, null,
                        userDetails.getAuthorities());
                authentication.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));

                SecurityContextHolder.getContext().setAuthentication(authentication);
            }
        } catch (Exception e) {
            log.error(""Cannot set user authentication: {}"", e.getMessage());
        }

        filterChain.doFilter(request, response);
    }

    private String parseJwt(HttpServletRequest request) {
        String headerAuth = request.getHeader(""Authorization"");

        if (StringUtils.hasText(headerAuth) && headerAuth.startsWith(""Bearer "")) {
            return headerAuth.substring(7, headerAuth.length());
        }

        return null;
    }
}

JWTUtils:
@Component
@Slf4j
public class JwtUtils {

    @Value(""${jwt.token.secret}"")
    private String jwtSecret;

    @Value(""${jwt.token.jwtExpirationMs}"")
    private int jwtExpirationMs;

    public String generateJwtToken(UserDetailsImpl userPrincipal) {
        return generateTokenFromUsername(userPrincipal.getUsername());
    }

    public String generateTokenFromUsername(String username) {
        return Jwts.builder().setSubject(username).setIssuedAt(new Date())
                .setExpiration(new Date((new Date()).getTime() + jwtExpirationMs)).signWith(SignatureAlgorithm.HS512, jwtSecret)
                .compact();
    }

    public String getUserNameFromJwtToken(String token) {
        return Jwts.parser().setSigningKey(jwtSecret).parseClaimsJws(token).getBody().getSubject();
    }

    public boolean validateJwtToken(String authToken) {
        try {
            Jwts.parser().setSigningKey(jwtSecret).parseClaimsJws(authToken);
            return true;
        } catch (SignatureException e) {
            log.error(""Invalid JWT signature: {}"", e.getMessage());
        } catch (MalformedJwtException e) {
            log.error(""Invalid JWT token: {}"", e.getMessage());
        } catch (ExpiredJwtException e) {
            log.error(""JWT token is expired: {}"", e.getMessage());
        } catch (UnsupportedJwtException e) {
            log.error(""JWT token is unsupported: {}"", e.getMessage());
        } catch (IllegalArgumentException e) {
            log.error(""JWT claims string is empty: {}"", e.getMessage());
        }

        return false;
    }

}

Controller method:
@GetMapping(""/requests"")
    @PreAuthorize(""hasRole('ROLE_USER')"")
    public UsernamesResponse getFriendRequests() {
        return userService.getFriendRequests();
    }

Stacktrace:
org.springframework.security.access.AccessDeniedException: Access Denied
    at org.springframework.security.authorization.method.AuthorizationManagerBeforeMethodInterceptor.attemptAuthorization(AuthorizationManagerBeforeMethodInterceptor.java:257) ~[spring-security-core-6.0.0.jar:6.0.0]
    at org.springframework.security.authorization.method.AuthorizationManagerBeforeMethodInterceptor.invoke(AuthorizationManagerBeforeMethodInterceptor.java:198) ~[spring-security-core-6.0.0.jar:6.0.0]
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184) ~[spring-aop-6.0.2.jar:6.0.2]
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:752) ~[spring-aop-6.0.2.jar:6.0.2]
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:703) ~[spring-aop-6.0.2.jar:6.0.2]
    at ru.effectivemobile.socialnetwork.controller.UserController$$SpringCGLIB$$0.getFriendRequests(<generated>) ~[classes/:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:207) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:152) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:884) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1080) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:973) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1003) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:895) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:705) ~[tomcat-embed-core-10.1.1.jar:6.0]
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:880) ~[spring-webmvc-6.0.2.jar:6.0.2]
    at jakarta.servlet.http.HttpServlet.service(HttpServlet.java:814) ~[tomcat-embed-core-10.1.1.jar:6.0]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:223) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-embed-websocket-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:185) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:110) ~[spring-web-6.0.2.jar:6.0.2]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:185) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.springframework.security.web.FilterChainProxy.lambda$doFilterInternal$3(FilterChainProxy.java:231) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:365) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.access.intercept.AuthorizationFilter.doFilter(AuthorizationFilter.java:100) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:126) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:120) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:131) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:85) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:100) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:179) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at ru.effectivemobile.socialnetwork.security.jwt.AuthTokenFilter.doFilterInternal(AuthTokenFilter.java:47) ~[classes/:na]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:107) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:93) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:91) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.header.HeaderWriterFilter.doHeadersAfter(HeaderWriterFilter.java:90) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:75) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.context.SecurityContextHolderFilter.doFilterInternal(SecurityContextHolderFilter.java:69) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:62) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.session.DisableEncodeUrlFilter.doFilterInternal(DisableEncodeUrlFilter.java:42) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:374) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:233) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:191) ~[spring-security-web-6.0.0.jar:6.0.0]
    at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:351) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267) ~[spring-web-6.0.2.jar:6.0.2]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:185) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:185) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:185) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) ~[spring-web-6.0.2.jar:6.0.2]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116) ~[spring-web-6.0.2.jar:6.0.2]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:185) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:158) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:197) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:542) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:119) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:357) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:400) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:861) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1739) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-10.1.1.jar:10.1.1]
    at java.base/java.lang.Thread.run(Thread.java:833) ~[na:na]

EDIT 1
Jwt token example:
eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJ0ZXN0IiwiaWF0IjoxNjg1NjIyMjc4LCJleHAiOjE2ODU2MjU4Nzh9.A_gw2V1c403vxANRcO5LCU7621TMYvYeBKKb-YJv_ZTjEPKym140YAlIjnqAOhoQtfhKRm2O-pJke-7zzglzSg

EDIT 2




",76384140,"Your problem relates to the difference in Spring between ROLES and Authorities and the Spring hack that treats Roles as Authorities prefixed with ""ROLE_""
In order to use the annotation @PreAuthorize(""hasRole('ROLE_USER')"") you need a Permission as follows:
ROLE_USER(""ROLE_USER"")

Which is related to your ERole enum as follows:
ROLE_USER(Set.of(Permission.ROLE_USER, Permission.READ))
If you are only using the Permission.READ, etc for Controller level authority checking then drop them.
Alternatively use annotation like this @PreAuthorize(""hasAuthotity('READ')"")and drop the roles.
I have made changes (on branch fix/authorities at https://github.com/KoosieDeMoer/social-network.git) with stubbed out repo that work.
",
76381531,Get product version from a custom action,"I have a Wix installer.
In the Product.wxs file, I have the below piece of code:
<Product Id=""*""
    Name=""$(var.PRODUCT_NAME)""
    Language=""1033""
    Version=""!(bind.FileVersion.myDLLfile)""
    Manufacturer=""$(var.CompanyName)""
    UpgradeCode=""{D00BA432-7798-588A-34DF-34A65378FD45}"">

In the Features.wxs, I have the myDLLfile defined as component:
<Component Id=""myDLLfile""
    Guid=""{30550881-053F-768D-88B7-BB9853B23C51}"">
    <File Id=""myDLLfile""
        Source=""$(var.dllDir)\myDLLfile.dll""
        KeyPath=""yes""
        Checksum=""yes""/>
</Component>

Now, I would like to know if from a custom action in C# I can get that same Product Version (which corresponds to the version of the myDllfile.dll). Is it possible? If so, how?
",76384167,"The easiest way is probably to store the value in a Property and read that property in the custom action. The bind variable syntax would make setting the property easy.
   <Property Id=""ThisIsASillyThingToNeedToDo"" Value=""!(bind.FileVersion.myDLLfile)"" />

",
76384063,Why does Mysql change query execution plan based on query parameters?,"I have the following query.
SELECT *
FROM user u
LEFT JOIN operator o ON o.id = u.id
WHERE u.user_type_id IN (2,4) AND u.is_enabled = 1 AND u.office_id = 225

If I run explain on the query above, it shows that it uses the index IX_user_type for the table user.
If I just change the office_id comparison value like the following, the execution plan changes.
SELECT *
FROM user u
LEFT JOIN operator o ON o.id = u.id
WHERE u.user_type_id IN (2,4) AND u.is_enabled = 1 AND u.office_id = 32365487

In this case, the explain shows that the indexes used for the table user are fk_user_office,IX_user_is_enabled.
I made some tests and would say, performance wise, the first execution plan is much better than the second one. Now, I know I can force Mysql to use the index I want but, I would like to understand why this happens. Why would Mysql pick an index instead of another based on a query parameter?
",76385989,"MySQL may decide not to use the index on office_id if the value you are searching for is too common.
By analogy, why doesn't a book include common words like ""the"" in the index at the back of the book? Because such common words occur on a majority of pages in the book. It's unnecessary to keep a list of those pages under the respective word in the index, because it's easier to tell the reader to read all the pages in the book, without the index lookup.
Similarly, if MySQL estimates that a given value you are searching for occurs on a high enough portion of the pages, it looks for another index if you have other conditions, and if none are found, then it resorts to a table-scan.
In this case, I'd ask if you can confirm that office_id 225 is very common in this table.

One more thought: The best index of all for the query you show would be a compound index on (office_id, is_enabled, user_type). Then it would be able to use that index to narrow down the search by all three columns at once.
You might like my presentation How to Design Indexes, Really or the video. I also have a chapter on index design in my book SQL Antipatterns, Volume 1:
Avoiding the Pitfalls of Database Programming.
",
76382547,Save the output of bigquery in JSON from python,"How can I modify this script to be able to see/print some of the results and write the output in JSON :
from google.cloud import bigquery


def query_stackoverflow(project_id=""gwas-386212""):
    client = bigquery.Client()
query_job = client.query(
        """"""
        WITH
  SNP_info AS (
  SELECT
    CONCAT(CAST(rs_id AS string)) AS identifier
  FROM
  `gwas-386212.gwas_dataset_1.SNPs_intergenic_vep_pha005199`)
SELECT
  *
FROM
  SNP_info
JOIN (
  SELECT
    CONCAT(CAST(rs_id AS string)) AS identifier,
chr_id AS chr_id,
position AS position, 
ref_allele AS ref,
alt_allele AS alt,
most_severe_consequence AS most_severe_consequence, 
gene_id_any_distance AS gene_id_any_distance,
gene_id_any AS gene_id_any,
gene_id_prot_coding_distance AS gene_id_prot_coding_distance, 
gene_id_prot_coding AS gene_id_prot_coding
 FROM
    `bigquery-public-data.open_targets_genetics.variants`) variants
ON
  SNP_info.identifier = variants.identifier"""""" 
  )
results = client.query(query)

for row in results:
    title = row['identifier']
    identifier = row['identifier']
    #print(f'{identifier}')

This is just printing a column the intentifier. i want to save the resulted table in JSON format. The JSOn from the google cloud platform should look something like this:
[{
  ""identifier"": ""rs62063022"",
  ""identifier_1"": ""rs62063022"",
  ""chr_id"": ""17"",
  ""position"": ""51134537"",
  ""ref"": ""T"",
  ""alt"": ""G"",
  ""most_severe_consequence"": ""intergenic_variant"",
  ""gene_id_any_distance"": ""13669"",
  ""gene_id_any"": ""ENSG00000008294"",
  ""gene_id_prot_coding_distance"": ""13669"",
  ""gene_id_prot_coding"": ""ENSG00000008294""
}, {
  ""identifier"": ""rs12944420"",
  ""identifier_1"": ""rs12944420"",
  ""chr_id"": ""17"",
  ""position"": ""42640692"",
  ""ref"": ""T"",
  ""alt"": ""C"",
  ""most_severe_consequence"": ""intergenic_variant"",
  ""gene_id_any_distance"": ""18592"",
  ""gene_id_any"": ""ENSG00000037042"",
  ""gene_id_prot_coding_distance"": ""18592"",
  ""gene_id_prot_coding"": ""ENSG00000037042""
},

",76384317,"Check out json documentation for further information.
records = [dict(row) for row in results]
out_file = open(""bigquery_response.json"", ""w"")
json.dump(records , out_file, indent = 6)
out_file.close()

",
76381598,How to rewrite sync method using SwiftUI?,"Now I'm using UIImage sync extension.
struct PostView: View {
    let url: String
    
    var body: some View {        
        PrivateImageView(image: UIImage(url:url))
     }
}

extension UIImage {
    public convenience init(url: String) {
        let url = URL(string: url)
        do {
            let data = try Data(contentsOf: url!)
            self.init(data: data)!
            return
        } catch let err {
            print(""Error : \(err.localizedDescription)"")
        }
        self.init()
    }
}

When I post a image, I got the error Synchronous URL loading of http://local.host/images/123.jpeg should not occur on this application's main thread as it may lead to UI unresponsiveness. Please switch to an asynchronous networking API such as URLSession. at try Data(contentsOf: url!).
In PostView I use PrivateImageView.
To use the view I have to designate the argument like this PrivateImageView(image: UIImage(xxxxxxxxx)).
I mean I have to use UIImage() not AsyncImage.
I don't know how to change convenience init to adjust to PrivateImageView.
Please tell me how to use an async function in this context.
",76384253,"There is no way to get a data from the inter/intranet synchronously you have to use an async method and account for the time it takes to download.
extension String {
    public func getUIImage() async throws -> UIImage {
        guard let url = URL(string: self) else {
            throw URLError(.badURL)
        }
        
        let (data, response) = try await URLSession.shared.data(from: url)
        
        guard let httpResponse = response as? HTTPURLResponse else {
            throw URLError(.badServerResponse)
        }
        
        guard httpResponse.statusCode == 200 else {
            throw URLError(URLError.Code(rawValue: httpResponse.statusCode))
        }
        
        guard let image = UIImage(data: data) else {
            throw URLError(.fileDoesNotExist)
        }
        return image
    }
}

extension UIImage {
    static public func fromURL(url: String) async throws -> UIImage {
        let image = try await url.getUIImage()
        return image
    }
}

You can rewrite PostView to something like.
struct PostView: View {
    let url: String
    @State private var uiImage: UIImage?
    var body: some View {
        Group{
            if let uiImage {
                PrivateImageView(image: uiImage)
            } else {
                ProgressView() //Show this while downloading
                    .task {
                        do {
                            self.uiImage = try await url.getUIImage()
                            // or
                            // self.uiImage = try await UIImage.fromURL(url: url)
                        } catch {
                            print(error)
                        }
                    }
            }
        }
    }
}

",
76385236,Self signed certificate not working on localhost IIS,"I have am setting up a very simple test site on my localhost under IIS which needs to be accessible locally from https.  IU have followed the steps below:

In IIS for my local server, I have created a self signed certificate and stored in the ""Personal"" store
I have added a https binding for my test site to this new certificate, the hostname is testite and the port 7001
Added the entry '127.0.0.1   testsite' to hosts file.

When I try to access https://testsite/index.html through the browser, the browser returns the following error:
NET::ERR_CERT_COMMON_NAME_INVALID
Same problem soccurs if I add the port number to the url, i.e: https://testsite:7001/index.html
More informatiom on the error shows the following:
This server could not prove that it is testsite; its security certificate is from Muzz2. This may be caused by a misconfiguration or an attacker intercepting your connection.

",76386062,"The solution involved using Powershell rather than IIS manager to generate the self signed certificate.  ISS always used the machine name rather than the sitename as the common name.
The powershell command I used was as follows:
New-SelfSignedCertificate -DnsName testsite -CertStoreLocation cert:\LocalMachine\My
After which mmc was used to place the certificate in the Trusted Root Certification Authorities, and the certificated bindings updated in IIS as you you usually would.
",
76378736,How do I implement SSL in Delphi to connect to a REDCap API server?,"I am trying to use XE7 to connect to an in-house REDCap server. REDCap has a detailed description of the API at https://education.arcus.chop.edu/redcap-api/ and a test server at https://bbmc.ouhsc.edu/redcap/api  with a test token key.  There is assistance at https://mran.microsoft.com/snapshot/2015-08-18/web/packages/REDCapR/vignettes/TroubleshootingApiCalls.html in R.
I can connect to the test site with Curl and PostMan. My problem is how to implement this in Delphi with SSL.
The Curl script from PostMan:
curl --location 'https://bbmc.ouhsc.edu/redcap/api/' \
--data-urlencode 'token=9A81268476645C4E5F03428B8AC3AA7B' \
--data-urlencode 'content=record' \
--data-urlencode 'action=export' \
--data-urlencode 'format=csv' \
--data-urlencode 'rawOrLabel=label'

After much searching, this is my Delphi code. What have I missed? IdLogFile1 is a component on the form.
function TForm1.IdSSLIOHandlerSocketOpenSSL1VerifyPeer(Certificate: TIdX509; AOk: Boolean; ADepth, AError: Integer): Boolean;
begin
   showmessage('at  IOhandler');                      
     Result := true;                             // always returns true
end;


procedure TForm1.idHTTP2BtnClick(Sender: TObject);
var post      : string;
    Params    : TStringList;
    idHTTP    : TIdHTTP;
    SSL1      : TIdSSLIOHandlerSocketOpenSSL;
    status    : integer;
    response : TstringStream;
begin
   params   := TStringList.Create;
   idHTTP   := TIdHTTP.Create(nil);
   SSL1     := TIdSSLIOHandlerSocketOpenSSL.Create(idHTTP);
   response  := TstringStream.create;


   SSL1.SSLOptions.Mode        := sslmClient ;
   SSL1.SSLOptions.SSLVersions := [sslvTLSv1, sslvTLSv1_1, sslvTLSv1_2 ];// [  sslvSSLv3,  sslvSSLv23,sslvSSLv2, sslvTLSv1, sslvTLSv1_1, sslvTLSv1_2];
   SSL1.SSLOptions.VerifyDepth := 0;
   SSL1.OnVerifyPeer           := IdSSLIOHandlerSocketOpenSSL1VerifyPeer;
   SSL1.SSLOptions.VerifyMode  := [ ];
   idHTTP.IOHandler            := SSL1;

   memo1.Lines.clear;

   idHTTP.ReadTimeout                 := 3000;
   idHTTP.ConnectTimeout              := 3000;
   idHttp.Request.BasicAuthentication := false;

   try

     idHTTP.HandleRedirects := true;
     idHTTP.Intercept       := IdLogFile1;
     IdLogFile1.Active      := true;

     IdHttp.Request.CustomHeaders.Clear;

 
     IdHttp.Request.CustomHeaders.Values['token']          := '9A81268476645C4E5F03428B8AC3AA7B';
     IdHttp.Request.CustomHeaders.Values['content']        := 'record';
     IdHttp.Request.CustomHeaders.Values['action']         := 'export';
     IdHttp.Request.CustomHeaders.Values['format']         := 'csv';
     IdHttp.Request.CustomHeaders.Values['rawOrLabel']     := 'label';
     IdHttp.Request.CustomHeaders.Values['verify_ssl']     := 'false';
     IdHttp.Request.CustomHeaders.Values['ssl_verify']     := 'false'; //various verify options ?
     IdHttp.Request.CustomHeaders.Values['ssl_verifypeer'] := 'false';

 
     idHTTP.Request.ContentType := 'application/x-www-form-urlencoded';
     IdHTTP.Request.Charset     := 'utf-8';
     idHTTP.HTTPOptions         := [hoKeepOrigProtocol, hoForceEncodeParams];

     idHTTP.Post('https://bbmc.ouhsc.edu/redcap/api/', params, response );


   finally
        memo1.Lines.add(' ');
        memo1.lines.add(idHTTP.ResponseText);
        memo1.Lines.add(' ');
        status           := idHTTP.ResponseCode;
        memo1.Lines.Add('code: ' + inttostr(status));
 
        idhttp.Disconnect;
 
   end;
   Params.Free;
   SSL1.Free;
   idHTTP.Free;
   response.Free;
end;

",76384351,"You are setting up the TLS connection correctly (provided the appropriate OpenSSL DLLs are available where Indy can find them).
What you are not setting up correctly is your data parameters. Curl's --data-urlencode command puts the data in the HTTP request body, not in the HTTP headers. So you need to put the data in the TStringList that you are posting (TIdHTTP will handle the url-encoding for you).
Try this instead:
procedure TForm1.idHTTP2BtnClick(Sender: TObject);
var
  params    : TStringList;
  idHTTP    : TIdHTTP;
  idSSL     : TIdSSLIOHandlerSocketOpenSSL;
  status    : integer;
  response  : string;
begin
  params := TStringList.Create;
  try
    idHTTP := TIdHTTP.Create(nil);
    try
      idSSL := TIdSSLIOHandlerSocketOpenSSL.Create(idHTTP);    

      idSSL.SSLOptions.Mode        := sslmClient ;
      idSSL.SSLOptions.SSLVersions := [sslvTLSv1, sslvTLSv1_1, sslvTLSv1_2 ];
      idSSL.SSLOptions.VerifyDepth := 0;
      idSSL.OnVerifyPeer           := IdSSLIOHandlerSocketOpenSSL1VerifyPeer;
      idSSL.SSLOptions.VerifyMode  := [ ];
      idHTTP.IOHandler := idSSL;

      Memo1.Lines.Clear;

      idHTTP.ReadTimeout                 := 3000;
      idHTTP.ConnectTimeout              := 3000;
      idHTTP.Request.BasicAuthentication := false;

      try    
        idHTTP.HandleRedirects := true;
        idHTTP.Intercept       := IdLogFile1;
        IdLogFile1.Active      := true;

        params.Add('token=9A81268476645C4E5F03428B8AC3AA7B');
        params.Add('content=record');
        params.Add('action=export');
        params.Add('format=csv');
        params.Add('rawOrLabel=label');

        idHTTP.Request.ContentType := 'application/x-www-form-urlencoded';
        idHTTP.Request.Charset     := 'utf-8';
        idHTTP.HTTPOptions         := [hoKeepOrigProtocol, hoForceEncodeParams];

        response := idHTTP.Post('https://bbmc.ouhsc.edu/redcap/api/', params);    
      finally
        Memo1.Lines.Add(' ');
        Memo1.Lines.Add(idHTTP.ResponseText);
        Memo1.Lines.Add(' ');
        status := idHTTP.ResponseCode;
        Memo1.Lines.Add('code: ' + IntToStr(status));
      end;
    finally
      idHTTP.Free;
    end;
  finally
    params.Free;
  end;
end;

",
76381369,Stored procedure with EF6 Code first - best practice?,"In my program I have to import data from a remote sqlserver database.
I am using ASP.NET MVC 5 and EF6 Code First (I'm new with EF and MVC 5).
First, I copy data from a remote view to a table in the local database.
For this part I use this code in the action method (names are in italian):
using (var source = new TimeWebDBContext())
{
 using (var target = new GESTPrefContext())
  {
   // 1 - Truncate table AnagraficaTimeWeb is exists
   target.Database.ExecuteSqlCommand(""IF EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME='AnagraficaTimeWeb') TRUNCATE TABLE AnagraficaTimeWeb"");
    
   // 2 - Copy from remote view  to  AnagraficaTimeWeb table
   var dati_importati = from i in source.VW_PREFNO_ANAGRAFICORUOLO
        select new AnagraficaTimeWeb()
        {
         Matricola = i.MATRICOLA,
         Cognome = i.COGNOME,
         Nome = i.NOME,
         Sesso = i.SESSO,
         Email = i.EMAIL,
         IdRuolo = i.IDRUOLO,
         Ruolo = i.RUOLO,
         DataInizio = i.DATAINIZIO,
         DataFine = i.DATAFINE,
         DataFineRapporto = i.DATALICENZ,
         DataUltimaImportazione = DateTime.Now
        };
        target.DatiAnagraficaTimeWeb.AddRange(dati_importati.ToList());
        target.SaveChanges();
   }
}

The view returns a list of employees with their role.
Roles have to be imported in a distinct local table called PROFILO, while employees data are saved in the IMPIEGATO table.
The remaining part of the import process consists of :
a) insert new data in the PROFILO table (data already saved are ignored)
b) update of employee data already present in the local IMPIEGATO table (name, email,etc. are overwritten)
c) insert new empoyee not yet in the IMPIEGATO table.
since I’m new to EF6 I thought I’d use SQL code .
In my opinion the possible solutions are :

execute SQL code directly in the actionmethod with db.Database.ExecuteSqlCommand
This is the code i write:

code for point a)
StringBuilder sql = new StringBuilder();
sql.AppendLine(""INSERT INTO PROFILO (IdTimeWeb, Descrizione, Ordinamento, Stato, Datainserimento)"");
sql.AppendLine("" SELECT DISTINCT IdRuolo, Ruolo, 1,"" + ((int)EnumStato.Abilitato) + "",'"" + DateTime.Now.ToShortDateString()+""'"");
sql.AppendLine("" FROM AnagraficaTimeWeb i"");
sql.AppendLine("" WHERE NOT EXISTS"");
sql.AppendLine(""(SELECT 1 FROM PROFILO p WHERE p.Descrizione = i.Ruolo)"");
target.Database.ExecuteSqlCommand(sql.ToString());

code for point b)
sql.Clear();
sql.Append(""UPDATE i "" + Environment.NewLine);
sql.Append("" SET i.Cognome = a.Cognome"" + Environment.NewLine);
sql.Append("" , i.Nome = a.Nome"" + Environment.NewLine);
sql.Append("" , i.Sesso = a.Sesso"" + Environment.NewLine);
sql.Append("" ,i.Email = a.Email"" + Environment.NewLine);
sql.Append("" ,i.DataModifica = '"" + DateTime.Now.ToShortDateString() +""'""+ Environment.NewLine);
sql.Append("" FROM Impiegato i "" + Environment.NewLine);
sql.Append("" JOIN AnagraficaTimeWeb a on i.Matricola=a.Matricola "" + Environment.NewLine);
sql.Append("" WHERE i.Stato ="" + ((int)EnumStato.Abilitato)  + Environment.NewLine);
target.Database.ExecuteSqlCommand(sql.ToString());
    

code for point c)
sql.Clear();
sql.Append(""INSERT INTO IMPIEGATO(Cognome, Nome, Matricola, Sesso, Email, Stato, DataInserimento) "" + Environment.NewLine);
sql.Append(""SELECT a.Cognome"" + Environment.NewLine);
sql.Append("", a.Nome"" + Environment.NewLine);
sql.Append("", a.Matricola"" + Environment.NewLine);
sql.Append("", a.Sesso"" + Environment.NewLine);
sql.Append("", a.Email"" + Environment.NewLine);
sql.Append("","" + ((int)EnumStato.Abilitato )+ Environment.NewLine);
sql.Append("",'""+ DateTime.Now.ToShortDateString() +""'"" + Environment.NewLine);
sql.Append("" FROM AnagraficaTimeWeb a "" + Environment.NewLine);
sql.Append("" LEFT OUTER JOIN IMPIEGATO on a.Matricola = Impiegato.Matricola "" + Environment.NewLine);
sql.Append(""  WHERE Impiegato.Matricola is null"" + Environment.NewLine);

target.Database.ExecuteSqlCommand(sql.ToString());      
                    
                


create a storedprocedure to call in the actionmetghod. in this case how to create the storedprocedure?

2.a) in the Up method of a migration ?
2.b) or running the storedprocedure creation script directly in the database (after it was first created ) and then run the stored from the action method?
",76384486,"You can create stored procedure directly in db and write all three remaining part of your process (point a,b,c) in one SP (stored procedure). SP is stored as object file in sql so it's fast and sql server don't spend time in making execution plan and other extra things.
In order to create sp, you need to more familiar with sql statements and it's control structures etc. It will feel like learning new language. You can start with below link
https://learn.microsoft.com/en-us/sql/relational-databases/stored-procedures/stored-procedures-database-engine?view=sql-server-ver16
To save that stored procedure as migration script you can create a blank migration using
add-migration 'SPName' -IgnoreChanges 
Then add your sp
public partial class SPName: DbMigration
{
    public override void Up()
    {
        Sql(@""Create Stored Procedure script"");

    }

    public override void Down()
    {
        Sql(@""Drop Stored Procedure script"")
    }
} 

don't forget to change migration script every time you modify/alter sp. You can also make migration script of modified sp.
To execute your sp you can refer the below code snippet. SqlPatameter helps you to make query clean,you can avoid sql injection.
List<SqlParameter> sqlParms = new List<SqlParameter>
  {
    new SqlParameter { ParameterName = ""@Id"", Value = employee.EmployeeID },
    new SqlParameter { ParameterName = ""@FirstName "", Value = employee.FirstName },
    new SqlParameter { ParameterName = ""@LastName"", Value = employee.LastName}
};
db.Database.ExecuteSqlRaw(""EXEC dbo.spName @Id, @FirstName, @LastName"" sqlParms.ToArray());

",
76384694,How to do conditional compilation with Zig?,"For example, I can add definitions for C/C++ preprocessor with CMake
add_definitions(-DFOO -DBAR ...)

and then I can use them for conditional compilation
#ifdef FOO
   code ...
#endif

#ifdef BAR
   code ...
#endif


Is there a way to do the same thing with Zig and its build system using compilation arguments or something like that?
",76386084,"You can do something similar using the build system. This requires some boilerplate code to do the option handling. Following the tutorial on https://zig.news/xq/zig-build-explained-part-1-59lf for the build system and https://ziggit.dev/t/custom-build-options/138/8 for the option handling:

Create a separate file called build.zig that contains a function build():

const std = @import(""std"");

pub fn build(b: *std.build.Builder) !void {
    const build_options = b.addOptions();
    // add command line flag
    // and set default value
    build_options.addOption(bool, ""sideways"", b.option(bool, ""sideways"", ""print sideways"") orelse false);

    // set executable name and source code
    const exe = b.addExecutable(""hello"", ""hello.zig"");
    exe.addOptions(""build_options"", build_options);
    // compile and copy to zig-out/bin
    exe.install();
}


Use the option for conditional compilation in a separate file hello.zig using @import(""build_options""):

const std = @import(""std"");

pub fn main() !void {
    const print_sideways = @import(""build_options"").sideways;
    const stdout = std.io.getStdOut().writer();
    if (print_sideways) {
       try stdout.print(""Sideways Hello, {s}!\n"", .{""world""});
    } else {
       try stdout.print(""Regular Hello, {s}!\n"", .{""world""});
    }
}


Compile with:

zig build -Dsideways=true


Executing zig-out/bin/hello gives the following output:

Sideways Hello, world!

",
76382999,Cognito JWT Authorize in ASP.NET Core 6 Web API,"How can I configure my ASP.NET Core 6 Web API controllers to use AWS Cognito authorization?
This is the code I wrote in my program.cs file:
var AWSconfiguration = builder.Configuration.GetSection(""AWS:Cognito"");
var userPoolId = AWSconfiguration[""UserPoolId""];
var clientId = AWSconfiguration[""ClientId""];
var region = AWSconfiguration[""Region""];

builder.Services.AddAuthentication(options =>
{
    options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;
    options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;
})
.AddJwtBearer(options =>
{
    options.Authority = $""https://cognito-idp.{region}.amazonaws.com/{userPoolId}"";
    options.TokenValidationParameters = new TokenValidationParameters
    {
        ValidateIssuerSigningKey = true,
        ValidateIssuer = true,
        ValidateAudience = true,
        ValidIssuer = $""https://cognito-idp.{region}.amazonaws.com/{userPoolId}"",
        ValidAudience = clientId,
        
    };
});

I'm getting this error:

www-authenticate: Bearer error=""invalid_token"",
error_description=""The audience 'empty' is invalid""

I validated my clientID in the AWS console.
Thanks for the help
",76384407,"Cognito access tokens don't have an audience claim - though ideally they should. In other authorization servers, APIs check the received access token has the expected logical name, such as api.mycompany.com.
For Cognito you will need to configure .NET to not validate the audience, similar to this. Other token validation parameters are derived from the metadata endpoint derived from the issuer base URL:
private void ConfigureOAuth(IServiceCollection services)
{
    services
        .AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
        .AddJwtBearer(options =>
        {
            options.Authority = this.configuration.IssuerBaseUrl;
            options.TokenValidationParameters = new TokenValidationParameters
            {
                ValidateAudience = false,
            };
        });

    services.AddAuthorization(options => 
    {
        options.FallbackPolicy = new AuthorizationPolicyBuilder().RequireAuthenticatedUser().Build();
    });
}


The FallbackPolicy then ensures that authentication is applied globally, except for endpoints annotated with [AllowAnonymous].
",
76381827,How to pass arguments in cron job function,"Im totaly new in wordpress php and cron. So, I have a task. I need to take data from a form and put it into a cron function.
This is me code. I created custom plugin page in wordpress admin panel and try to run this code.
actually the code works if I enter a article id instead of a variable $post_id;

function cron_add_one_minute( $schedules ) {
    $schedules['one_minute'] = array(
        'interval' => 60,
        'display'  => 'One in minute'
    );
    return $schedules;
};
    if(!empty($_POST) && ($_POST['btnaup']) && !wp_next_scheduled( 'update_post' )) {
        wp_schedule_event( time(), 'one_minute', 'update_post');
    }

    if(isset( $_POST['btnaup'])) {
        $post_id = $_POST['id'];
        $b = $_POST['days'];
    }

add_action( 'update_post', 'update_my_post', 10, 1);

function update_my_post( $post_id ){
        $time = current_time('mysql');
 
        wp_update_post( array (
            'ID'            => $post_id, 
            'post_date'     => $time,
            'post_date_gmt' => get_gmt_from_date( $time ),
            'post_modified' => $time,
            'post_modified_gmt' => get_gmt_from_date($time),
        )  );
}



",76384517,"Per the docs for wp_schedule_event, the fourth parameter, which you are 't currently using, is $args

Array containing arguments to pass to the hook's callback function. Each value in the array is passed to the callback as an individual parameter.
The array keys are ignored.
Default: array()

This means you should be able to use:
if (!empty($_POST) && ($_POST['btnaup']) && !wp_next_scheduled('update_post')) {
    $data = [
        $_POST['id'],
    ];

    wp_schedule_event(time(), 'one_minute', 'update_post', $data);
}

Your update_my_post function should just work then.
",
76381875,How to start a GUI executable and a console program and terminate the console application on GUI application closed by the user?,"I am executing a script which starts an executable proc1.exe. When proc1.exe is running, the batch file has to start another executable proc2.exe.
Example: File A.bat runs proc1.exe with following requirements.

When proc1.exe is running, proc2.exe should run.
When proc1.exe is closed, proc2.exe should be terminated.

I tried this code:
tasklist /fi ""ImageName eq proc1.exe"" /fo csv 2>NUL|find /I ""proc1.exe"">NUL

if ""%ERRORLEVEL%""==""0"" 
echo proc1.exe  is running
start /wait proc2.exe

else

taskkill /F /IM proc2.exe 

When I run the script, the command window displays the error message:

The syntax of tasklist command is incorrect.

What is the issue with the above tasklist command line?
I am not sure whether the else part would be achieved. How do I come back to else part in the script and kill proc2.exe after proc1.exe is terminated.
How could this be achieved?

Here are more information after post of first version of Mofi´s answer.
This is what I have tried and I think, I am near to the solution but need your support.
Batch file A.bat is modified to:
start bin\proc1.exe 2>&1
rem proc1.exe is running. Proc1.exe is a GUI application with buttons and
rem widgets. The cmd window and the GUI application window are opened now.

start /wait proc2.exe 2>&1
rem proc2.exe is running now too. It is a Windows console application.
rem A second console window is opened for that reason.
rem Both proc1.exe and proc2.exe are running as processes.
    
rem Now I want to check whether Proc1.exe is running or not.
rem If it is running do nothing, else terminate proc2.exe.
rem I have written a loop for that purpose. The issue is that
rem the control is not coming to the loop.

rem When I close the GUI application, the ELSE part should
rem be executed and proc2.exe should be terminated/killed.

rem How do I bring the control to loop label or how 
rem to signal proc1.exe is closed and run taskkill?

:loop
tasklist /FI ""IMAGENAME eq proc1.exe"" 2>NUL | find /I /N ""proc1.exe"">NUL
if ""%ERRORLEVEL%""==""0"" (
    timeout /T 1 /NOBREAK >NUL
    goto loop
) else (
    taskkill /F /IM proc2.exe  >NUL
)

",76384775,"The task is very unclear.

Is proc1.exe started outside of the batch file or also by the batch file?
Is proc1.exe a Windows console or a Windows GUI application and does it open files for read/write operations or makes it registry reads/writes or does it open connections to other processes or even other devices?
Is proc2.exe a Windows console or a Windows GUI application and does it open files for read/write operations or makes it registry reads/writes or does it open connections to other processes or even other devices?
Is it possible to start proc2.exe a second before proc1.exe or does proc2.exe depend on an already running proc1.exe for a successful start?
Can be more instances of proc1.exe and/or proc2.exe already running before the batch file starts just proc2.exe or both applications?
Is it possible that even more instances of proc1.exe or proc2.exe are started by a user or any other process while the batch file observes the one or two processes started during execution of the batch file.
Is it really okay forcing a brutal kill of all running instances of proc2.exe by the operating system by using TASKKILL with the options /F /IM proc2.exe giving none of the running proc2 processes the chance to gracefully terminate with closing connections, finishing all read/write operations, saving unsaved data and closing files?

Let me assume the answers on these seven questions are as follows:

The batch file always starts proc1.exe.
It is unknown what proc1.exe is and what it does.
It is unknown what proc2.exe is and what it does.
Yes, proc2.exe starts also successful on proc1.exe not already running.
Yes, that is indeed possible.
Yes, that is possible, too.
No, that is not okay. proc2.exe should close itself and should not be killed by the OS.

In this case can be used the following commented batch file:
@echo off
setlocal EnableExtensions DisableDelayedExpansion
rem Define the two programs to run which can be even twice the same program.
set ""ProgStart_FullName=%SystemRoot%\Notepad.exe""
set ""ProgWait_FullName=%SystemRoot%\Notepad.exe""

rem Get just the file name of the first program with file extension.
for %%I in (""%ProgStart_FullName%"") do set ""ProgStart_FileName=%%~nxI""

rem Delete all environment variables of which name starts with
rem #PID_ in the local environment of this batch file context.
for /F ""delims=="" %%I in ('set #PID_ 2^>nul') do ""set %%I=""

rem Get all process identifiers of all already running processes
rem of the executable which is started next by the the batch file.
for /F ""tokens=2"" %%I in ('%SystemRoot%\System32\tasklist.exe /FI ""IMAGENAME eq %ProgStart_FileName%"" /NH') do set ""#PID_%%I=1""

rem Start the program which should run as separate process.
start """" ""%ProgStart_FullName%""

rem Find out the process identifier of just started program. There are
rem hopefully not started two instances of this program at the same time.
for /F ""tokens=2"" %%I in ('%SystemRoot%\System32\tasklist.exe /FI ""IMAGENAME eq  %ProgStart_FileName%"" /NH') do if not defined #PID_%%I set ""#PID_OBSERVE=%%I"" & goto StartNext
echo ERROR: Failed to start ""%ProgStart_FullName%""!& echo(& pause & goto EndBatch

:StartNext
rem Start the next program and wait for its self-termination.
""%ProgWait_FullName%""

rem Check if the first started program is still running and send it in
rem this case the WM_CLOSE message for a graceful self-termination giving
rem the process the chance to close all connections, finish all file and
rem registry accesses with saving all unsaved data and close all files.
%SystemRoot%\System32\tasklist.exe /FI ""PID eq %#PID_OBSERVE%"" | %SystemRoot%\System32\find.exe /I ""%ProgStart_FileName%"" >nul || goto EndBatch
%SystemRoot%\System32\taskkill.exe /PID %#PID_OBSERVE% >nul 2>nul

rem Wait five seconds for the self-termination of the first started program.
rem A Windows GUI program should get even more time, especially if a user uses
rem that GUI program and the program asks the user if unsaved data should be
rem saved before exiting. How much time to wait depends on the application.
echo Wait for exit of ""%ProgStart_FileName%"" with PID %#PID_OBSERVE%"" ...
set ""LoopCount=5""
:WaitLoop
%SystemRoot%\System32\timeout.exe /T 1 /NOBREAK >nul
%SystemRoot%\System32\tasklist.exe /FI ""PID eq %#PID_OBSERVE%"" | %SystemRoot%\System32\find.exe /I ""%ProgStart_FileName%"" >nul || goto EndBatch
set /A LoopCount-=5
if not %LoopCount% == 0 goto WaitLoop

rem Force a brutal kill of the first started program by the operating system.
%SystemRoot%\System32\taskkill.exe /F /PID %#PID_OBSERVE% >nul 2>nul

:EndBatch
endlocal

This batch file demonstrates the process management by starting two instances of Windows Notepad. A user can start other Notepad instances before running the batch file and can also start even more Notepad processes while the two instances of Notepad started by the Windows Command Processor during processing of the batch file are still running and wait for user actions. The user can close the first batch started instance and later the second batch started instance of Notepad, but the opposite is also possible. If the user entered text into new file of first batch started instance without saving that text and closes first the second batch started instance, the first batch started instance of Notepad prompts the user if the unsaved text should be saved now. The user has five seconds time for the choice as otherwise the batch file runs TASKKILL with option /F to force a kill of the first batch started Notepad resulting in a loss of the input text.
The batch file as is cannot be used for executables with a space in file name.
The batch file cannot be used as posted here if ProgStart_FullName is %CompSpec% or %SystemRoot%\System32\cmd.exe.
The environment variable ProgStart_FullName must be defined with the fully qualified file name of proc2.exe while the environment variable ProgWait_FullName must be defined with the fully qualified file name of proc1.exe. So, proc2.exe is started first as separate, parallel running process and next is started proc1.exe on which cmd.exe halts the batch file execution until proc1.exe exits itself. Then the batch file terminates also proc2.exe on still running or finally kills it if proc2.exe does not close itself within five seconds for whatever reason.

The task became more clear with the additional information added to the question.
The seven questions are answered as follows:

The batch file always starts proc1.exe.
The application proc1.exe is a Windows GUI application.
The application proc2.exe is a Windows console application.
proc2.exe must be started after proc1.exe.
There is neither proc1.exe nor proc2.exe started before batch file execution.
There are running never more than one proc1.exe one proc2.exe.
proc2.exe should close itself and should not be killed by the OS.

The commented batch file for this task with proc1.exe and proc2.exe in subdirectory bin of the batch file directory could be:
@echo off
setlocal EnableExtensions DisableDelayedExpansion
set ""FullNameProcess1=%~dp0bin\proc1.exe""
set ""FullNameProcess2=%~dp0bin\proc2.exe""

rem Get just the file name of the two programs with file extension.
for %%I in (""%FullNameProcess1%"") do set ""FileNameProcess1=%%~nxI""
for %%I in (""%FullNameProcess2%"") do set ""FileNameProcess2=%%~nxI""

rem Start the GUI program which should run as separate process in foreground.
start """" ""%FullNameProcess1%"" 2>nul
rem Could the first program not be started at all?
if errorlevel 9059 echo ERROR: Failed to start ""%FullNameProcess1%""!& echo(& pause & goto EndBatch

rem Start the console program which should run as separate process
rem without opening a console window.
start """" /B ""%FullNameProcess2%""

rem Define an endless running loop searching once per second in the
rem task list if the first started GUI application is still running.
:WaitLoop
%SystemRoot%\System32\timeout.exe /T 1 /NOBREAK >nul
%SystemRoot%\System32\tasklist.exe /FI ""IMAGENAME eq %FileNameProcess1%"" /NH | %SystemRoot%\System32\find.exe ""%FileNameProcess1%"" >nul && goto WaitLoop

rem The first started GUI program is not running anymore. Check now if the
rem console program is still running and if that is the case, send it the
rem message to close itself.
%SystemRoot%\System32\tasklist.exe /FI ""IMAGENAME eq %FileNameProcess2%"" /NH | %SystemRoot%\System32\find.exe ""%FileNameProcess2%"" >nul || goto EndBatch
%SystemRoot%\System32\taskkill.exe /IM ""%FileNameProcess2%"" >nul 2>nul

rem Wait one second and check if the console program really terminated itself.
rem Otherwise force a brutal kill of the console program by the operating system.
%SystemRoot%\System32\timeout.exe /T 1 /NOBREAK >nul
%SystemRoot%\System32\tasklist.exe /FI ""IMAGENAME eq %FileNameProcess2%"" /NH | %SystemRoot%\System32\find.exe ""%FileNameProcess2%"" >nul && %SystemRoot%\System32\taskkill.exe /F /IM ""%FileNameProcess2%"" >nul 2>nul

:EndBatch
endlocal

The batch file as is cannot be used for executables with a space in file name.
The batch file cannot be used if proc2.exe is in real cmd.exe because of taskkill.exe /IM ""cmd.exe"" results in termination of all running cmd processes including the one processing the batch file.
To understand the commands used and how they work, open a command prompt window, execute there the following commands, and read the displayed help pages for each command, entirely and carefully.

echo /?
endlocal /?
find /?
for /?
goto /?
if /?
pause /?
rem /?
set /?
setlocal /?
start /?
taskkill /?
tasklist /?
timeout /?

Read the Microsoft documentation about Using command redirection operators for an explanation of >nul and 2>nul and |. The redirection operators > and | must be escaped with caret character ^ on the FOR command lines to be interpreted as literal characters when Windows command interpreter processes these command lines before executing command FOR which executes the embedded command line with using a separate command process started in background with %ComSpec% /c and the command line within ' with ^ appended as additional arguments.
See also single line with multiple commands using Windows batch file for an explanation of unconditional command operator & and conditional command operator || and correct syntax for an IF condition with an ELSE branch. The usage of ""%ERRORLEVEL%""==""0"" is neither described by the usage help of command IF nor is it ever a good idea to use this string comparison for exit code evaluation in a batch file.
",
76383269,the trait `FromSql` is not implemented for `DateTime`,"I'm getting the following error when trying to compile my rust diesel project despite the diesel docs showing that this trait is implemented

the trait bound DateTime<Utc>: FromSql<diesel::sql_types::Timestamptz, Pg> is not satisfied

My schema.rs
pub mod offers {
    diesel::table! {
        offers.offers (id) {
            id -> Int4,
            #[max_length = 255]
            offername -> Varchar,
            #[max_length = 255]
            offertypeid -> Nullable<Varchar>,
            startdate -> Nullable<Timestamptz>,
            enddate -> Nullable<Timestamptz>,
            frequency -> Nullable<Int4>,
            #[max_length = 255]
            createdby -> Nullable<Varchar>,
            createdAt -> Nullable<Timestamptz>
        }
    }
}


Here is my models.rs:
use diesel::prelude::*;
use chrono::{DateTime, Utc};
use crate::schema::offers::offers as offerTable;

#[derive(Queryable, Selectable)]
#[diesel(table_name = offerTable)]
#[diesel(check_for_backend(diesel::pg::Pg))]
pub struct Offer {
    pub id: i32,
    pub enddate: Option<DateTime<Utc>>,
    pub createdAt: Option<DateTime<Utc>>,
    pub createdby: Option<String>,
    pub frequency: Option<i32>,
    pub offername: String,
    pub startdate: Option<DateTime<Utc>> <--- ERROR FOR THIS LINE
}

In the docs for diesel here is shows that this mapping should work and I haven't been able to figure out why it's not working

",76384410,"You need to enable the ""chrono"" feature for the implementation for DateTime<UTC> from the chrono crate to be provided. This is shown as an annotation in the docs and is not enabled by default. You can read more about this feature and others in Diesel's crate feature flags section of the docs.
So your Cargo.toml should contain at least this:
[dependencies]
diesel = { version = ""2.1.0"", features = [""postgres"", ""chrono""] }

This goes for Diesel version 1.x as well.
",
76381239,"Is there a faster algorithm for max(ctz(x), ctz(y))?","For min(ctz(x), ctz(y)), we can use ctz(x | y) to gain better performance. But what about max(ctz(x), ctz(y))?
ctz represents ""count trailing zeros"".
C++ version (Compiler Explorer)
#include <algorithm>
#include <bit>
#include <cstdint>

int32_t test2(uint64_t x, uint64_t y) {
    return std::max(std::countr_zero(x), std::countr_zero(y));
}

Rust version (Compiler Explorer)
pub fn test2(x: u64, y: u64) -> u32 {
    x.trailing_zeros().max(y.trailing_zeros())
}

",76385356,"These are equivalent:

max(ctz(a),ctz(b))
ctz((a|-a)&(b|-b))
ctz(a)+ctz(b)-ctz(a|b)

The math-identity ctz(a)+ctz(b)-ctz(a|b) requires 6 CPU instructions, parallelizable to 3 steps on a 3-way superscalar CPU:

3× ctz
1× bitwise-or
1× addition
1× subtraction

The bit-mashing ctz((a|-a)&(b|-b)) requires 6 CPU instructions, parallelizable to 4 steps on a 2-way superscalar CPU:

2× negation
2× bitwise-or
1× bitwize-and
1× ctz

The naïve max(ctz(a),ctz(b)) requires 5 CPU instructions, parallelizable to 4 steps on a 2-way superscalar CPU:

2× ctz
1× comparison
1× conditional branch
1× load/move (so that the ""output"" is always in the same register)

... but note that branch instructions can be very expensive.
If your CPU has a conditional load/move instruction, this reduces to 4 CPU instructions taking 3 super-scalar steps.
If your CPU has a max instruction (e.g. SSE4), this reduces to 3 CPU instructions taking 2 super-scalar steps.
All that said, the opportunities for super-scalar operation depend on which instructions you're trying to put against each other. Typically you get the most by putting different instructions in parallel, since they use different parts of the CPU (all at once). Typically there will be more ""add"" and ""bitwise or"" units than ""ctz"" units, so doing multiple ctz instructions may actually be the limiting factor, especially for the ""math-identity"" version.
If ""compare and branch"" is too expensive, you can make a non-branching ""max"" in 4 CPU instructions. Assuming A and B are positive integers:

C = A-B
subtract the previous carry, plus D, from D itself (D is now either 0 or -1, regardless of whatever value it previously held)
C &= D (C is now min(0, A-B))
A -= C (A' is now max(A,B))

",
76384813,"Using Office365 Excel array formulas, how to convert this standard formula?","These two formulas are the same, except the first one is not an array formula and the second one is.  How can the first formula be converted to an array formula?  Getting circular logic when using the array formula.
Standard Formula (works fine, no circular logic):
=LET(a, A2, b, B2, c, C2, d, D1, e, E1,
    dd, IF(a = 1, 0, d),
    ee, IF(a = 1, 0, e),
(b * c + dd * ee ) / (b + dd)  )

Array Formula (circular logic error):
=LET(a, A9:A12, b, B9:B12, c, C9:C12, d, D8:D11, e,E8:E11,
    dd, IF(a = 1, 0, d),
    ee, IF(a = 1, 0, e),
(b * c + dd * ee ) / (b + dd)  )

The formula is a fairly simple weighted average of two sets of numbers.  Trying to convert it to an array formula.  The previous result is used on the following row, except on the first row where there is no previous result.
The difficulty is in how to reference the previous result cell when calculating the current cell.

Data:




Seq
B
C
D




1
100
1.00
-


2
100
3.00
800


3
250
2.00
200


4
400
5.00
300



",76386149,"INDEX each array and use SCAN to return the values:
=LET(
    a, A9:A12, 
    b, B9:B12, 
    c, C9:C12, 
    d, D8:D11,
    dd, IF(a = 1, 0, d),
    SCAN(0,a,LAMBDA(z,y,(INDEX(b,y)*INDEX(c,y)+INDEX(dd,y)*z)/(INDEX(b,y)+INDEX(dd,y)))))


",
76383596,Area of each cell covered by polygons,"Is there a way/function to calculate the proportion of each raster cell covered by a polygon? The polygons are usually larger than single cells and the landscape I'm working on is pretty big. I'll like to do it without converting the raster into cell-polygons and st_union/st_join, but I'm not sure if it's possible.
The output I'm looking for is a raster with cell values showing the proportion of each cell covered by the polygons layer.
",76384510,"Thanks for the comments.
At the end the terra::rasterize() function with the cover = T parameter applied on the polygons layer does exactly what I was looking for... and it's super fast.
I was able to keep it all on the ""raster side"" and avoid the more intense processing of vectorizing the raster template and doing intersects/spatial joins.
",
76381200,How to create Custom progress bar minimum value 0 starts from the bottom left corner?,"I want the starting point 0% and the ending point 100% of the progress bar to be in the lower left corner. And the progress of the value changes to display normally. How can I accomplish it?
I want the progress bar to increase and decrease in value along the circle.
The result of my test is not correct, I don't know where is the problem and how to fix it.
Xaml:
<Window.Resources>
    <local:AngleToPointConverter x:Key=""prConverter""/>
    <local:AngleToIsLargeConverter x:Key=""isLargeConverter""/>
  
    <Style x:Key=""circularProgressBar"" TargetType=""local:CircularProgressBar"">
        <Setter Property=""Value"" Value=""10""/>
        <Setter Property=""Maximum"" Value=""100""/>
        <Setter Property=""StrokeThickness"" Value=""10""/>
        <Setter Property=""Template"">
            <Setter.Value>
                <ControlTemplate TargetType=""local:CircularProgressBar"">
                  
                        
                        <Canvas Width=""100"" Height=""130"">
                            <Ellipse Width=""101"" Height=""101"" Stroke=""LightGray"" Opacity=""0.7"" StrokeThickness=""4""  />

                            <Path Stroke=""{TemplateBinding Background}"" 
                              StrokeThickness=""{TemplateBinding StrokeThickness}"">
                                <Path.Data>
                                    <PathGeometry>
                                        <PathFigure x:Name=""fig"" StartPoint=""20,90"">
                                            <ArcSegment RotationAngle=""0"" SweepDirection=""Clockwise""
                                                    Size=""50,50""
                                                    Point=""{Binding Path=Angle, Converter={StaticResource prConverter}, RelativeSource={RelativeSource FindAncestor, AncestorType=ProgressBar}}""
                                                    IsLargeArc=""{Binding Path=Angle, Converter={StaticResource isLargeConverter}, RelativeSource={RelativeSource FindAncestor, AncestorType=ProgressBar}}""
                                                    >
                                            </ArcSegment>
                                        </PathFigure>
                                    </PathGeometry>
                                </Path.Data>
                            </Path>
                            <Border Width=""100"" Height=""100"">
                                <Grid>
                                    <Ellipse Width=""50"" Height=""50""   Fill=""White""  />
                                    <Image Width=""20"" Height=""20"" Margin=""30,25,30,30"" Source=""bulb1.PNG""/>

                                    <TextBlock Width=""50"" Foreground=""Black"" Height=""20""  Margin=""10,40,10,5"" TextAlignment=""Center""
                                   Text=""{Binding Path=Value, StringFormat={}{0}%,  
                            RelativeSource={RelativeSource TemplatedParent}}""
                                       FontSize=""{TemplateBinding FontSize}""/>
                                </Grid>

                            </Border>
                            <Canvas  Canvas.Top=""110"">
                                <Button x:Name=""decrease""  Margin=""20,0,0,0"" Command=""{Binding DecreaseCommand}"" >
                                        <Button.Style>
                                            <Style TargetType=""{x:Type Button}"">
                                                <Setter Property=""Template"">
                                                    <Setter.Value>
                                                        <ControlTemplate>
                                                            <Grid>
                                                                <Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1""  />
                                                                <Border Width=""20"" Height=""20"" >
                                                                    <TextBlock Foreground=""LightGray"" Text=""-"" FontWeight=""Bold"" HorizontalAlignment=""Center"" VerticalAlignment=""Center"" />
                                                                </Border>
                                                            </Grid>
                                                        </ControlTemplate>
                                                    </Setter.Value>
                                                </Setter>
                                            </Style>
                                        </Button.Style>
                                    </Button>
                                    <Button x:Name=""increase""  Margin=""60,0,0,0"" Grid.Column=""1"" Command=""{Binding IncreaseCommand}"" >
                                        <Button.Style>
                                            <Style TargetType=""{x:Type Button}"">
                                                <Setter Property=""Template"">
                                                    <Setter.Value>
                                                        <ControlTemplate>
                                                            <Grid>
                                                                <Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1"" />
                                                                <Border Width=""20"" Height=""20""   Grid.Column=""1"">
                                                                    <TextBlock Foreground=""LightGray"" Text=""+"" FontWeight=""Bold""  VerticalAlignment=""Center"" HorizontalAlignment=""Center"" />

                                                                </Border>
                                                            </Grid>
                                                        </ControlTemplate>
                                                    </Setter.Value>
                                                </Setter>
                                            </Style>
                                        </Button.Style>
                                    </Button>
                                    <!--<Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1""  Margin=""20,0,0,0""/>
                                    <Border Width=""20"" Height=""20"" Margin=""20,0,0,0"">
                                        <TextBlock Foreground=""LightGray"" Text=""-"" HorizontalAlignment=""Center"" />
                                    </Border>
                                    <Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1""  Margin=""60,0,0,0""/>
                                    <Border Width=""20"" Height=""20"" Margin=""60,0,0,0"">
                                        <TextBlock Foreground=""LightGray"" Text=""+"" HorizontalAlignment=""Center"" />

                                    </Border>-->

                            </Canvas>
                        </Canvas>
                  
                </ControlTemplate>
            </Setter.Value>
        </Setter>
    </Style>
</Window.Resources>
<Grid Background=""DarkBlue"">
    <local:CircularProgressBar Background=""White"" Style=""{StaticResource  circularProgressBar }""
                                  Value=""{Binding ElementName=CirularSlider, Path= Value}""  Foreground=""Black"" FontWeight=""Bold""
                                 
                                   StrokeThickness=""4""  
                               
                                   BorderBrush=""LightGray""/>

    <Slider Minimum=""0"" Maximum=""100"" 
                x:Name=""CirularSlider"" IsSnapToTickEnabled=""True""
                VerticalAlignment=""Top"" Value=""10""/>
</Grid>

Codebedhind:
public class CircularProgressBar : ProgressBar
    {
        public CircularProgressBar()
        {
            this.ValueChanged += CircularProgressBar_ValueChanged;
        }

        void CircularProgressBar_ValueChanged(object sender, RoutedPropertyChangedEventArgs<double> e)
        {
            CircularProgressBar bar = sender as CircularProgressBar;
            double currentAngle = bar.Angle;
            double targetAngle = e.NewValue / bar.Maximum * 359.999;
          //  double targetAngle = e.NewValue / bar.Maximum * 179.999;

            DoubleAnimation anim = new DoubleAnimation(currentAngle, targetAngle, TimeSpan.FromMilliseconds(500));
            bar.BeginAnimation(CircularProgressBar.AngleProperty, anim, HandoffBehavior.SnapshotAndReplace);
        }

        public double Angle
        {
            get { return (double)GetValue(AngleProperty); }
            set { SetValue(AngleProperty, value); }
        }

        // Using a DependencyProperty as the backing store for Angle.  This enables animation, styling, binding, etc...
        public static readonly DependencyProperty AngleProperty =
            DependencyProperty.Register(""Angle"", typeof(double), typeof(CircularProgressBar), new PropertyMetadata(0.0));

        public double StrokeThickness
        {
            get { return (double)GetValue(StrokeThicknessProperty); }
            set { SetValue(StrokeThicknessProperty, value); }
        }

        // Using a DependencyProperty as the backing store for StrokeThickness.  This enables animation, styling, binding, etc...
        public static readonly DependencyProperty StrokeThicknessProperty =
            DependencyProperty.Register(""StrokeThickness"", typeof(double), typeof(CircularProgressBar), new PropertyMetadata(10.0));
    }


   
   public class AngleToPointConverter : IValueConverter
    {

        public object Convert(object value, Type targetType, object parameter, System.Globalization.CultureInfo culture)
        {
            double angle = (double)value;
            double radius = 50;
            double piang = angle * Math.PI / 180;
            //double piang = angle * Math.PI / 310;

            double px = Math.Sin(piang) * radius + radius;
            double py = -Math.Cos(piang) * radius + radius;

            return new System.Windows.Point(px, py);
        }

        public object ConvertBack(object value, Type targetTypes, object parameter, System.Globalization.CultureInfo culture)
        {
            throw new NotImplementedException();
        }
    }

    public class AngleToIsLargeConverter : IValueConverter
    {

        public object Convert(object value, Type targetType, object parameter, System.Globalization.CultureInfo culture)
        {
            double angle = (double)value;

           return angle > 180;
           // return angle > 300;
        }

        public object ConvertBack(object value, Type targetTypes, object parameter, System.Globalization.CultureInfo culture)
        {
            throw new NotImplementedException();
        }
    }

The result:

Edit:

Update:
How can I change the progress value by dragging the ball?
<Style x:Key=""circularProgressBar1"" TargetType=""local:CircularProgressBar"">
            <Setter Property=""Value"" Value=""10""/>
            <Setter Property=""Maximum"" Value=""100""/>
            <Setter Property=""StrokeThickness"" Value=""7""/>
            <Setter Property=""Template"">
                <Setter.Value>
                    <ControlTemplate TargetType=""local:CircularProgressBar"">


                        <Canvas Width=""100"" Height=""130"">
                            <Ellipse Width=""105"" Height=""104"" Margin=""-2.4,-1.5,0,0"" Stroke=""LightGray"" Opacity=""0.7""  StrokeThickness=""8""  />


                           
                            <Path Stroke=""{TemplateBinding Background}""  StrokeStartLineCap=""Round"" StrokeEndLineCap=""Round""
                              StrokeThickness=""{TemplateBinding StrokeThickness}"">
                                <Path.Data>
                                    <PathGeometry>
                                        <PathFigure x:Name=""fig"" StartPoint=""20,90"">
                                            <ArcSegment RotationAngle=""0"" SweepDirection=""Clockwise"" 
                                                    Size=""50,50""
                                                    Point=""{Binding Path=Angle, Converter={StaticResource prConverter}, RelativeSource={RelativeSource FindAncestor, AncestorType=ProgressBar}}""
                                                    IsLargeArc=""{Binding Path=Angle, Converter={StaticResource isLargeConverter}, RelativeSource={RelativeSource FindAncestor, AncestorType=ProgressBar}}""
                                                    >
                                            </ArcSegment>
                                        </PathFigure>
                                    </PathGeometry>
                                </Path.Data>
                            </Path>
                            <Button>
                                <Button.Style>
                                    <Style TargetType=""Button"">
                                        <Setter Property=""Template"">
                                            <Setter.Value>
                                                <ControlTemplate>
                                                    <Path Stroke=""Black"" StrokeThickness=""10"" StrokeStartLineCap=""Round"" StrokeEndLineCap=""Round"">
                                                        <Path.Data>
                                                            <PathGeometry>
                                                                <PathGeometry.Figures>
                                                                    <PathFigure StartPoint=""{Binding Path=Angle, Converter={StaticResource prConverter}, RelativeSource={RelativeSource FindAncestor, AncestorType=ProgressBar}}"">
                                                                        <PathFigure.Segments>
                                                                            <LineSegment Point=""{Binding Path=Angle, Converter={StaticResource prConverter}, RelativeSource={RelativeSource FindAncestor, AncestorType=ProgressBar}}"" />
                                                                        </PathFigure.Segments>
                                                                    </PathFigure>
                                                                </PathGeometry.Figures>
                                                            </PathGeometry>
                                                        </Path.Data>
                                                    </Path>
                                                </ControlTemplate>
                                            </Setter.Value>
                                        </Setter>
                                    </Style>
                                </Button.Style>
                            </Button>
                        


                            <Border Width=""100"" Height=""100"">
                                <Grid>
                                    <Ellipse Width=""50"" Height=""50""   Fill=""White""  />
                                    <Image Width=""20"" Height=""20"" Margin=""30,25,30,30"" Source=""bulb1.PNG""/>

                                    <TextBlock Width=""50"" Foreground=""Black"" Height=""20""  Margin=""10,40,10,5"" TextAlignment=""Center""
                                   Text=""{Binding Path=Value, StringFormat={}{0}%,  
                            RelativeSource={RelativeSource TemplatedParent}}""
                                       FontSize=""{TemplateBinding FontSize}""/>
                                </Grid>

                            </Border>
                            <Canvas  Canvas.Top=""110"">
                                <Button x:Name=""decrease""  Margin=""20,0,0,0"" Command=""{Binding DecreaseCommand}"" >
                                    <Button.Style>
                                        <Style TargetType=""{x:Type Button}"">
                                            <Setter Property=""Template"">
                                                <Setter.Value>
                                                    <ControlTemplate>
                                                        <Grid>
                                                            <Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1""  />
                                                            <Border Width=""20"" Height=""20"" >
                                                                <TextBlock Foreground=""LightGray"" Text=""-"" FontWeight=""Bold"" HorizontalAlignment=""Center"" VerticalAlignment=""Center"" />
                                                            </Border>
                                                        </Grid>
                                                    </ControlTemplate>
                                                </Setter.Value>
                                            </Setter>
                                        </Style>
                                    </Button.Style>
                                </Button>
                                <Button x:Name=""increase""  Margin=""60,0,0,0"" Grid.Column=""1"" Command=""{Binding IncreaseCommand}"" >
                                    <Button.Style>
                                        <Style TargetType=""{x:Type Button}"">
                                            <Setter Property=""Template"">
                                                <Setter.Value>
                                                    <ControlTemplate>
                                                        <Grid>
                                                            <Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1"" />
                                                            <Border Width=""20"" Height=""20""   Grid.Column=""1"">
                                                                <TextBlock Foreground=""LightGray"" Text=""+"" FontWeight=""Bold""  VerticalAlignment=""Center"" HorizontalAlignment=""Center"" />

                                                            </Border>
                                                        </Grid>
                                                    </ControlTemplate>
                                                </Setter.Value>
                                            </Setter>
                                        </Style>
                                    </Button.Style>
                                </Button>
                                <!--<Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1""  Margin=""20,0,0,0""/>
                                    <Border Width=""20"" Height=""20"" Margin=""20,0,0,0"">
                                        <TextBlock Foreground=""LightGray"" Text=""-"" HorizontalAlignment=""Center"" />
                                    </Border>
                                    <Ellipse Width=""20"" Height=""20"" Stroke=""LightGray""  StrokeThickness=""1""  Margin=""60,0,0,0""/>
                                    <Border Width=""20"" Height=""20"" Margin=""60,0,0,0"">
                                        <TextBlock Foreground=""LightGray"" Text=""+"" HorizontalAlignment=""Center"" />

                                    </Border>-->

                            </Canvas>
                        </Canvas>

                    </ControlTemplate>
                </Setter.Value>
            </Setter>
        </Style>


",76385707,"change the piang calculation in the convert function so that the starting point at the bottom left is taken into account in the calculation
double piang = (angle - 143.2) * Math.PI / 180;

so the class looks like this
public class AngleToPointConverter : IValueConverter
{

    public object Convert(object value, Type targetType, object parameter, System.Globalization.CultureInfo culture)
    {
        double angle = (double)value;
        double radius = 50;
        double piang = (angle - 143.2) * Math.PI / 180;
        //double piang = angle * Math.PI / 310;

        double px = Math.Sin(piang) * radius + radius;
        double py = -Math.Cos(piang) * radius + radius;

        return new System.Windows.Point(px, py);
    }

    public object ConvertBack(object value, Type targetTypes, object parameter, System.Globalization.CultureInfo culture)
    {
        throw new NotImplementedException();
    }
}

you may have to adjust the angle (143.2).
",
76384224,Why is 'CustomerOrOrganizationInNoUpdateDocRestrictor' inaccessible in my Acumatica project?,"Good morning!  I have a customer ID field included in the table DAC.  I would like for this selector to operate identically to the customer ID field in the sales order form.  I used all pertinent DAC code from the sales order to create the field in my custom screen; however, when I attempt to use all the same attributes for the restrictor, the following attribute cannot be accessed.  I have all the appropriate references included in the project(PX.Objects.AR).  Any assistance or work around for this issue would be greatly appreciated.  Thank you!
#region CustomerID
        public abstract class customerID : BqlInt.Field<customerID>
        {
            public class PreventEditBAccountCOrgBAccountID<TGraph> :
                PreventEditBAccountRestrictToBase<BAccount.cOrgBAccountID, TGraph, NXBOL,
                    SelectFrom<NXBOL>
                    .Where<NXBOL.bolType.IsNotEqual<NXBOLType.nonProductMovement>.
                        And<NXBOL.customerID.IsEqual<BAccount.bAccountID.FromCurrent>>>>
                where TGraph : PXGraph
            {
                protected override string GetErrorMessage(BAccount baccount, NXBOL document, string documentBaseCurrency)
                {
                    return PXMessages.LocalizeFormatNoPrefix(Messages.CannotChangeRestricToIfShipmentExists,
                        documentBaseCurrency, baccount.AcctCD, document.BOLNbr);
                }
            }

            public class PreventEditBAccountCOrgBAccountIDOnVendorMaint : PreventEditBAccountCOrgBAccountID<VendorMaint>
            {
                public static bool IsActive()
                    => PXAccess.FeatureInstalled<FeaturesSet.multipleBaseCurrencies>();
            }

            public class PreventEditBAccountCOrgBAccountIDOnCustomerMaint : PreventEditBAccountCOrgBAccountID<CustomerMaint>
            {
                public static bool IsActive()
                    => PXAccess.FeatureInstalled<FeaturesSet.multipleBaseCurrencies>();
            }
        }
        protected Int32? _CustomerID;
        [CustomerActive(
            typeof(Search<BAccountR.bAccountID, Where<True, Equal<True>>>), // TODO: remove fake Where after AC-101187
            Visibility = PXUIVisibility.SelectorVisible, Required = true)]
        [CustomerOrOrganizationInNoUpdateDocRestrictor]
        [PXForeignReference(typeof(Field<NXBOL.customerID>.IsRelatedTo<BAccount.bAccountID>))]
        public virtual Int32? CustomerID
        {
            get
            {
                return this._CustomerID;
            }
            set
            {
                this._CustomerID = value;
            }
        }
        #endregion
        

It appears that the [CustomerOrOrganizationInNoUpdateDocRestrictor] is not accessible.  The project will not build and receive the following error:
Error   CS0122  'CustomerOrOrganizationInNoUpdateDocRestrictor' is inaccessible due to its protection level
I have tried manipulating references, I would expect the attribute which is defined in PX.Objects.AR to work the same in my project as the PX.Objects.SO.
",76386230,"CustomerOrOrganizationInNoUpdateDocRestrictor is an internal class, so you can't access it.
You can use this restrictor instead:
[PXRestrictor(
        typeof(Where<Customer.type, IsNotNull, Or<Current<PX.Objects.SO.SOOrder.aRDocType>,
            Equal<ARDocType.noUpdate>, And<Current<PX.Objects.SO.SOOrder.behavior>, Equal<SOBehavior.tR>,
                And<Where<BAccountR.type, In3<BAccountType.branchType, BAccountType.organizationType>,
                    Or<PX.Objects.CR.BAccount.isBranch, Equal<True>>>>>>>),
        ""Only a customer or company business account can be specified."")]

",
76383243,Chain assertions together in kotlin,"I have an operation that I want to succeed with two different conditions. For example, if the status code is 501 OR message is not 'FAILED'. Is there a way to have assertions grouped together logically like AND/OR. If assertion 1 passes OR assertion 2 passes, I want my test case to succeed.
",76384553,"@cactustictacs suggests ""naming"" your assertions and then chaining them.
I'll suggest an answer by showing code that validates 4x REST interface inputs that have some complex permutations that allowed / not allowed.  This code is arguably easier to read than a classic if ... else structure.
See how they are evaluated using the when construct.   Perhaps you can take these ideas for your assertions...
val userIdNotNull = userId != null
        && channelType == null
        && primaryMentorId == null
        && primaryClinicianId == null

val userIdAndChannelTypeNotNull = userId != null
        && channelType != null
        && primaryMentorId == null
        && primaryClinicianId == null

val primaryMentorIdNotNull = userId == null
        && channelType == null
        && primaryMentorId != null
        && primaryClinicianId == null

val primaryClinicianIdNotNull = userId == null
        && channelType == null
        && primaryMentorId == null
        && primaryClinicianId != null

val channels = when {
    userIdNotNull -> getChannelsByUserId(userId)
    userIdAndChannelTypeNotNull -> channelRepository.findByMemberIdAndChannelType(userId!!, channelType!!)
        .ifEmpty { throw NotFoundException() }

    primaryMentorIdNotNull -> channelRepository.findByPrimaryMentorId(primaryMentorId)
        .ifEmpty { throw NotFoundException() }

    primaryClinicianIdNotNull -> channelRepository.findByPrimaryClinicianId(primaryClinicianId)
        .ifEmpty { throw NotFoundException() }

    else -> throw InvalidRequestParameterException(""This combination of request parameters is not supported"")
}

",
76381084,use surface pressure to mask 4D netcdf variable,"I've merged a 3D surface pressure field (ERA5, converted from Pa to hPa, function of lat,lon and time) with a 4D variable which is also a function of pressure levels (lat,lon,time,level).
So, my netcdf file has two fields, Temperature which is 4D:
float t(time, level, latitude, longitude)

surface pressure, which is 3d:
float sp(time, latitude, longitude)

The pressure dimension ""level"" is of course a vector:
int level(level)

What I want to do is make a mask for temperature for all locations where the pressure exceeds the surface pressure.
I know how to use nco to make a mask using a simple threshold:
 ncap2 -s 'mask=(level>800)' t_ps.nc mask.nc

But of course when I try to use the surface pressure
ncap2 -s 'mask=(level>sp)' t_ps.nc mask.nc 

I get the error
ncap2: ERROR level and template sp share no dimensions

I think what I need to do is make a new variable like ""level3d"" which duplicates the pressure ""level"" to be a function of lat and lon, which I can then use to efficiently make the mask, yes?  But I'm not sure how to do this with a dimension (I thought about cdo enlarge but couldn't get it to work).
By the way, instead of posting the data, this is the python api script I used to retrieve it
import cdsapi
c = cdsapi.Client()
c.retrieve(
    'reanalysis-era5-single-levels-monthly-means',
    {
        'format': 'netcdf',
        'product_type': 'monthly_averaged_reanalysis',
        'variable': 'surface_pressure',
        'year': '2020',
        'month': '03',
        'time': '00:00',
    },
    'ps.nc')

c.retrieve(
    'reanalysis-era5-pressure-levels-monthly-means',
    {
        'format': 'netcdf',
        'product_type': 'monthly_averaged_reanalysis',
        'variable': 'temperature',
        'pressure_level': [
            '1', '2', '3',
            '5', '7', '10',
            '20', '30', '50',
            '70', '100', '125',
            '150', '175', '200',
            '225', '250', '300',
            '350', '400', '450',
            '500', '550', '600',
            '650', '700', '750',
            '775', '800', '825',
            '850', '875', '900',
            '925', '950', '975',
            '1000',
        ],
        'year': '2020',
        'month': '03',
        'time': '00:00',
    },
    't.nc')

",76386067,"Your diagnosis of the NCO behavior is essentially correct. The ""broadcast""
ncap2 -s 'mask=(level>sp)' t_ps.nc mask.nc

fails because level and sp are arrays (not scalars) that share no dimensions. The fix would be to create and use a temporary 3D version of level with something like
ncap2 -s 'level_3D[level,latitude,longitude]=level;mask=(level_3D>sp)' t_ps.nc mask.nc

",
76385355,How to set invert_if_negative to fill bars to a solid color in python xlswriter,"I am making graphics on excel from python using xls writer and want to make a graphic with green colored bars for positive values, and red for negative.
Current code seems like this:
chart3 = workbook.add_chart({'type': 'column'})
chart3.add_series({
    'values': '=Summary!$W$2:$W$76',
    'categories': '=Summary!$A$2:$A$76',
    'gap': 4,
    'line': {'width': 1},
    'name': '=Summary!$W$1',
    'fill':   {'color': 'green'},
    'invert_if_negative': True
})

It differientiates positive and negative values in the graph but the negative ones are just no-colored. Is there a way to make the inverted color to be red?
Already tried with other properties like inverted_color or any syntax like that but does not work
",76386251,"You will need version >= 3.1.1 of XlsxWriter which supports the invert_if_negative_color parameter:
from xlsxwriter import Workbook


workbook = Workbook(""chart.xlsx"")

worksheet = workbook.add_worksheet()
chart = workbook.add_chart({""type"": ""column""})

worksheet.write_column(""A1"", [3, 2, -3, 4, -2])

chart.add_series(
    {
        ""values"": ""=Sheet1!$A$1:$A$5"",
        ""fill"": {""color"": ""green""},
        ""invert_if_negative"": True,
        ""invert_if_negative_color"": ""red"",
    }
)

worksheet.insert_chart(""C1"", chart)

workbook.close()


Output:

",
76380701,Custom CSS drop caps in Wordpress 6.2.2 with Twenty Twenty Three theme,"I'm trying to do drop caps in the Twenty Twenty-Three theme on WordPress 6.2.2.
All the docs I find when I google it are for older versions of WordPress, and possibly on an older theme. It used to be easy, but I can't find relevant docs for how to do this with the Twenty Twenty-Three theme.
And following on from that, how do I add custom CSS to use a different font for the drop caps?
I have a couple of older posts from an earlier version of WP that have drop caps and I used to have them styled via a child theme I was using, but I upgraded to the Twenty Twenty Three theme and I lost all my customisations.
I've added the following code via the ""Tools > Theme file editor"", but it doesn't seem to be working.
p.has-drop-cap:not(:focus)::first-letter
{
  font-family: 'Fredericka the Great', cursive;
}

",76386331,"Neither TwentyTwentyTwo nor TwentyTwentyThree currently support dropCaps. Since the layout looks undesirable on certain user systems, it was agreed that dropcap support is not mandatory for either theme. Read more - WordPress issues: https://github.com/WordPress/twentytwentytwo/issues/180
But there's a workaround available. Since you have to touch the theme core files, the use of a child theme is probably not a bad idea. Otherwise, adjustments could be overwritten when updating.
The workaround was first pointed out in a comment by @colorful-tones a user on GitHub, in this thread. The related CSS is from @justintadlock, another GitHub user, you can read more here.
So here are the steps you need to take to enable dropCap support:

Since you are using the theme file editor, go there and open       theme.json.
At about line 109, under typography, change the value for dropCap from false to true.
Save the file.
Open the theme's style.css and add:

.has-drop-cap:not(:focus)::first-letter {
    font-family: var( --wp--custom--drop-cap--typography--font-family, inherit );
    font-size: var( --wp--custom--drop-cap--typography--font-size, 5.5em );
    font-weight: var( --wp--custom--drop-cap--typography--font-weight, 700 );
    font-style: var( --wp--custom--drop-cap--typography--font-style, normal );
    line-height: var( --wp--custom--drop-cap--typography--line-height, .85 );
    margin: var( --wp--custom--drop-cap--spacing--margin, 0.05em 0.1em 0 0 );
    padding: var( --wp--custom--drop-cap--spacing--paddig, 0 );
}


Save the file.

NOTE: If you want to use a custom font you may have to add your font to the typography section in theme.json. A support topic from WordPress.org could be helpful hereby. You can also try to replace all variables directly in the CSS part with your own values. But I'm sorry I can't remember if it worked like that, because I used this workaround only once and it was some time ago and the page doesn't exist like that anymore. You'll just have to test it yourself.
Finally, don't forget to properly include your Fredericka the Great font into Wordpress.
Hope this works for you.
",
76383382,Axis labeling for subplots,"The following code generates this image 
I want the ""y""-axis label to be ""Space"" and the ""x""-axis label to be ""time"" for the left subplot. However, I am failing to achieve this. Why does my plotting code not do as I desire?
p1 = surface(sol.t, x, z, xlabel=""Time"", ylabel=""Space"", zlabel=""|u|²"", colorbar = false)
p2 = contourf(sol.t,x,z, xlabel=""Time"", ylabel=""Space"")
plt = plot(p1,p2,layout=(1,2), size=(1200,800))


using DifferentialEquations, LinearAlgebra, Plots, SparseArrays
plotlyjs()

N₁=31 # Number of waveguides / size of solution vector
γ=1  # Nonlinear term strength parameter
h=1 # Grid spacing 

centerGrid  = (N₁-1)/2;
x = -centerGrid:centerGrid;

# Coefficient matrix of second-order centered-difference operator (δ²u)ₙ
M           = spdiagm(-1 => fill(1,N₁-1), 0 => fill(-2,N₁), 1 => fill(1,N₁-1))
M[N₁,1]     = 1; # Periodic boundary conditions
M[1,N₁]     = 1;

# RHS of DNLS. The solution vector u is a N₁x1 complex vector
g₁(u,p,t)   = 1*im*(p[1]*M*u + @.(γ*((abs(u))^2).*u) )

# Julia is explicitly typed (e.g, cannot have Int and Complex in same array) and so we must convert the object containing the initial data to be complex
u0  = Complex.(sech.(x))

tspan = (0.0,200)
prob = ODEProblem(g₁,u0,tspan, [h])
sol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)

z= [abs(sol.u[i][j])^2  for j=1:N₁, i=1:size(sol)[2]] # |u|²

p1 = surface(sol.t, x, z, xlabel=""Time"", ylabel=""Space"", zlabel=""|u|²"", colorbar = false)
p2 = contourf(sol.t,x,z, xlabel=""Time"", ylabel=""Space"")
plt = plot(p1,p2,layout=(1,2), size=(1200,800))

",76384575,"Setting custom axis labels for 3d plots doesn't work with Plots and plotlyjs() backend. Only with GR backend your labels are displayed.
You can try this version using PLotlyJS.jl instead Plots.jl:
fig=make_subplots(rows=1, cols=2, specs =[Spec(kind=""scene"") Spec(kind=""xy"")], 
                  horizontal_spacing=-0.1, column_widths=[0.65, 0.35])
add_trace!(fig, PlotlyJS.surface(x=sol.t, y=collect(x), z=z', showscale=false), row=1, col=1)
add_trace!(fig, PlotlyJS.contour(x=sol.t, y=collect(x), z=z), row=1, col=2)
relayout!(fig, template=templates[""plotly_white""], font_size=11,
                width=1000, height=600, scene=attr(xaxis_title=""Time"", yaxis_title=""Space"", 
                zaxis_title=""|u|²"", camera_eye=attr(x=1.8, y=1.8, z=1)),
                xaxis2_title=""Time"", yaxis2_title=""Space"",  margin_l=15) 
display(fig)


",
76381599,newEmail' box that comes with identity is autopopulating and I can't stop it,"I have created a website and included identity for logging in. On the manage your account page, the new email box keeps autopopulating and I can't figure out how to stop it.

I have tried to add the 'autocomplete=off' to the  tag (see below code) but it still populates.
@page
@using FarmersPortal.Areas.Identity.Pages.Account.Manage;
@model EmailModel
@{
    ViewData[""Title""] = ""Manage Email"";
    ViewData[""ActivePage""] = ManageNavPages.Email;
}

<style>
    body {
        background-image: url('http://10.48.1.215/PORTAL/hero-range-1.jpg');
        height: 100%;
        background-position: center;
        background-repeat: no-repeat;
        background-size: cover;
        /*        background-color: white;*/
    }
</style>

<h3 style=""color:white"">@ViewData[""Title""]</h3>
<partial name=""_StatusMessage"" for=""StatusMessage"" />
<div class=""row"">
    <div class=""col-md-6"">
        <form id=""email-form"" method=""post"">
            <div asp-validation-summary=""All"" class=""text-danger""></div>
            <div class=""form-floating input-group"">
                <input asp-for=""Email"" class=""form-control"" disabled />
                <div class=""input-group-append"">
                    <span class=""h-100 input-group-text text-success font-weight-bold"">✓</span>
                </div>
                <label asp-for=""Email"" class=""form-label""></label>
            </div>


            <div class=""form-floating"">
                <input asp-for=""Input.NewEmail"" class=""form-control"" autocomplete=""off"" aria-required=""true"" />
                <label asp-for=""Input.NewEmail"" class=""form-label""></label>
                <span asp-validation-for=""Input.NewEmail"" class=""text-danger""></span>
            </div>
            <button id=""change-email-button"" type=""submit"" asp-page-handler=""ChangeEmail"" class=""w-100 btn btn-lg btn-primary"">Change email</button>
        </form>
    </div>
</div>

@section Scripts {
    <partial name=""_ValidationScriptsPartial"" />
}

",76386720,"asp-for sets the id, name and validation related attributes, and it also sets the value of the input element if there is already a value within the model passed to the view.
From your code, You are using:
<input asp-for=""Input.NewEmail"" class=""form-control"" autocomplete=""off"" aria-required=""true"" />

to input the value of Input.NewEmail, I think before you render this view, Input.NewEmail already has the value, asp-for will get this value and set value=""xxx"" attribute in input tag.
So if you don't want show the value, You can just use name property instead of asp-for, Change your code to:
<input name=""Input.NewEmail"" class=""form-control"" autocomplete=""off"" aria-required=""true"" />

Then when render this view, Input tag will show nothing.
",
76384911,stick footer to bottom if actual page height is greater than html height,"I have a situation where I use several sliders on the page due to which the actual height of the page changes
Let's say my html height is 600px but due to some sliders the actual page height is 1000px
And because of this, when I try to stick the footer to the bottom using position: absolute and bottom: 0, I have it placed at the end of the html height
I used an example to show how everything looks like for me
If I use position: relative then on other pages where the height is small, it will not be at the bottom
How can I stick the footer to the bottom of the page in this case?
I have also tried wrapping the entire html content in a class .wrapper { height: 100%; display: flex; flex-direction: column; } and for the footer use position: relative and margin-top: auto
This kind of helped, but then there were problems with the blocks that come after the html, they lost their width


html {
  height: 500px;
}

.main-content {
  padding: 200px;
  text-align: center;
}

.content {
  padding: 300px;
  text-align: center;
}

.footer {
  padding: 40px 0;
  position: absolute;
  width: 100%;
  bottom: 0;
  text-align: center;
  background: gray;
}
<html>
<div class=""main-content""> MAIN CONTENT</div>
<div class=""content"">CONTENT</div>
<footer class=""footer"">FOOTER</footer>
</html>



",76386287,"You can use flexbox. Uncomment height property in body to see the changes.
Check the elements html, body, main and footer in the code below.
Resources: 
CSS Tricks | Flexbox 
CSS Tricks | Flexbox and Auto Margins 
Dev | Stick Footer to The Bottom of The Page


* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}

html, body {
  height: 100%;
}

body {
  display: flex;
  flex-direction: column;
  /*height: 1000px;*/
}

header {
  height: 50px;
  background-color: cyan;
}

main {
  flex: 1;
}

footer {
  margin-top: auto;
  height: 70px;
  background-color: red;
}
<html>
  <body>
    <header>Header</header>
    <main>Main</main>
    <footer>Footer</footer>
  </body>
</html>



",
76387119,guard/if let in Java - declare a property or an object if it can be unwrapped,"In swift there is the guard let / if let pattern allowing us to declare an object or a property only if it can be unwrapped.
it works a follow:
func getMeaningOfLife() -> Int? {
    42
}

func printMeaningOfLife() {
    if let name = getMeaningOfLife() {
        print(name)
    }
}

func printMeaningOfLife() {
    guard let name = getMeaningOfLife() else {
        return
    }

    print(name)
}

My question here is: Is there a Java version of it ?
",76387120,"The answer is No.
Apparently this syntax also exists in Clojure and according to this Stack Overflow answer there is no way to declare a property if it can be unwrapped in Java.
",
76387153,"Powershell GUI, How to take password input without exposing","I have a situation I am writing a powershell GUI.
I need to take the password in a secure input.. How can I do it ?
The passwd input need to be secured in the window
below is my code

$Passwd                         = New-Object system.Windows.Forms.TextBox
$Passwd.multiline               = $false
$Passwd.width                   = 150
$Passwd.height                  = 20
$Passwd.location                = New-Object System.Drawing.Point(169,26)
$Passwd.Font                    = New-Object System.Drawing.Font('Microsoft Sans Serif',10)
$Passwd.ForeColor               = [System.Drawing.ColorTranslator]::FromHtml(""#7ed321"")
$Passwd.BackColor               = [System.Drawing.ColorTranslator]::FromHtml(""#000000"")

this is the code
the passwd input nee to be secured input ?
",76387187,"You need to set UseSystemPasswordChar to $true:
$Passwd.UseSystemPasswordChar = $true

",
76384259,String text alignment by decimal point (Swift),"Is there a way to achieve such kind of alignment of numbers in multiple strings, preferably in interface builder? Please see the attached screenshot.

",76386348,"One approach to achieve this is by using a table view and programmatically adding constraints to align the separator symbol. This method offers scalability as it only includes elements visible on the screen. An interesting aspect of this approach is that the separator's position may change based on the largest offset currently on the screen. Whether this behavior is desired or not depends on your specific requirements.
In this solution, I suggest splitting the string into three components and placing them into three separate labels: one for the content before the separator, one for the separator itself, and another for the content after the separator. Additionally, create an invisible reference view at the top level, which will be used to connect the separator label.
Although the following code is implemented programmatically for clarity in understanding constraint connections, it is recommended to move most of the code into the storyboard for better organization and maintainability.
I hope this code snippet helps you solve your problem.
class OffsetNumberViewController: UIViewController {
    
    var values: [NSDecimalNumber] = [] {
        didSet {
            tableView.reloadData()
        }
    }
    
    private lazy var tableView: UITableView = {
        let view = UITableView()
        view.delegate = self
        view.dataSource = self
        return view
    }()
    
    private lazy var numberFormatter: NumberFormatter = {
        let formatter = NumberFormatter()
        formatter.decimalSeparator = "".""
        formatter.usesGroupingSeparator = true
        formatter.groupingSeparator = "",""
        formatter.groupingSize = 3
        formatter.maximumFractionDigits = 5
        return formatter
    }()
    
    private lazy var referenceView = {
        let view = UIView()
        view.isHidden = true
        self.view.addSubview(view)
        view.translatesAutoresizingMaskIntoConstraints = false
        self.view.addConstraints([
            .init(item: view, attribute: .trailing, relatedBy: .lessThanOrEqual, toItem: self.view, attribute: .trailing, multiplier: 1.0, constant: 0.0),
            .init(item: view, attribute: .top, relatedBy: .equal, toItem: self.view, attribute: .top, multiplier: 1.0, constant: 0.0),
            .init(item: view, attribute: .bottom, relatedBy: .equal, toItem: self.view, attribute: .bottom, multiplier: 1.0, constant: 0.0)
        ])
        return view
    }()
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        tableView.register(NumberTableViewCell.self, forCellReuseIdentifier: ""amountCell"")
        tableView.frame = view.bounds
        view.addSubview(tableView)
        
        values = generateRandomValues(count: 1000)
    }
    
    private func generateRandomValues(count: Int) -> [NSDecimalNumber] {
        (0..<count).map { index in
            let startValue: Int = 1234567890
            let maximumDivision = 5
            
            let randomDivision: Int = 1<<Int.random(in: 0...maximumDivision)
            return .init(integerLiteral: startValue).dividing(by: .init(integerLiteral: randomDivision))
        }
    }
    
    private func formatValue(_ value: NSDecimalNumber) -> String {
        numberFormatter.string(for: value) ?? ""NaN""
    }
    
}

// MARK: - UITableViewDelegate, UITableViewDataSource

extension OffsetNumberViewController: UITableViewDelegate, UITableViewDataSource {
    
    func tableView(_ tableView: UITableView, numberOfRowsInSection section: Int) -> Int {
        values.count
    }
    
    func tableView(_ tableView: UITableView, cellForRowAt indexPath: IndexPath) -> UITableViewCell {
        if let cell = tableView.dequeueReusableCell(withIdentifier: ""amountCell"", for: indexPath) as? NumberTableViewCell {
            cell.setup(withNumberAsString: formatValue(values[indexPath.row]), decimalSeparator: numberFormatter.decimalSeparator)
            return cell
        } else {
            return UITableViewCell()
        }        
    }
    
    func tableView(_ tableView: UITableView, willDisplay cell: UITableViewCell, forRowAt indexPath: IndexPath) {
        (cell as? NumberTableViewCell)?.attachCenterTo(referenceView, parent: self.view)
    }
    
    func tableView(_ tableView: UITableView, didEndDisplaying cell: UITableViewCell, forRowAt indexPath: IndexPath) {
        (cell as? NumberTableViewCell)?.detachExternalConstraints()
    }
    
}

// MARK: - NumberTableViewCell

extension OffsetNumberViewController {
    
    class NumberTableViewCell: UITableViewCell {
        
        lazy private var leftSideLabel: UILabel = UILabel()
        lazy private var rightSideLabel: UILabel = UILabel()
        lazy private var separatorLabel: UILabel = UILabel()
        
        private var currentExternalConstraints: [NSLayoutConstraint] = []
        
        lazy private var stackView: UIStackView = {
            let stackView = UIStackView()
            stackView.translatesAutoresizingMaskIntoConstraints = false
            stackView.alignment = .fill
            stackView.axis = .horizontal
            stackView.distribution = .fill
            
            stackView.addArrangedSubview(leftSideLabel)
            stackView.addArrangedSubview(separatorLabel)
            stackView.addArrangedSubview(rightSideLabel)
            
            addSubview(stackView)
            
            addConstraints([
                .init(item: stackView, attribute: .right, relatedBy: .equal, toItem: self, attribute: .right, multiplier: 1.0, constant: -12.0),
                .init(item: stackView, attribute: .top, relatedBy: .equal, toItem: self, attribute: .top, multiplier: 1.0, constant: 0.0),
                .init(item: stackView, attribute: .bottom, relatedBy: .equal, toItem: self, attribute: .bottom, multiplier: 1.0, constant: 0.0),
            ])
            
            return stackView
        }()
        
        func setup(withNumberAsString numberString: String, decimalSeparator: String) {
            let components = numberString.components(separatedBy: decimalSeparator)
            
            let _ = stackView
            
            separatorLabel.text = decimalSeparator
            if components.count == 1 {
                leftSideLabel.text = components[0]
                rightSideLabel.text = """"
                separatorLabel.alpha = 0
            } else if components.count == 2 {
                leftSideLabel.text = components[0]
                rightSideLabel.text = components[1]
                separatorLabel.alpha = 1
            } else {
                // Something went wrong
                leftSideLabel.text = """"
                rightSideLabel.text = ""error""
                separatorLabel.alpha = 0
            }
        }
        
        func detachExternalConstraints() {
            currentExternalConstraints.forEach { constrain in
                constrain.isActive = false
            }
            currentExternalConstraints = []
        }
        
        func attachCenterTo(_ referenceView: UIView, parent: UIView) {
            currentExternalConstraints = [
                .init(item: separatorLabel,
                                           attribute: .centerX,
                                           relatedBy: .greaterThanOrEqual,
                                           toItem: referenceView,
                                           attribute: .centerX,
                                           multiplier: 1.0,
                                           constant: 0.0),
                .init(item: referenceView,
                                           attribute: .centerX,
                                           relatedBy: .greaterThanOrEqual,
                                           toItem: separatorLabel,
                                           attribute: .centerX,
                                           multiplier: 1.0,
                                           constant: 0.0)
            ]
            parent.addConstraints(currentExternalConstraints)
        }
        
    }
    
}

",
76385322,Banno External Plugin S3 Hosting Question,"I am developing 2 plugins for Banno and have a hosting question.
My client will be hosting the plugins on S3. Can I use 1 bucket for both plugins or will they each need a bucket?
Thank you.
I haven't uploaded anything to S3 yet.
",76386379,"We don't have material specific to Amazon Web Services (e.g., S3) but here's some general guidance which may be helpful.
No matter what, you'll need to make sure that your plugin's content is hosted by your public-facing web server. This means your web server must be accessible via the internet and cannot require the user to be on a specific network or VPN.
See these resources for more info:

Plugin Framework / Architecture / Hosting
Plugin Framework / Guides/ Designing and Developing Plugins

The gist is that how you build your plugin's web service is at your discretion.

Whether you have separate S3 buckets or a single S3 bucket, that's up to you to decide what's best for your plugin.

",
76382443,Google Sheets - Data Validation - Conditional based on Column in another sheet,"Im currently designing a menu for a food festival. I use google sheets. I have a sheet filled with food choices. the menu for a given week should not have food items from the previous weeks. This is a mandatory requirement & I'm not able to get the drop-down if I use Data validation & custom formula.
I use =FILTER('Item Suggestions'!A:A,'Item Suggestions'!E:E=""Y"") as the custom formula for validating the data.
Is there any other way (or a tweak to get the data validation drop down) to get the drop-down & keep the drop-down list filtered?
Each of the columns in the ""Menu"" sheet should pick from column A of the ""Item Suggestions"" sheet. But the data should be based on column E. If Column E is Y, then that respective data in Column A should be shown in the drop-down
Menu Sheet:




Column  A
Column B
Column C
Column D
Column E




Appetizers
Mains
Course 2
Dessert
Drinks


Soup
Chicken
Creamy Pasta
Strawberry Mousse
Fire and Ice


Broccoli
Rice & Curry
Roti & Subzi
Icecream Sundae
Mojito




Item Suggestions sheet:




Column A
Column B
Column C
Column D
Column E




Dish
Course
Allergens
Type
Used in Previous weeks?


Creamy Pasta
Main
Gluten
Vegetarian
Y


Chocolate Marble Cake
Dessert
Wheat
Vegan
N



",76384657,"Validation Helper Columns
I added some 'helper' columns for the validation.

They can be on the same sheet or a different one.
There is one for each course: Appetizer, Main,  Dessert, and Drink. I assume Main and Course 2 both share the same dishes.
The FILTER formula would return an array of Dishes that match the correct Course, and haven't yet been used:

=IFERROR(FILTER(Dishes,Courses=thisCourse,isUsed<>""Y""),""-"")


For the validation rule,

the criteria would be ""Dropdown (from a range)"" with the range being the appropriate helper column for each Course
The ""Apply to range"" value would be the appropriate Course column in your Menu table


Please note that all populated menu items will 'always' show the error flag (red triangle in the top right corner).  This is because the moment they are used, they are no longer valid values.  This doesn't affect the functionality of the menu dropdowns.  Used menu items will be filtered from the dropdowns, and you will not be able to add a used menu item manually with the dropdown properties set to reject.  Just a visual distraction.

Dropdown Formula
Single Formula
Will generate all dropdowns at once and centralizes modifications
=BYCOL(M2:P2, LAMBDA(c, 
   IFERROR(FILTER(G:G,H:H=c,K:K<>""Y""),""-"")))

Individual Formula
Needs to be manually copied to each column
=IFERROR(FILTER($G:$G,$H:$H=M2,$K:$K<>""Y""),""-"")

Filtering Formula

Dropdown with Filtered Dishes

Formula to Mark Dishes When Used

Your Master List of dishes includes a column to mark if a dish has been used previously
Your menu's dropdowns are based on that and it makes sense to update the ""used/not used"" status dynamically when a dish is added or removed from a menu.
This can be achieved using a formula in the Master List that marks each dish based on whether it exists in the menu.

Dish ""Is Used"" Formula
Note that the 'Single Formula' includes the column heading ""Used in previous weeks?"".

This was intentional in order to place the formula a line above the Master List data
Offsetting the formula from the data by a row, allows the data to be sorted without impacting the formula. For example, you could sort the Master List by any of Dishes, Courses, Allergens, or Type

Single Formula
={""Used in previous weeks?"";
  BYROW(G3:G51, LAMBDA(r, 
    IFERROR(IF(ROWS(FILTER(r,COUNTIF(A:E,r)))>0,""Y""))))}

Individual Formulas
Needs to be copied to each row
=IFERROR(IF(ROWS(FILTER(G3,COUNTIF(A:E,G3)))>0,""Y""))

Sorted Asc. by Dishes

Sorted Asc. by Courses then Dishes

",
76382591,Google Container Registry: Permission issue while trying to pull/push images with authenticated credentials,"In Ubuntu-22, google-cloud has been installed through snap store;
> whereis gcloud
gcloud: /snap/bin/gcloud
> snap list | grep google
google-cloud-sdk           432.0.0                     346    latest/stable    google-cloud-sdk**  classic

Docker has been installed via snap too;
> snap list | grep docker
docker                     20.10.24                    2893   latest/stable    canonical**

And I have authenticated my account to a private GCR as below;
> gcloud auth login
Your browser has been opened to visit:

    https://accounts.google.com/o/oauth2/auth?...<long_url>


You are now logged in as [<my_email@address.com>].
Your current project is [<desired_project_name>].  You can change this setting by running:
  $ gcloud config set project PROJECT_ID

Double-checked the login process;
> gcloud auth list
           Credentialed Accounts
ACTIVE             ACCOUNT
*                  <my_email@address.com>

To set the active account, run:
    $ gcloud config set account `ACCOUNT`

But, when I try to pull or push any image, I hit the following permission issue;
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication

I am able to access to the image which I try to pull from the private GCR in my browser, this makes me think that it is an issue related to creds while performing docker pull in my terminal.
What am I missing here?
PS: The solution in this question did not work for me Unable to push to Google Container Registry - Permission issue

EDIT:
As it is asked in the comments, I need to mention that I have performed one more step before auth login which is gcloud auth configure-docker;
> gcloud auth configure-docker
Adding credentials for all GCR repositories.
WARNING: A long list of credential helpers may cause delays running 'docker build'. We recommend passing the registry name to configure only the registry you are using.
After update, the following will be written to your Docker config file located at 
[/home/<user>/.docker/config.json]:
 {
  ""credHelpers"": {
    ""gcr.io"": ""gcloud"",
    ""us.gcr.io"": ""gcloud"",
    ...
  }
}

Do you want to continue (Y/n)?  

Docker configuration file updated.

",76384670,"Removing snap installation and installing docker with package manager apt has fixed my issue.
The difference I have observed between two installations;

With snap, once gcloud auth login directs me to browser, authentication was completed by choosing google account only (Please see the 3rd code block in my question, no authorization code was asked).
With apt, after choosing the desired google account, I was directed to another page where the authorization code was provided which needed to be entered in the terminal;

> gcloud auth login
Your browser has been opened to visit:

    https://accounts.google.com/o/oauth2/auth?...<long_url>

Enter authorization code: <Code_from_browser>  // This is the difference!!

You are now logged in as [<my_email@address.com>].
Your current project is [<desired_project_name>].  You can change this setting by running:
  $ gcloud config set project PROJECT_ID

Thank you @JohnHanley pointed out that docker recommends apt installation.
",
76385271,Optimizing joins in a postgres/postgis query,"I'm trying to optimize my Postgres query. I'm running into problems with some of the joins here. My main issue is around the filter h.type='inNetwork' and my geometry search ST_Intersects(ST_MakeValid(ser.boundaries)::geography, ST_MakeValid(ST_SetSRID(ST_GeomFromGeoJson(<INSERT_GEOMETRY_JSON>)). Something about that specific filter increase the search time by ~10x. The other filters don't seem to have much of an effect on the search speed. As a side note, there are additional filters that are used conditionally and that's why some of the join tables here seem irrelevant.
Query in question:
SELECT DISTINCT r.id, r.profitability
FROM rolloff_pricing as r
LEFT JOIN service_areas ser on r.service_area_id = ser.id
LEFT JOIN sizes as s on r.size_id = s.id
LEFT JOIN sizes as sa on r.sell_as = sa.id
LEFT JOIN waste_types w on w.id = r.waste_type_id
LEFT JOIN regions reg on reg.id = ser.region_id
LEFT JOIN haulers h on h.id = reg.hauler_id
LEFT JOIN current_availability ca on ca.region_id = reg.id
LEFT JOIN regions_availability ra on ra.region_id = reg.id
LEFT JOIN current_availability_new_deliveries cand on ca.id = cand.current_availability_id and r.size_id = cand.size_id
LEFT JOIN exceptions ex on ex.region_id = reg.id
WHERE ser.active is true
and ST_Intersects(ST_MakeValid(ser.boundaries)::geography, ST_MakeValid(ST_SetSRID(ST_GeomFromGeoJson('{
    ""type"": ""POINT"",
    ""coordinates"": [
        ""-95.3595563"",
        ""29.7634871""
    ]
}'),4326))::geography)
and h.active is true
and ra.delivery_type='newDeliveries'
and h.type='inNetwork'
GROUP BY r.id ORDER BY profitability desc OFFSET 0 ROWS FETCH NEXT 8 ROWS ONLY

Here is the EXPLAIN (ANALYZE, BUFFERS):
Limit  (cost=246.23..246.29 rows=8 width=21) (actual time=3711.860..3711.866 rows=8 loops=1)
  Buffers: shared hit=15048
  ->  Unique  (cost=246.23..246.29 rows=8 width=21) (actual time=3711.859..3711.860 rows=8 loops=1)
        Buffers: shared hit=15048
        ->  Sort  (cost=246.23..246.25 rows=8 width=21) (actual time=3711.858..3711.858 rows=8 loops=1)
""              Sort Key: r.profitability DESC, r.id""
              Sort Method: quicksort  Memory: 28kB
              Buffers: shared hit=15048
              ->  Group  (cost=246.07..246.11 rows=8 width=21) (actual time=3711.820..3711.841 rows=48 loops=1)
                    Group Key: r.id
                    Buffers: shared hit=15048
                    ->  Sort  (cost=246.07..246.09 rows=8 width=21) (actual time=3711.817..3711.823 rows=216 loops=1)
                          Sort Key: r.id
                          Sort Method: quicksort  Memory: 41kB
                          Buffers: shared hit=15048
                          ->  Hash Left Join  (cost=154.30..245.95 rows=8 width=21) (actual time=3711.508..3711.745 rows=216 loops=1)
                                Hash Cond: ((reg.id)::text = (ex.region_id)::text)
                                Buffers: shared hit=15048
                                ->  Hash Join  (cost=150.45..242.05 rows=8 width=37) (actual time=3711.490..3711.705 rows=144 loops=1)
                                      Hash Cond: ((ra.region_id)::text = (reg.id)::text)
                                      Buffers: shared hit=15045
                                      ->  Seq Scan on regions_availability ra  (cost=0.00..89.11 rows=643 width=16) (actual time=0.006..0.186 rows=643 loops=1)
                                            Filter: ((delivery_type)::text = 'newDeliveries'::text)
                                            Rows Removed by Filter: 1286
                                            Buffers: shared hit=65
                                      ->  Hash  (cost=150.34..150.34 rows=9 width=53) (actual time=3711.461..3711.461 rows=144 loops=1)
                                            Buckets: 1024  Batches: 1  Memory Usage: 21kB
                                            Buffers: shared hit=14980
                                            ->  Hash Right Join  (cost=73.72..150.34 rows=9 width=53) (actual time=3711.218..3711.442 rows=144 loops=1)
                                                  Hash Cond: ((ca.region_id)::text = (reg.id)::text)
                                                  Buffers: shared hit=14980
                                                  ->  Seq Scan on current_availability ca  (cost=0.00..69.02 rows=2002 width=32) (actual time=0.009..0.124 rows=2002 loops=1)
                                                        Buffers: shared hit=49
                                                  ->  Hash  (cost=73.68..73.68 rows=3 width=69) (actual time=3711.173..3711.173 rows=48 loops=1)
                                                        Buckets: 1024  Batches: 1  Memory Usage: 13kB
                                                        Buffers: shared hit=14931
                                                        ->  Nested Loop  (cost=0.84..73.68 rows=3 width=69) (actual time=2262.438..3711.145 rows=48 loops=1)
                                                              Buffers: shared hit=14931
                                                              ->  Nested Loop  (cost=0.55..44.90 rows=1 width=48) (actual time=2262.424..3710.955 rows=7 loops=1)
                                                                    Buffers: shared hit=14877
                                                                    ->  Nested Loop  (cost=0.28..38.20 rows=3 width=16) (actual time=0.012..4.723 rows=609 loops=1)
                                                                          Buffers: shared hit=1418
                                                                          ->  Seq Scan on haulers h  (cost=0.00..21.60 rows=2 width=16) (actual time=0.003..0.698 rows=439 loops=1)
                                                                                Filter: ((active IS TRUE) AND ((type)::text = 'inNetwork'::text))
                                                                                Rows Removed by Filter: 89
                                                                                Buffers: shared hit=15
                                                                          ->  Index Scan using regions_hauler_id_idx on regions reg  (cost=0.28..8.29 rows=1 width=32) (actual time=0.006..0.007 rows=1 loops=439)
                                                                                Index Cond: ((hauler_id)::text = (h.id)::text)
                                                                                Buffers: shared hit=1403
                                                                    ->  Index Scan using service_areas_region_id_idx on service_areas ser  (cost=0.28..2.22 rows=1 width=32) (actual time=6.035..6.085 rows=0 loops=609)
                                                                          Index Cond: ((region_id)::text = (reg.id)::text)
""                                                                          Filter: ((active IS TRUE) AND ((st_makevalid(boundaries))::geography && '0101000020E610000087646DF802D757C0FA6AFDE373C33D40'::geography) AND (_st_distance((st_makevalid(boundaries))::geography, '0101000020E610000087646DF802D757C0FA6AFDE373C33D40'::geography, '0'::double precision, false) < '1.00000000000000008e-05'::double precision))""
                                                                          Rows Removed by Filter: 3
                                                                          Buffers: shared hit=13459
                                                              ->  Index Scan using rolloff_pricing_service_area_id_idx on rolloff_pricing r  (cost=0.29..28.70 rows=8 width=83) (actual time=0.013..0.019 rows=7 loops=7)
                                                                    Index Cond: ((service_area_id)::text = (ser.id)::text)
                                                                    Buffers: shared hit=54
                                ->  Hash  (cost=3.38..3.38 rows=38 width=48) (actual time=0.012..0.012 rows=39 loops=1)
                                      Buckets: 1024  Batches: 1  Memory Usage: 10kB
                                      Buffers: shared hit=3
                                      ->  Seq Scan on exceptions ex  (cost=0.00..3.38 rows=38 width=48) (actual time=0.003..0.007 rows=39 loops=1)
                                            Buffers: shared hit=3
Planning Time: 1.031 ms
Execution Time: 3711.956 ms

My understanding of what's happening here is that my filter is bringing in additional rows (the large majority of the h table is true for the condition h.type='inNetwork', which is making my geometry query run for a much larger set of rows than intended.
Something that I've tried is putting the geometry query into a subquery (because the geometry query actually runs pretty quickly itself) to get a set of r.id's that I could use in an where in clause. This seems to not work as well though. Here is my modified query that also is too slow:
SELECT DISTINCT r.id, r.profitability
FROM rolloff_pricing as r
LEFT JOIN service_areas ser on r.service_area_id = ser.id
LEFT JOIN sizes as s on r.size_id = s.id
LEFT JOIN sizes as sa on r.sell_as = sa.id
RIGHT JOIN waste_types w on w.id = r.waste_type_id
RIGHT JOIN regions reg on reg.id = ser.region_id
RIGHT JOIN haulers h on h.id = ser.hauler_id
RIGHT JOIN current_availability ca on ca.region_id = reg.id
RIGHT JOIN regions_availability ra on ra.region_id = reg.id
LEFT JOIN current_availability_new_deliveries cand on ca.id = cand.current_availability_id and r.size_id = cand.size_id
RIGHT JOIN exceptions ex on ex.region_id = reg.id
WHERE r.id in (
    select r2.id
    from rolloff_pricing as r2
    LEFT JOIN service_areas ser2 on r2.service_area_id = ser2.id
    WHERE
    ST_Intersects(ST_MakeValid(ser2.boundaries)::geography, ST_MakeValid(ST_SetSRID(ST_GeomFromGeoJson('{
        ""type"": ""POINT"",
        ""coordinates"": [
            ""-95.3595563"",
            ""29.7634871""
        ]
    }'),4326))::geography)
    and ser2.active is true
)
and h.active is true
and ra.delivery_type='newDeliveries'
and h.type='inNetwork'
GROUP BY r.id ORDER BY profitability desc OFFSET 0 ROWS FETCH NEXT 8 ROWS ONLY

It's interesting to me because the subquery here resolves by itself very quickly. And if I sub out the subquery for the returned results, the whole thing resolves very quickly as well. So I'm not sure how to approach this exactly. My next guess is to just run a completely seperate query for the r.ids and then pass them through to the ""main"" query.
Maybe relevant info: this query is being generated and executed in an eloquent-based api
How can I go about approaching improving the speed here?
",76386421,"
the large majority of the h table is true for the condition h.type='inNetwork', which is making my geometry query run for a much larger set of rows than intended.

I don't understand.  If most of the table meets the condition h.active is true and h.type='inNetwork', then most of the table gets processed.  What else could have possibly been intended? The estimate there is pretty horrible (estimated rows 2, actual 439) but that must be just because your stats are horribly out of date.  There is really no good reason for the estimate to be off by so much. You should run VACUUM ANALYZE on all tables involved in this query, after making sure there are no transactions being held open. If it doesn't fix the query directly, it will at least produce plans which are easier to understand.
->  Index Scan using service_areas_region_id_idx on service_areas ser  (cost=0.28..2.22 rows=1 width=32) (actual time=6.035..6.085 rows=0 loops=609)
    Index Cond: ((region_id)::text = (reg.id)::text)
    Filter: ((active IS TRUE) AND ((st_makevalid(boundaries))::geography && '0101000020E610000087646DF802D757C0FA6AFDE373C33D40'::geography) AND (_st_distance((st_makevalid(boundaries))::geography, '0101000020E610000087646DF802D757C0FA6AFDE373C33D40'::geography, '0'::double precision, false) < '1.00000000000000008e-05'::double precision))""
    Rows Removed by Filter: 3
    Buffers: shared hit=13459

This is the one place which takes pretty much all of the time.  And it is hard to understand what it is actually doing.  Why on earth would it take 13459 buffers hits to run this scan 609 times?  that is 22 buffers for each loop, but descending an index should only take 3 to 4 buffer accesses.  You could be filtering out a lot of rows due to the filter condition, but in that case Rows Removed by Filter would need to be a lot more than 3.  Maybe the index is stuffed full of dead entries, which then get filtered out but don't get tabulated in 'Rows Removed by Filter'.  (A VACUUM when there are no transactions being held open would fix that).  Or maybe the geometry column is very large and so gets TOASTed over many pages.
And to be clear here, I'm not saying the large number of buffer accesses are causing the slowness, they are all ""buffer hits"" afterall and so should be fast. But it is an oddity that should be investigated.
",
76383772,Link list sidebar widget remove button not working in jquery,"I am creating a link list widget. Whenever we clicked on Title button, it adds a list with add link option. Everything is working fine but whenever I am trying to click the remove title button. The button is not working anymore. Where the alert is working and logs are showing also in console but the action is not working. Can you please check?
Code Pen Codepen Reference Link
",76384721,"At line 23 you are only selecting a parent HTML element when the remove-title button is clicked, but doing nothing with it.


if ($("".remove-title"").length) {
  $(""body"").on(""click"", "".remove-title"", function() {
    console.log(""clicked"");
    $(this).parents("".btn-options"");
  });
}



First you should make sure you're not deleting the only section left. If so, the .add-title button is triggered first.


if ($("".remove-title"").length) {
  $(""body"").on(""click"", "".remove-title"", function() {
    console.log(""clicked"");
    var $this = $(this);
    var parent = $this.parents("".title-area"");
    if (!parent.siblings("".title-area"").length) {
      $this.siblings("".add-title"").trigger(""click"");
    }
    var title = parent.find(""input.title-text"").eq(0).val();
    var toast = parent.siblings("".title-area"").find("".btn-toast"");
    toast.show().html(""Title removed: <span>"" + title + ""</span>"")
      .delay(400).fadeOut(""slow"");
    parent.remove();
  });
}



As you can see, I've also added a ""toast"" element to the btn-options div to briefly display the title of the deleted section. You can find the rest of those changes here: Codepen
",
76387124,What is the equivalent of add.classList in React for toggling visibility?,"Adding and removing classes to elements in React
I’m a complete newb with React but basically I’ve been trying to make it so that a container goes from hidden to visible when you click a button.
Usually I’d just do an eventListner and add.classList or remove.classList but I can’t find the equivalent to that in react ?
I’ve tried figuring it out with useState and everything but I feel like I’m just overlooking something simple.
I would really appreciate some help it’s been like hours and my tiny brain is gonna explode
",76387192,"I would recommend adding a condition to render the element/component instead of using classes.
const [visible, setVisible] = useState(false);

return (
  <div>
    <button onClick={() => setVisible(!visible)}>toggle</button>
    {visible && <span>hello</span>}
  </div>
);

",
76383780,Find points inside region delimited by two lines,"I have the following point configuration:
import numpy as np 

T=np.array([9,9])
X1=np.array([8,16])
X2=np.array([16,3])
points=np.array([[4, 15],
                 [13,17],
                 [2, 5],
                 [16,8]])

This can be represented as:

Given T, X1, and X2, I want to find all points of the array points that are inside the yellow region. This yellow region is always in the ""opposite side"" of the points X1 and X2.
How can I achieve this in a simple and efficient way?
Edit1 (trying B Remmelzwaal solution)
T=np.array([9,9])
X1=np.array([10,2])
X2=np.array([2,15])
points=np.array([[2, 5]])

valid_points = list()

# calculating y = mx + b for line T, X1
slope1 = np.diff(list(zip(T, X1)))
m1 = np.divide(slope1[1], slope1[0])
b1 = T[1] - m1*T[0]

# calculating y = mx + b for line T, X2
slope2 = np.diff(list(zip(T, X2)))
m2 = np.divide(slope2[1], slope2[0])
b2 = T[1] - m2*T[0]

for point in points:
    # check if point is under both lines
    for m, b in (m1, b1), (m2, b2):
        if point[1] > m*point[0] + b:
            break
    else:
        # only append if both checks pass
        valid_points.append(point)
        
print(valid_points)

The configuration is the following:

and the code returns returns [2,5] and it should return []. This is not correct since the region of interest is now in the opposite region (see image)
",76384730,"The naive solution to this can be thought of as a series of stages

embed the values into equations in a Two-Point Form
for each line defined by the Equations

for each point in the collection to compare

at X, see if Y is below the line value




boolean AND on the results, such that only values below both lines match

However, this can be much faster with NumPy's powerful numeric methods, as you can directly use the values in collections without bothering to create the intermediate equations, but need to then pose it in a manner it expects and would make more sense to do for a great number of lines (hundreds, millions..)
very extended approach
import numpy as np 

T=np.array([9,9])
X1=np.array([8,16])
X2=np.array([16,3])
points=np.array([[4, 15],
                 [13,17],
                 [2, 5],
                 [16,8]])

equations = []
for point_pair in [(T, X1), (T, X2)]:
    # extract points
    (x1, y1), (x2, y2) = point_pair  # unpack
    # create equation as a function of X to get Y
    fn = lambda x, x1=x1, y1=y1, x2=x2, y2=y2: (y2-y1)/(x2-x1)*(x-x1)+y1
    equations.append(fn)

results = {}  # dict mapping lines to their point comparisons
for index, equation in enumerate(equations):
    key_name = ""line_{}"".format(index + 1)
    results_eq = []
    for point in points:
        point_x, point_y = point  # unpack
        line_y = equation(point_x)
        results_eq.append(point_y < line_y)  # single bool
    array = np.array(results_eq)             # list -> array of bools
    results[key_name] = array                # dict of arrays of bools

# & is used here to compare boolean arrays such that both are True
final_truthyness = results[""line_1""] & results[""line_2""]
print(final_truthyness)

>>> print(final_truthyness)
[False False  True False]


Alternatively, you can carefully order your points and take the Cross Product
NOTE that the point ordering matters here such that points below are really to the right of the line (vector), you can calculate this by comparing the X values of the points
>>> X1[0] < T[0], X2[0] < T[0]             # determine point ordering
(True, False)
>>> a = np.cross(points - X1, T - X1) > 0
>>> b = np.cross(points - T, X2 - T) > 0
>>> a,b ; a&b                              # individual arrays ; AND
(array([ True, False,  True, False]), array([False, False,  True, False]))
array([False, False,  True, False])

Finally, you might take some caution in a larger program to special case point pairs which are exactly the same point
",
76384948,Xdebug not stopping at breakpoints,"Xdebug does not stop at breakpoints.
I tried different versions of Xdebug. (current v1.32.1, v1.32.0,  v1.31.1, v1.31.0, v1.30.0)
This is my configuration at the launch.json file:
{
// Use IntelliSense to learn about possible attributes.
// Hover to view descriptions of existing attributes.
// For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
""version"": ""0.2.0"",
""configurations"": [
    {
        ""name"": ""Listen for Xdebug"",
        ""type"": ""php"",
        ""request"": ""launch"",
        ""port"": 9003
    },
    {
        ""name"": ""Launch currently open script"",
        ""type"": ""php"",
        ""request"": ""launch"",
        ""program"": ""${file}"",
        ""cwd"": ""${fileDirname}"",
        ""port"": 0,
        ""runtimeArgs"": [
            ""-dxdebug.start_with_request=yes""
        ],
        ""env"": {
            ""XDEBUG_MODE"": ""debug,develop"",
            ""XDEBUG_CONFIG"": ""client_port=${port}""
        }
    },
    {
        ""name"": ""Launch Built-in web server"",
        ""type"": ""php"",
        ""request"": ""launch"",
        ""runtimeArgs"": [
            ""-dxdebug.mode=debug"",
            ""-dxdebug.start_with_request=yes"",
            ""-S"",
            ""localhost:0""
        ],
        ""program"": """",
        ""cwd"": ""${workspaceRoot}"",
        ""port"": 9003,
        ""serverReadyAction"": {
            ""pattern"": ""Development Server \\(http://localhost:([0-9]+)\\) started"",
            ""uriFormat"": ""http://localhost:%s"",
            ""action"": ""openExternally""
        }
    }
]}

Could there be a conflict with the web server?
This is my configuration in the php.ini (all the way at the bottom of the file):
[xdebug]
zend_extension=C:\xampp\php\ext\php_xdebug.dll
xdebug.mode = debug
xdebug.start_with_request = yes
xdebug.client_port = 9003        // it also doesnt work without this line.

Installation Wizard
Summary from <https://xdebug.org/wizard> 

    • Xdebug installed: 3.2.1
    • Server API: Apache 2.0 Handler
    • Windows: yes 
    • Compiler: MS VS16
    • Architecture: x64
    • Zend Server: no 
    • PHP Version: 8.2.4
    • Zend API nr: 420220829
    • PHP API nr: 20220829
    • Debug Build: no
    • Thread Safe Build: yes
    • OPcache Loaded: no
    • Configuration File Path: no value
    • Configuration File: C:\xampp\php\php.ini
    • Extensions directory: C:\xampp\php\ext





I've downloaded the file from the wizard and renamed it correctly.
Port 9003 is correct according to the documentation. But I also tried port 9000 as well.
If I go to https://portchecker.co/checking and check for Port 9000 or 9003 they are closed.

I reinstalled XAMPP
I reinstalled VS Code

I also went to Settings -> Features -> Debug -> Allow Breakpoints Everywhere.
",76386605,"I went ahead and installed xampp and xdebug.
Our launch.json files and wizard output are identical and it seems to work ok for me.
My php.ini doesn't include the last line with the port number or the apostrophe after the xdebug closing brace that you have.
[xDebug]
zend_extension = xdebug
xdebug.mode = debug
xdebug.start_with_request = yes

You shouldn't need the entire file path for your zend_extension considering that the extensions directory is already mapped for you.
I would double check that you've defined your ""php.debug.executablePath"": in vscode settings as well.
Here's what mine looks like after

Running Listen for Xdebug in vscode
Navigating to the file I used here at http://localhost/info/index.php in the browser


If making those changes still doesn't work, maybe double check that the breakpoint is actually reachable.
",
76382864,How do I prevent 'NotImplementedError' and 'TypeError' when using numeric aggregate functions in Pandas pivot tables with string columns?,"I have tried severally to perform some numeric aggregation methods on numeric data with pandas. However, I have received a NotImplementedError, which then throws a TypeError, whenever I do so. I hypothesize that pandas is refusing to ignore the string columns when performing said numerical tasks. How do I prevent this?
Given a pivot table named matrix_data, and with pandas imported as pan:
  Account Number  Company      Contact Account Manager     Product  Licenses   
0         2123398   Google  Larry Pager    Edward Thorp   Analytics       150  
1         2123398   Google  Larry Pager    Edward Thorp  Prediction       150   
2         2123398   Google  Larry Pager    Edward Thorp    Tracking       300   
3         2192650     BOBO  Larry Pager    Edward Thorp   Analytics       150   
4          420496     IKEA    Elon Tusk    Edward Thorp   Analytics       300   

   Sale Price        Status  
0     2100000     Presented  
1      700000     Presented  
2      350000  Under Review  
3     2450000          Lost  
4     4550000           Won  

Trying to aggregate all numerical values by company:
pan.pivot_table(matrix_data, index = ""Company"", aggfunc=""mean"");

throws an exception like so:
NotImplementedError                       Traceback (most recent call last)
File ~\AppData\Roaming\Python\Python311\site-packages\pandas\core\groupby\groupby.py:1490, in GroupBy._cython_agg_general..array_func(values)
   1489 try:
-> 1490     result = self.grouper._cython_operation(
   1491         ""aggregate"",
   1492         values,
   1493         how,
   1494         axis=data.ndim - 1,
   1495         min_count=min_count,
   1496         **kwargs,
   1497     )
   1498 except NotImplementedError:
   1499     # generally if we have numeric_only=False
   1500     # and non-applicable functions
...
   1698             # e.g. ""foo""
-> 1699             raise TypeError(f""Could not convert {x} to numeric"") from err
   1700 return x

TypeError: Could not convert Larry PagerLarry PagerLarry Pager to numeric

dataframe.groupby([""col_name1""]).mean() will throw an identical error
I'm on windows 10, python 3.11, with pandas version 2.0.1. All this was performed on Jupyter Notebook with VScode
",76384737,"This has been deprecated in Pandas 2.0. This is the warning pandas 1.5.3 gives:

FutureWarning: pivot_table dropped a column because it failed to
aggregate. This behavior is deprecated and will raise in a future
version of pandas. Select only the columns that can be aggregated.

You now have to select the specific columns you want to aggregate.
cols = ['Licenses', 'Sale Price']
pd.pivot_table(matrix_data, values=cols, index=""Company"", aggfunc=""mean"")

",
76387117,Can I be guaranteed that a variable value will be passed along to a new running Task?,"Can I assume the following code will always pass my assertions? I'm a worried about the index value. I'm not sure if the scoped value will be passed along to the Task.Run lambda expression. I think it will be scoped just like the attributesChunked value seems to be. But I'd like some confirmation.
var tasks = new List<Task>();
var attributes = new string[4]{ ""att1"", ""att2"", ""att3"", ""att4"" };
var chunkIndex = -1;
foreach (var attributesChunked in attributes.Chunk(2))
{
    var index = Interlocked.Increment(ref chunkIndex);
    tasks.Add(Task.Run(() =>
    {
        if (index == 0)
             Assert.Equal(""attr1"", attributesChunked.First());
        if (index == 2)
             Assert.Equal(""attr3"", attributesChunked.First());
    }
}
await Task.WhenAll(tasks);

",76387222,"For each task you create, you are introducing a closure on the index variable declared in that iteration. So yes your Task will use the right value, as can be tested using following code:
static async Task Main(string[] args)
{
    var tasks = new List<Task>();
    var attributes = new string[4] { ""att1"", ""att2"", ""att3"", ""att4"" };
    var chunkIndex = -1;
    foreach (var attributesChunked in attributes.Chunk(2))
    {
        var index = Interlocked.Increment(ref chunkIndex);
        tasks.Add(Task.Run(async () =>
        {
            //simulate some work...
            await Task.Delay(50);
            Console.WriteLine($""index: {index}"");
        }));
    }
    await Task.WhenAll(tasks);
}

The result:

index: 0
index: 1

Note that the sequence of the output is entirely dependent on the scheduling of the individual Tasks and cannot be predicted...
Useful reference: What are 'closures' in .NET?
",
76384430,Strange behavior from CMake when importing a STATIC library,"I'm experiencing a strange (bug?) when importing the yaml-cpp static library with CMake.
main.cpp
#include ""yaml-cpp/yaml.h""

CMakeLists.txt (working)
add_library(yaml-cpp ${PROJECT_BINARY_DIR}/path/to/libyaml-cpp.a)
target_link_libraries(main yaml-cpp)

CMakeLists.txt (not working)
add_library(yaml-cpp STATIC IMPORTED)                                
set_target_properties(yaml-cpp PROPERTIES IMPORTED_LOCATION ${PROJECT_BINARY_DIR}/path/to/libyaml-cpp.a)  
target_link_libraries(main yaml-cpp)  

When I use the second CMakeLists.txt, my main.cpp cannot find yaml-cpp/yaml.h. When I use the first CMakeLists.txt, it can, however, I get the ""ar: no archive members specified"" message every time I configure the project, which is annoying. Would like to import it the second way to get rid of that message.
",76386630,"For both of you who answered, I appreciate it. Turns out I should have provided more information in my question. The issue was arising basically from the fact that I am attempting to create a portable installation, with the entire source of each of the dependencies within the project folder-structure, which is something that I haven't attempted before. It seemed logical to me that the library files (.a, .dylib, etc..) would contain all of the headers within them, but apparently that is not the case. I will provide a few details on how I was able to fix the issue.

Building the libraries before the rest of the project was the right move, but I forgot to install them. cmake -> make -> make install

BuildLibraries.txt (cmake file)
    set(yaml-cpp_cmakelists ""${CMAKE_SOURCE_DIR}/external/yaml-cpp-master"")
    set(yaml-cpp_build_location ""${CMAKE_BINARY_DIR}/external/yaml-cpp-master"")        
    file(MAKE_DIRECTORY ${yaml-cpp_build_location})  

    execute_process(                                                                     
        COMMAND ${CMAKE_COMMAND} -S ${yaml-cpp_cmakelists} -B ${yaml-cpp_build_location} -D CMAKE_INSTALL_PREFIX=${CMAKE_LIBRARY_OUTPUT_DIRECTORY} -D BUILD_SHARED_LIBS=OFF
        WORKING_DIRECTORY ${yaml-cpp_build_location}
        RESULT_VARIABLE result
    )
    if(NOT result EQUAL 0)
        message(FATAL_ERROR ""Failed to configure yaml-cpp"")
    endif()

    execute_process(                                                                     
        COMMAND make -C ${yaml-cpp_build_location} -j4
        WORKING_DIRECTORY ${yaml-cpp_build_location}
        RESULT_VARIABLE result
    )
    if(NOT result EQUAL 0)
        message(FATAL_ERROR ""Failed to generate yaml-cpp"")
    endif()

    execute_process(                                                                     
        COMMAND make install -C ${yaml-cpp_build_location} -j4
        WORKING_DIRECTORY ${yaml-cpp_build_location}
        RESULT_VARIABLE result
    )
    if(NOT result EQUAL 0)
        message(FATAL_ERROR ""Failed to install yaml-cpp"")
    endif()


Inside project-root directory CMakeLists.txt:

ensure that find_package() knows where to look for the libraries that you installed using set(CMAKE_PREFIX_PATH ...)
go ahead and use find_package(). It is, as these users suggested, much easier.



CMakeLists.txt (in root project directory)
       cmake_minimum_required(VERSION 3.26.0)                           
       set(CMAKE_CXX_STANDARD 17)
       set(CMAKE_CXX_STANDARD_REQUIRED ON)

       set(REBUILD_LIBS ON)                                             # choose wether to rebuild libraries during the generate phase

       set(PROJECT_BINARY_DIR ""${CMAKE_SOURCE_DIR}/build"")              # root build directory
       set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ""${PROJECT_BINARY_DIR}"")      # static libraries
       set(CMAKE_INSTALL_LIBDIR ${PROJECT_BINARY_DIR}/lib)
       set(CMAKE_INSTALL_BINDIR ${PROJECT_BINARY_DIR})
       set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ""${PROJECT_BINARY_DIR}"")      # shared libraries
       set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ""${PROJECT_BINARY_DIR}"")      # executables
       set(CMAKE_PREFIX_PATH                                            # define search-paths for find_package()
           ""${PROJECT_BINARY_DIR}/lib/cmake""
       )  

       if(REBUILD_LIBS)
           include(${CMAKE_SOURCE_DIR}/CMakeFiles/BuildLibraries.txt)   # Build external libraries using the BuildLibraries.txt CMakeLists.txt file
       endif()


       project(myProject)     
       add_executable(main main.cpp) 

       find_package (yaml-cpp)

       target_link_libraries(main yaml-cpp)  

",
76384877,Rshiny-Use slider values to cacluate a dataset and plot the calculated values,"I'm fairly new to Rshiny and looking for some help to understand how to create a plot using slider values as input. The user selected slider values are displayed as a table, and used as inputs to calculate an equation. The resulting calculated values are stored in a table (if possible I'd like to be able to download the generated values as a csv file) and used to generate a simple plot. This is what I have so far:
ui <- fluidPage(titlePanel(p(""title"", style = ""color:#3474A7""),
 sidebarLayout(
                sidebarPanel(
                    sliderInput(""Max"", ""Max:"",
                                min = 0, max = 1000,
                                value = 116.8, step=0.1),
                    sliderInput(""Rate"", ""Rate:"",
                                min = 0, max = 5,
                                value = 0.12, step=0.01),
                    sliderInput(""Inflection"", ""Inflection:"",
                                min = 0, max = 20,
                                value = 11.06, step=0.01),
                    sliderInput(""Area"", ""Area:"",
                                min = 0, max = 10000,
                                value = 180, step=20),
                         p(""Made with"", a(""Shiny"",
                        href = ""http://shiny.rstudio.com""), "".""),
                  ),
                  mainPanel(
                    
                    # Output: Table summarizing the values entered ----
                    tableOutput(""values""),
                    plotOutput(""plot"")
                    
                  )
                )
)


#use the slider values to estimate growth for any given year using the equation
Growth = function(x, A, B, C, R) 
{R *(A *exp(-B * C^x))} 


#create a Table Ouptut with selected Slider values 
server <- function(input, output){ 
  
  # Reactive expression to create data frame of all input values ----
  
  sliderValues <- reactive({
    data.frame(
      Name = c(""Max"",
               ""Inflection"",
               ""Rate"",
               ""Area""),
      Value = as.character(c(input$Max,
                             input$Inflection,
                             input$Rate,
                             input$Area)),
      stringsAsFactors = FALSE)
  })
  
# Show the values in an HTML table ----
  output$values <- renderTable({
    sliderValues()
  })
    
#reactive expression to let users download the data as a table 
##restC <- reactive({
  #run the code for all time with the selected parameters from the slider
  mylist<-list(c(1:50))
  mydata<-data.frame(lapply(mylist, Growth, A=values$Max, B=values$Rate, C=values$Inflection, R=values$Area),mylist)
  names(mydata)[1]<-""Pop""
  names(mydata)[2]<-""Time""
  
 #output$my_table<-renderDataTable({
   #restC() 
# })
#plot the values in a graph
 output$plot <- renderPlot({
  ggplot(mydata, aes(Time,Pop)) + geom_line()
})

 }

shinyApp(ui = ui, server = server)

",76386661,"Made some tweakings in your code; now it does what you want:
library(shiny)
library(ggplot2)

ui <- fluidPage(titlePanel(p(""title"", style = ""color:#3474A7"")),
                           sidebarLayout(
                             sidebarPanel(
                               sliderInput(""Max"", ""Max:"",
                                           min = 0, max = 1000,
                                           value = 116.8, step=0.1),
                               sliderInput(""Rate"", ""Rate:"",
                                           min = 0, max = 5,
                                           value = 0.12, step=0.01),
                               sliderInput(""Inflection"", ""Inflection:"",
                                           min = 0, max = 20,
                                           value = 11.06, step=0.01),
                               sliderInput(""Area"", ""Area:"",
                                           min = 0, max = 10000,
                                           value = 180, step=20),
                               downloadButton(""download"", ""Download data""),
                               p(""Made with"", a(""Shiny"",
                                                href = ""http://shiny.rstudio.com""), "".""),
                             ),
                             mainPanel(
                               
                               # Output: Table summarizing the values entered ----
                               tableOutput(""values""),
                               plotOutput(""plot"")
                               
                             )
                           )
)

#use the slider values to estimate growth for any given year using the equation
Growth = function(x, A, B, C, R) 
{R *(A *exp(-B * C^x))} 

#create a Table Ouptut with selected Slider values 
server <- function(input, output){ 
  
  # Reactive expression to create data frame of all input values ----
  
  sliderValues <- reactive({
    data.frame(
      Name = c(""Max"",
               ""Inflection"",
               ""Rate"",
               ""Area""),
      Value = as.character(c(input$Max,
                             input$Inflection,
                             input$Rate,
                             input$Area)),
      stringsAsFactors = FALSE)
  })
  
  # Show the values in an HTML table ----
  output$values <- renderTable({
    sliderValues()
  })
  
  #reactive expression to let users download the data as a table 
  restC <- reactive({
  #run the code for all time with the selected parameters from the slider
  mylist<-list(c(1:50))
  mydata<-data.frame(lapply(mylist, Growth, A=input$Max, B=input$Rate, C=input$Inflection, R=input$Area),mylist)
  names(mydata)[1]<-""Pop""
  names(mydata)[2]<-""Time""
  mydata
  })
  #output$my_table<-renderDataTable({
  #restC() 
  # })
  #plot the values in a graph
  output$plot <- renderPlot({
    ggplot(restC(), aes(Time,Pop)) + geom_line()
  })
  
  output$download <- downloadHandler(
    filename = function() {
      paste(""data-"", Sys.Date(), "".csv"", sep="""")
    },
    content = function(file) {
      write.csv(restC(), file)
    }
  )
  
}

shinyApp(ui = ui, server = server)


",
76382956,React - integrate custom Bootstrap theme,"I've been trying for couple days to integrate a poor Bootstrap theme template in a React app with no success.
So, I've created a new application in my folder. All good. Installed all the packages required by the theme and upgraded to the latest version. All good.

Now, let's customize the App.js component in React with some custom code:
function App() {
  return (
    <div className=""App"">
      <section className=""slice slice-lg delimiter-top delimiter-bottom"">
        <div className=""container"">
          <div className=""row mb-6 justify-content-center text-center"">
            <div className=""col-lg-8 col-md-10"">
              <span className=""badge badge-primary badge-pill"">
                What we do
              </span>
              <h3 className=""mt-4"">Leading digital agency for <span className=""text-warning typed"" id=""type-example-1"" data-type-this=""business, modern, dedicated""></span> solutions</h3>
            </div>
          </div>
          <div className=""row row-grid"">
            <div className=""col-md-4"">
              <div className=""pb-4"">
                <div className=""icon"">
                  <img alt=""Image placeholder"" src=""../../assets/img/svg/icons/Apps.svg"" className=""svg-inject img-fluid"" />
                </div>
              </div>
              <h5>Designed for developers</h5>
              <p className=""text-muted mb-0"">Quick contains components and pages that are built to be customized and used in any combination.</p>
            </div>
            <div className=""col-md-4"">
              <div className=""pb-4"">
                <div className=""icon"">
                  <img alt=""Image placeholder"" src=""../../assets/img/svg/icons/Ballance.svg"" className=""svg-inject img-fluid"" />
                </div>
              </div>
              <h5>Responsive and scalable</h5>
              <p className=""text-muted mb-0"">Scalable and easy to maintain, Quick enables consistency while developing new features and pages.</p>
            </div>
            <div className=""col-md-4"">
              <div className=""pb-4"">
                <div className=""icon"">
                  <img alt=""Image placeholder"" src=""../../assets/img/svg/icons/Book.svg"" className=""svg-inject img-fluid"" />
                </div>
              </div>
              <h5 className="""">Very well documented</h5>
              <p className=""text-muted mb-0"">No matter you are a developer or new to web design, you will find our theme very easy to customize with an intuitive code.</p>
            </div>
          </div>
        </div>
      </section>
    </div>
  );
}

export default App;

Now, let's import everything we need from the custom theme in index.js:
...

import './assets/css/quick-website.css';
import './assets/js/quick-website.js';
import './assets/libs/@fortawesome/fontawesome-free/css/all.min.css';
import './assets/libs/jquery/dist/jquery.min.js';

...

However, when I import the core JS (quick-website.js) file of the time, I get this types of errors:

From quick-website.js



From 'jquery.min.js`


What am I missing here?
",76384761,"Rather than importing jquery.min.js from assets, you should use npm to install the jquery package and then import relevant modules in the files where you need them. This is a more ""react"" way of doing things and it's much easier to update dependencies from the command line.

Run npm install jquery from the command line
Now, just import modules where you need them (in this case, quick-website.js):

import $ from 'jquery'

In general, if you see the error 'Something' is not defined, checking if you have imported the module is a good place to start.
",
76383351,Google.Apis.Oauth2.v2 + UWP: Access to path c:\users is denied,"After updating Google.Apis.Oauth2.v2 NuGet package to v. 1.60.0.1869, I start getting exception Access to the path C:\Users is denied when trying login with Google in my UWP app. Here's my code:
string fname = @""Assets\User\Auth\google_client_secrets.json"";
StorageFolder InstallationFolder = Windows.ApplicationModel.Package.Current.InstalledLocation;
var stream = await InstallationFolder.OpenStreamForReadAsync(fname);
credential = await GoogleWebAuthorizationBroker.AuthorizeAsync(
    stream,
    new[] { ""profile"", ""email"" },
    ""me"",
    CancellationToken.None);

The exception occurs in GoogleWebAuthorizationBroker.AuthorizeAsync call.
This code (with some light changes) worked well before with Google.Apis.Oauth2.v2 package v. 1.25.0.859, but now this package is obsolete and doesn't work anymore.
How to login with Google in my UWP app?
NOTE: I understand that UWP app doesn't have access to c:\Users, but my code never request anything in the folder. google_client_secrets.json exists and I can read it in the app from the stream, so this file is unrelated to the issue.
UPDATE
After I set the 5th parameter of AuthorizeAsync like this:
credential = await GoogleWebAuthorizationBroker.AuthorizeAsync(
    stream,
    new[] { ""profile"", ""email"" },
    ""me"",
    CancellationToken.None
    new FileDataStore(ApplicationData.Current.LocalCacheFolder.Path, true));

the exception is gone. Now the execution thread just dies inside the AuthorizeAsync and I start getting the following error popup:

",76384817,"After some tried, I failed to make Google.Apis.Oauth2.v2 NuGet package v. 1.60.0.1869 to work with UWP. I made Google Login works by removing the NuGet and implementing OAuth flow myself as described in my answer here.
",
76387092,What is the cause of the deadlock in my Go code using WaitGroups and Buffered Channels?,"WaitGroups, Buffered Channels, and Deadlocks
I have this bit of code which results in a deadlock and I'm not certain why. I have tried using mutex locking in a few different places, closing channels in and outside of separate go routines, but the result is still the same.
I'm trying to send data through one channel (inputChan), and then read it from another (outputChan)
package main

import (
    ""fmt""
    ""sync""
)

func listStuff(wg *sync.WaitGroup, workerID int, inputChan chan int, outputChan chan int) {
    defer wg.Done()

    for i := range inputChan {
        fmt.Println(""sending "", i)
        outputChan <- i
    }
}

func List(workers int) ([]int, error) {
    _output := make([]int, 0)

    inputChan := make(chan int, 1000)
    outputChan := make(chan int, 1000)

    var wg sync.WaitGroup
    wg.Add(workers)

    fmt.Printf(""+++ Spinning up %v workers\n"", workers)
    for i := 0; i < workers; i++ {
        go listStuff(&wg, i, inputChan, outputChan)
    }

    for i := 0; i < 3000; i++ {
        inputChan <- i
    }

    done := make(chan struct{})
    go func() {
        close(done)
        close(inputChan)
        close(outputChan)
        wg.Wait()
    }()

    for o := range outputChan {
        fmt.Println(""reading from channel..."")
        _output = append(_output, o)
    }

    <-done
    fmt.Printf(""+++ output len: %v\n"", len(_output))
    return _output, nil
}

func main() {
    List(5)
}

",76387227,"The code in your main function is sequential and first tries to write 3k values into inputChan then will read values from outputChan.
Your code blocks on the first of those steps:

nothing drains from outputChan before 3k values are succesfully sent to inputChan, so the workers end up stuck on outputChan <- i after the first 1k  value
once the workers stop draining from inputChan, main will get stuck on inputChan <- i after ~2k values


One way to fix this can be to have the producer (inputChan <- i) and the end consumer (for o := range outputChan {) run in separate goroutines.
You can keep one of these actors in the main goroutine, and spin a new one for the other. For example :
go func(inputChan chan<- int){
    for i := 0; i < 3000; i++ {
        inputChan <- i
    }
    close(inputChan)
}(inputChan)

done := make(chan struct{})
go func() {
    close(done)
    // close(inputChan) // I chose to close inputChan above, don't close it twice
    close(outputChan)
    wg.Wait()
}()

...

https://go.dev/play/p/doBgfkAbyaO
one extra note: the order of actions around signaling done is important ; channels done and outputChan should only be closed after wg.Done() indicates that all workers are finished
    // it is best to close inputChan next to the code that controls
    // when its input is complete.
    close(inputChan)
    // If you had several producers writing to the same channel, you
    // would probably have to add a separate waitgroup to handle closing,
    // much like you did for your workers

    go func() {
        wg.Wait()
        // the two following actions must happen *after* workers have
        // completed
        close(done)
        close(outputChan)
    }()

",
76387085,mat-select panel still shows overlapped like version 14 and below even after upgrade to angular material 15,"we have used mat-select in our project, the panel shows overlapping on the trigger text like version 14 and before (material version 12 was used in our project)

after finding few hacks, the panel showed below the trigger text, but it was not consistent with different screen sizes, specially the mobile view.
We found that from material version 15, the select panel showed below the trigger text - 
so we upgraded to material version 15, and also upgraded the angular version to 15. But even after the upgrade the mat-select is still showing in the previous style.
Can someone please suggest what could be going wrong here and what needs to be done to get working like material version 15 mat-select
",76387252,"Based on the comments, you have only updated to Angular material 15, but is running in legacy mode.
To fully migrate you need to run.
schematic: ng generate @angular/material:mdc-migration
however due to class name changes to the mdc- prefix etc and structual changes in some of the componets. you should follow their migration guide.
https://rc.material.angular.io/guide/mdc-migration
",
76383522,"ASP.NET Core 7.0 container: Failed to create CoreCLR, HRESULT: 0x8007000E","That's a result when I receive after trying to run ASP.NET Core 7.0 runtime image in Amazon ECS container (AWS Fargate service).
My project in on ASP.NET Core 7.0 Web API.
Here is a docker file for image which built on Jenkins and sent to Amazon ECS
FROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build
WORKDIR /src
COPY [""src/MetaGame/MyMetaGame.API/MyMetaGame.API.csproj"", ""src/MetaGame/MyMetaGame.API/""]
RUN dotnet restore ""src/MetaGame/MyMetaGame.API/MyMetaGame.API.csproj""
COPY . .
WORKDIR ""/src/src/MetaGame/MyMetaGame.API""
RUN dotnet build ""MyMetaGame.API.csproj"" -c Release -o /app/build

FROM build AS publish
RUN dotnet publish ""MyMetaGame.API.csproj"" -c Release -o /app/publish /p:UseAppHost=false

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [""dotnet"", ""MyMetaGame.API.dll""] 

After some googling I've found a decision to increase GC Heap limit by environment parameter. I set it to 512Mb
DOTNET_GCHeapHardLimit=20000000

But it hasn't fixed my problem.
No idea what's the problem
Here is the configuration of csproj file
<PropertyGroup>
    <TargetFramework>net7.0</TargetFramework>
    <Nullable>enable</Nullable>
    <ImplicitUsings>enable</ImplicitUsings>
    <DockerDefaultTargetOS>Linux</DockerDefaultTargetOS>
</PropertyGroup>

",76384890,"So, the problem is fixed by adding
ENV COMPlus_EnableDiagnostics=0

in the final stage of Dockerfile.
So, it should looks like
FROM base AS final
ENV COMPlus_EnableDiagnostics=0
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [""dotnet"", ""MyMetaGame.API.dll""]

Found the solution here
",
76384192,How to make sure that my virtual machine does not change its IP address when connecting to different networks?,"I have two virtual machines. First has apache server, and wordpress on virtualhost. Second has mysql server for my wordpress in first vm. Mysql configuration use ip adress of first vm. But when i leave my home and in result leave my home wi-fi, ip adress of first vm is changing when i connect to internet from my phone and my web-site crashes.
",76386662,"It is said that by default IP addresses are assigned dynamically, so the IP changes if you are in different network (that is, when you access the Internet through mobile data instead of home wifi). You may assign static IP address to your VM. This is done from inside of your VM.
",
76384852,How can I change the size of the text in TextComponent using Flutter Flame?,"How do you change the size of the text in the TextComponent in Flame?  I want to make the text larger
I used the Flame documentation to get this, but I don't know how to modify it for a larger text size (like 20pt vs 14pt).  Also, what is the anchor?
final style = TextStyle(color: BasicPalette.darkBlue.color);
final regular = TextPaint(style: style);

TextComponent startText = TextComponent(text: 'test text', textRenderer: regular)
  ..anchor = Anchor.topCenter
  ..x = (width * 0.2)
  ..y = (height - (height*0.5))
  ..priority = 300;

",76386672,"final style = TextStyle(
  color: BasicPalette.darkBlue.color,
  fontSize: 20.0, // Change the font size here
);
final regular = TextPaint(style: style);

TextComponent startText = TextComponent(
  text: 'test text',
  textRenderer: regular,
)
  ..anchor = Anchor.topCenter
  ..x = (width * 0.2)
  ..y = (height - (height * 0.5))
  ..priority = 300;

What's anchor?
The anchor in Flame's TextComponent determines where the text is positioned relative to its given coordinates (x and y). You can choose from options like top-left, top-center, top-right, center-left, center, center-right, bottom-left, bottom-center, and bottom-right to align the text as you want. Adjust the anchor and position values to position the text as needed.
",
76384267,How to clear cache for a route with the 'apicache' package,"I'm using NPM install API cache for my MERN application. I want one of my requests to be cached and then when a request is made to another route it refreshes the cache (it fetches the new data). I've managed to get one of my requests to be cached but I can't manage to clear it.
This is my code
const apicache = require(""apicache"")

let cache = apicache.middleware

app.get(""/api/users/checkJWT"",cache(""2 minutes""), async (req,res) => {
  const token = req.headers.authorization?.split("" "")[1]

  const decoded = jwt.verify(token,SECRET_KEY)

  if(decoded != null && await User.findById(decoded.id) != null) {
    return res.status(200).json({valid: true, user: await User.findById(decoded.id)})
  }
  else {
    return res.status(400).json({valid: false})
  }
})

app.patch(""/api/users/user"",authenticateJWTUser, async (req, res) => {
  apicache.clear('/api/users/checkJWT')
  const user = await User.findOne({ name: req.body.username });
  if (req.body.name != null) {
    user.name = req.body.name;
  }
  if (req.body.email != null) {
    user.email = req.body.email;
  }
  if (req.body.password != null) {
    user.password = await bcrypt.hash(req.body.password, 10);
  }
  if (req.body.tasks != null) {
    user.tasks = req.body.tasks;
  }
  try {
    const updatedUser = await user.save();
    res.json(updatedUser);
  } catch (error) {
    res.status(400).json({ message: error.message });
  }
});

Link to my Git repo: https://github.com/JesseOgunlaja/Task-Tracker-MERN/tree/ae1b0027bfa40f90ab7a3d6208c05a9eb41a4478
",76386693,"I was able to get this work using the apicache-plus package.
const apicache = require(""apicache-plus"");

router.get(
  ""/api/users/getName/1"",
  apicache.middleware(""10 minutes""),
  async (req, res, next) => {
    req.apicacheGroup = ""toClear"";
    const someData = { someName: ""Amy"" };
    res.json(someData);
  }
);

router.get(""/api/users/user/1/2"", async (req, res, next) => {
  console.log(""something"");
  apicache.clear(""toClear"");

  const user = { name: ""Jeff"" };
  res.json(user);
});

",
76387202,Razor page redirect to another folder page,"I'm facing an issue with razor page navigation. I'm in the https://localhost:7154/Application/setupAccount page and i have to redirect to https://localhost:7154/Loan/setupLoan page.
So I used RedirectToPage(""setupLoan"", new{id=123}. but the issue is that it's looking like a setupLoan page inside the Application folder. ( URL - https://localhost:7154/Application/setupLoan )
Then I'm getting an error like page not found
I can use redirect functionality. But the thing is, I can't send dynamic query parameters.
So let me know if anyone know a correct path to navigate razor page into another folder
",76387264,"Try:
   return RedirectToPage(""setupLoan"", new { area = ""Application"",id=1 }); 

result:

You can read Areas with Razor Pages to know more.
Update
 return RedirectToPage(""setupLoan"", new { area = ""Loan"",id=1 }); 

Update 2
Loan is a page folder. not area folder ,
try:
  return RedirectToPage(""/Loan/setupLoan"", new { id = 1 });

",
76383257,Is there a way to stop old observable and set new after event in Nest Gateways?,"I'm developing a simple message server app, using WebSockets and NestJs gateways. Server receives an event to indentify a listener with name, and then sends websocket updates to that listener.
Right now I'm using subject/observable approach, server after listen-as event returns a new observable, which filters all upcoming messages:
export class ListenAsDto {
  name: string;
}

export interface PersonalisedMessage {
  from: string;
  title: string;
  content: string;
}

export class MessagesService {
  // ...
  listenAs(listener: ListenAsDto): Observable<WsResponse<PersonalisedMessage>> {
    return this.messageObservable.pipe(
      filter((item) => item.to === listener.name),
      map((item) => {
        const copy = { ...item };
        delete copy.to;
        return { event: INBOX_MESSAGE_NAME, data: copy };
      }),
    );
  }
}

But this only works for the first time, after another request client now receives both updates from 2 users at the time.
Is there a way to stop previous observable for current user an start new?
EDIT: I'm using ws platform
",76384992,"Found a solution by using a native WebSocket instance. It passes the same object if request was made by same client. So, using a WeakMap, we can manually track all subscriptions objects, and unsubscribe them, if we get the same WebSocket client
export class MessagesService {
  private subs = new WeakMap<WebSocket, Subscription>();

  listenAs(ip: WebSocket, dto: ListenAsDto) {
    if (this.subs.has(ip)) this.subs.get(ip).unsubscribe();
    // 'createObservable' is basically question`s 'listenAs' function
    const obs = this.createObservable(dto);
    this.subs.set(
      ip,
      obs.subscribe({
        next(value) {
          ip.send(JSON.stringify(value));
        },
      }),
    );

    return dto;
  }
}

",
76385199,How to start Jupyter Notebook on a GCP VM with predefined token automatically on VM start?,"OK I have set up jupyter notebook on a gcp VM, I want it to start automatically every time VM is started with a predefined token. (OS in Centos)
Does any one have any idea how to solve this final issue so that we can get the jupyter notebook started under the user test1 , everytime the VM starts ?
Here is what I have done so far

Created a local test1 user and created a Virtual env for it, installed jupyter notebook inside the virtual env for isolation of python

pyton3 -m venv tvenv
source tvenv\bin\activate
pip install jupyter


Understood that I would need to generate config and update token / ip / port etc inside the config becuase dynamically generated token cannot be used

jupyter notebook --generate-config 


modified NotebookApp.token/NotebookApp.port/NotebookApp.ip etc attributes to get that to work.
based on an existing question created a shell script to be added to the start up to the gcp VM

sudo -u test1 bash -c 'cd ~/; nohup tvenv/bin/jupyter-notebook &'

When i run the command manually from root it works when I update in GCP startup it constantly fails saying
Account or password is expired, reset your password and try again

Note : Helpful debugging tip. You can get the startup script logs by running below command once inside the VM
sudo journalctl -u google-startup-scripts.service

",76386831,"As John has already pointed out to you in the comment, sudo is not a good option, since the Startup scripts run as root, su can be used to switch to another profile.
Few other things that you should be aware of is that as you are using virtual enviornment, you have to set up similar enviornment variables as 'activate'
does.
one possible solution that you can try is below
 su -c 'PATH=""/home/test1/tvenv/bin:$PATH"";export PATH; cd ~/; nohup tvenv/bin/jupyter-notebook &' -s /bin/sh test1

What it does is sets the path to the virtual enviornment ( activity that is done via the activate among other tasks ) moves to home directory of the user and runs the notebook in background with test1 user profile.
I would also suggest that you use shutdown script in the gcp vm to shutdown notebook gracefully, make sure that you replace the port with the port that you are using in your config
jupyter notebook stop 8888

Similar to start up you can check the shutdown logs by using the command below
sudo journalctl -u google-shutdown-scripts.service

Let me know if you still face any issues.
",
76383937,How do I remove deleted Python environments in PyCharm?,"When I'm creating a new project in PyCharm, I can see some environments I created for old projects that have been deleted:

but I can't find a way to delete these entries. Where are they? How do I delete them?
I tried going to ""Python Interpreters"" following https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#removing_interpreter but they are just not there:


",76388079,"The Interpreters list from the New Project dialogue has a different entry point from the project specific list. Go to File > New Projects Setup > Settings for New Projects (instead of going to File > Settings as usual):

Going through the other entry point you can already notice the Python Interpreter item in the sidebar is slightly different visually from if you had gone the other route. From there on the renaming dialogues are the same as usual but the lists are different. Continue to Python Interpreter > Show All... at the bottom of the drop down list:

",
76387278,How to remove 3 dots after end of line limit,"In SwiftUI,
Text(""a \n b \n c \n d \n e"")
     .lineLimit(3)

In SwiftUI, the above code shows output including 3 dots in the end.
Output:
a
b
c...

But my target is to show the output without dots like this -
Target:
a
b
c

",76387294,"implement the following way.
Text(""1\n 2 \n 3 \n 4 \n 5"".truncateToLineLimit(3))

extension String {
    func truncateToLineLimit(_ lineLimit: Int) -> String {
        var truncatedString = """"
        let lines = self.components(separatedBy: ""\n"").prefix(lineLimit)
        for line in lines {
            truncatedString += line.trimmingCharacters(in: .whitespacesAndNewlines)
            if line != lines.last {
                truncatedString += ""\n""
            }
        }
        return truncatedString
    }
}


",
76382914,How to remove gap between a frame and its scrollbar?,"I have three frames; left_frame, middle_frame and right_frame. I would like to have no gap between left_frame and its scrollbar. How can I achieve it?
import tkinter as tk
from tkinter import ttk

def create_dummy_chart(frame, index):
    # Create a dummy chart
    label = tk.Label(frame, text=f""Chart {index}"", width=10, height=2, relief=""solid"")
    label.pack(side=""top"", pady=5)

# Create the main window
window = tk.Tk()
window.title(""Chart Display"")
window.geometry(""800x300"")

# Create the left frame
left_frame = tk.Frame(window, width=200, height=300, relief=""solid"", bd=1)
left_frame.pack(side=""left"", fill=""y"")

# Create a canvas to hold the left_frame and the scrollbar
canvas = tk.Canvas(left_frame, width=200, height=300)
canvas.pack(side=""left"", fill=""y"")

# Create the scrollbar
scrollbar = ttk.Scrollbar(left_frame, orient=""vertical"", command=canvas.yview)
scrollbar.pack(side=""right"", fill=""y"")

# Configure the canvas to use the scrollbar
canvas.configure(yscrollcommand=scrollbar.set)
canvas.bind(""<Configure>"", lambda e: canvas.configure(scrollregion=canvas.bbox(""all"")))

# Create a frame inside the canvas to hold the charts
chart_frame = tk.Frame(canvas)

# Add the frame to the canvas
canvas.create_window((0, 0), window=chart_frame, anchor=""nw"")

# Create dummy charts in the chart_frame
num_charts = 200
for i in range(num_charts):
    create_dummy_chart(chart_frame, i)

# Update the canvas scrollable area
chart_frame.update_idletasks()
canvas.configure(scrollregion=canvas.bbox(""all""))

# Create the middle frame
middle_frame = tk.Frame(window, width=200, height=300, relief=""solid"", bd=1)
middle_frame.pack(side=""left"", fill=""y"")

# Create the right frame
right_frame = tk.Frame(window, width=400, height=300, relief=""solid"", bd=1)
right_frame.pack(side=""left"", fill=""both"", expand=True)

# Run the Tkinter event loop
window.mainloop()


I have left_frame and vertical scrollabr. But, I ended up having a gap between them. I expect to have no gap between them. How can I achieve this?
What I have:

What I expect:

",76385022,"If you're referring to the one or two pixel space between the canvas and the scrollbar, you can set the highlightthickness attribute of the canvas to zero, or you can set the highlightcolor to the same color as the background of the canvas so that the highlight ring is not visible when the canvas has the keyboard focus.
You might also need to explicitly set the padx value to zero when packing the canvas and scrollbar. On some platforms it might default to 1.
If you're referring to the fact that the canvas is wider than the items in the canvas, then you can set up a binding to set the width of the canvas to always match the width of the items on the canvas.
You can do that like this:
chart_frame.bind(""<Configure>"", lambda event: canvas.configure(width=chart_frame.winfo_width()))

",
76385290,What is the point of the ILoggingBuilder interface in ASP.NET Core?,"I looked inside the the built in WebApplicationBuilder class and noticed the
public ILoggingBuilder Logging { get; }

property. Here is the entire ILoggingBuilder interface:
public interface ILoggingBuilder
{
    IServiceCollection Services { get; }
}

It just stores a single property so what is the point of this interface? Couldn't the WebApplicationBuilder just store an instance of IServiceCollection directly?
",76388158,"It's about scoping and reducing intellisense hell. If everything was on IServiceCollection it would get cluttered quickly. Subsystems that are highly configurable and are themselves extensible need a ""target"" to extend. This is one of the patterns the platform employs to solve this problem.
Some of the patterns you'll see:
Simple options via a callback:
services.AddComponent((ComponentOptions options) => 
{
    options.SomeFlag = true;
});

This pattern works well when the number of options on component is small, self-contained and has limited extensibility.
Complex builder returned from the API:
IComponentBuilder builder = services.AddComponent();

builder.AddExtensibleThing1();
builder.AddExtensibleThing2();

Breaks the fluent API pattern but reduces nested lambdas.
OR
services.AddComponent((IComponentBuilder builder) =>
{
    builder.AddExtensibleThing1();
    builder.AddExtensibleThing2();
});

Similar to the above but nested instead of a return value. Keeps the fluent API working at the cost of more callbacks.
",
76387293,How to seach text not translate in VSCode?,"I'm added i18next React for old project. So many text not can be translate. How can I search all of them in VSCode?
Some case:
<Button className=""w-full"" onClick={onClick}>
  Save
<Button>

<Button
  type=""primary""
  onClick={onCLick}
  className=""ml-2""
>
  Save
</Button>

<Button>Save</Button>

<ContentWrapper title=""Connection"" headerAction={<Button size=""small"">Add</Button>}>
  <div>{`Connection`}</div>
</ContentWrapper>

<p className=""mt-2 text-center"">
  Drag and drop files here or <span className=""text-blue-500"">{t('browse')}</span>
</p>

<div>
  {mediaRecorder?.state === 'recording' ? (
    <Button onClick={handleStop}>{t('Stop')}</Button>
  ) : (
    <Button onClick={handleStart}>Start</Button>
  )}
</div>

Translated:
<Button>{t('Save')}</Button>

<ContentWrapper title=""Connection"" headerAction={<Button size=""small"">{t('Add')}</Button>}>
      <div>{t('Connection')}</div>
    </ContentWrapper>

",76387324,"Find:
(<Button[\s\S\n]*?>[\n\s]*)(\w+)([\n\s]*</Button>)

Replace:
$1{t('$2')}$3

",
76384368,Bokeh plot is missing in layout when checkbox group is used,"Bokeh scatter plot disappears when checkbox is added to layout.
If I exclude layout = row([checkbox_group, p]) and do show(p), I get the intended scatter plot with the color bar.
But when I include layout = row([checkbox_group, p]) and do show(layout), the scatter plot disappears whereas the checkbox and color bar appear.
import pandas as pd
import numpy as np
from bokeh.plotting import figure, show, curdoc
from bokeh.models import ColumnDataSource, ColorBar, HoverTool, CustomJS
from bokeh.models.widgets import CheckboxGroup
from bokeh.transform import linear_cmap
from bokeh.palettes import Iridescent18
from bokeh.models.mappers import LinearColorMapper
from bokeh.layouts import row

gene_list = ['A', 'B', 'C']
file_paths = [r""home/File1.feather"", r""home/File2.feather""]
checkbox_group = CheckboxGroup(labels=['File 1', 'File 2'], active = [0])

def checkbox_change(attr, old, new):
    if new:
        selected_file_index = new[0] 
        if selected_file_index >= len(file_paths):
            print(""Selected file index is out of range."")
            return
        selected_file_path = file_paths[selected_file_index]
        df = pd.read_feather(selected_file_path, columns=gene_list + ['umap1', 'umap2', 'index'])
        df = df.replace({'index' : 'cell_type'})

        median_score = []
        for idx, r in df.iterrows():
            score = np.sum(r[gene_list])
            median_score.append(score)
        df['score'] = median_score

        source.data = df.to_dict(orient='list')

    checkbox_group.on_change('active', checkbox_change)

# Create the initial plot
df = pd.read_feather(file_paths[0], columns=gene_list + ['umap1', 'umap2', 'index'])

median_score = []
for idx, r in df.iterrows():
    score = np.sum(r[gene_list])
    median_score.append(score)
df['score'] = median_score

source = ColumnDataSource(df)

mapper = linear_cmap(
    field_name='score', palette=Iridescent18, low=df['score'].min(), high=df['score'].max()
)

p = figure(
    title='UMAP Visualization',
    x_axis_label='umap1',
    y_axis_label='umap2',
    sizing_mode='stretch_width',
    height=1500,
    toolbar_location='above'
)
hover = HoverTool(tooltips=[('Cell Name', '@index')], mode='mouse')
p.add_tools(hover)

p.scatter(""umap1"", ""umap2"", color=mapper, source=source)

color_mapper = LinearColorMapper(
    palette=Iridescent18, low=df['score'].min(), high=df['score'].max()
)
color_bar = ColorBar(
    color_mapper=color_mapper, label_standoff=12, location=(0, 0), title='Score'
)
p.add_layout(color_bar, 'right')

layout = row([checkbox_group, p])

show(layout) #show(p)

",76388195,"Your problem comes from the selected sizing_modes in the figure and the row function calls.
Your are setting the sizing_mode of the figure to stretch_width and you are using the default sizing_mode of the row-layout which is fixed. This leads to a behavoir where the figure is shrunk to a minimal width of 0.
To fix this, you can

set the sizing_mode of the row to stretch_width or
set the figure to a fixed width

Minimal Example
source = ColumnDataSource(dict(A=[1,2,3], B=[1,2,3], score=[2,4,6]))

p = figure(sizing_mode='stretch_width')
mapper = linear_cmap(
    field_name='score', palette=Iridescent18, low=df['score'].min(), high=df['score'].max()
)
p.scatter(""A"", ""B"", color=mapper, source=source)
checkbox_group = CheckboxGroup(labels=['Checkbox 1', 'Checkbox 2'], active = [0])
color_mapper = LinearColorMapper(palette=Iridescent18, low=1, high=3)
color_bar = ColorBar(
    color_mapper=color_mapper, label_standoff=12, location=(0, 0), title='Score'
)
p.add_layout(color_bar, 'right')
layout = row([checkbox_group, p], sizing_mode='stretch_width')
show(layout)

Output

Comment
The output was created with bokeh 3.1.1. I am not sure if this will work for older versions.
",
76378492,How to calculate rotation matrix for an accelerometer using only basic algebraic operations,"There is a C function that gets the acceleration values of x,y,z of an accelerometer sensor (MEMS IMU) as input, and calculates the rotation matrix in a way that the z axis is aligned with the gravity. It is being used for calibrating the accelerometer data.

#define X_AXIS (0u)
#define Y_AXIS (1u)
#define Z_AXIS (2u)

static float matrix[3][3];
void calculate_rotation_matrix(float raw_x, float raw_y, float raw_z)
{ 
  const float r = sqrtf(raw_x * raw_x + raw_y * raw_y + raw_z * raw_z);
  const float x = raw_x / r;
  const float y = raw_y / r;
  const float z = raw_z / r;

  const float x2 = x * x;
  const float y2 = y * y;

  matrix[X_AXIS][X_AXIS] = (y2 - (x2 * z)) / (x2 + y2);
  matrix[X_AXIS][Y_AXIS] = ((-x * y) - (x * y * z)) / (x2 + y2);
  matrix[X_AXIS][Z_AXIS] = x;
  matrix[Y_AXIS][X_AXIS] = ((-x * y) - (x * y * z)) / (x2 + y2);
  matrix[Y_AXIS][Y_AXIS] = (x2 - (y2 * z)) / (x2 + y2);
  matrix[Y_AXIS][Z_AXIS] = y;
  matrix[Z_AXIS][X_AXIS] = -x;
  matrix[Z_AXIS][Y_AXIS] = -y;
  matrix[Z_AXIS][Z_AXIS] = -z;
}



float result[3];
void apply_rotation(float x, float y, float z)
{
  result[AXIS_X] = matrix[X_AXIS][X_AXIS] * x
                 + matrix[X_AXIS][Y_AXIS] * y
                 + matrix[X_AXIS][Z_AXIS] * z;
  
  result[AXIS_Y] = matrix[Y_AXIS][X_AXIS] * x
                 + matrix[Y_AXIS][Y_AXIS] * y
                 + matrix[Y_AXIS][Z_AXIS] * z;
  
  result[AXIS_Z] = matrix[Z_AXIS][X_AXIS] * x
                 + matrix[Z_AXIS][Y_AXIS] * y
                 + matrix[Z_AXIS][Z_AXIS] * z;

}

I'm trying to wrap my head around how it works and why there is no use of trigonometric functions here?
is it just simplifying the trigonometric functions by normalizing the input values and using the equivalent equations to calculate the trigonometric functions?
What are the limitations of this method? for example when the denominator calculated is zero, we will have division by zero. Anything else?
Tried to search on the internet and stackoverflow, but couldn't find a similar method to calculate the rotation matrix.
UPDATE:
Just simplified the calculations so they are more readable.
To add more context this code is used to rotate the readings of an accelerometer in a way that regardless of the orientation of the device, the z-axis is perpendicular to the ground.
The calculate_rotation_matrix() is called when we know that the object is stationary and is on a flat surface. This results in calculating the 3x3 matrix. Then the apply_rotation() is used to rotate subsequent readings.
",76385025,"A quaternion representation can apply rotations without trig functions.  But this appears to be a version of: https://math.stackexchange.com/a/476311 . The math appears to be a variation thereof, where ""a"" and ""b"" are the accelerometer and gravity vectors.
The method also appears to assume measurements will not be perfect.  And MEMS sensors fit that description.  Otherwise, as you stated, if x and y are both zero then you have a divide by zero condition.
",
76384085,Kotlin Firebase Realtime Database correct return value for setValue() in MVVM,"What would be the correct return value of my setMyBirthday function in the HomeRepository class for the code to work? Void doesn't seem to be the right return value.
Fragment:
private fun HomeViewModel.setupObserver() {
    myBirthday.observe(viewLifecycleOwner) { response ->
        when(response) {
            is Resource.Error -> {
                response.message?.let { message ->
                    Log.e(TAG, ""An error occurred: $message"")
                }
            }
            is Resource.Loading -> {
                binding.buttonChangeBirthday.isClickable = false
            }
            is Resource.Success -> {
                binding.buttonChangeBirthday.isClickable = true
            }
        }
    }
}

HomeViewModel:
val myBirthday: MutableLiveData<Resource<Void>> = MutableLiveData()
fun setMyBirthday(birthday: String) = viewModelScope.launch {
    try {
        myBirthday.postValue(Resource.Loading())
        val response = homeRepository.setMyBirthday(birthday)
        myBirthday.postValue(Resource.Success(response))
    } catch (e: Exception) {
        myBirthday.postValue(Resource.Error(e.message!!))
    }
}

HomeRepository:
    suspend fun setMyBirthday(birthday: String) =
    databaseReference
        .child(""users"")
        .child(myUserID)
        .child(""birthday"")
        .setValue(birthday)
        .await()

",76388372,"
What would be the correct return value of my function in the HomeRepository class for the code to work? Void doesn't seem to be the right return value.

Indeed Void is the type of object the setMyBirthday function returns. So your function should look like this:
//                                            👇
suspend fun setMyBirthday(birthday: String): Void =
databaseReference
    .child(""users"")
    .child(myUserID)
    .child(""birthday"")
    .setValue(birthday)
    .await()

While your approach will work, it will be more convenient to return a Resource<Boolean> as in the following lines of code:
suspend fun setMyBirthday(birthday: String): Resource<Boolean> = try {
    databaseReference
        .child(""users"")
        .child(myUserID)
        .child(""birthday"")
        .setValue(birthday)
        .await()
    Resource.Success(true)
} catch (e: Exception) {
    Resource.Error(e)
}

This is more useful because you can pass the error further, otherwise, you won't know if something fails.
",
76387171,Beautiful Soup Img Src Scrape,"Problem: I am trying to scrape the image source locations for pictures on a website, but I cannot get Beautiful Soup to scrape them successfully.
Details:

Here is the website

The three images I want have the following HTML tags:

<img src=""https://ik.imagekit.io/02fmeo4exvw/exercise-library/large/14-1.jpg"" style=""display: none;"">
<img src=""https://ik.imagekit.io/02fmeo4exvw/exercise-library/large/14-2.jpg"" style=""display: none;"">
<img src=""https://ik.imagekit.io/02fmeo4exvw/exercise-library/large/14-3.jpg"" style=""display: none;"">



Code I've Tried:

soup.find_all('img')
soup.select('#imageFlicker')
soup.select('#imageFlicker > div')
soup.select('#imageFlicker > div > img:nth-child(1)')
soup.find_all('div', {'class':'exercise-post__step-image-wrap'})
soup.find_all('div', attrs={'id': 'imageFlicker'})
soup.select_all('#imageFlicker > div > img:nth-child(1)')

The very first query of soup.find_all('img') gets every image on the page except the three images I want. I've tried looking at the children and sub children of each of the above, and none of that works either.
What am I missing here? I think there may be javascript that is changing the css display attribute from block to none and back so the three images look like a gif instead of three different images. Is that messing things up in a way I'm not understanding? Thank you!
",76387347,"The content is provided dynmaically via JavaScript, but not rendered by requests per se, unlike in the browser.
However, you can search for the JavaScript variable:
var data = {""images"":[""https://ik.imagekit.io/02fmeo4exvw/exercise-library/large/14-1.jpg"",""https://ik.imagekit.io/02fmeo4exvw/exercise-library/large/14-2.jpg"",""https://ik.imagekit.io/02fmeo4exvw/exercise-library/large/14-3.jpg""],""interval"":600};

with regex re.search() and convert its content string with json.loads() to JSON, so that you can access it easily.
Example
import requests
import re, json

url = 'https://www.acefitness.org/resources/everyone/exercise-library/14/bird-dog/'

json.loads(re.search(r'var data = (.*?);', requests.get(url).text).group(1))['images']

",
76383602,Javascript event.offset alternative for final value during CSS scale & transition,"I have an HTML image that I want to pan & zoom programatically. In order to improve the experience, I added a CSS transition for smoothness.
When I click on the image, I need to determine the mouse position within the image. Currently, event.offsetX gives me the mouse position within the image at the current frame of the animation.
I would like to know the mouse position as if the animation finished already (even though it hadn't).
Here is another way of explaining the problem. I created below an example where an image zooms in after we click on the button. The zoom takes 5 seconds. During the zoom, if I keep my mouse fixed and keep clicking, the offset value changes as the image moves on the screen and then it stabilizes after 5 seconds and returns the same offset value. I would like to know that final offset value before the animation finishes if possible.


<button onclick=""onButton()"">Zoom</button>
<br>
<img
    id='img'
    onclick=""onImage(event)""
    src=""https://picsum.photos/id/237/200""
    style=""transition: 5s ease""
>

<script>
function onButton() {
    const img = document.querySelector('#img')
    img.style.scale = 5.0
}
function onImage(event) {
    console.log(event.offsetX, event.offsetY)
}
</script>



",76385062,"If you don't mind adding some additional HTML and CSS:


function onButton() {
  const img = document.querySelector(""#img"");
  img.style.scale = 5.0;
  const target = document.querySelector(""#target"");
  target.style.scale = 5.0;// make sure target's style matches img's style
}
function onImage(event) {
  console.log(event.offsetX, event.offsetY);
}
#wrapper {
  display: inline-block;
  position:relative;
  font-size:0;
}
#target{
  position:absolute;
  width:100%;
  height:100%;
}
#img {
  transition: 5s ease;
  z-index:2;
  pointer-events:none;
}
<button onclick=""onButton()"">Zoom</button>
<br>
<div id=""wrapper"">
  <div id=""target"" onclick=""onImage(event)""></div>
  <img id=""img"" src=""https://picsum.photos/id/237/200"">
</div>



This work by zooming an invisible div behind the image without transition, and transfer the click event to that div instead of the image itself
",
76383390,How to ignore safe area for a background with a linear gradient and image in swiftUI?,"I have a VStack like this:
    var body: some View{
        VStack{
            //some view
        }
        .background(
            ZStack{
                LinearGradient(gradient: Gradient(
                    colors: [
                        .yellow_500,
                        .yellow_500,
                        .yellow_500,
                        .yellow_50
                    ]), startPoint: .top, endPoint: .bottom)
                .ignoresSafeArea(.container, edges: [.top])
                
                
                Image(R.image.home_bg_try.name)
                    .resizable()
                    .scaledToFill()
                    .frame(maxWidth: .infinity, maxHeight: .infinity)
                    .ignoresSafeArea(.container, edges: [.top])

            }
//                .offset(y: -20)
        )
    }


I want the image to be displayed without considering the safe area, which currently results in a white space appearing at the top, as shown in this image: (https://i.stack.imgur.com/knRdm.png).
I attempted to fix this issue by adding an offset, but I noticed that the height of the white space varies across different models.
How can I resolve this problem?
",76385275,"Your current setup has the right idea, but if you want to completely ignore the safe area for the VStack's background, you should apply the .ignoresSafeArea() modifier on each of the backgrounds.
The white space you are seeing might be a result of the VStack not filling up the whole screen.
Try moving .ignoresSafeArea() modifier outside of the ZStack, and apply it to the whole VStack. Here is the updated code:
var body: some View {
    VStack{
        //some view
    }
    .background(
        ZStack {
            LinearGradient(gradient: Gradient(
                colors: [
                    .yellow_500,
                    .yellow_500,
                    .yellow_500,
                    .yellow_50
                ]), startPoint: .top, endPoint: .bottom)
                .resizable()
                .aspectRatio(contentMode: .fill)

            Image(R.image.home_bg_try.name)
                .resizable()
                .scaledToFill()
                .frame(maxWidth: .infinity, maxHeight: .infinity)
        }
    )
    .ignoresSafeArea()
}

By using .ignoresSafeArea(), you instruct SwiftUI to layout the VStack and its background across the whole screen, including under the status bar and home indicator on iPhone models with edge-to-edge screens.
Additionally, you might want to apply .edgesIgnoringSafeArea(.all) instead of .ignoresSafeArea() in certain situations. For example, if your view doesn't play well with safe area insets when it's embedded in another view hierarchy, you might find .edgesIgnoringSafeArea(.all) works better. However, in SwiftUI 2.0 and later, .ignoresSafeArea() is generally recommended.
",
76387297,How to conditionally assign a value to Flutter List,"How to conditionally assign a value to ""ingList"" based on the value of recipeList?
 class Calculator extends StatefulWidget {
    List<IngredientList> recipeList;
    Calculator(this.recipeList, {Key? key}) : super(key: key);

  @override
  State<Calculator> createState() => _CalculatorState();
}

class _CalculatorState extends State<Calculator> {
  List<IngredientList> ingList = [];

",76387372,"You can use initState to do that
class Calculator extends StatefulWidget {
  List<IngredientList> recipeList;
  Calculator(this.recipeList, {Key? key}) : super(key: key);

  @override
  State<Calculator> createState() => _CalculatorState();
}

class _CalculatorState extends State<Calculator> {
  List<IngredientList> ingList = [];

  @override
  void initState() {
    super.initState();

    ingList = widget.recipeList; // you can do any condition here
  }
}

",
76384547,Passing a variable to a function within another function,"I am learning Rust. I am working on an embedded Rust project to interface with an I2C LED driver.
I have defined a pub enum LedRegister that defines all of the I2C registers used to control each LED. These register definitions are always one byte, they set by the chip's datasheet, and they will never change. However, I like having them in an enum, because it allows me to create a function that will only accept LedRegisters as inputs.
pub enum LedRegister {
    // 0-255 PWM value. 255 = max LED brightness.
    CpuStatusR    = 0x03,
    CpuStatusG    = 0x02,
    CpuStatusB    = 0x01,
    Ch1Cor        = 0x04,
    Ch1Ptt        = 0x06,
    Ch1SpareR     = 0x05,
    Ch1StatusR    = 0x0C,
    Ch1StatusG    = 0x0B,
    Ch1StatusB    = 0x0A,
    Ch2Cor        = 0x07,
    Ch2Ptt        = 0x09,
    Ch2SpareR     = 0x08,
    Ch2StatusR    = 0x0E,
    Ch2StatusG    = 0x0D,
    Ch2StatusB    = 0x0F,
    SpareLedR     = 0x12,
    SpareLedG     = 0x11,
    SpareLedB     = 0x10,
}

In the same scope, I have a pub fn enable_led to toggle each LED on or off:
pub fn enable_led(i2c: &mut I2c, led: LedRegister, state: bool) -> Result<(), Box<dyn Error>> {
    if state {
        i2c.write(&[led as u8, 0xFF])?;
    } else {
        i2c.write(&[led as u8, 0x00])?;
    }

    i2c.write(&[ControlRegister::Update as u8, 0x00])?;

    Ok(())
}

From main, I can call this function and see that my LED turns on:
led_driver::enable_led(&mut i2c, led_driver::LedRegister::SpareLedG, true);
I would like to write another function that allows me to blink the LED by calling enable_led multiple times with a delay in between:
pub fn blink_led(i2c: &mut I2c, led: LedRegister, duration_ms: u64) -> Result<(), Box<dyn Error>> {
    enable_led(i2c, led, true);
    // add a delay
    enable_led(i2c, led, false);
    // add a delay

    Ok(())
}

Here, Rust complains because the value of 'led' has moved into the first call of enable_led. I understand that there can only be one owner of the value - so I get why Rust is complaining.
I have attempted to 'borrow' the value of LedRegister:
pub fn enable_led(i2c: &mut I2c, led: &LedRegister, state: bool) -> Result<(), Box<dyn Error>> {
    if state {
        i2c.write(&[led as u8, 0xFF])?;
    } else {
        i2c.write(&[led as u8, 0x00])?;
    }

    i2c.write(&[ControlRegister::Update as u8, 0x00])?;

    Ok(())
}

led_driver::enable_led(&mut i2c, &led_driver::LedRegister::SpareLedG, true);


This gives me E0606 and a suggestion to implement casting through a raw pointer and unsafe blocks, which seems needlessly complicated for this use case.
I have also considered implementing copy on my LedRegister, so I can make copies of the initial reference before calling led_enable() with the same LED. This compiles, but it also seems more difficult than it should be. I don't think I actually need multiple copies of my register definition byte in RAM. Is this actually necessary?
I get the feeling that I'm not using the right features of Rust for this task. I am not strongly tied to using an enum here if that is the wrong tool for the job. However, I do want to limit the inputs to enable_led and blink_led so they only accept valid LEDs.
What is the best approach to accomplish my objective here in Rust?
",76388486,"Casting a reference to an integer is like casting a pointer to an integer, which gives you the address and not the value inside casted, except it is not allowed and you need to go through a raw pointer explicitly. This doesn't require unsafe (led as *const LedRegister as u8), but this also doesn't do what you want.
The correct solution is to implement Copy for your enum. You should always implement Copy for simple enums, unless there is a strong reason not to. Taking references to the value is not in any way better than copying it: it is slower, requiring dereferences and storing in the stack instead of potentially in registers, and it complicates the source code for no reason.
",
76383164,Z Index complexity - How to position a component inside of a div to have a higher z-index value against a component outside to its level?,"

.App {
  font-family: sans-serif;
  text-align: center;
}

.section-01 {
  position: relative;
  z-index: 0;
  background-color: red;
  color: white;
  padding: 2rem;
}

.section-02 {
  position: fixed;
  z-index: 1;
  background-color: blue;
  color: white;
  padding: 1rem;
  top: 25vh;
  left: 30vh;
}

.div-01 {
  z-index: 2;
  position: absolute;
  background-color: purple;
  padding: 1rem;
  top: 15vh;
  left: 25vh;
}
<section class=""section-01"">
        Section-01
        <div class=""div-01"">Div-01</div>
      </section>
      <section class=""section-02"">
        <div>Section - 02</div>
</section>



I have a component like this:
<section id='section-01' style={{zIndex: 0}}>
    <div id='div-01' style={{zIndex: 2}}>Hello World</div>
</section>
<section id='section-02' style={{zIndex: 1}}>
</section>

Now my problem is that due to absolute positioning of #section-02 and #div-01, #section-02 takes the precedence and shows above #div-01. However I don't want that, I want #div-01 to show up above #section-02 and #section-02 show up above #section-01.
Please note that I don't have access to alter code of #section-01 or #section-02. I can only control the code of #div-01.
I know that if parent element has lower z-index then its child won't be able to show above any other entity in hierarchy of parent element and so no matter how high I set the value of div-01 it will always show below section-02 so I wanted to know how can I solve this problem?
Adding code sandbox for example: https://codesandbox.io/s/ecstatic-flower-u5plui?file=/src/styles.css
Here's how it shows atm:

And here's how I want it to be:

Please note that you're only allowed to make changes in
.div-01 {
  position: relative;
  z-index: 2;
  position: absolute;
  background-color: purple;
  padding: 1rem;
  top: 6vh;
  left: 25vh;
}

or in JSX part (HTML):
<div className=""div-01"">Div-01</div>

I don't have access to section-01 or section-02.
I cannot restructure the HTML as these comes from different App owners and I have only control to div and its inner content.
",76385416,"Objective
Given 2 sibling positioned elements (.section-01 and .section-02), of which the first element sits at a lower stacking context than the second element. Place the child positioned element of the first element (.section-01 > .div-01) above the second element (.section-02) in the z-axis.
Criterias
Only the child element of the first element (.section-01 > .div-01) may be modified by CSS (apparently by stylesheet?) and HTML (by JSX). The 2 sibling elements are generated by an app and for some reason are beyond reach (please read the XY problem).
Taking the aforementioned criterias into consideration, we cannot:

add, remove, or modify HTML (.div-01 is the exception)
add, remove, or modify CSS (.div-01 is the exception)

Problem
There are two stacking contexts:

.section-01 sits at z-index: 0 without any extended positioning (top, right, bottom, and left) nor does it reference any other positioned elements (position: relative).

.section-02 sits above everything else at z-index: 1 and it's position references the viewport (top, left, and position: fixed).


Naturally #2 stacking context will occupy the foreground. Even if .section-02 had z-index: 0 it would still be in the foreground because in HTML layout it proceeds .section-01. The only way to affect stacking context is to change the element that started it (ie. .section-01 and .section-02).
Solutions
If we could just resolve this issue without having to consider the impossible restrictions mentioned under Criterias, we can simply place .section-01 back into the normal flow of the document by assigning it position: static (or removing position: all together), thereby removing it as an origin of a stacking context and allowing it's child element, .div-01 to be the origin of it's own stacking context. (see Figure I).
Figure I
.section-01 {
  position: static;
  /* As it was originally, there was no real reason for it to have any `position` 
  property */;
}

.div-01 {
  position: fixed /* or `absolute` */;
  z-index: 2;
  /* ... */
}

Putting aside my doubts about the criterias being a genuine concern, (maybe it's too difficult to target the app's HTML/CSS because it assigns randomly determined #ids or .classes), there is a solution with the same result but in an indirect way.
Since the only element in our complete control is .div-01, we can target it's parent by using the .has() pseudo-class. (see Figure II)/
Figure II
/* This will target anything that is a direct ancestor (aka parent) of
.div-01 */
:has(> .div-01) {
  position: static
} 

.div-01 {
  position: fixed /* or absolute */;
  z-index: 2;

Example


html,
body {
  margin: 0;
  padding: 0;
  min-height: 100vh;
  font: 2ch/1.15 ""Segoe UI"";
}

.section-01 {
  position: relative;
  z-index: 0;
  padding: 2rem;
  color: white;
  background-color: red;
}

.section-02 {
  position: fixed;
  top: 10vh;
  left: 30vw;
  z-index: 1;
  Width: 40vw;
  height: 30vh;
  padding: 6rem 1rem 1rem;
  text-align: right;
  color: white;
  background-color: blue;
}

.div-01 {
  position: fixed;
  top: 5vh;
  left: 25vw;
  z-index: 2;
  width: 30vw;
  height: 30vh;
  padding: 1rem;
  color: white;
  background-color: purple;
}

:has(> .div-01) {
  position: static;
}
<section class=""section-01"">
  Section-01
  <div class=""div-01"">Div-01</div>
</section>
<section class=""section-02"">
  <div>Section - 02</div>
</section>



",
76387354,How can I use url query parameter to set the value of a 'Parameters' in PowerBI?,"In PowerBI, I have a managed parameter which is a list of text.
abc, def, ghi.
And that parameter is being use to call a custom function in powerbi i.e. MyCustomFunction(@Name)
My question is how can I change the value of the parameter from the url when I load the report (after i published it to PowerBI service)?
i.e. I would like to
https://mypowerbi url?filter Name eq 'abc'  
`MyCustomFunction('abc')' will be called.

https://mypowerbi url?filter Name eq 'def'  
`MyCustomFunction('def')' will be called.


",76387413,"
My question is how can I change the value of the parameter from the url when I load the report (after i published it to PowerBI service)?

You can't. In Power BI reports, the URL query parameters can be used to filter the report. See Filter a report using query string parameters in the URL
Parameters can be set from the URL in paginated reports only (legacy from SSRS). See Pass a report parameter within a URL for a Power BI paginated report.
For Power BI reports published to Power BI Service, the parameter values can be changed from the dataset's settings. See Edit parameter settings in the Power BI service. Keep in mind that this affects all users, who see the report, and it is not something that can affect your viewing session only. Changing parameter values of imported datasets usually will take a dataset refresh for the changes to take effect.
Parameters of published reports can also be updated using the Power BI's REST API - see Update Parameters and Update Parameters In Group.
",
76387183,Need help personalizing my CSS navbar: how do I highlight the  element on mouseover?,"I am trying to make a simple navbar I managed to make the base but when I try to personalize I am getting stuck.
My objective is that when the mouse goes over each element(home, etc) it highlights like a box, but currently it only highlights the <a> I tried and not the <li> holding it.
I'm trying to make the <li> an anchor that can be clicked and highlighted similar to stack's navbar.


.nav-top {
  background: rgb(151, 138, 63);
  margin: 0;
  padding: 1rem 0;
  display: flex;
  justify-content: flex-end;
  align-items: center;
}

.nav-item {
  list-style: none;
  margin-right: 1.2rem;
  padding: 5px 10px;
}

.nav-top ul {
  margin: 0 auto;
  padding: 0;
  list-style: none
}

.nav-top li:hover {
  display: block
}

.nav-top a:hover {
  background: #F2F2F2;
  color: #444444;
}

.nav-item a {
  text-decoration: none;
  color: white;
  width: 100px;
}

.nav-item:first-child {
  margin-right: auto;
  margin-left: 2rem;
}

.nav-item a:hover {
  color: aqua;
}
<navbar>
  <ul class=""nav-top"">
    <li class=""nav-item""><label class=""logo"">LoremX</label></li>
    <li class=""nav-item""><a href=""index.html"">Home</a></li>
    <li class=""nav-item""><a href=""#"">About</a></li>
    <li class=""nav-item"" id=""contact""><a href=""#"">Contact</a></li>
  </ul>
</navbar>



",76387423,"The only :hover declaration you have for li is the default value of display: block while the color change declarations are made only for a. However, the effect that I believe you are trying to achieve is better accomplished by making the anchors block-level with padding.
Not related to the hover effect, just correcting your markup:

You have .nav-top ul selector but the example doesn't include a nested ul
I suspect that you are misusing the label element.
navbar is not an HTML element and I suspect you want nav



.nav-top {
  display: flex;
  margin: 0;
  padding: 0;
  list-style-type: none;
  background: rgb(151, 138, 63);
}

.logo {
  margin-right: auto;
  margin-left: 2rem;
  padding: 5px 10px;
  align-self: center;
}

.nav-item {
  margin-right: 1.2rem;
  padding: 5px 10px;
}

.nav-item a {
  display: block;
  padding: .5em;
  text-decoration: none;
  color: white;
}

.nav-item a:hover {
  color: aqua;
  background: #F2F2F2;
}
<nav>
  <ul class=""nav-top"">
    <li class=""logo"">LoremX</li>
    <li class=""nav-item""><a href=""#"">Home</a></li>
    <li class=""nav-item""><a href=""#"">About</a></li>
    <li class=""nav-item"" id=""contact""><a href=""#"">Contact</a></li>
  </ul> 
</nav>



",
76383862,Mysql in docker container doesn't accept credentials,"I have a Dockerfile which I later want to feed some initial sql dump into:
FROM mysql:debian

EXPOSE 3306

ENTRYPOINT [""docker-entrypoint.sh""]

CMD [""mysqld""]

Then there is my docker-compose.yaml:
version: '3'

services:
  mysql:
    container_name: ${CONTAINER_NAME}
    build:
      context: .
    restart: always
    environment:
      - MYSQL_ROOT_PASSWORD=${ROOT_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_DATABASE=${MYSQL_DATABASE}

I can docker exec -it sqlcontainer mysql -uroot -p into my container just fine (works with the $MYSQL_USER as well), but when I try to do the same from outside, e.g. with  mysql -uroot -p, I always get ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES).
I'm out of my wits. What am I doing wrong? The communication to mysqld inside the container apparently works, otherwise I wouldn't get back any response.
I'm on Windows, if that's a hint for anyone. But the above is true for PowerShell, CMD and even Git Bash.
",76388595,"It is embarrassing to admit, but both commenters @RiggsFolly and @DavidMaze are right in what they said: I hadn't used MySQL for quite a while on my machine. When I checked the task manager, I indeed found a local mysqld running already, which of course was primed with different credentials. After killing the process (and adding the missing port in the compose file), I was able to connect to my containerized instance.
Hope this will help out somebody else in the future who falls into the same trap.
",
76387405,Why seize queue cant be zero in AnyLogic,"Let's imagine a situation where we have a queue positioned before the seize block, and this queue has the timeout property activated.
In such cases, it is important to note that the seizure queue cannot be reduced to zero. The seize queue always has a minimum capacity of 1. If the seize queue capacity were set to 1, one agent would be taken out of the timeout property of the original queue and placed directly into the seize queue.
This behavior is generally not desirable, particularly when dealing with a large number of agents or running the simulation for extended periods, such as years, to collect the statistics, especially for the timeout property of the original queue.

So how can I avoid having an agent in the seize queue, or is there another way to achieve this?
",76387433,"You can manually avoid having agents in the Seize queue by using a Hold block upstream and only releasing agents into the Seize block if its queue is empty.
",
76382756,Efficiently match all values of a vector in another vector,"I'm looking to find an efficient method of matching all values of vector x in vector y rather than just the first position, as is returned by match().  What I'm after essentially is the default behavior of pmatch() but without partial matching:
x <- c(3L, 1L, 2L, 3L, 3L, 2L)
y <- c(3L, 3L, 3L, 3L, 1L, 3L)

Expected output:
pmatch(x, y)  
[1]  1  5 NA  2  3 NA

One way is to use ave() however this becomes slow and very memory inefficient as the number of groups increases:
ave(x, x, FUN = \(v) which(y == v[1])[1:length(v)])
[1]  1  5 NA  2  3 NA

Can anyone recommend an efficient way to achieve this in preferably (but not mandatory) base R?
Larger dataset for benchmarking:
set.seed(5)
x <- sample(5e3, 1e5, replace = TRUE)
y <- sample(x, replace = TRUE)

",76385488,"A variant in base using split.
split the indices of both vectors by its value. Subset the second list with the names of the first, that both have the same order. Change NULL to NA and bring the lengths of the second list to those from the first. Reorder the indices of the second list by those of the first.
x <- c(3L, 1L, 2L, 3L, 3L, 2L)
y <- c(3L, 3L, 3L, 3L, 1L, 3L)

a <- split(seq_along(x), x)
b <- split(seq_along(y), y)[names(a)]
b[lengths(b)==0] <- NA
b <- unlist(Map(`length<-`, b, lengths(a)), FALSE, FALSE)
`[<-`(b, unlist(a, FALSE, FALSE), b)
#[1]  1  5 NA  2  3 NA

I tried to exchange the part
b <- split(seq_along(y), y)[names(a)]
b[lengths(b)==0] <- NA

with
b <- list2env(split(seq_along(y), y))
b <- mget(names(a), b, ifnotfound = NA)

But it was not faster.
An RCPP version.
Store the indices of the second vector ín a queue for each unique value in an unordered_map. Iterate over all values of the first vector and take the indices from the queue.
Rcpp::sourceCpp(code=r""(
#include <Rcpp.h>
#include <unordered_map>
#include <queue>

using namespace Rcpp;
// [[Rcpp::export]]
IntegerVector pm(const std::vector<int>& a, const std::vector<int>& b) {
  IntegerVector idx(no_init(a.size()));
  std::unordered_map<int, std::queue<int> > lut;
  for(int i = 0; i < b.size(); ++i) lut[b[i]].push(i);
  for(int i = 0; i < idx.size(); ++i) {
    auto search = lut.find(a[i]);
    if(search != lut.end() && search->second.size() > 0) {
      idx[i] = search->second.front() + 1;
      search->second.pop();
    } else {idx[i] = NA_INTEGER;}
  }
  return idx;
}
)"")
pm(x, y)
#[1]  1  5 NA  2  3 NA

A for this case specialized RCPP version.
Create a vector of the length of the maximum value of the first vector and count how many times a value is present. Create another queue vector of the same length and sore there the indices of the values of the second vector until it has reached the number of the first. Iterate over all values of the first vector and take the indices from the queue.
Rcpp::sourceCpp(code=r""(
#include <Rcpp.h>
#include <vector>
#include <array>
#include <queue>
#include <algorithm>

using namespace Rcpp;
// [[Rcpp::export]]
IntegerVector pm2(const std::vector<int>& a, const std::vector<int>& b) {
  IntegerVector idx(no_init(a.size()));
  int max = 1 + *std::max_element(a.begin(), a.end());
  std::vector<int> n(max);
  for(int i = 0; i < a.size(); ++i) ++n[a[i]];
  std::vector<std::queue<int> > lut(max);
  for(int i = 0; i < b.size(); ++i) {
    if(b[i] < max && n[b[i]] > 0) {
      --n[b[i]];
      lut[b[i]].push(i);
    }
  }
  for(int i = 0; i < idx.size(); ++i) {
    auto & P = lut[a[i]];
    if(P.size() > 0) {
      idx[i] = P.front() + 1;
      P.pop();
    } else {idx[i] = NA_INTEGER;}
  }
  return idx;
}
)"")
pm2(x,y)
#[1]  1  5 NA  2  3 NA


Benchmark
set.seed(5)
x <- sample(5e3, 1e5, replace = TRUE)
y <- sample(x, replace = TRUE)

library(data.table)

matchall <- function(x, y) {
  data.table(y, rowid(y))[
    data.table(x, rowid(x)), on = .(y = x, V2), which = TRUE
  ]
}

rmatch <- function(x, y) {
  xp <- cbind(seq_along(x), x)[order(x),]
  yp <- cbind(seq_along(y), y)[order(y),]
  result <- numeric(length(x))
  
  xi <- yi <- 1
  Nx <- length(x)
  Ny <- length(y)
  while (xi <= Nx) {
    if (yi > Ny) {
      result[xp[xi,1]] <- NA
      xi <- xi + 1
    } else if (xp[xi,2] == yp[yi,2]) {
      result[xp[xi,1]] = yp[yi,1]
      xi <- xi + 1
      yi <- yi + 1
    } else if (xp[xi,2] < yp[yi,2]) {
      result[xp[xi,1]] <- NA
      xi <- xi + 1
    } else if (xp[xi,2] > yp[yi,2]) {
      yi <- yi + 1
    }
  }
  result  
}

bench::mark(
ave = ave(x, x, FUN = \(v) which(y == v[1])[1:length(v)]),
rmatch = rmatch(x, y),
make.name = match(make.names(x, TRUE), make.names(y, TRUE)),
paste = do.call(match, lapply(list(x, y), \(v) paste(v, ave(v, v, FUN = seq_along)))),
make.unique = match(make.unique(as.character(x)), make.unique(as.character(y))),
split = {a <- split(seq_along(x), x)
  b <- split(seq_along(y), y)[names(a)]
  b[lengths(b)==0] <- NA
  b <- unlist(Map(`length<-`, b, lengths(a)), FALSE, FALSE)
  `[<-`(b, unlist(a, FALSE, FALSE), b)},
data.table = matchall(x, y),
RCPP = pm(x, y),
RCPP2 = pm2(x, y)
)

Result
  expression       min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc
  <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> <int> <dbl>
1 ave            1.66s    1.66s     0.603    3.73GB    68.7      1   114
2 rmatch      258.29ms 259.35ms     3.86     5.34MB    30.8      2    16
3 make.name   155.69ms 156.82ms     6.37    14.06MB     1.59     4     1
4 paste         93.8ms 102.06ms     9.74    18.13MB     7.79     5     4
5 make.unique  81.67ms   92.8ms    10.4      9.49MB     5.22     6     3
6 split        12.66ms  13.16ms    65.8      7.18MB    16.0     33     8
7 data.table    6.22ms   6.89ms   114.       5.13MB    28.0     57    14
8 RCPP          3.06ms    3.2ms   301.     393.16KB     3.98   151     2
9 RCPP2         1.64ms   1.82ms   514.     393.16KB     8.00   257     4

In this case the C++ version is the fastest and allocates the lowest amount of memory. In case using base the splitB variant is the fastest and rmatch allocates the lowest amount of memory.
",
76378680,R - How to rename xml parent node based on child element (or associate child elements)?,"I'm trying to extract a couple of elements from XML files into an R dataframe but the parent nodes are all named the same, so I don't know how to associate child elements. I'm very new to xml (about 3 hours) so apologies if I use the wrong terminology. I did not find any R-based solutions.
This is the general structure of the xml files:
<Annotations>
    <Version>1.0.0.0</Version>
    <Annotation>
        <MicronLength>14.1593438418</MicronLength>
        <MicronHeight>0.0000000000</MicronHeight>
        <ObjIndex>1</ObjIndex>
    </Annotation>
    <Annotation>
        <MicronLength>5.7578076896</MicronLength>
        <MicronHeight>0.0000000000</MicronHeight>
        <ObjIndex>2</ObjIndex>
    </Annotation>
</Annotations>

There are many ""Annotation"" nodes. There are also several other children node names in there but they don't matter as I'm just trying to extract MicronLength and ObjIndex into a dataframe. So I need to either:

Associate and get both elements from within each ""Annotation"" node

OR

Rename each ""Annotation"" based on the ObjIndex within (e.g. ""Annotation 1"", ""Annotation 2"", etc.) and then get parent name and child element into the df.

I also have several xml files so I want to iterate over each one to eventually create a DF like the example below.
| filename           | ObjIndex | MicronLength  |

| ------------------ | -------- | ------------- |

| examplefile1(.xml) | 1        | 14.1593438418 |

| examplefile1       | 2        | 5.7578076896  |

| examplefile2       | 1        | 12.6345661343 |


The filenames (with or without extension) will then be str_split into some more columns but I can do that myself.
Much appreciated!
",76385544,"I have previously used xml_find_all() for this kind of simple conversion.
This works as long as each Annotation node always has exactly
one ObjIndex and MicronLength child node:
library(xml2)

xml <- read_xml(""
<Annotations>
    <Version>1.0.0.0</Version>
    <Annotation>
        <MicronLength>14.1593438418</MicronLength>
        <MicronHeight>0.0000000000</MicronHeight>
        <ObjIndex>1</ObjIndex>
    </Annotation>
    <Annotation>
        <MicronLength>5.7578076896</MicronLength>
        <MicronHeight>0.0000000000</MicronHeight>
        <ObjIndex>2</ObjIndex>
    </Annotation>
</Annotations>
"")

data.frame(
  ObjIndex = xml_integer(xml_find_all(xml, ""Annotation/ObjIndex"")),
  MicronLength = xml_double(xml_find_all(xml, ""Annotation/MicronLength""))
)
#>   ObjIndex MicronLength
#> 1        1    14.159344
#> 2        2     5.757808

",
76387362,How to set UID and GID for the container using python sdk?,"How to set UID and GID for the container when Python SDK is used to spin up the container?
",76387470,"As the documentation you've linked says, pass in user and group_add to run():
client.containers.run('alpine', 'echo hello world', user='foo', group_add=[123])

Both accept both IDs and names, but group_add needs to be a list.
",
76387121,How to align text vertically according to the highest point of the text and lowest point?,"I want to center a text vertically such that the highest point of the text and the lowest point of text are on equal distance from the ending div that it is enclosed in.
I have the following css code:
padding-left: 30px;
font-family: 'Playfair Display', serif;
font-weight: 100;
width: 140%;
line-height: 68px;
color: #fff;
font-size: 46px;
border-style: solid;
border-color: #1faf2d;
background-color: #1faf2d;
margin-bottom: 0 !important;
margin-top: 38px;

This is what gives the colour and height etc to the font and it look like the below image.

",76387471,"This can only be done by adding padding at the bottom or Top and this may vary as per different Fonts. I question is why do you want to do this? As anigning center as per your way would further discurb the balance.
Also, The positioning of certain letters, such as ""p,"" ""g,"" ""y,"" and ""q,"" are lower than the center line is due to their design and the rules of typography. These letters have descenders, which are the parts of the letterforms that extend below the baseline.
Descenders are common in lowercase letters and serve to maintain the overall balance and legibility of the text. They allow for differentiation between similar letterforms and help to create a visually pleasing text block.
When aligning text vertically, the descenders are taken into account to ensure proper spacing and balance. By aligning the baseline of the text, which includes the descenders, it maintains the overall visual harmony of the text and prevents the descenders from interfering with the line spacing or overlapping other elements.
In the case of vertically aligning text to the highest point and lowest point, the descenders will naturally make the text appear lower as they extend below the baseline. This is considered the correct alignment for maintaining legibility and preserving the intended design of the typeface.
It's worth noting that the alignment of text can vary depending on the context and the specific design choices made by the typographer or designer. Different fonts may have slightly different alignments, and some artistic or decorative fonts may intentionally deviate from traditional alignment principles for creative purposes.
",
76388879,Timer isn't dealing with seconds properly,"Whenever I change the minutes on the timer it works perfectly, but when I change the seconds on the timer, no matter what, it instantly stops. I'm not sure what I'm doing wrong. This program is apart of a codepen exercise. When I had the timer on a countdown setting it worked perfectly, but when I changed it to countup it stopped working for the seconds.


window.addEventListener('DOMContentLoaded', documentLoaded, false);

var startTime;
var limit;
var timer;

function documentLoaded() {
  ""use strict"";

  var timerElement = document.getElementById(""timer"");
  timerElement.addEventListener(""keydown"", function (event) {
    if (event.key === 'Enter') {
      event.preventDefault();
      startTimer();
      timerElement.blur();
    }
  });
}

function startTimer() {
  startTime = new Date();
  limit = parseInt(document.getElementById(""timer"").innerHTML);

  clearInterval(timer);
  timer = setInterval(updateTime, 1000);
}

function updateTime() {
  var currentTime = new Date();
  var elapsed = (currentTime.getTime() - startTime.getTime()) / 1000;

  var minutes = Math.floor(elapsed / 60);
  var seconds = Math.floor(elapsed % 60);

  if (minutes < 10) {
    minutes = ""0"" + minutes;
  }
  if (seconds < 10) {
    seconds = ""0"" + seconds;
  }

  document.getElementById(""timer"").innerHTML = minutes + "":"" + seconds;

  var totalSeconds = minutes * 60 + seconds;
  if (totalSeconds >= limit * 60) {
    document.getElementById(""timer"").classList.add(""red"");
    clearInterval(timer); // Stop the timer
  } else {
    document.getElementById(""timer"").classList.remove(""red"");
  }
}
body {
  display: flex;
  justify-content: center;
  align-items: center;
}

#timer-container {
  width: 200px;
  height: 200px;
  border-radius: 50%;
  background-color: #0781D4;
  display: flex;
  justify-content: center;
  align-items: center;
}

#timer {
  font-size: 36px;
  text-align: center;
}

.red {
  background-color: red;
}
<!DOCTYPE html>
<html>
<head>
  <meta charset=""UTF-8"">
  <title>Ejercio No. 3</title>
  <link rel=""stylesheet"" href=""style.css"">
</head>
<body>
  <div id=""timer-container"">
    <div id=""timer"" contenteditable=""true"">00:00</div>
  </div>
  <script src=""script.js""></script>
</body>
</html>



",76388972,"You don't set the limit correctly - you set it directly to the content of the timer, which includes non numeric characters such as "":"". When using parseInt on something that isn't all digits, anything after the first non numeric character is discarded. Therefore, by using anything under a minute to test with, limit will be set to 0, as ""00:30"" would result in 0 from parseInt, as only text from before the colon is used. To fix this, split the text at the colon and convert the minutes into seconds, as shown in the snippet
let time = document.getElementById(""timer"").innerHTML.split("":"");
//if time = ""01:30"", limit = 1 + 30/60 = 1.5 minutes
limit = parseInt(time[0]) + parseInt(time[1])/60;



window.addEventListener('DOMContentLoaded', documentLoaded, false);

var startTime;
var limit;
var timer;

function documentLoaded() {
  ""use strict"";

  var timerElement = document.getElementById(""timer"");
  timerElement.addEventListener(""keydown"", function (event) {
    if (event.key === 'Enter') {
      event.preventDefault();
      startTimer();
      timerElement.blur();
    }
  });
}

function startTimer() {
  startTime = new Date();
  let time = document.getElementById(""timer"").innerHTML.split("":"");
  limit = parseInt(time[0]) + parseInt(time[1])/60;

  clearInterval(timer);
  timer = setInterval(updateTime, 1000);
}

function updateTime() {
  var currentTime = new Date();
  var elapsed = (currentTime.getTime() - startTime.getTime()) / 1000;

  var minutes = Math.floor(elapsed / 60);
  var seconds = Math.floor(elapsed % 60);

  if (minutes < 10) {
    minutes = ""0"" + minutes;
  }
  if (seconds < 10) {
    seconds = ""0"" + seconds;
  }

  document.getElementById(""timer"").innerHTML = minutes + "":"" + seconds;

  var totalSeconds = minutes * 60 + seconds;
  if (totalSeconds >= limit * 60) {
    document.getElementById(""timer"").classList.add(""red"");
    clearInterval(timer); // Stop the timer
  } else {
    document.getElementById(""timer"").classList.remove(""red"");
  }
}
body {
  display: flex;
  justify-content: center;
  align-items: center;
}

#timer-container {
  width: 200px;
  height: 200px;
  border-radius: 50%;
  background-color: #0781D4;
  display: flex;
  justify-content: center;
  align-items: center;
}

#timer {
  font-size: 36px;
  text-align: center;
}

.red {
  background-color: red;
}
<!DOCTYPE html>
<html>
<head>
  <meta charset=""UTF-8"">
  <title>Ejercio No. 3</title>
  <link rel=""stylesheet"" href=""style.css"">
</head>
<body>
  <div id=""timer-container"">
    <div id=""timer"" contenteditable=""true"">00:00</div>
  </div>
  <script src=""script.js""></script>
</body>
</html>



",
76383134,Is it possible to run a sudo command in go without running the program itsself as sudo,"The program runs multiple commands that require sudo privileges (such as sudo dnf update). Since the program should be installed using the go install command, it can't be run as sudo its self without configuration done by the user (afaik).
The program doesn't show the output to the user to keep the output clean. To show that a process is running, it uses a spinner from the spinner library.
Is it possible to do any of these things?

Obtain sudo privileges from within the program
Make the program runnable as sudo, even when installed using go install
Show the output of the sudo command (including the password request) without it being overwritten by the spinner

Here is a shortened version of what I would like my code to do:
package main

import (
    ""fmt""
    ""os""
    ""os/exec""
    ""time""

    ""github.com/briandowns/spinner""
)

func main() {
    // Spinner to show that it's running
    s := spinner.New(spinner.CharSets[14], time.Millisecond*100)
    s.Start()

    // Somehow execute this with sudo
    _, err := exec.Command(os.Getenv(""SHELL""), ""-c"", ""dnf update -y"").Output()

    // Stop the spinner and handle any error
    if err != nil {
        fmt.Printf(""Err: %s"", err)
        os.Exit(1)
    }
    s.Stop()

    // Finish
    fmt.Println(""Success"")
}

",76385590,"_, err := exec.Command(os.Getenv(""SHELL""), ""-c"", ""sudo dnf update -y"").Output()
In this exapmle, with adding sudo before the command that you want run as sudo, and after running the program, will ask password for sudo, If you apply this to your example code you can't see password request message, because the spinner graphics will overwrite it, but if you try this without spinner graphics you can see it. Even you don't see the message, if you type your correct password and press enter your commands will work as sudo. With this, you don't need run your application as sudo. I have ran similar commands with using this and they have worked.
",
76387535,error: could not find `sui` in https://github.com/MystenLabs/sui.git?branch=devnet with version `*`,"When trying to install sui binaries using
cargo install --locked --git https://github.com/MystenLabs/sui.git --branch devnet sui

as suggested by the official docs,
gives the below error
Updating git repository `https://github.com/MystenLabs/sui.git` 
error: could not find `sui` in https://github.com/MystenLabs/sui.git?branch=devnet with version `*`

What could be the possible reason?
",76387552,"I used the below command which includes the tag to install it
cargo install --locked --git https://github.com/MystenLabs/sui.git --branch devnet --tag devnet-<version> sui

where you can replace version as required (e.g v1.3.0)
",
76387474,Wierd behavior with raising intgers to the negative powers in python,"I am observing this weird behavior when I am raising an integer to the negative power using an np.array. Specifically, I am doing
import numpy as np

a = 10**(np.arange(-1, -8, -1))

and it results in the following error.

ValueError: Integers to negative integer powers are not allowed.

This is strange as the code 10**(-1) works fine. However the following workaround (where 10 is a float instead of integer) works fine.
import numpy as np

a = 10.**(np.arange(-1, -8, -1)
print(a)  # Prints array([1.e-01, 1.e-02, 1.e-03, 1.e-04, 1.e-05, 1.e-06, 1.e-07])

Why is it not valid for integers? Any explanation is appreciated.
",76387579,"This is happening because the input 10 is an integer.
10**(np.arange(-1, -8, -1))

numpy.arange() is designed such a way that that has to give 10**(np.arange(-1, -8, -1)) integers or nothing since input is an integer.
On the contrary;
a = 10.**(np.arange(-1, -8, -1)

gives results happily as 10.0 is a float

Edit: found an answer to back my point; voted for a duplicate:
Why can't I raise to a negative power in numpy?
",
76383413,Get value of a field from a foreigKey in Django Models,"I have two classes in my Database in Django
class Test(models.Model):
    sequenzaTest = models.ForeignKey(""SequenzaMovimento"", null=True, on_delete=models.SET_NULL)

class SequenzaMovimento(models.Model):
    nomeSequenza = models.CharField(max_length=50, blank=False, null=False)
    serieDiMovimenti = models.TextField(blank=False, null=False, default="""")

Now, every Test object created can be associated with just one SequenzaMovimento object. Different Test objects can have the same SequenzaMovimento
Now, I know the primary key of my Test. How do I find the serieDiMovimenti inside the SequenzaMovimento object which is linked to the Test object?
I can get the sequenzaTest from the Test object with
testo_sequenza = Test.objects.get(pk=idObj)
testo_sequenza.sequenzaTest

but I can't find to understand how access serieDiMovimenti
",76385687,"This should work:
try:
    testo_sequenza = Test.objects.get(pk=idObj)
except Test.DoesNotExist:
    # do here what you need the program to do if not found. maybe a return if function or a continue/break if you're in a loop
    print(""Testo Sequenza not found"")
sequenza_test = testo_sequenza.sequenzaTest
serie_di_movimenti = sequenza_test.serieDiMovimenti

Also you should check
serieDiMovimenti = models.TextField(blank=False, null=False, default="""")

Because if you want it to be ""optional field"" then it would be blank=True
",
76388965,"How to use integer methods using `method`, not their infix form, in Ruby","I am looking to programmatically apply a typically infixed operation (eg: +, -, *, /) on two integers, where the operation is specified via a string. I have had success accessing the method itself using .method
This is a pattern that works, 1.+(2) which correctly resolves to 3
By extension, I'd to define a way that could take a variable string for the operator, like so: 1 + 2 as 1.method('+')(2)
The above causes a syntax error, though up til the point of retrieving the method this way does work, I'm not sure what the syntax needs to be to then pass the second integer argument. e.g:
1.method('+')     # <Method: Integer#+(_)>
1.method('+') 2   # syntax error, unexpected integer literal, expecting end-of-input
1.method('+')(2)  # syntax error, unexpected '(', expecting end-of-input

What is the right syntax to perform an operation 1 + 2 in this way?
I am using: ruby 3.1.2p20 (2022-04-12 revision 4491bb740a) [x86_64-linux]
",76389014,"The Method class has several instance method of it's own. The one you're looking for here is call
It is also aliased to [] and ===
1.method('+').call(2) #=> 3
1.method('+')[2] #=> 3
1.method('+') === 2 #=> 3

",
76387189,"Fatal error relating to ""include S.h"" when installing R 'scalop' package","I am attempting to install an R package named 'infercna', the github repository to which is linked here.
The install process attempts to load another package named 'scalop', which is linked here.
Specifically, this command:
devtools::install_github(""jlaffy/infercna"")

returns
Downloading GitHub repo jlaffy/infercna@HEAD
── R CMD build ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
✔  checking for file ‘/private/var/folders/hj/1wvjfb692c3gswybcg8xdcwm0000gn/T/RtmpqQIEYL/remotes7d75586a9ac5/jlaffy-infercna-98a8db8/DESCRIPTION’ (343ms)
─  preparing ‘infercna’:
✔  checking DESCRIPTION meta-information ...
─  checking for LF line-endings in source and make files and shell scripts
─  checking for empty or unneeded directories
     NB: this package now depends on R (>= 3.5.0)
     WARNING: Added dependency on R >= 3.5.0 because serialized objects in
     serialize/load version 3 cannot be read in older versions of R.
     File(s) containing such objects:
       ‘infercna/data-raw/genes.rda’
─  building ‘infercna_1.0.0.tar.gz’
   
* installing *source* package ‘infercna’ ...
** using staged installation
** R
** data
*** moving datasets to lazyload DB
** byte-compile and prepare package for lazy loading
Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : 
  there is no package called ‘scalop’
Calls: <Anonymous> ... loadNamespace -> withRestarts -> withOneRestart -> doWithOneRestart
Execution halted
ERROR: lazy loading failed for package ‘infercna’
* removing ‘/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/infercna’

As such, I backtracked and attempted to install scalop, like so:
remotes::install_github(""jlaffy/scalop"")

This is where things start to really get hairy. To install, scalop requires 95 dependencies. Upon successful installation of all 95, the installation for scalop will eventually still fail, like so:
checking for file ‘/private/var/folders/hj/1wvjfb692c3gswybcg8xdcwm0000gn/T/RtmpqQIEYL/remotes7d757fe15404/jlaffy-scalop-021999d/DESCRIPTION’ ...
─  preparing ‘scalop’: (385ms)
✔  checking DESCRIPTION meta-information ...
─  cleaning src
─  checking for LF line-endings in source and make files and shell scripts
─  checking for empty or unneeded directories
─  building ‘scalop_1.1.0.tar.gz’
   
* installing *source* package ‘scalop’ ...
** using staged installation
** libs
using C compiler: ‘Apple clang version 11.0.3 (clang-1103.0.32.62)’
using SDK: ‘MacOSX10.15.sdk’
clang -arch x86_64 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/Rcpp/include' -I/opt/R/x86_64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c init.c -o init.o
clang -arch x86_64 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/Rcpp/include' -I/opt/R/x86_64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c nd.c -o nd.o
nd.c:24:10: fatal error: 'S.h' file not found
#include ""S.h""
         ^~~~~
1 error generated.
make: *** [nd.o] Error 1
ERROR: compilation failed for package ‘scalop’
* removing ‘/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/scalop’

I am writing to ask if anyone knows enough about this output to know what to do to fix the ""fatal error, 'S.h' file not found"" error, which ultimately kills the download.
Several people have reached out to the author, as per the issues posted on scalop; specifically issues 4 and 5, but no reply. Additionally, posting the error message into google does not return useful hits, so far as I can see.
Finally, I am happy to provide any and all necessary info; e.g. sessionInfo(), R version (4.3) Mac OS (11.7) etc.
Help me Stack Overflow-Kenobi, you're my only hope.
",76387609,"The ""S.h"" headers file is from the ""S"" language (the precursor to R); replacing ""S.h"" with ""R.h"" fixes the 'cant find S.h' error, but causes other issues. Clearly this package is not being maintained :(
I've forked the repository and made a couple of changes to the source code (commits fe15cf9 and ab9fe5c). I successfully installed both the scalop and infercna packages via Bioconductor, but there are a lot of warnings when they compile. I used gcc to compile them, rather than Apple Clang, with these flags:
cat ~/.R/Makevars
LOC=/usr/local/gfortran
CC=$(LOC)/bin/gcc -fopenmp
CXX=$(LOC)/bin/g++ -fopenmp
CXX11=$(LOC)/bin/g++ -fopenmp

CFLAGS=-g -O3 -Wall -pedantic -std=gnu99 -mtune=native -pipe
CXXFLAGS=-g -O3 -Wall -pedantic -std=c++11 -mtune=native -pipe
LDFLAGS=-L$(LOC)/lib -Wl,-rpath,$(LOC)/lib,-L/usr/local/lib
CPPFLAGS=-I$(LOC)/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/usr/local/include

FLIBS=-L/usr/local/gfortran/lib/gcc/x86_64-apple-darwin19/10.2.0 -L/usr/local/gfortran/lib -lgfortran -lquadmath -lm
CXX1X=/usr/local/gfortran/bin/g++
CXX98=/usr/local/gfortran/bin/g++
CXX11=/usr/local/gfortran/bin/g++
CXX14=/usr/local/gfortran/bin/g++
CXX17=/usr/local/gfortran/bin/g++

If you have problems installing the scalop package from source using Apple Clang, and you have an intel processor, my instructions for compiling R packages from source are here: https://stackoverflow.com/a/65334247/12957340
If you have an Apple silicon processor, you can try the instructions here: https://stackoverflow.com/a/68275558/12957340

This is how I installed the packages:
install.packages(""BiocManager"")
library(BiocManager)
BiocManager::install(""Homo.sapiens"")
BiocManager::install(""jpmam1/scalop"") # my forked copy
BiocManager::install(""jlaffy/infercna"")

The example from the vignette runs, but some of the functions no longer work as expected:
library(infercna)
#> 
#> 
#> Warning: replacing previous import 'AnnotationDbi::select' by 'dplyr::select'
#> when loading 'scalop'
#> 
#> Attaching package: 'infercna'
#> The following object is masked from 'package:graphics':
#> 
#>     clip
set.seed(1014)
useGenome('hg19')
#> Genome has been set to hg19
retrieveGenome()
#> Retrieving: hg19
#> # A tibble: 33,575 × 8
#>    symbol     start_position end_position chromosome_name arm   band   strand
#>    <chr>               <dbl>        <dbl> <fct>           <fct> <chr>   <int>
#>  1 DDX11L1             11869        14412 1               1p    p36.33      1
#>  2 WASH7P              14363        29806 1               1p    p36.33     -1
#>  3 MIR1302-11          29554        31109 1               1p    p36.33      1
#>  4 FAM138A             34554        36081 1               1p    p36.33     -1
#>  5 OR4G4P              52473        54936 1               1p    p36.33      1
#>  6 OR4G11P             62948        63887 1               1p    p36.33      1
#>  7 OR4F5               69091        70008 1               1p    p36.33      1
#>  8 CICP27             131025       134836 1               1p    p36.33      1
#>  9 RNU6-1100P         157784       157887 1               1p    p36.33     -1
#> 10 CICP7              329431       332236 1               1p    p36.33     -1
#> # ℹ 33,565 more rows
#> # ℹ 1 more variable: ensembl_gene_id <chr>
m = useData(mgh125)
dim(m)
#> [1] 8556 1266
range(m)
#> [1]  0.000 15.328
lengths(refCells)
#> oligodendrocytes      macrophages 
#>              219              707

cna = infercna(m = m, refCells = refCells, n = 5000, noise = 0.1, isLog = TRUE, verbose = FALSE)
cnaM = cna[, !colnames(cna) %in% unlist(refCells)]

cnaScatterPlot(cna = cna,
               signal.threshold = NULL,
               main = 'Default')



obj = cnaPlot(cna = cna,
              order.cells = TRUE,
              subtitle = 'Copy-Number Aberrations in a patient with Glioblastoma')
#> Error in if (class(x) == ""matrix"") {: the condition has length > 1

Depending on your use-case, you'll probably need to make further changes to the source code to get your desired output. If you have further errors/questions please post them in the comments and I'll take a look at them when I have some time.

sessionInfo()
#> R version 4.3.0 (2023-04-21)
#> Platform: x86_64-apple-darwin20 (64-bit)
#> Running under: macOS Ventura 13.3.1
#> 
#> Matrix products: default
#> BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib 
#> LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0
#> 
#> locale:
#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
#> 
#> time zone: Australia/Melbourne
#> tzcode source: internal
#> 
#> attached base packages:
#> [1] stats     graphics  grDevices utils     datasets  methods   base     
#> 
#> other attached packages:
#> [1] infercna_1.0.0
#> 
#> loaded via a namespace (and not attached):
#>   [1] splines_4.3.0                          
#>   [2] BiocIO_1.10.0                          
#>   [3] bitops_1.0-7                           
#>   [4] ggplotify_0.1.0                        
#>   [5] filelock_1.0.2                         
#>   [6] tibble_3.2.1                           
#>   [7] R.oo_1.25.0                            
#>   [8] polyclip_1.10-4                        
#>   [9] graph_1.78.0                           
#>  [10] reprex_2.0.2                           
#>  [11] XML_3.99-0.14                          
#>  [12] lifecycle_1.0.3                        
#>  [13] rstatix_0.7.2                          
#>  [14] edgeR_3.42.4                           
#>  [15] Homo.sapiens_1.3.1                     
#>  [16] lattice_0.21-8                         
#>  [17] MASS_7.3-60                            
#>  [18] OrganismDbi_1.42.0                     
#>  [19] backports_1.4.1                        
#>  [20] magrittr_2.0.3                         
#>  [21] limma_3.56.1                           
#>  [22] plotly_4.10.1                          
#>  [23] rmarkdown_2.22                         
#>  [24] yaml_2.3.7                             
#>  [25] metapod_1.8.0                          
#>  [26] cowplot_1.1.1                          
#>  [27] DBI_1.1.3                              
#>  [28] RColorBrewer_1.1-3                     
#>  [29] abind_1.4-5                            
#>  [30] zlibbioc_1.46.0                        
#>  [31] Rtsne_0.16                             
#>  [32] R.cache_0.16.0                         
#>  [33] GenomicRanges_1.52.0                   
#>  [34] purrr_1.0.1                            
#>  [35] mixtools_2.0.0                         
#>  [36] R.utils_2.12.2                         
#>  [37] msigdbr_7.5.1                          
#>  [38] ggraph_2.1.0                           
#>  [39] BiocGenerics_0.46.0                    
#>  [40] RCurl_1.98-1.12                        
#>  [41] styler_1.10.0                          
#>  [42] yulab.utils_0.0.6                      
#>  [43] tweenr_2.0.2                           
#>  [44] rappdirs_0.3.3                         
#>  [45] GenomeInfoDbData_1.2.10                
#>  [46] IRanges_2.34.0                         
#>  [47] S4Vectors_0.38.1                       
#>  [48] enrichplot_1.20.0                      
#>  [49] ggrepel_0.9.3                          
#>  [50] irlba_2.3.5.1                          
#>  [51] tidytree_0.4.2                         
#>  [52] dqrng_0.3.0                            
#>  [53] DelayedMatrixStats_1.22.0              
#>  [54] codetools_0.2-19                       
#>  [55] DelayedArray_0.26.3                    
#>  [56] scuttle_1.10.1                         
#>  [57] DOSE_3.26.1                            
#>  [58] xml2_1.3.4                             
#>  [59] ggforce_0.4.1                          
#>  [60] tidyselect_1.2.0                       
#>  [61] aplot_0.1.10                           
#>  [62] farver_2.1.1                           
#>  [63] ScaledMatrix_1.8.1                     
#>  [64] viridis_0.6.3                          
#>  [65] matrixStats_0.63.0                     
#>  [66] stats4_4.3.0                           
#>  [67] BiocFileCache_2.8.0                    
#>  [68] GenomicAlignments_1.36.0               
#>  [69] jsonlite_1.8.4                         
#>  [70] BiocNeighbors_1.18.0                   
#>  [71] tidygraph_1.2.3                        
#>  [72] survival_3.5-5                         
#>  [73] segmented_1.6-4                        
#>  [74] tools_4.3.0                            
#>  [75] progress_1.2.2                         
#>  [76] treeio_1.24.1                          
#>  [77] TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2
#>  [78] Rcpp_1.0.10                            
#>  [79] glue_1.6.2                             
#>  [80] gridExtra_2.3                          
#>  [81] xfun_0.39                              
#>  [82] qvalue_2.32.0                          
#>  [83] MatrixGenerics_1.12.0                  
#>  [84] GenomeInfoDb_1.36.0                    
#>  [85] dplyr_1.1.2                            
#>  [86] withr_2.5.0                            
#>  [87] BiocManager_1.30.20                    
#>  [88] fastmap_1.1.1                          
#>  [89] bluster_1.10.0                         
#>  [90] fansi_1.0.4                            
#>  [91] rsvd_1.0.5                             
#>  [92] caTools_1.18.2                         
#>  [93] digest_0.6.31                          
#>  [94] R6_2.5.1                               
#>  [95] gridGraphics_0.5-1                     
#>  [96] colorspace_2.1-0                       
#>  [97] GO.db_3.17.0                           
#>  [98] biomaRt_2.56.0                         
#>  [99] RSQLite_2.3.1                          
#> [100] R.methodsS3_1.8.2                      
#> [101] utf8_1.2.3                             
#> [102] tidyr_1.3.0                            
#> [103] generics_0.1.3                         
#> [104] data.table_1.14.8                      
#> [105] rtracklayer_1.60.0                     
#> [106] prettyunits_1.1.1                      
#> [107] graphlayouts_1.0.0                     
#> [108] httr_1.4.6                             
#> [109] htmlwidgets_1.6.2                      
#> [110] S4Arrays_1.0.4                         
#> [111] scatterpie_0.2.0                       
#> [112] pkgconfig_2.0.3                        
#> [113] gtable_0.3.3                           
#> [114] blob_1.2.4                             
#> [115] SingleCellExperiment_1.22.0            
#> [116] XVector_0.40.0                         
#> [117] shadowtext_0.1.2                       
#> [118] clusterProfiler_4.8.1                  
#> [119] htmltools_0.5.5                        
#> [120] carData_3.0-5                          
#> [121] fgsea_1.26.0                           
#> [122] scalop_1.1.0                           
#> [123] RBGL_1.76.0                            
#> [124] scales_1.2.1                           
#> [125] Biobase_2.60.0                         
#> [126] png_0.1-8                              
#> [127] scran_1.28.1                           
#> [128] ggfun_0.0.9                            
#> [129] knitr_1.43                             
#> [130] rstudioapi_0.14                        
#> [131] reshape2_1.4.4                         
#> [132] rjson_0.2.21                           
#> [133] nlme_3.1-162                           
#> [134] curl_5.0.0                             
#> [135] org.Hs.eg.db_3.17.0                    
#> [136] cachem_1.0.8                           
#> [137] stringr_1.5.0                          
#> [138] parallel_4.3.0                         
#> [139] HDO.db_0.99.1                          
#> [140] AnnotationDbi_1.62.1                   
#> [141] restfulr_0.0.15                        
#> [142] pillar_1.9.0                           
#> [143] grid_4.3.0                             
#> [144] vctrs_0.6.2                            
#> [145] ggpubr_0.6.0                           
#> [146] BiocSingular_1.16.0                    
#> [147] car_3.1-2                              
#> [148] beachmat_2.16.0                        
#> [149] dbplyr_2.3.2                           
#> [150] cluster_2.1.4                          
#> [151] evaluate_0.21                          
#> [152] zeallot_0.1.0                          
#> [153] GenomicFeatures_1.52.0                 
#> [154] locfit_1.5-9.7                         
#> [155] cli_3.6.1                              
#> [156] compiler_4.3.0                         
#> [157] Rsamtools_2.16.0                       
#> [158] rlang_1.1.1                            
#> [159] crayon_1.5.2                           
#> [160] ggsignif_0.6.4                         
#> [161] plyr_1.8.8                             
#> [162] fs_1.6.2                               
#> [163] stringi_1.7.12                         
#> [164] viridisLite_0.4.2                      
#> [165] BiocParallel_1.34.2                    
#> [166] babelgene_22.9                         
#> [167] munsell_0.5.0                          
#> [168] Biostrings_2.68.1                      
#> [169] lazyeval_0.2.2                         
#> [170] GOSemSim_2.26.0                        
#> [171] Matrix_1.5-4.1                         
#> [172] patchwork_1.1.2                        
#> [173] hms_1.1.3                              
#> [174] sparseMatrixStats_1.12.0               
#> [175] bit64_4.0.5                            
#> [176] ggplot2_3.4.2                          
#> [177] statmod_1.5.0                          
#> [178] KEGGREST_1.40.0                        
#> [179] SummarizedExperiment_1.30.1            
#> [180] kernlab_0.9-32                         
#> [181] igraph_1.4.3                           
#> [182] broom_1.0.4                            
#> [183] memoise_2.0.1                          
#> [184] ggtree_3.8.0                           
#> [185] fastmatch_1.1-3                        
#> [186] bit_4.0.5                              
#> [187] downloader_0.4                         
#> [188] gson_0.1.0                             
#> [189] ape_5.7-1

Created on 2023-06-02 with reprex v2.0.2
",
76383296,Broadcast receiver change UI,"I have searched widely but not found an answer to this question:
Is it possible to change a variable in a Jetpack compose user interface from a broadcast receiver?
",76385696,"You can't modify your compose ui from Broadcast receiver directly. Instead, your Broadcast receiver should change some data in your data layer - datastore, preferences, database or just in memory in some Repository singleton class. Then you should make this data observable and observe them from your compose ui.
",
76389017,Automatic template argument deduction fails for return type,"I have a template function
template <typename B, typename A>
B func(A x) {/*do something*/}

And in my code I am doing something like
uint32_t a[6] = {1,2,3,4,5,6};
uint64_t b = func(a);

But this fails with
test.cxx:10:16: error: no matching function for call to 'func'
  uint64_t b = func(a);
               ^~~~~~~~~
test.cxx:5:6: note: candidate template ignored: couldn't infer template argument 'B'

So it seems that the type of a is automatically deduced but the type of b not. I can specify it (uint64_t b = <uint64_t>func(a);), sure, but why the compiler is not able to deduce it?
EDIT
Following comments also posting the content of the function (which is actually taking 2 other parameters)
template <typename B, typename A>
B bit_slice(A words[], int start, int end) {
  int wordSize = sizeof(A)*8;
  B s = 0;
  int n = end / wordSize;
  for(int i= 0; i <= n; ++i){
    s = (s << wordSize) + words[i];
  }
  s >>= (n+1) * wordSize - (end+1);
  B mask = (((T)1) << (end - start + 1))- 1;
  s &= mask;
  return s;
};

So basically a bit slicing of an array (representing something).
",76389067,"If the argument cannot be deduced from func(a); then it cannot be deduced. auto return type can be deduced from the return statement, but not here. What you assign the return value to is not relevant. It simply doesnt work like this.
It does work however with a templated conversion:
#include <iostream>
#include <cstdio>

template <typename A,typename B>
A the_implementation(B x) { return x*2; }

template <typename B>
struct proxy {
    B b;
    template <typename A>
    operator A() {
        return the_implementation<A>(b);
    }
};

template<typename B>
proxy<B> func(B x) {
    return {x};
}


int main() { 
    int x = 42;
    int y = func(x);
}

func returns a proxy<B>, B is deduced from x. Then proxy<B> can be converted to int via its conversion operator, which calls the actual function (because only now A and B have been deduced).
However, I have doubts that the above is a good idea in your code. Consider to write instead:
auto b = func<uint64_t>(a);

",
76381199,How to increase allocated memory in a k3d cluster,"I am on a learning curve with Kubernetes and recently started working with K3D. I am trying to deploy my project on a local K3D cluster. When I created a pod to run an application, I saw it hang in the pending state for some time, and below is the kubectl describe pod output (events).

application.yaml file's resource requirements are as below
resources:
        requests:
          memory: ""4Gi""
          cpu: ""2""

The output of kubectl decribe node is as below: I assumed this was due to the fact that my node has around 3 GB of memory and the app is requesting 4 GB. I am getting the error in Pod. I looked for an answer to increase the memory but had no luck so far.

How can I increase the memory to get the application up and running? If I reduce the app.yaml resource to --> memory: 3 Gi or 2 Gi, the app starts running, but the actual functionality of the app is not there. Whenever I try to do something in the app, it then gives me Not enough CPU and/or memory is available for error in my application.
I am running this on Linux and k3d version k3d version v5.5.1 k3s version v1.26.4-k3s1 (default)
Thanks!
",76387620,"Assuming the machine where you are running has more than 3GB of RAM (you can check by running lsmem or free), you can try re-creating the Kubernetes cluster using k3d, by passing an explicit memory limit. E.g.
k3d cluster create --agents-memory 8G

Or if you are doing a multi-node deployment, by adding a node with sufficient memory, e.g.
k3d node create --memory 8G

But when running on Linux, you typically would not have a memory limit applied to the Kubernetes cluster, unless that limit was requested explicitly. So I would suggest checking your previous cluster creation commands, or double-check any scripts you may have used.
If you are running Linux, another option is to run k3s directly, without k3d. That is unlikely to see limits applied to it as well.
Finally, an alternative is to use an ephemeral cloud environment for this type of testing. For example, using https://namespace.so you can create a Kubernetes cluster with 8GB of RAM in a few seconds, and use it to test your application.
",
76382660,update PyqtGraph plot in PyQt5,"I'm writing a small interface in PyQT5 that has a graph that I use PyQtGraph to create. The graph starts to be drawn by pressing the ""Start"" button, and at first everything looks fine:
But over time and an increase in the number of points, the entire graph shrinks to the width of the screen and becomes not informative:

In this regard, there are two questions:
How can I make the window not try to fit the whole graph at once, squeezing it, but rather move behind it, showing the latest data?
Now the data comes in once a second and I redraw the graph every time. Is it possible to make it a partial update so that I just pass only new data into it?
from pyqtgraph import PlotWidget
import pyqtgraph
from PyQt5 import QtCore
from PyQt5.QtCore import Qt, QThread, QTimer, QObject, pyqtSignal, QTimer
from PyQt5.QtWidgets import QHBoxLayout, QMainWindow,  QPushButton, QVBoxLayout, QWidget, QApplication
import sys
import random


def get_kl_test():
    choices = [50, 50, 50, 51, 51, 51, 52, 52, 52]
    list = [random.choice(choices) for i in range(11)]
    return list


def get_iopd_test():
    choices = [40, 40, 40, 50, 50, 50, 60, 60, 60]
    return random.choice(choices)


class Graph(PlotWidget):
    def __init__(self):
        super().__init__()
        self.setBackground('white')
        self.addLegend()
        self.showGrid(x=True, y=True)
        self.setYRange(0, 255, padding=0)


class ReadingWorker(QObject):
    update_graph = pyqtSignal(list, list, list, list)

    def __init__(self):
        super().__init__()
        self.time_from_start = 0
        self.time_values = []
        self.green_values = []
        self.blue_values = []
        self.red_values = []

    def run(self):
        self.read()
        self.update_time()

    def read(self):
        ipd_values = get_kl_test()
        iopd_value = get_iopd_test()

        self.green_values.append(ipd_values[0])
        self.blue_values.append(ipd_values[1])
        self.red_values.append(iopd_value)
        self.time_values.append(self.time_from_start)

        self.update_graph.emit(
            self.green_values, self.blue_values, self.red_values, self.time_values)
        QTimer.singleShot(1000, self.read)

    def update_time(self):
        self.time_from_start += 1
        QTimer.singleShot(1000, self.update_time)


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.central_widget = QWidget(self)
        self.setGeometry(50, 50, 1300, 700)
        self.setCentralWidget(self.central_widget)
        self.layout_main_window = QVBoxLayout()
        self.central_widget.setLayout(self.layout_main_window)

        # конфигурация тулбара
        self.layout_toolbar = QHBoxLayout()
        self.layout_toolbar.addStretch(1)
        self.btn_start = QPushButton(""Старт"")
        self.btn_start.clicked.connect(self.start)
        self.layout_toolbar.addWidget(self.btn_start)
        self.layout_main_window.addLayout(self.layout_toolbar)

        # конфигурация графика
        self.graph = Graph()
        self.layout_main_window.addWidget(self.graph)

    def start(self):
        self.reading_thread = QThread(parent=self)
        self.reading_widget = ReadingWorker()
        self.reading_widget.moveToThread(self.reading_thread)
        self.reading_widget.update_graph.connect(self.draw_graph)
        self.reading_thread.started.connect(self.reading_widget.run)
        self.reading_thread.start()

    @QtCore.pyqtSlot(list, list, list, list)
    def draw_graph(self, ipd_1_values, ipd_2_values, iopd_values, time_values):
        
        self.graph.plotItem.clearPlots()
        pen_ipd_1 = pyqtgraph.mkPen(color='green', width=4)
        pen_ipd_2 = pyqtgraph.mkPen(color='blue', width=4, style=Qt.DashDotLine)
        pen_iopd = pyqtgraph.mkPen(color='red', width=4, style=Qt.DashLine)

        line_ipd_1 = self.graph.plotItem.addItem(pyqtgraph.PlotCurveItem(
            time_values, 
            ipd_1_values,
            pen=pen_ipd_1,
            name='1'
        ))
        line_ipd_2 = self.graph.plotItem.addItem(pyqtgraph.PlotCurveItem(
            time_values, 
            ipd_2_values,
            pen=pen_ipd_2,
            name='2'
        ))
        line_iopd = self.graph.plotItem.addItem(pyqtgraph.PlotCurveItem(
            time_values, 
            iopd_values,
            pen=pen_iopd,
            name='3'
        ))


if __name__ == '__main__':
    app = QApplication(sys.argv)
    app.setStyle('Fusion')
    main_window = MainWindow()
    main_window.show()
    sys.exit(app.exec_())

",76385756,"
step 1: add the PlotCurveItems as members of Mainwindow, set them up in the constructor, so you can access them later
step 2: in the draw_graph function use the getData() and setData() functions of the PlotcurveItems, update them
step 3: if you have enough x-values set the xRange, so not all data is shown, I use a maximal xRange of 20 here (self.window_size)

In the code below I only use the last entry in your lists (e.g. ipd_1_values[-1]), you can just pass scalars and remove the [-1].
Also I used import numpy as np for the np.append().
    class MainWindow(QMainWindow):
        def __init__(self):
            super().__init__()
            self.central_widget = QWidget(self)
            self.setGeometry(50, 50, 1300, 700)
            self.setCentralWidget(self.central_widget)
            self.layout_main_window = QVBoxLayout()
            self.central_widget.setLayout(self.layout_main_window)

            # конфигурация тулбара
            self.layout_toolbar = QHBoxLayout()
            self.layout_toolbar.addStretch(1)
            self.btn_start = QPushButton(""Старт"")
            self.btn_start.clicked.connect(self.start)
            self.layout_toolbar.addWidget(self.btn_start)
            self.layout_main_window.addLayout(self.layout_toolbar)

            # конфигурация графика
            self.graph = Graph()
            self.layout_main_window.addWidget(self.graph)

            self.setup_graphs() # step 1
            self.window_size = 20 # step 3

        def start(self):
            self.reading_thread = QThread(parent=self)
            self.reading_widget = ReadingWorker()
            self.reading_widget.moveToThread(self.reading_thread)
            self.reading_widget.update_graph.connect(self.draw_graph)
            self.reading_thread.started.connect(self.reading_widget.run)
            self.reading_thread.start()

        def setup_graphs(self):
            pen_ipd_1 = pyqtgraph.mkPen(color='green', width=4)
            pen_ipd_2 = pyqtgraph.mkPen(color='blue', width=4, style=Qt.DashDotLine)
            pen_iopd = pyqtgraph.mkPen(color='red', width=4, style=Qt.DashLine)
            self.line_ipd_1 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_1, name='1')
            self.line_ipd_2 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_2, name='2')
            self.line_iopd = pyqtgraph.PlotCurveItem([], [], pen=pen_iopd, name='3')
            self.graph.plotItem.addItem(self.line_ipd_1)
            self.graph.plotItem.addItem(self.line_ipd_2)
            self.graph.plotItem.addItem(self.line_iopd)

        @QtCore.pyqtSlot(list, list, list, list)
        def draw_graph(self, ipd_1_values, ipd_2_values, iopd_values, time_values): # step 2
            x, y = self.line_ipd_1.getData()
            x = np.append(x, time_values[-1])
            self.line_ipd_1.setData(y=np.append(y, ipd_1_values[-1]), x=x)
            _, y = self.line_ipd_2.getData()
            self.line_ipd_2.setData(y=np.append(y, ipd_2_values[-1]), x=x)
            _, y = self.line_iopd.getData()
            self.line_iopd.setData(y=np.append(y, iopd_values[-1]), x=x)
            if (len(x)>0 and x[-1]-x[0]>self.window_size): # step 3
                self.graph.plotItem.setXRange(x[-1]-self.window_size, x[-1])

",
76388928,Can not update @State variable via delegate set in View.init(),"The following is a minimum example code with the problem.
struct TestView: View {
    @State var text = ""Hello""
    let useCase = TestUseCase()

    init() {
        useCase.output = self
    }

    var body: some View {
        Text(text)
            .onAppear {
                // ① useCase.output = self
                useCase.show()
            }
    }
}
extension TestView: TestUseCaseOutput {
    func showText(text: String) {
        self.text = text
    }
}

class TestUseCase {
    var output: TestUseCaseOutput?
    func show() {
        output?.showText(text: ""Changed"")
    }
}
protocol TestUseCaseOutput {
    func showText(text: String)
}

This code changes the text from ""Hello"" to ""Changed"" when the view is displayed, but the change is not reflected. The showText method was called, but it had no effect. I also found that if I set the delegate at ①, the text was updated correctly.
Can anyone tell me the cause of this problem?
",76389086,"SwiftUI views are structs, and therefore immutable.  Whenever a SwiftUI changes, a new instance of that view struct is created.
When you update text, SwiftUI needs to create a new instance of TestView.  But, the new instance  has text set to Hello (and it was also have a new instance of TestUseCase) so you don't see any change.
The sequence of events is:

You create an instance of TestView - This is initialised with text = ""Hello""
You update text which triggers SwiftUI to recreate TestView
The newly created instance of TestView is initialised with text = ""Hello""`

In SwiftUI no object should ever need to hold a reference to a View.
init() {
    useCase.output = self
}

The self you store in your TestUseCase instance will be thrown away as soon as the view is updated.  It simply isn't useful to try and hold references to SwiftUI views.
You should structure your code so that views respond to changes in your model (Via @Published for example).  Your model should never try and update a view directly.
",
76387595,react-number-format Showing Format On inserting value,"react-number-format Showing Format On inserting value
I have used this package for the Phone Input format
import { PatternFormat } from 'react-number-format';

<PatternFormat 
   value={value} 
   className='form-control'   
   format=""(###) ###-####"" 
 />

Due to this format whenever I add any single value, The formatted value show in the input. before the value reaches there.

I want to show the value in This format but when the value reaches there,
when I have entered only a single value it should not show that '-' there.
I want something like below

This is the link to the npm package I am using:
https://s-yadav.github.io/react-number-format/docs/intro
",76387683,"You could build the pattern diffrent if the value reaches said length then change   (empty space) for a - (dash)
import { PatternFormat } from 'react-number-format';

let pattern;
if (value.length >= 9) {
    pattern = ""(###) ###-####"";
} else {
    pattern = ""(###) ### ####"";
}

<PatternFormat 
   value={value} 
   className='form-control'   
   format={pattern} 
 />


",
76387605,Compiling C++ code using libssh library through vcpkg,"I installed libssh through vcpkg on my Windows 8.1 machine.
vcpkg install libssh

Now I am trying to compile my c++ code making use of libssh.
#include <libssh/libssh.h>
#include <stdlib.h>
 
int main()
{
  ssh_session my_ssh_session = ssh_new();
  if (my_ssh_session == NULL)
    exit(-1);
  ...
  ssh_free(my_ssh_session);
}

But I am receiving following error.
D:\remoteDesktopTimeZone>gcc sample.cpp -o sampl
sample.cpp:1:10: fatal error: libssh/libssh.h: No such file or directory
    1 | #include <libssh/libssh.h>
      |          ^~~~~~~~~~~~~~~~~
compilation terminated.

",76387686,"First, you should ensure that you are installing libraries with correct ""triplet"" matching your compiler and architecture.
I don't know if your gcc is MingW or Cygwin.
See instructions here.
Second, you should either use CMake as described here, or manually point the compiler where to find the library headers and static libraries using the -I and -L command line flags.
",
76388987,How can I sort a JSON array by a key inside of it?,"I have an unknown number of items and item categories in a json array like so:
[
    {
        ""x_name"": ""Some Name"",
        ""x_desc"": ""Some Description"",
        ""id"": 1,
        ""category"": ""Email""
    },
    {
        ""x_name"": ""Another name here"",
        ""x_desc"": ""Another description"",
        ""id"": 2,
        ""category"": ""Email""
    },
    {
        ""x_name"": ""Random Name"",
        ""x_desc"": ""Random Description"",
        ""id"": 3,
        ""category"": ""Email""
    },
    {
        ""x_name"": ""Owner Meetings"",
        ""x_desc"": ""Total count"",
        ""id"": 167,
        ""category"": ""Owner Specific""
    },
    {
        ""x_name"": ""Owner Tasks"",
        ""x_desc"": ""Total count of tasks"",
        ""id"": 168,
        ""category"": ""Owner Specific""
    },
    {
        ""x_name"": ""Owner Calls"",
        ""x_desc"": ""Total count of calls"",
        ""id"": 169,
        ""category"": ""Owner Specific""
    },
    {
        ""x_name"": ""Overall Total Views"",
        ""x_desc"": ""The total views"",
        ""id"": 15,
        ""category"": ""Totals Report""
    }
    ......
]

I need to group these JSONObjects based on the property ""category"".
I've seen similar examples in JS using the reduce function but couldn't get a similar python solution. How can I efficiently do this in Python?
The desired outcome would be:
{
    ""category"": ""Email"",
    ""points"": [
        {
            ""x_name"": ""Some Name"",
            ""x_desc"": ""Some Description"",
            ""id"": 1,
            ""category"": ""Email""
        },
        {
            ""x_name"": ""Another name here"",
            ""x_desc"": ""Another description"",
            ""id"": 2,
            ""category"": ""Email""
        },
        {
            ""x_name"": ""Random Name"",
            ""x_desc"": ""Random Description"",
            ""id"": 3,
            ""category"": ""Email""
        }
    ]
}

and then:
{
    ""category"": ""Owner Specific"",
    ""points"": [
        {
            ""x_name"": ""Owner Meetings"",
            ""x_desc"": ""Total count"",
            ""id"": 167,
            ""category"": ""Owner Specific""
        },
        {
            ""x_name"": ""Owner Tasks"",
            ""x_desc"": ""Total count of tasks"",
            ""id"": 168,
            ""category"": ""Owner Specific""
        },
        {
            ""x_name"": ""Owner Calls"",
            ""x_desc"": ""Total count of calls"",
            ""id"": 169,
            ""category"": ""Owner Specific""
        }
    ]
}

and so on.
I do not know the value of the key ""category"" or the number of ""categories"" in the original JSON array.
",76389120,"Here is a small script I made for that.
Script
def sort_by_category():
    categories = {}
    output = []

    a = [
        {
            ""x_name"": ""Some Name"",
            ""x_desc"": ""Some Description"",
            ""id"": 1,
            ""category"": ""Email"",
        },
        ...
    ]

    for i in a:
        if i[""category""] in categories:
            categories[i[""category""]].append(i)
        else:
            categories[i[""category""]] = [i]

    for c in categories:
        o = {""category"": c, ""points"": categories[c]}
        output.append(o)
    return(output)

It browses your a array and creates another array based on categories, then it formats the output as you asked.
Output
[
    {
        ""category"":""Email"",
        ""points"":[
            {
                ""x_name"":""Some Name"",
                ""x_desc"":""Some Description"",
                ""id"":1,
                ""category"":""Email""
            },
            {
                ""x_name"":""Another name here"",
                ""x_desc"":""Another description"",
                ""id"":2,
                ""category"":""Email""
            },
            {
                ""x_name"":""Random Name"",
                ""x_desc"":""Random Description"",
                ""id"":3,
                ""category"":""Email""
            }
        ]
    },
    {
        ""category"":""Owner Specific"",
        ""points"":[
            {
                ""x_name"":""Owner Meetings"",
                ""x_desc"":""Total count"",
                ""id"":167,
                ""category"":""Owner Specific""
            },
            {
                ""x_name"":""Owner Tasks"",
                ""x_desc"":""Total count of tasks"",
                ""id"":168,
                ""category"":""Owner Specific""
            },
            {
                ""x_name"":""Owner Calls"",
                ""x_desc"":""Total count of calls"",
                ""id"":169,
                ""category"":""Owner Specific""
            }
        ]
    },
    {
        ""category"":""Totals Report"",
        ""points"":[
            {
                ""x_name"":""Overall Total Views"",
                ""x_desc"":""The total views"",
                ""id"":15,
                ""category"":""Totals Report""
            }
        ]
    }
]

",
76387588,Casting a void pointer to a 2D String array pointer (C/CPP),"I'm using a library which requires a function with a void* pointer as a parameter. I have a 2D string array and I want to pass that array through that parameter and extract it inside the function. I successfully passed the array as a pointer but I don't know how to convert that pointer back to my array.
This is my current code:
String str_array[100][10];

int callback(void* data) {

  String* str_array_ptr[100][10] = (String* [100][10])data;

  (*str_array_ptr)[0][0] = ""text"";

  return 0;

}

void test() {
  callback(&str_array);
}

However, when compiling, I obtain the following error message:

error: ISO C++ forbids casting to an array type 'String* [100][10]' [-fpermissive]

PS: I'm trying to use the SQLite library's sqlite3_exec() function and store the result of a ""SELECT SQL query"" into a 2D string array.
SQLite C Interface - One-Step Query Execution Interface
",76387690,"You cannot cast a pointer to an array. Instead you access your array through another pointer. That pointer has type String (*)[10]. Like this
String str_array[100][10];

int callback(void* data) { 

    String (*str_array_ptr)[10] = (String (*)[10])data;

    str_array_ptr[0][0] = ""text""; // Note no '*'

    return 0;

}

void test() {
    callback(str_array); // Note no '&'
}

Both the way you create the pointer, you don't need to use &, and the way you access the pointer, you don't need to use *, are also wrong in your code. See the code above for details.
The fundamental issue here (and maybe the issue you are misunderstanding) is the difference between String *x[10]; and String (*x)[10];. In the first case x is an array of 10 pointers to String, in the second case x is a pointer to an array of ten String. It's the second option that you want.
",
76382813,BizTalk 2020 with BTDF & Azure Pipelines - Application dependencies,"Currently transitioning from BizTalk 2013r2 to 2020, and implementing Azure Pipelines to automate deployment with BTDF.
So far, we're able to deploy our Core applications, but we've just realised there are dependencies with the 'child applications' (applications that take schemas from the Core apps).

How should I refer to the Core application within Visual Studio in our dev environment (we used to reference the .dll from the local repos solution)
How can we configure BTDF and CI/CD pipelines to point to the correct application when deploying the child?
What happens if we need to update the Core application via BTDF/Pipeline - we'll need to undeploy all child applications that reference it before we can deploy surely - can this be done via the BTDF config?

",76385918,"No, I don't believe that BTDF can take care of that.
You should either.
Version increase your assembly version number of your Core Application and do a side by side deployment (e.g. leave the original ones in place).
Later on when you need the newer version in the dependent application then reference the later DLL (and yes, just have the DLL as an external assembly in the solution).
Or
Have your Core Application Pipeline undeploy all the dependent applications, before undeploying the Core Application and deploying it, and then re-deploying all the dependent applications.
My preference would be for the first option, less complicated.
",
76387658,Nattable Display converter shifts columns when the table is scrolled horizontally,"I have used a custom DisplayConverter on some columns of my Nattable.This displayconverter shows long values as hex strings.
Whenever I scroll my Nattable horizontally, this converter shifts one/mulitple columns. This results in columns which show hex values to be shown in default numeral format. On the other hand, columns which should be showing numerals show hex values.
In the following images, the first image shows how it should be displayed, that is column number 2 and 7 should show hex values (these are just long values with my custom converter applied).
When I scroll my table to the right, this converter is then applied to column number 3 and 8.
Default (as it should be)
Scrolled right 
I have applied my CustomDisplayConverter ( column override HEX_FORMAT) to certain columns. LinkerMapHexAddressDisplayConverter is the custom display converter which converts long values to hex strings for display.
'columnLabelAccumulator.registerColumnOverrides(
            pnames.indexOf(ILinkerMapConstants.PROP_KEY_SECTION_SIZE), NUMBER_FORMAT);//column 3
    columnLabelAccumulator.registerColumnOverrides(
            pnames.indexOf(ILinkerMapConstants.PROP_KEY_OBJECT_SIZE), NUMBER_FORMAT);//column 8

    configRegistry.registerConfigAttribute(CellConfigAttributes.DISPLAY_CONVERTER,
            new DefaultLongDisplayConverter(), DisplayMode.NORMAL, NUMBER_FORMAT);

    columnLabelAccumulator.registerColumnOverrides(
            pnames.indexOf(ILinkerMapConstants.PROP_KEY_SECTION_ADDRESS), HEX_FORMAT);//column 2
    columnLabelAccumulator.registerColumnOverrides(
            pnames.indexOf(ILinkerMapConstants.PROP_KEY_OBJECT_MODULE_ADDRESS),
            HEX_FORMAT);//column 7

    configRegistry.registerConfigAttribute(CellConfigAttributes.DISPLAY_CONVERTER,
            new LinkerMapHexAddressDisplayConverter(), DisplayMode.NORMAL,
            HEX_FORMAT);'

",76387707,"This happens if you apply your custom labels (HEX_FORMAT in your case) on the ViewportLayer or above. If you have a strong relation on the structure, you should apply the labels on the DataLayer as there is no index-position conversion.
",
76383599,Getting SQLSTATE[23000] error with Laravel global scope in many-to-many relationship,"Subject 1: I am using Laravel version 7 in my project, and in order to add a query to all my models in Laravel, I have added a global scope to all my models. In this way, all my models inherit from another model. The mentioned model is provided below.
namespace App\Models;

use Illuminate\Database\Eloquent\Builder;

class Model extends \Illuminate\Database\Eloquent\Model
{
    /**
     * The ""booting"" method of the model.
     * @return void
     */
    protected static function boot()
    {
        parent::boot();
        if(!empty(Client::$Current_Token)) {
            static::addGlobalScope('client_token', function (Builder $builder) {
                $builder->where('client_token', Client::$Current_Token);
            });
        }
    }
}

Subject 2: There is a model named ""user"" and a model named ""role"", and there is a many-to-many relationship between these 2 tables. Now, imagine that I want to retrieve all the roles associated with a user in the ""user"" model using the belongsToMany method, based on their relationship defined in the intermediate table.
/**
* The roles that belong to the user.
*/
public function roles(): BelongsToMany
{
    return $this->belongsToMany(Role::class, 'role_user', 'user_id', 'role_id');
}

Problem: I encounter the following error: SQLSTATE\[23000\]: Integrity constraint violation: 1052 Column 'client_token' in the WHERE clause is ambiguous and I know it is related to the condition I added in the global scope.
",76386234,"I believe you got that error because a number of your tables are having that client_token column. So when you got a database query involving multiple tables, it just doesn't know which client_token column you are talking about.
Lets create a scope class so we can access the table name of the model:
<?php
 
namespace App\Scopes;
 
use Illuminate\Database\Eloquent\Scope;
use Illuminate\Database\Eloquent\Model;
use Illuminate\Database\Eloquent\Builder;
 
class ClientTokenScope implements Scope
{
    /**
     * Apply the scope to a given Eloquent query builder.
     *
     * @param  \Illuminate\Database\Eloquent\Builder  $builder
     * @param  \Illuminate\Database\Eloquent\Model  $model
     * @return void
     */
    public function apply(Builder $builder, Model $model)
    {
        $builder->where(""{$model->getTable()}.client_token"", Client::$Current_Token);
    }
}

Then applying the scope in the boot method:
namespace App\Models;

use App\Scopes\ClientTokenScope;

use Illuminate\Database\Eloquent\Builder;

class Model extends \Illuminate\Database\Eloquent\Model
{
    /**
     * The ""booting"" method of the model.
     * @return void
     */
    protected static function boot()
    {
        parent::boot();
        if(!empty(Client::$Current_Token)) {
            static::addGlobalScope(new ClientTokenScope);
        }
    }
}

",
76389075,Return data from database and show in calendar,"I'm creating a calendar for event bookings. The calendar is already working at the event registration level. I'm having trouble then showing the events registered in the database on the calendar.
To show the data in the database I am trying this way:
var datta = [
   {PequenoAlm:  ""Peq_Almoço"", Valencia: ""Teste1"", Ano: ""2023"", Mes: ""6"", Dia: ""27"", },
];

var verif = [];
var verif1 = [];
var verif2 = [];
var verif3 = [];
var verif4 = [];
for (var i = 0; i < datta.length; i++) {
  var PequenoAlm = datta[0].PequenoAlm;
  var Valencia = datta[0].Valencia;
  var Ano = datta[0].Ano;
  var Mes = datta[0].Mes;
  var Dia = datta[0].Mes;
  
  verif.push(PequenoAlm);
  verif1.push(Valencia);
  verif2.push(Ano);
  verif3.push(Mes);
  verif4.push(Dia);
}
    
 var event_data = {
   ""events"": [
     {
       ""occasion"": verif,
       ""invited_count"": verif1,
       ""year"": verif2,
       ""month"": verif3,
       ""day"": verif4,
       ""cancelled"": true
     }
   ]
 };

The problem is that it doesn't show any information on the calendar.
I leave the complete code of how I am doing it and with the part of the code that I am trying to return the data from the database to the calendar.
Snippet below:


$(document).ready(function() {
  var date = new Date();
  var today = date.getDate();

  $("".right-button"").click({
    date: date
  }, next_year);
  $("".left-button"").click({
    date: date
  }, prev_year);
  $("".month"").click({
    date: date
  }, month_click);
  $("".right-button1"").click({
    date: date
  }, next_mes);
  $("".left-button1"").click({
    date: date
  }, prev_mes);
  $(""#add-button"").click({
    date: date
  }, new_event);
  $("".months-row"").children().eq(date.getMonth()).addClass(""active-month"");
  init_calendar(date);
  var events = check_events(today, date.getMonth() + 1, date.getFullYear());
  show_events(events, months[date.getMonth()], today);
});

function init_calendar(date) {
  $("".tbody"").empty();
  $("".events-container"").empty();
  var calendar_days = $("".tbody"");
  var month = date.getMonth();
  var year = date.getFullYear();
  var day_count = days_in_month(month, year);
  var row = $(""<tr class='table-row'></tr>"");
  var today = date.getDate();

  date.setDate(1);
  var first_day = date.getDay();
  for (var i = 0; i < 35 + first_day; i++) {
    var day = i - first_day + 1;
    if (i % 7 === 0) {
      calendar_days.append(row);
      row = $(""<tr class='table-row'></tr>"");
    }

    if (i < first_day || day > day_count) {
      var curr_date = $(""<td class='table-date nil'>"" + ""</td>"");
      row.append(curr_date);
    } else {
      var curr_date = $(""<td class='table-date'>"" + day + ""</td>"");
      var events = check_events(day, month + 1, year);

      if (today === day && $("".active-date"").length === 0) {
        curr_date.addClass(""active-date"");
        show_events(events, months[month], day);
      }

      if (events.length !== 0) {
        curr_date.addClass(""event-date"");
      }

      curr_date.click({
        events: events,
        month: months[month],
        day: day
      }, date_click);
      row.append(curr_date);
    }
  }

  calendar_days.append(row);
  $("".year"").text(year);

  calendar_days.append(row);
  if (month == 0) {
    $("".mes"").text(""Janeiro"");
  }
  if (month == 1) {
    $("".mes"").text(""Fevereiro"");
  }
  if (month == 2) {
    $("".mes"").text(""Março"");
  }
  if (month == 3) {
    $("".mes"").text(""Abril"");
  }
  if (month == 4) {
    $("".mes"").text(""Maio"");
  }
  if (month == 5) {
    $("".mes"").text(""Junho"");
  }
  if (month == 6) {
    $("".mes"").text(""Julho"");
  }
  if (month == 7) {
    $("".mes"").text(""Agosto"");
  }
  if (month == 8) {
    $("".mes"").text(""Setembro"");
  }
  if (month == 9) {
    $("".mes"").text(""Outubro"");
  }
  if (month == 10) {
    $("".mes"").text(""Novembro"");
  }
  if (month == 11) {
    $("".mes"").text(""Dezembro"");
  }
}

function days_in_month(month, year) {
  var monthStart = new Date(year, month, 1);
  var monthEnd = new Date(year, month + 1, 1);
  return (monthEnd - monthStart) / (1000 * 60 * 60 * 24);
}

function date_click(event) {
  $("".events-container"").show(250);
  $(""#diaalog"").hide(250);
  $("".active-date"").removeClass(""active-date"");
  $(this).addClass(""active-date"");
  show_events(event.data.events, event.data.month, event.data.day);
};

function month_click(event) {
  $("".events-container"").show(250);
  $(""#diaalog"").hide(250);
  var date = event.data.date;
  $("".active-month"").removeClass(""active-month"");
  $(this).addClass(""active-month"");
  var new_month = $("".month"").index(this);
  date.setMonth(new_month);
  init_calendar(date);
}

function next_year(event) {
  $(""#diaalog"").hide(250);
  var date = event.data.date;
  var new_year = date.getFullYear() + 1;
  $(""year"").html(new_year);
  date.setFullYear(new_year);
  init_calendar(date);
}

function prev_year(event) {
  $(""#diaalog"").hide(250);
  var date = event.data.date;
  var new_year = date.getFullYear() - 1;
  $(""year"").html(new_year);
  date.setFullYear(new_year);
  init_calendar(date);
}

function next_mes(event) {
  $(""#diaalog"").hide(250);
  var date = event.data.date;
  var new_mes = date.getMonth() + 1;
  $(""mes"").html(new_mes);
  date.setMonth(new_mes);
  init_calendar(date);
}

function prev_mes(event) {
  $(""#diaalog"").hide(250);
  var date = event.data.date;
  var new_mes = date.getMonth() - 1;
  $(""mes"").html(new_mes);
  date.setMonth(new_mes);
  init_calendar(date);
}

function new_event(event) {
  if ($("".active-date"").length === 0)
    return;
  $(""inpuut"").click(function() {
    $(this).removeClass(""error-inpuut"");
  })
  $(""#diaalog input[type=text]"").val('');
  $(""#diaalog input[type=number]"").val('');
  $("".events-container"").hide(250);
  $(""#diaalog"").show(250);

  $(""#cancel-button"").click(function() {
    $(""#reff"").removeClass(""error-inpuut"");
    $(""#reff1"").removeClass(""error-inpuut"");
    $(""#reff2"").removeClass(""error-inpuut"");
    $(""#almm"").removeClass(""error-inpuut"");
    $(""#almm1"").removeClass(""error-inpuut"");
    $(""#almm2"").removeClass(""error-inpuut"");
    $(""#almm3"").removeClass(""error-inpuut"");
    $(""#valref"").removeClass(""error-inpuut"");
    $(""#Dataref"").removeClass(""error-inpuut"");
    $(""#diaalog"").hide(250);
    $("".events-container"").show(250);
  });
}

function show_events(events, month, day) {
  $("".events-container"").empty();
  $("".events-container"").show(250);
  console.log(event_data[""events""]);

  if (events.length === 0) {
    var event_card = $(""<div class='event-card'></div>"");
    var event_name = $(""<div class='event-name'>Não há refeições marcadas para "" + day + ""  "" + month + "".</div>"");
    $(event_card).css({
      ""border-left"": ""10px solid #FF1744""
    });
    $(event_card).append(event_name);
    $("".events-container"").append(event_card);
  } else {
    for (var i = 0; i < events.length; i++) {
      var event_card = $(""<div class='event-card'></div>"");
      var event_name = $(""<div class='event-name'>"" + events[i][""occasion""] + "":</div>"");
      var event_count = $(""<div class='event-count'>"" + events[i][""invited_count""] + "" Invited</div>"");

      if (events[i][""cancelled""] === true) {
        $(event_card).css({
          ""border-left"": ""10px solid #FF1744""
        });
        event_count = $(""<div class='event-cancelled'>Cancelled</div>"");
      }
      $(event_card).append(event_name).append(event_count);
      $("".events-container"").append(event_card);
    }
  }
}

function check_events(day, month, year) {
  var events = [];
  for (var i = 0; i < event_data[""events""].length; i++) {
    var event = event_data[""events""][i];
    if (event[""day""] === day &&
      event[""month""] === month &&
      event[""year""] === year) {
      events.push(event);
    }
  }
  return events;
}

var datta = [{
  PequenoAlm: ""Peq_Almoço"",
  Valencia: ""Teste1"",
  Ano: ""2023"",
  Mes: ""6"",
  Dia: ""27"",
}, ];

var verif = [];
var verif1 = [];
var verif2 = [];
var verif3 = [];
var verif4 = [];
for (var i = 0; i < datta.length; i++) {
  var PequenoAlm = datta[0].PequenoAlm;
  var Valencia = datta[0].Valencia;
  var Ano = datta[0].Ano;
  var Mes = datta[0].Mes;
  var Dia = datta[0].Mes;

  verif.push(PequenoAlm);
  verif1.push(Valencia);
  verif2.push(Ano);
  verif3.push(Mes);
  verif4.push(Dia);
}

var event_data = {
  ""events"": [{
    ""occasion"": verif,
    ""invited_count"": verif1,
    ""year"": verif2,
    ""month"": verif3,
    ""day"": verif4,
    ""cancelled"": true
  }]
};

const months = [
  ""Janeiro"",
  ""Fevereiro"",
  ""Março"",
  ""Abril"",
  ""maio"",
  ""Junho"",
  ""Julho"",
  ""Agosto"",
  ""Setembro"",
  ""Outubro"",
  ""Novembro"",
  ""Dezembro""
];
.conteent {
  overflow: none;
  max-width: 790px;
  padding: 0px 0;
  height: 500px;
  position: relative;
  margin: 20px auto;
  background: #52A0FD;
  background: -moz-linear-gradient(right, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  background: -webkit-linear-gradient(right, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  background: linear-gradient(to left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  border-radius: 3px;
  box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
  -moz-box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
  -webkit-box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
}


/*  Events display */

.events-container {
  overflow-y: scroll;
  height: 100%;
  margin: 0px auto;
  font: 13px Helvetica, Arial, sans-serif;
  display: inline-block;
  padding: 0 10px;
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}

.events-container:after {
  clear: both;
}

.event-card {
  padding: 20px 0;
  width: 350px;
  margin: 20px auto;
  display: block;
  background: #fff;
  border-left: 10px solid #52A0FD;
  border-radius: 3px;
  box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
  -moz-box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
  -webkit-box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
}

.event-count,
.event-name,
.event-cancelled {
  display: inline;
  padding: 0 10px;
  font-size: 1rem;
}

.event-count {
  color: #52A0FD;
  text-align: right;
}

.event-name {
  padding-right: 0;
  text-align: left;
}

.event-cancelled {
  color: #FF1744;
  text-align: right;
}


/*  Calendar wrapper */

.calendar-container {
  position: relative;
  margin: 0px auto;
  height: 100%;
  background: #fff;
  font: 13px Helvetica, Arial, san-serif;
  display: inline-block;
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}

.calendar-container:after {
  clear: both;
}

.calendar {
  display: table;
}


/* Calendar Header */

.year-header {
  background: #52A0FD;
  background: -moz-linear-gradient(left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  background: -webkit-linear-gradient(left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  background: linear-gradient(to right, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  font-family: Helvetica;
  box-shadow: 0 3px 6px rgba(0, 0, 0, 0.16), 0 3px 6px rgba(0, 0, 0, 0.23);
  -moz-box-shadow: 0 3px 6px rgba(0, 0, 0, 0.16), 0 3px 6px rgba(0, 0, 0, 0.23);
  -webkit-box-shadow: 0 3px 6px rgba(0, 0, 0, 0.16), 0 3px 6px rgba(0, 0, 0, 0.23);
  height: 40px;
  text-align: center;
  position: relative;
  color: #fff;
  border-top-left-radius: 3px;
}

.year-header span {
  display: inline-block;
  font-size: 20px;
  line-height: 40px;
}

.left-button,
.right-button {
  cursor: pointer;
  width: 28px;
  text-align: center;
  position: absolute;
}

.left-button1,
.right-button1 {
  cursor: pointer;
  width: 28px;
  text-align: center;
  position: absolute;
}

.left-button {
  left: 0;
  -webkit-border-top-left-radius: 5px;
  -moz-border-radius-topleft: 5px;
  border-top-left-radius: 5px;
}

.left-button1 {
  left: 0;
  -webkit-border-top-left-radius: 5px;
  -moz-border-radius-topleft: 5px;
  border-top-left-radius: 5px;
}

.right-button {
  right: 0;
  top: 0;
  -webkit-border-top-right-radius: 5px;
  -moz-border-radius-topright: 5px;
  border-top-right-radius: 5px;
}

.right-button1 {
  right: 0;
  top: 0;
  -webkit-border-top-right-radius: 5px;
  -moz-border-radius-topright: 5px;
  border-top-right-radius: 5px;
}

.left-button:hover {
  background: #3FADFF;
}

.left-button1:hover {
  background: #3FADFF;
}

.right-button:hover {
  background: #00C1FF;
}

.right-button1:hover {
  background: #00C1FF;
}

.ajustebot {
  margin-top: -5%;
}


/* Buttons */

.bbuutton {
  cursor: pointer;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  outline: none;
  font-size: 1rem;
  border-radius: 25px;
  padding: 0.65rem 1.9rem;
  transition: .2s ease all;
  color: white;
  border: none;
  box-shadow: -1px 10px 20px #9BC6FD;
  background: #52A0FD;
  background: -moz-linear-gradient(left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  background: -webkit-linear-gradient(left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  background: linear-gradient(to right, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
}

#cancel-button {
  box-shadow: -1px 10px 20px #FF7DAE;
  background: #FF1744;
  background: -moz-linear-gradient(left, #FF1744 0%, #FF5D95 80%, #FF5D95 100%);
  background: -webkit-linear-gradient(left, #FF1744 0%, #FF5D95 80%, #FF5D95 100%);
  background: linear-gradient(to right, #FF1744 0%, #FF5D95 80%, #FF5D95 100%);
}

#add-button {
  display: block;
  position: absolute;
  right: 20px;
  bottom: 20px;
}

#add-button:hover,
#ok-button:hover,
#cancel-button:hover {
  transform: scale(1.03);
}

#add-button:active,
#ok-button:active,
#cancel-button:active {
  transform: translateY(3px) scale(.97);
}


/* Days/months tables */

.days-table,
.dates-table {
  border-collapse: separate;
  text-align: center;
}

.day {
  height: 26px;
  width: 26px;
  padding: 0 10px;
  line-height: 26px;
  border: 2px solid transparent;
  text-transform: uppercase;
  font-size: 90%;
  color: #9e9e9e;
}

.active-month {
  font-weight: bold;
  font-size: 14px;
  color: #FF1744;
  text-shadow: 0 1px 4px RGBA(255, 50, 120, .8);
}


/*  Dates table */

.table-date {
  cursor: default;
  color: #2b2b2b;
  height: 26px;
  width: 26px;
  font-size: 15px;
  padding: 10px;
  line-height: 26px;
  text-align: center;
  border-radius: 50%;
  border: 2px solid transparent;
  transition: all 250ms;
}

.table-date:not(.nil):hover {
  border-color: #FF1744;
  box-shadow: 0 2px 6px RGBA(255, 50, 120, .9);
}

.event-date {
  border-color: #52A0FD;
  box-shadow: 0 2px 8px RGBA(130, 180, 255, .9);
}

.active-date {
  background: #FF1744;
  box-shadow: 0 2px 8px RGBA(255, 50, 120, .9);
  color: #fff;
}

.event-date.active-date {
  background: #52A0FD;
  box-shadow: 0 2px 8px RGBA(130, 180, 255, .9);
}


/* input dialog */

.diaalog {
  z-index: 5;
  background: #fff;
  position: absolute;
  width: 438px;
  height: 500px;
  left: 352px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
  display: none;
  border-left: 1px #aaa solid;
  top: 0%;
}

.diaalog-header {
  margin: 20px;
  color: #333;
  text-align: center;
}

.form-ccontainer {
  margin-top: 5%;
}

.form-labeel {
  color: #333;
}

.inpuut {
  border: none;
  background: none;
  border-bottom: 1px #aaa solid;
  display: block;
  margin-bottom: 50px;
  width: 200px;
  height: 40px;
  text-align: center;
  transition: border-color 250ms;
}

.inpuut:focus {
  outline: none;
  border-color: #00C9FB;
}

.error-inpuut {
  border-color: #FF1744;
}


/* Tablets and smaller */

@media only screen and (max-width: 780px) {
  .conteent {
    overflow: visible;
    position: relative;
    max-width: 100%;
    width: 370px;
    height: 100%;
    background: #52A0FD;
    background: -moz-linear-gradient(left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
    background: -webkit-linear-gradient(left, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
    background: linear-gradient(to right, #52A0FD 0%, #00C9FB 80%, #00C9FB 100%);
  }
  .diaalog {
    width: 370px;
    height: 450px;
    border-radius: 3px;
    top: 0;
    left: 0;
  }
  .events-container {
    float: none;
    overflow: visible;
    margin: 0 auto;
    padding: 0;
    display: block;
    left: 0;
    border-radius: 3px;
  }
  .calendar-container {
    float: none;
    padding: 0;
    margin: 0 auto;
    margin-right: 0;
    display: block;
    left: 0;
    border-radius: 3px;
    box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
    -moz-box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
    -webkit-box-shadow: 3px 8px 16px rgba(0, 0, 0, 0.19), 0 6px 6px rgba(0, 0, 0, 0.23);
  }
}


/* Small phone screens */

@media only screen and (max-width: 400px) {
  .conteent,
  .events-container,
  .year-header,
  .calendar-container {
    width: 320px;
  }
  .diaalog {
    width: 320px;
  }
  .days-table {
    width: 320px;
    padding: 7px 7px;
  }
  .event-card {
    width: 300px;
  }
  .day {
    padding: 7px 7px;
  }
  .table-date {
    width: 320px;
    height: 20px;
    line-height: 20px;
  }
  .event-name,
  .event-count,
  .event-cancelled {
    font-size: .8rem;
  }
  #add-button {
    bottom: 2px;
    right: 5px;
    padding: 0.5rem 1.5rem;
  }
  .ajustebot {
    margin-top: -12%;
  }
}
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<div class=""conteent"">
  <div class=""calendar-container"">
    <div class=""calendar"">
      <div class=""year-header"">
        <span class=""left-button"" id=""prev""> &lang; </span>
        <span class=""year"" id=""label""></span>
        <span class=""right-button"" id=""next""> &rang; </span>
      </div>
      <div class=""year-header"">
        <span class=""left-button1"" id=""prev1""> &lang; </span>
        <span class=""mes"" id=""label""></span>
        <span class=""right-button1"" id=""next1""> &rang; </span>
      </div>
      <table class=""days-table"">
        <td class=""day"">Dom</td>
        <td class=""day"">Seg</td>
        <td class=""day"">Ter</td>
        <td class=""day"">Qua</td>
        <td class=""day"">Qui</td>
        <td class=""day"">Sex</td>
        <td class=""day"">Sab</td>
      </table>
      <div class=""frame"">
        <table class=""dates-table"">
          <tbody class=""tbody"">
          </tbody>
        </table>
      </div>
      <button class=""bbuutton"" id=""add-button"">Marcação</button>
    </div>
  </div>
  <div class=""events-container""></div>
  <div class=""diaalog"" id=""diaalog"">
    <h2 class=""diaalog-header""> Adicionar Nova Refeição </h2>
    <form class=""fform"" id=""fform"">
      <div class=""form-ccontainer"" align=""center"">
        <p class=""form-labeel"">Pequenas Refeições <span style=""color: red;"">*</span></p>
        <div class=""radio_containner"">
          <input type=""checkbox"" class=""inradio"" name=""reff"" id=""reff"" value=""Peq_Almoço"">
          <label for=""reff"" class=""labradio"">Pequeno-Almoço</label>
          <input type=""checkbox"" class=""inradio"" name=""reff1"" id=""reff1"" value=""Lanche"">
          <label for=""reff1"" class=""labradio"" style=""margin-left: 3%;"">Lanche</label>
          <input type=""checkbox"" class=""inradio"" name=""reff2"" id=""reff2"" value=""Ceia"">
          <label for=""reff2"" class=""labradio"" style=""margin-left: 3%;"">Ceia</label>
        </div>
        <p class=""form-labeel"">Refeição <span style=""color: red;"">*</span></p>
        <div class=""radio_containner"">
          <input type=""checkbox"" class=""inradio"" name=""almm"" id=""almm"" value=""Almoço"">
          <label for=""almm"" class=""labradio"">Almoço</label>
          <input type=""checkbox"" class=""inradio"" name=""almm1"" id=""almm1"" value=""Almoço_(Dieta)"">
          <label for=""almm1"" class=""labradio"" style=""margin-left: 3%;"">Almoço Dieta</label>
          <input type=""checkbox"" class=""inradio"" name=""almm2"" id=""almm2"" value=""Jantar"">
          <label for=""almm2"" class=""labradio"" style=""margin-left: 3%;"">Jantar</label>
          <input type=""checkbox"" class=""inradio"" name=""almm3"" id=""almm3"" value=""Jantar_(Dieta)"">
          <label for=""almm3"" class=""labradio"" style=""margin-left: 3%;"">Jantar Dieta</label>
        </div>
        <div class=""form-group"">
          <p class=""form-labeel""> Valência <span style=""color: red;"">*</span></p>
          <select class=""js-states form-control ajuste sssinglett"" name=""valref"" id=""valref"">
            <option></option>
            <option value=""3"" selected> ERPI</option>
          </select>
        </div>
        <label for=""Dataref"" class=""form-labeel"">Período de Marcação </label>
        <input type=""date"" class=""inpuut"" name=""Dataref"" id=""Dataref"">
        <div class=""ajustebot"">
          <input type=""button"" value=""Cancel"" class=""bbuutton"" id=""cancel-button"">
          <input type=""button"" value=""OK"" class=""bbuutton"" id=""ok-button"">
        </div>
      </div>
    </form>
  </div>
</div>



codepen
Can you help overcome this difficulty?
I'm trying like this:
var event_data = {
  ""events"": []
};

$(document).ready(function () {

  $.getJSON('consrefeicoes.php', function (datta) {
  
    for (var i = 0; i < datta.length; i++) {

      PequenoAlm = datta[i].PequenoAlm;     
      Valencia = datta[i].Valencia;
      Ano = datta[i].Ano;
      mes = datta[i].mes;
      dia = datta[i].dia;
      
      event_data.events.push({
        ""occasion"": PequenoAlm,
        ""invited_count"": Valencia,
        ""year"": Number(Ano),
        ""month"": Number(mes),
        ""day"": Number(dia),
        ""cancelled"": true
      })
      
    };

  });
});

php:
$Colaborador = $_SESSION['usuarioId'];

$query = $conn->prepare(""SELECT PequenoAlm, Alm, Lan, jant, Ceia, Valencia, YEAR(Data) AS Ano, Colaborador, MONTH(Data) AS mes, DAY(Data) AS dia

FROM raddb.MarcErpi WHERE raddb.MarcErpi.Colaborador = ?"");

$query->execute([$Colaborador]);

$json = [];
while($row=$query->fetch(PDO::FETCH_ASSOC)){
    extract($row);

    $json[]= ['PequenoAlm' =>(string)$PequenoAlm, 'Alm' =>(string)$Alm, 'Lan' =>(string)$Lan, 'jant' =>(string)$jant, 'Ceia' =>(string)$Ceia, 'Valencia' =>(string)$Valencia, 
    'Ano' =>(string)$Ano, 'Colaborador' =>(string)$Colaborador, 'mes' =>(string)$mes, 'dia' =>(string)$dia];
}

echo json_encode($json);


(3) [{…}, {…}, {…}]  
0 :  {PequenoAlm: 'Peq_Almoço', Alm: 'Almoço', Lan: 'Lanche', jant: '', Ceia: '', …}  
1 :  {PequenoAlm: 'Peq_Almoço', Alm: 'Almoço', Lan: '', jant: '', Ceia: '', …}  
2 :  {PequenoAlm: '', Alm: '', Lan: 'Lanche', jant: 'Jantar', Ceia: '', …} length :   
3 [[Prototype]] :  Array(0)

",76389126,"You are configuring your event incorrectly - when defining event_data, you define a single event that has arrays of all of the events. For example, the first event has the date of all of them. Since there is only one, you end up with day = [""23""] instead of 23, which is breaking the calendar code. You also need to convert each element to a number, as they are currently strings. Here is the correction:
var event_data = {
    ""events"": []
};

for (var i = 0; i < verif.length; i++) {
    event_data.events.push({
        ""occasion"": verif[i],
        ""invited_count"": Number(verif1[i]),
        ""year"": Number(verif2[i]),
        ""month"": Number(verif3[i]),
        ""day"": Number(verif4[i]),
        ""cancelled"": true
    });
}

This code adds an event to event data for each item in verif.
In addition, earlier in your code, you define Dia incorrectly, with the month rather than the day.
Correction:
var Dia = datta[0].Dia;

rather than
var Dia = datta[0].Mes;

Updated pen: https://codepen.io/CoderMuffin/pen/RweXKJM
",
76381797,Get Calendar Events of all group members via Power Automate,"I want to get the Calendar Events of all of the members in the organization/team via Power Automate. As I see now the ""Get Calendars (V2)"" and the ""Get Calendar View of Events (V3)"" only return the corresponding values of the user itself (the one who owns the flow). I was wondering if there's a way to get the calendar events of the group members given the fact that they've given the corresponding permission to share all the calendar data with the organization members.
Any help on this will be much appreciated.
Thanks!
",76387767,"The built-in connector will use the current user.
I don't think you can impersonate a user this way.
The best approach or alternative is to use Graph API with a servcie account (Azure AD app registration). You can get events from anyone, so from each users part of a group.
GET /users/{id | userPrincipalName}/calendar/events

GET /groups/{id}/members

To call Graph API with Power Automate you will need HTTP connector (it's a premium connector and the connector must be enable for your environment)
You have to declare a service account in Azure AD (or Microsoft Entra):

Create a new app registration (Applications > App registrations)

Copy the client id
Copy the tenant id





Add the following permissions (Applications > App registrations > API permissions)

Graph API > Application > Calendars.Read.All
Graph API > Application > Directory.Read.All
Graph API > Application > GroupMember.Read.All


Grant admin consent for the permissions



Generate a client secret (Applications > App registrations > Certificate & secrets > Client secrets)

In Power Automate, create a new cloud flow, for example a button with the group id as input.
Declare variables:

Client ID on the app registration
Client secret for the app registration
Tenant ID
Array to store all the events


Next, call Graph API to get group members with the generic HTTP connector
Method: GET
URI: https://graph.microsoft.com/v1.0/groups/<your group id input>/members
Headers: Content-Type application/json
Headers: Accept application/json
Authentication: Active Directory OAuth
Tenant: your tenant id variable
Audience: https://graph.microsoft.com
Client ID: your client id variable
Crendential Type: Secret
Secret: your client secret variable


Transform the result in JSON using the schema:
{
""type"": ""object"",
""properties"": {
    ""@@odata.context"": {
        ""type"": ""string""
    },
    ""value"": {
        ""type"": ""array"",
        ""items"": {
            ""type"": ""object"",
            ""properties"": {
                ""id"": {
                    ""type"": ""string""
                },
                ""mail"": {
                    ""type"": ""string""
                }
            },
            ""required"": [
                ""id"",
                ""mail""
            ]
        }
    }
}
}

For each members, call Graph API to get user's events
Method: GET
URI: https://graph.microsoft.com/v1.0/users/<user id fron json foreach>/event
Headers: Content-Type application/json
Headers: Accept application/json
Authentication: Active Directory OAuth
Tenant: your tenant id variable
Audience: https://graph.microsoft.com
Client ID: your client id variable
Crendential Type: Secret
Secret: your client secret variable


Transform the result in JSON using the schema:
{
""type"": ""object"",
""properties"": {
    ""@@odata.context"": {
        ""type"": ""string""
    },
    ""value"": {
        ""type"": ""array"",
        ""items"": {
            ""type"": ""object"",
            ""properties"": {
                ""@@odata.etag"": {
                    ""type"": ""string""
                },
                ""id"": {
                    ""type"": ""string""
                },
                ""createdDateTime"": {
                    ""type"": ""string""
                },
                ""lastModifiedDateTime"": {
                    ""type"": ""string""
                },
                ""changeKey"": {
                    ""type"": ""string""
                },
                ""categories"": {
                    ""type"": ""array""
                },
                ""transactionId"": {
                    ""type"": ""string""
                },
                ""originalStartTimeZone"": {
                    ""type"": ""string""
                },
                ""originalEndTimeZone"": {
                    ""type"": ""string""
                },
                ""iCalUId"": {
                    ""type"": ""string""
                },
                ""reminderMinutesBeforeStart"": {
                    ""type"": ""integer""
                },
                ""isReminderOn"": {
                    ""type"": ""boolean""
                },
                ""hasAttachments"": {
                    ""type"": ""boolean""
                },
                ""subject"": {
                    ""type"": ""string""
                },
                ""bodyPreview"": {
                    ""type"": ""string""
                },
                ""importance"": {
                    ""type"": ""string""
                },
                ""sensitivity"": {
                    ""type"": ""string""
                },
                ""isAllDay"": {
                    ""type"": ""boolean""
                },
                ""isCancelled"": {
                    ""type"": ""boolean""
                },
                ""isOrganizer"": {
                    ""type"": ""boolean""
                },
                ""responseRequested"": {
                    ""type"": ""boolean""
                },
                ""seriesMasterId"": {},
                ""showAs"": {
                    ""type"": ""string""
                },
                ""type"": {
                    ""type"": ""string""
                },
                ""webLink"": {
                    ""type"": ""string""
                },
                ""onlineMeetingUrl"": {},
                ""isOnlineMeeting"": {
                    ""type"": ""boolean""
                },
                ""onlineMeetingProvider"": {
                    ""type"": ""string""
                },
                ""allowNewTimeProposals"": {
                    ""type"": ""boolean""
                },
                ""occurrenceId"": {},
                ""isDraft"": {
                    ""type"": ""boolean""
                },
                ""hideAttendees"": {
                    ""type"": ""boolean""
                },
                ""responseStatus"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""response"": {
                            ""type"": ""string""
                        },
                        ""time"": {
                            ""type"": ""string""
                        }
                    }
                },
                ""body"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""contentType"": {
                            ""type"": ""string""
                        },
                        ""content"": {
                            ""type"": ""string""
                        }
                    }
                },
                ""start"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""dateTime"": {
                            ""type"": ""string""
                        },
                        ""timeZone"": {
                            ""type"": ""string""
                        }
                    }
                },
                ""end"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""dateTime"": {
                            ""type"": ""string""
                        },
                        ""timeZone"": {
                            ""type"": ""string""
                        }
                    }
                },
                ""location"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""displayName"": {
                            ""type"": ""string""
                        },
                        ""locationType"": {
                            ""type"": ""string""
                        },
                        ""uniqueIdType"": {
                            ""type"": ""string""
                        },
                        ""address"": {
                            ""type"": ""object"",
                            ""properties"": {}
                        },
                        ""coordinates"": {
                            ""type"": ""object"",
                            ""properties"": {}
                        }
                    }
                },
                ""locations"": {
                    ""type"": ""array""
                },
                ""recurrence"": {},
                ""attendees"": {
                    ""type"": ""array""
                },
                ""organizer"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""emailAddress"": {
                            ""type"": ""object"",
                            ""properties"": {
                                ""name"": {
                                    ""type"": ""string""
                                },
                                ""address"": {
                                    ""type"": ""string""
                                }
                            }
                        }
                    }
                },
                ""onlineMeeting"": {},
                ""calendar@odata.associationLink"": {
                    ""type"": ""string""
                },
                ""calendar@odata.navigationLink"": {
                    ""type"": ""string""
                }
            },
            ""required"": [
                ""@@odata.etag"",
                ""id"",
                ""createdDateTime"",
                ""lastModifiedDateTime"",
                ""changeKey"",
                ""categories"",
                ""transactionId"",
                ""originalStartTimeZone"",
                ""originalEndTimeZone"",
                ""iCalUId"",
                ""reminderMinutesBeforeStart"",
                ""isReminderOn"",
                ""hasAttachments"",
                ""subject"",
                ""bodyPreview"",
                ""importance"",
                ""sensitivity"",
                ""isAllDay"",
                ""isCancelled"",
                ""isOrganizer"",
                ""responseRequested"",
                ""seriesMasterId"",
                ""showAs"",
                ""type"",
                ""webLink"",
                ""onlineMeetingUrl"",
                ""isOnlineMeeting"",
                ""onlineMeetingProvider"",
                ""allowNewTimeProposals"",
                ""occurrenceId"",
                ""isDraft"",
                ""hideAttendees"",
                ""responseStatus"",
                ""body"",
                ""start"",
                ""end"",
                ""location"",
                ""locations"",
                ""recurrence"",
                ""attendees"",
                ""organizer"",
                ""onlineMeeting"",
                ""calendar@odata.associationLink"",
                ""calendar@odata.navigationLink""
            ]
        }
    }
}
}


Activate and start the flow with a group id as input:

",
76387472,tidyr::pivot_longer() with duplicate problems with no apparent duplicate column names or dataset in R,"My goal is to change value 99999 with the value adjacent to it unless it's 99999 again.
I took the advice from here before, now I am having a new problem.
MRE:
'as' is a dataframe with 9 different cohort datasets; 10030 obs of 7060 variables. I am mainly (as of now) dealing with as$AS1_WEIGHT ... as$AS9_WEIGHT
> as %>%
+     select(starts_with(""AS"") & ends_with(""_WEIGHT"")) %>% head() %>% dput()

structure(list(AS1_WEIGHT = c(72, 59, 50, 55.2, 82.1, 50.4), 
    AS2_WEIGHT = c(74.8, NA, NA, 54.8, 84.5, 52.5), AS3_WEIGHT = c(75.2, 
    NA, NA, 55.9, 81.7, 54.6), AS4_WEIGHT = c(75, NA, NA, 55.1, 
    80.6, NA), AS5_WEIGHT = c(75.4, NA, NA, 58.8, 89.5, NA), 
    AS6_WEIGHT = c(77.3, NA, NA, NA, NA, NA), AS7_WEIGHT = c(70.7, 
    NA, NA, 56, NA, NA), AS8_WEIGHT = c(73.8, NA, NA, 55.5, NA, 
    NA), AS9_WEIGHT = c(74.5, NA, NA, 54.8, NA, 52)), row.names = c(NA, 
-6L), class = c(""tbl_df"", ""tbl"", ""data.frame""))



as %<>%
  mutate(row = row_number()) %>%
  tidyr::pivot_longer(starts_with(""AS"") & ends_with(""_WEIGHT"")) %>%
  mutate(value = if_else(value == '99999', lead(value), value), .by = row) %>%
  pivot_wider(names_from = name, values_from = value)

returns:
Error in tidyr::pivot_longer():
! Names must be unique.
✖ These names are duplicated:

""name"" at locations 7049 and 7053.
""value"" at locations 7050 and 7054.
ℹ Use argument names_repair to specify repair strategy.
Run rlang::last_trace() to see where the error occurred.

So I ran this code to see which columns are duplicated:
> dup_col <- duplicated(base::as.list(as))
colnames(as[dup_col])

character(0)

I ran another code to see if I am referring to the right columns
> as %>%
  select(starts_with(""AS"") & ends_with(""_WEIGHT"")) %>%
  colnames()

[1] ""AS1_WEIGHT"" ""AS2_WEIGHT"" ""AS3_WEIGHT"" ""AS4_WEIGHT"" ""AS5_WEIGHT"" ""AS6_WEIGHT"" ""AS7_WEIGHT"" ""AS8_WEIGHT""
[9] ""AS9_WEIGHT""


Thank you in advance!

",76387866,"I suspect you already have a column named name or value before you run pivot_longer, which by default tries to create columns with those names. As noted here, the error message isn't necessarily clear that's the problem.
Try grep(""name"", colnames(as)) and grep(""value"", colnames(as)) to find those columns.
Either rename in your data frame or use pivot_longer( ... names_to = ""a_new_name_col"", values_to = ""a_new_value_col"")
data.frame(a = 1:2, name = 3:4, value = 7:8) %>%
  tidyr::pivot_longer(a)
#Error in `vec_cbind()`:
#! Names must be unique.
#✖ These names are duplicated:
#  * ""name"" at locations 1 and 3.
#  * ""value"" at locations 2 and 4.
#ℹ Use argument `names_repair` to specify repair strategy.
#Run `rlang::last_trace()` to see where the error occurred.

data.frame(a = 1:2, name2 = 3:4, value2 = 7:8) %>%
  tidyr::pivot_longer(a)
## A tibble: 2 × 4
#  name2 value2 name  value
#  <int>  <int> <chr> <int>
#1     3      7 a         1
#2     4      8 a         2

",
76383552,"Create Record With ""has_many_through"" Association – Ruby on Rails","I have a has_many_through association where Users have many Projects through ProjectUsers. Rails has some magic that allows updating this relationship with:
u = User.first
u.update(project_ids: [...])

Is there a clean way to do the same thing with create?
Running User.create(name: ..., project_ids: [...]) fails with Validation failed: Project users is invalid.
I suspect this is because Rails tries to create the ProjectUser record before creating the User record and has some built-in validation on join tables to validate that both sides of the join already exist. The ProjectUser model has no custom validation.
class ProjectUser < ApplicationRecord
  belongs_to :project
  belongs_to :user
end

Is there a simple way to get around this?
",76386291,"Active Record supports automatic identification for most associations with standard names. However, Active Record will not automatically identify bi-directional associations that contain the :through or :foreign_key options. (You can check here)
So you have to define inverse_of explicitly.
class Project < ApplicationRecord
  has_many :project_users, foreign_key: :project_id, inverse_of: :project
  has_many :users, through: :project_users
end

class User < ApplicationRecord
  has_many :project_users, foreign_key: :user_id, inverse_of: :user
  has_many :projects, through: :project_users
end

class ProjectUser < ApplicationRecord
  belongs_to :project
  belongs_to :user
end


",
76387583,SQL Function DIFFERENCE returns interesting scores,"Can anyone explain why the SQL function (using SQL Server 2019) returns results that to me appear to be counter intuitive? Here are the queries and the scores:
SELECT 
    DIFFERENCE('Good', 'Good Samaritans'); --Result is 4 (High score match)

SELECT 
    DIFFERENCE('Samaritans', 'Good Samaritans'); --Result is 1 (Low score match)

SELECT 
    DIFFERENCE('Sam', 'Good Samaritans'); --Result is 2 (A higher score than above!)

I understand DIFFERENCE uses SOUNDEX to match consonants phonetically, but the results above seem very odd particularly with the second query. Is it something to do with the space and the proceeding string?
",76387869,"If you change the order of words you will see there is a bias on the first word. Then if you consider SOUNDEX you will begin to understand why. Also read the reference below.

SOUNDEX converts an alphanumeric string to a four-character code that is based on how the string sounds when spoken in English. The
first character of the code is the first character of
character_expression, converted to upper case. The second through
fourth characters of the code are numbers that represent the letters
in the expression. The letters A, E, I, O, U, H, W, and Y are ignored
unless they are the first letter of the string. Zeroes are added at
the end if necessary to produce a four-character code. For more
information about the SOUNDEX code, see The Soundex Indexing
System

-- Bias based on left to right order of words
SELECT 10 id, DIFFERENCE('Good', 'Good Samaritans') --Result is 4 (High score match)
union all
SELECT 11 id, DIFFERENCE('Good', 'Samaritans Good') --Result is 1 (Low score match)
union all
SELECT 20, DIFFERENCE('Samaritans', 'Good Samaritans') --Result is 1 (Low score match)
union all
SELECT 21, DIFFERENCE('Samaritans', 'Samaritans Good') --Result is 4 (High score match)
union all
SELECT 30, DIFFERENCE('Sam', 'Good Samaritans') --Result is 2 (On the upper low side)






id
(No column name)




10
4


11
1


20
1


21
4


30
2




fiddle
Your expectations from difference may be too high.
",
76389016,How to get the index with the minimum value in a column avoiding duplicate selection,"I have the following dataframe:
import pandas as pd
pd.DataFrame({'index': {0: 'x0',
  1: 'x1',
  2: 'x2',
  3: 'x3',
  4: 'x4',
  5: 'x5',
  6: 'x6',
  7: 'x7',
  8: 'x8',
  9: 'x9',
  10: 'x10'},
 'distances_0': {0: 0.42394711275317537,
  1: 0.40400179114038315,
  2: 0.4077213959237454,
  3: 0.3921048592156785,
  4: 0.25293154279281627,
  5: 0.2985576890173001,
  6: 0.0,
  7: 0.32563550923886675,
  8: 0.33341592647322754,
  9: 0.30653189426783256,
  10: 0.31749957588191197},
 'distances_1': {0: 0.06684300576184829,
  1: 0.04524728117549289,
  2: 0.04896118088709522,
  3: 0.03557204741075342,
  4: 0.10588973399963886,
  5: 0.06178330590643222,
  6: 0.0001,
  7: 0.6821440376099591,
  8: 0.027074111335967314,
  9: 0.6638424898747833,
  10: 0.674718181953208},
 'distances_2': {0: 0.7373816871931514,
  1: 0.7184619375104593,
  2: 0.7225072199147892,
  3: 0.7075191710741303,
  4: 0.5679436864793461,
  5: 0.6142446533143044,
  6: 0.31652743219529056,
  7: 0.010859948083988706,
  8: 0.6475070638933254,
  9: 0.010567926115431175,
  10: 0.0027932480510772413}}

)
index   distances_0 distances_1 distances_2
0   x0  0.423947    0.066843    0.737382
1   x1  0.404002    0.045247    0.718462
2   x2  0.407721    0.048961    0.722507
3   x3  0.392105    0.035572    0.707519
4   x4  0.252932    0.105890    0.567944
5   x5  0.298558    0.061783    0.614245
6   x6  0.000000    0.000100    0.316527
7   x7  0.325636    0.682144    0.010860
8   x8  0.333416    0.027074    0.647507
9   x9  0.306532    0.663842    0.010568
10  x10 0.317500    0.674718    0.002793

I would like to get, for every distances_ column, the index with the minimum value.
The requirement is that each distances_ column, should have a different index: For instance index==""x6"" has the minimum value for both distances_0 and distances_1, columns, but it should be chosen only for one (and in this case it should be chosen for distances_0, since 0.000000 < 0.000100).
How could I do that ?
",76389173,"Use Series.idxmin with filter out existing values in ouput list:
df1 = df.set_index('index')

out = []
for c in df1.columns:
    out.append(df1.loc[~df1.index.isin(out), c].idxmin())
print (out)
['x6', 'x8', 'x10']

",
76381633,Ensuring a minimum time interval between successive observations in a Pandas dataframe,"I have a pandas dataframe which is sorted by a date column. However I wish to ensure a minimum time interval between observations. Say for simplicity this window is 10 minutes, what this means is that if my first observation occurred at 8:05 AM then the second observation must occur at at least 8:15 AM. Any observations occurring between 8:05-8:15 AM must be dropped from the dataframe. Say without loss of generality that after dropping observations the second observation occurs at 8:17 AM. Then all observations between 8:17-8:27 AM are dropped to find the third data point and this process continues.
I have a script which works but iterates over the rows one at a time and is excruciatingly slow as the dataframe has hundreds of thousands of rows. My current script (window is the minimum threshold in minutes):
cur_time = df.iloc[0].Date

for idx, row in df[1:].iterrows():

time_diff = (row.Date - cur_time).total_seconds()

if time_diff > window*60:

    cur_time = row.Date

else:

    trades_df.drop(idx, inplace=True)

Can anyone think of a more speed optimized way of doing this operation? If I switch to the Date column as the index are there functions readily available for performing this function?
Edit: After doing further research the function that I'm looking for is similar to df.resample(window + 'M').first(). However the issue with using this is that my data set is sparsely spaced. I.e. I don't have data for every minute, the gap between data points could be 1 second or it could be 1 month.
",76387897,"With your condition mentioned in comments, I think you can't vectorize whole code. However, you can browse through the dataset faster:
window = 10
# convert date as numpy array (in seconds)
arr = df['Date'].values.astype(float) / 1e9
# compute dense matrix using numpy broadcasting
m = arr - arr[:, None] > window * 60
locs = []  # list of valid observations
idx = 0  # first date is always valid

while True:
    # append the current observation
    locs.append(idx)
    if m[idx].sum() == 0:
        # no more observations to check
        break
    # next valid observation
    idx = np.argmax(m[idx])

out = df.iloc[locs]

Output:
>>> out
                  Date
0  2023-06-01 00:02:10
3  2023-06-01 00:14:20
8  2023-06-01 00:24:42
11 2023-06-01 00:35:35
13 2023-06-01 00:48:39

>>> locs
[0, 3, 8, 11, 13]

Minimal Reproducible Example:
import numpy as np
import pandas as pd

np.random.seed(42)
offsets = pd.to_timedelta(np.random.randint(0, 60*60, 20), unit='S')
df = (pd.DataFrame({'Date': pd.Timestamp('2023-06-01') + offsets})
        .sort_values('Date', ignore_index=True))
print(df)

# Output
                  Date
0  2023-06-01 00:02:10  # OK, first value is always valid
1  2023-06-01 00:05:30
2  2023-06-01 00:07:46
3  2023-06-01 00:14:20  # OK, 00:02:10 + 10min < 00:14:20
4  2023-06-01 00:18:15
5  2023-06-01 00:18:50
6  2023-06-01 00:20:38
7  2023-06-01 00:21:34
8  2023-06-01 00:24:42  # OK, 00:14:20 + 10min < 00:24:42
9  2023-06-01 00:27:18
10 2023-06-01 00:28:05
11 2023-06-01 00:35:35  # OK, 00:24:42 + 10min < 00:35:35
12 2023-06-01 00:36:09
13 2023-06-01 00:48:39  # OK, 00:35:35 + 10min < 00:48:39
14 2023-06-01 00:51:32
15 2023-06-01 00:52:51
16 2023-06-01 00:52:54
17 2023-06-01 00:56:20
18 2023-06-01 00:57:24
19 2023-06-01 00:58:27

",
76382810,Have difficulty understanding the syntax of generic lambdas for SFINAE-based traits,"I am reading some examples of SFINAE-based traits, but unable to make sense out of the one related to generic lambdas in C++17 (isvalid.hpp).
I can understand that it roughly contains some major parts in order to implement a type trait such as isDefaultConstructible or hasFirst trait (isvalid1.cpp):
1. Helper functions using SFINAE technique:
#include <type_traits>

// helper: checking validity of f(args...) for F f and Args... args:
template<typename F, typename... Args,
         typename = decltype(std::declval<F>()(std::declval<Args&&>()...))>
std::true_type isValidImpl(void*);

// fallback if helper SFINAE'd out:
template<typename F, typename... Args>
std::false_type isValidImpl(...);

2. Generic lambda to determine the validity:
// define a lambda that takes a lambda f and returns whether calling f with args is valid
inline constexpr
auto isValid = [](auto f) {
                 return [](auto&&... args) {
                          return decltype(isValidImpl<decltype(f),
                                                      decltype(args)&&...
                                                     >(nullptr)){};
                        };
               };

3. Type helper template:
// helper template to represent a type as a value
template<typename T>
struct TypeT {
    using Type = T;
};

// helper to wrap a type as a value
template<typename T>
constexpr auto type = TypeT<T>{};

// helper to unwrap a wrapped type in unevaluated contexts
template<typename T>
T valueT(TypeT<T>);  // no definition needed

4. Finally, compose them into isDefaultConstructible trait to check whether a type is default constructible:
constexpr auto isDefaultConstructible
    = isValid([](auto x) -> decltype((void)decltype(valueT(x))()) {
        });

It is used like this (Live Demo):
struct S {
    S() = delete;
};

int main() {
    std::cout << std::boolalpha;
    std::cout << ""int: "" << isDefaultConstructible(type<int>) << std::endl;    // true
    std::cout << ""int&: "" << isDefaultConstructible(type<int&>) << std::endl;  // false
    std::cout << ""S: "" << isDefaultConstructible(type<S>) << std::endl;        // false

    return 0;
}

However, some of the syntax are so complicated and I cannot figure out.
My questions are:

With respect to 1, as for std::declval<F>()(std::declval<Args&&>()...), does it mean that it is an F type functor taking Args type constructor? And why it uses forwarding reference Args&& instead of simply Args?

With respect to 2, as for decltype(isValidImpl<decltype(f), decltype(args)&&...>(nullptr)){} , I also cannot understand why it passes forwarding reference decltype(args)&& instead of simply decltype(args)?

With respect to 4, as for decltype((void)decltype(valueT(x))()), what is the purpose of (void) casting here? ((void) casting can also be found in isvalid1.cpp for hasFirst trait) All I can find about void casting is Casting to void to avoid use of overloaded user-defined Comma operator, but it seems it is not the case here.


Thanks for any insights.

P.S. For one who wants more detail could check C++ Templates: The Complete Guide, 2nd - 19.4.3 Using Generic Lambdas for SFINAE. The author also mentioned that some of the techniques are used widely in Boost.Hana, so I also listen to Louis Dionne's talk about it. Yet, it only helps me a little to understand the code snippet above. (It is still a great talk about the evolution of C++ metaprogramming)
",76386312,"
F is a function object callable with Args...
For the sake of mental model, picture std::declval<F>() as a ""fully constructed object of type F"". std::declval is there just in case F is not default-constructible and still needs to be used in unevaluated contexts.
For a default-constructible type this would be equivalent:
F()(std::declval<Args&&>()...); In essence it's a call to F's constructor and then call to its operator() with forwarded Args. But imagine one type is constructible with int, another one is default-constructible, yet another one requires a string. Without some unevaluated constructor-like metafunction it would be impossible to cover all those cases.
You can read more on that in Alexandrescu's Modern C++ Design: Generic Programming and Design Patterns Applied.

Adding && to the argument type is effectively perfect-forwarding it. It may look obscure, but it's just a shorthand for decltype(std::forward<decltype(args)>(args)). See the implementation of std::forward and reference collapsing rules for more details. Keep in mind though, that this snippet adds rvalue-reference that collapses to the correct one when combined with the original type, not a forwarding one.

As it was stated in the comments: the type is not really needed, possibilty exists it cannot be returned, its presence there is just to check expression's correctness, afterwards it can be discarded.


",
76383773,Taildwind css align bottom,"Hello I just tring send to bottom of its parent:
 <form class=""flex flex-col w-full"" (submit)=""updatePhoto(title, description)"">
        <div class=""w-full block"">
            <input type=""text"" class=""shadow appearance-none border rounded w-full py-2 px-3 text-gray-700 leading-tight focus:outline-none focus:shadow-outline"" placeholder=""Photo's Title"" [value]=""photo.title"" #title>
        </div>
        <div class=""my-4 w-full"">
            <textarea rows=""2"" class=""shadow appearance-none border rounded w-full py-2 px-3 text-gray-700 leading-tight focus:outline-none focus:shadow-outline resize-none"" placeholder=""Photo's Description"" [value]=""photo.description"" #description></textarea>
        </div>
        <div class=""grid justify-items-end  mt-auto  border "">
            <div>
                <button class=""text-white bg-gradient-to-r from-red-400 via-red-500 to-red-600 hover:bg-gradient-to-br focus:ring-4 focus:outline-none focus:ring-red-300 dark:focus:ring-red-800 shadow-lg shadow-red-500/50 dark:shadow-lg dark:shadow-red-800/80 font-medium rounded-lg text-sm px-5 py-2.5 text-center mr-2 mb-2"" (click)=""deletePhoto(photo._id)"">
                    Delete
                </button>
                <button class=""text-white bg-gradient-to-r from-blue-500 via-blue-600 to-blue-700 hover:bg-gradient-to-br focus:ring-4 focus:outline-none focus:ring-blue-300 dark:focus:ring-blue-800 shadow-lg shadow-blue-500/50 dark:shadow-lg dark:shadow-blue-800/80 font-medium rounded-lg text-sm px-5 py-2.5 text-center mr-2 mb-2 "">
                    Update
                </button>
            </div>
        </div>
 </form>

so I want align element(buttons) to the bottom of its parent, I'm using flex flex-col in parent and child I'm using grid justify-items-end  mt-auto but don't work, so I just added border to see the position, I'm getting this:

you can see the buttons are up, what's wrong? why
",76386482,"It works fine just use grid on the outer container.  There's also a lot of opportunity to streamline both the markup and the classes.


<script src=""https://cdn.tailwindcss.com""></script>

<div class=""m-4 grid max-w-4xl grid-cols-2 gap-3 rounded border p-4 shadow"">
  <img class=""w-full"" src=""https://picsum.photos/id/237/900"" />
  <form class=""flex flex-col"" (submit)=""updatePhoto(title, description)"">
    <input type=""text"" class=""focus:shadow-outline mb-4 w-full appearance-none rounded border px-3 py-2 leading-tight text-gray-700 shadow focus:outline-none"" placeholder=""Photo's Title"" [value]=""photo.title"" #title />
    <textarea rows=""2"" class=""focus:shadow-outline w-full resize-none appearance-none rounded border px-3 py-2 leading-tight text-gray-700 shadow focus:outline-none"" placeholder=""Photo's Description"" [value]=""photo.description"" #description></textarea>
    <div class=""ml-auto mt-auto"">
      <button class=""mb-2 mr-2 rounded-lg bg-gradient-to-r from-red-400 via-red-500 to-red-600 px-5 py-2.5 text-center text-sm font-medium text-white shadow-lg shadow-red-500/50 hover:bg-gradient-to-br focus:outline-none focus:ring-4 focus:ring-red-300 dark:shadow-lg dark:shadow-red-800/80 dark:focus:ring-red-800"" (click)=""deletePhoto(photo._id)"">Delete</button>
      <button class=""mb-2 mr-2 rounded-lg bg-gradient-to-r from-blue-500 via-blue-600 to-blue-700 px-5 py-2.5 text-center text-sm font-medium text-white shadow-lg shadow-blue-500/50 hover:bg-gradient-to-br focus:outline-none focus:ring-4 focus:ring-blue-300 dark:shadow-lg dark:shadow-blue-800/80 dark:focus:ring-blue-800"">Update</button>
    </div>
  </form>
</div>



",
76389093,How do I display Javascript variable using,"According to the chrome console there are no problems however there are as my variable is ""undefined"" despite it being set as ""0""
I have used the following resources:
Resource 1
Resource 2
I am trying to make it so when the user clicks my image it will add +1 to the points variable (Game.points) here is my code (for context purposes):
<!DOCTYPE html>

<script>
function Game() {
var points = ""0""
document.getElementById(""myText"").innerHTML = Game.points;

}
</script>
<center>  <img onclick=""Game.points = Game.points + 1"" src=""https://static.wikia.nocookie.net/villains/images/5/5d/Frieza.png/revision/latest?cb=20200625063534"" width=""350px""> </center>
<body onload=""Game()"">

    <h1>""The value for number is: "" <span id=""myText""></span></h1>

",76389203,"The points variable is not a member of the Game object. To reference it, just use the points keyword.
There's also a couple of changes you can make to improve the code quality:

Remove the inline onclick and onload attributes. They are outdated and not good practice. Use addEventListener() to bind events within JS, instead of HTML.
Set points to be a numeric value, not a string. This way there's no type coercion needed when you increment its value.
Put any CSS styling in a separate stylesheet, not the HTML



const myText = document.querySelector('#myText');
const img = document.querySelector('img');
let points = 0;

document.addEventListener('DOMContentLoaded', () => {
  updateText();
});

img.addEventListener('click', () => {
  points++;
  updateText();
});

updateText = () => {
  myText.textContent = points;
};
img {
  width: 350px;
}
<center>
  <img src=""https://static.wikia.nocookie.net/villains/images/5/5d/Frieza.png/revision/latest?cb=20200625063534"" />
</center>

<h1>
  The value for number is: 
  <span id=""myText""></span>
</h1>



",
76382922,Dask map_partition does no use all workers on client,"I have very CPU heavy process and would like to use as many workers are possible in Dask.
When I read the csv file using the read_csv from dask and then process the dataframe using map_partitions only one worker is used. If I use read_csv from pandas and then convert the file to a Dask dataframe, all my workers are used. See code below.
Could someone explain the difference in behavior?
Ideally, I would like to use read_csv from Dask so that I dont have to have a conversion step. Could anyone help me with that?
import dask as d
import pandas as pd

def fWrapper(x):
            p = doSomething(x.ADDRESS, param)
            return(pd.DataFrame(p, columns=[""ADDRESS"", ""DATA"",""TOKEN"", ""CLASS""]))

# only use 1 worker instead of the available 8
dask_df = d.dataframe('path\to\file')
dask_df.set_index(UID, npartitions = 8,   drop = False)
ddf2 = dask_df.map_partitions(fWrapper, meta={""ADDRESS"" : object, ""DATA"" : object, ""TOKEN"" : object, ""CLASS"" : object}).compute() 

#uses all 8 workers
df = pd.read_csv('path\to\file')
df.set_index('UID', drop=False)
dask_df2 =d.dataframe.from_pandas(df, npartitions=dask_params['df_npartitions'], sort=True)
ddf3 = dask_df2.map_partitions(fWrapper, meta={""ADDRESS"" : object, ""DATA"" : object, ""TOKEN"" : object, ""CLASS"" : object}).compute() 

",76386636,"The DataFrame.set_index method in both dask.dataframe and pandas returns the updated dataframe, so it must be assigned to a label. pandas does have a convenience kwarg inplace, but that's not available in dask. This means that in your snippet, the first approach should look like this:
dask_df = dask_df.set_index(UID, npartitions = 8,   drop = False)

This will make sure that the new indexed dask dataframe has 8 partitions, so downstream work should be allocated across multiple workers.
",
76378627,User can't create objects in the page,"I made a program where users can keep track of their expenses by entering them into the system after creating their own account. The system requires users to fill in four fields: customer, category, price, and month. I would like the first field (customer) to automatically populate with the username of the logged-in user, so that users don't have to choose from all available customers.
However, I encountered an issue where users are unable to create objects in the system. I can only create objects through the admin dashboard. When I try to create an object on the page, it throws an error message saying,
RelatedObjectDoesNotExist at / User has no customer.

I suspect this problem is related to the fact that users are added in the authentication and authorization section under Users in the admin page, instead of being created under Customers by my app, alongside the Finance section where the objects are stored.
To summarize my two main issues:

I want the first field to automatically populate with the username of the logged-in user.
I want users to be able to create objects directly from the page, as they could in the past when this issue didn't occur.

Thank you so much for your help.
models.py:
class Customer(models.Model):
    user = models.OneToOneField(User, null=True, on_delete=models.CASCADE)
    name = models.CharField(max_length=200, null=True)
    email = models.CharField(max_length=200, null=True, blank=True)
    date_created = models.DateTimeField(auto_now_add=True, null=True)

    # def __str__(self):
    #     return self.name #I don't now if it's correct


class Finance(models.Model):
    expenses_category = [
        (""Saving"", ""Saving""),
        (""Food"", ""Food""),
        (""Bills"", ""Bills""),
        (""Rent"", ""Rent""),
        (""Extra"", ""Extra""),
    ]

    expenses_month = [
        (""January"", ""January""),
        (""February"", ""February""),
        (""March"", ""March""),
        (""April"", ""April""),
        (""May"", ""May""),
        (""June"", ""June""),
        (""July"", ""July""),
        (""August"", ""August""),
        (""September"", ""September""),
        (""October"", ""October""),
        (""November"", ""November""),
        (""December"", ""December""),
    ]

    customer = models.ForeignKey(User, on_delete=models.CASCADE, null=True, blank=True)
    category = models.CharField(choices=expenses_category, max_length=200)
    price = models.IntegerField()
    month = models.CharField(choices=expenses_month, max_length=200)


views.py:
@csrf_exempt
def registerPage(request):
    if request.user.is_authenticated:
        return redirect('home')
    else:
        form = CreateUserForm()
        if request.method == 'POST':
            form = CreateUserForm(request.POST)
            if form.is_valid():
                # form.instance.user = request.user
                user = form.save()
                username = form.cleaned_data.get('username')

                group = Group.objects.get(name='customer')
                user.groups.add(group)

                messages.success(request, 'Account was created for ' + username)

                return redirect('login')

        context = {'form': form}
        return render(request, 'app_finance/register.html', context)


def loginPage(request):
    username = None
    if request.user.is_authenticated:
        username = request.user.customer
        return redirect('home')
    else:
        if request.method == 'POST':
            username = request.POST.get('username')
            password = request.POST.get('password')

            user = authenticate(request, username=username, password=password)

            if user is not None:
                login(request, user)
                return redirect('home')
            else:
                messages.info(request, 'Username or password incorrect.')

        context = {}
        return render(request, 'app_finance/login.html', context)


def logoutUser(request):
    logout(request)
    return redirect('login')


def userPage(request):
    return render(request, 'app_finance/user.html')


@login_required(login_url='login')
def homeView(request):
    # customer = Customer.objects.get(id=pk) (not sure#)
    username = None
    items = Finance.objects.filter(customer_id=request.user.id)
    form = FinanceForm(initial={'customer': User})
    if request.method == 'POST':
        username = request.user.customer
        form = FinanceForm(request.POST)  # initial={'customer': user}
        if form.is_valid():
            form.save()
            return HttpResponseRedirect('/')
    else:
        form = FinanceForm()
        return render(request, 'app_finance/home.html', {'form': form, 'items': items})


forms.py:
class CustomerForm(ModelForm):
    class Meta:
        model = Customer
        fields = '__all__'
        exclude = ['user']


class CreateUserForm(UserCreationForm):
    class Meta:
        model = User
        fields = ['username', 'email', 'password1', 'password2']


class FinanceForm(ModelForm):
    class Meta:
        model = Finance
        fields = '__all__'


templates/home.html:
<div>
    <span>Hello, {{ request.user }}</span> <br>
    <span><a class=""hello-msg"" href=""{% url 'logout' %}"">Logout</a></span>
</div>


<form action="""" method=""post"">
    {% csrf_token %}
    {{ form }}
    <!-- {{ form }} -->

    <input type=""submit"" value=""Submit"">
</form>

<br>
<div class=""row"">
    <div class=""col-md"">
        <div class=""card card-body"">
            <h1>Expenses</h1>
        </div>
        <div class=""card card-body"">
            <table class=""table"">
                <tr>
                    <th>User</th>
                    <th>Category</th>
                    <th>Price</th>
                    <th>Month</th>
                </tr>
                {% for i in items %}
                    <tr>
                        <td>{{ i.customer }}</td>
                        <td>{{ i.category }}</td>
                        <td>{{ i.price }}</td>
                        <td>{{ i.month }}</td>
                        <td>&nbsp;&nbsp;</td>
                        <td><a class=""btn btn-sm btn-info"" href="""">Update</a></td>
                        <td><a class=""btn btn-sm btn-danger"" href="""">Delete</a></td>
                    </tr>
                {% endfor %}
            </table>
        </div>
    </div>
</div>


Thanks again for helping.
",76387024,"This is the solution i found and worked perfect too.
I made 2 changes:

views.py:

    @login_required(login_url='login')
def homeView(request):
    items = Finance.objects.filter(customer=request.user).order_by(Cast('month', IntegerField())).reverse()

    if request.method == 'POST':
        form = FinanceForm(request.POST, request=request)  # Pass the request object to the form
        if form.is_valid():
            finance_obj = form.save(commit=False)
            finance_obj.customer = request.user
            finance_obj.save()
            return redirect('home')
    else:
        form = FinanceForm(request=request)  # Pass the request object to the form

    context = {'form': form, 'items': items}
    return render(request, 'app_finance/home.html', context)


forms.py

    class FinanceForm(ModelForm):
    def __init__(self, *args, **kwargs):
        self.request = kwargs.pop('request')  # Retrieve the request object
        super().__init__(*args, **kwargs)
        self.fields['customer'].initial = self.request.user

",
76387813,"Find Min/max for X and Y, then interpolate for 5000 txt files with python","I have this working code to process a single file:
import pandas as pd
import pygmt

#import table
df = pd.read_table(""file1.txt"", sep="" "", names=['X', 'Y', 'Z'] ) 

#min/max
Xmin = df['X'].min()
Xmax = df['X'].max()
Ymin = df['Y'].min()
Ymax = df['Y'].max()
#print(Xmin, Xmax)
#print(Ymin, Ymax)

#gridding with pyGMT
grid = pygmt.surface(data=df, spacing=1, region=[Xmin, Xmax, Ymin, Ymax]) 

#print(grid) 
#export
grid.to_netcdf('file1.nc') 

Now I want to repeat this code for all *.txt files in a directory. How can I do that? I tried writing a loop like:
for file in glob.glob(""*.txt""):

But how can I make the respective input (.txt) and output (.nc) have the same name?
",76387951,"As said before in comment section you also can do it by iterating all .txt filenames and changes their formats to .nc with saving names.
import glob
import pandas as pd
import pygmt

filenames_in = glob.glob(""*.txt"")

for filename_in in filenames_in:
    filename_out = filename_in.replace('.txt', '.nc')
    # YOUR CODE
    # import table
    df = pd.read_table(filename_in, sep="" "", names=['X', 'Y', 'Z'])

    # min/max
    Xmin = df['X'].min()
    Xmax = df['X'].max()
    Ymin = df['Y'].min()
    Ymax = df['Y'].max()
    # print(Xmin, Xmax)
    # print(Ymin, Ymax)

    # gridding with pyGMT
    grid = pygmt.surface(data=df, spacing=1, region=[Xmin, Xmax, Ymin, Ymax])

    # print(grid)
    # export
    grid.to_netcdf(filename_out)



",
76383379,How do you manually calculate imul -1 * 3?,"I am learning x86-64 assembly with an online course and the course is horribly unclear in detail. I've searched online and read several SO questions but couldn't get an answer.
I tried to figure out how to calculate binary multiplication by hand, but I am stuck with imul .
Given this example of binary multiplication, 11111111 * 00000011, it can be viewed as 255 * 3 unsigned or -1 * 3 signed.
255*3
   mov al, 255
   mov bl, 3
   mul bl

this is easy and here's how I calculate by hand, just like decimal multiplication:
     11111111
x    00000011 
--------------
     11111111       
    11111111
--------------
   1011111101  

The result overflows, the upper half is 10 in ah and the lower half is 11111101 in al. My manual calculation matches the program result.
-1*3
when it comes to signed,
   mov al, -1
   mov bl, 3
   imul bl

the program result is 11111111 in ah and 11111101 in al.
How can I calculate this result by hand? I was told that sign extension is involved in imul, but I really don't know how it works here.
I am using SASM IDE and NASM Assembler.
",76387122,"Honestly, I can't fully understand the other two answers. It's over complicated for me. I just need a dumb, simple and universal rule.
I'd like to just pick up what works for me.

The result is equivalent to sign-extending (imul) or zero-extending
(mul) both inputs to the destination width and then doing a
non-widening multiply
@Peter Cordes


To manually compute there are several approaches: you can sign extend
both inputs to 16-bits and do 16 × 16 keeping only the low 16-bits
@Erik Eidt

I tried and verified and this rule works for me.
-1*3,
sign extended
  11111111 11111111
 x00000000 00000011
-------------------
  11111111 11111111
 111111111 1111111
-------------------
1011111111 11111101

keep the low 16 bits, I get the correct result 11111111(ah) 11111101(al).
Try a 4 bit example:
-2 * 3, 1110 imul 0011
sign extended
  1111 1110
 x0000 0011
-----------
  1111 1110
 11111 110
-----------
101111 1010

keep the lower 8 bits, the result is 1111 1010, -6.
Before I wasn't sure how sign extended works in imul, now I get it and it's easy to verify. Btw, if you find manual calculation tedious for some examples (e.g. 4 bit -7 * -1, 1001 x 1111, sign extended 1111 1001 x 1111 1111, many lines to add), you can use the Windows Calculator (programmer mode) and verify the result very quickly.
",
76388975,How do I eliminate this circular dependency,"The circular dependency between self.buttons and self.radiobuttons has been impossible for me to solve
class Program(tk.Tk):

    def __init__(self, title, size):
        super().__init__()
        self.title(title)
        self.geometry(f""{size[0]}x{size[1]}"")
        
        self.frame = tk.Frame(self, background = ""#D3D3D3"")
        self.frame.pack(expand = 1, fill = tk.BOTH)

        self.entries = Entries(self) 
        self.output = Output(self)
        self.buttons = Buttons(self, self.entries, self.output, self.radiobuttons) # here
        self.radiobuttons = (self, self.buttons) # here


        self.filemenu = FileMenu(self)


        self.mainloop()

I am trying to use functools among other things to fix my problem but I can't seem to fix it. I could always just move the entirety of one class into the other but it would make using classes redundant
",76389211,"You can create another class method of Buttons to pass self.radiobuttons to it after self.radiobuttons is created.
Below is an example:
class Buttons(ttk.Frame):
    def __init__(self, master, entries, output):
        super().__init__(master)
        self.entries = entries
        self.output = output

    # another class method to pass radiobuttons
    def set_radiobuttons(self, radiobuttons):
        self.radiobuttons = radiobuttons
        # do whatever you want on radiobuttons

class Radiobuttons(ttk.Frame):
    def __init__(self, master, buttons):
        super().__init__(master)
        self.buttons = buttons

class Program(tk.Tk):

    def __init__(self, title, size):
        super().__init__()
        self.title(title)
        self.geometry(f""{size[0]}x{size[1]}"")

        self.frame = tk.Frame(self, background = ""#D3D3D3"")
        self.frame.pack(expand = 1, fill = tk.BOTH)

        self.entries = Entries(self)
        self.output = Output(self)
        self.buttons = Buttons(self, self.entries, self.output)
        self.radiobuttons = Radiobuttons(self, self.buttons)
        self.buttons.set_radiobuttons(self.radiobuttons) # pass self.radiobuttons to class Buttons

        self.filemenu = FileMenu(self)

if __name__ == ""__main__"":
    app = Program(""Hello World"", (600,400))
    app.mainloop() # call mainloop() here instead of inside __init__()

",
76387859,Maps containing keys with spaces are not properly validated when those keys are used as resource names,"I faced an issue when I had to terraform import some role_assignment resources, specially regarding the APP CONFIG DATA READER role assignment, in terraform.
the problem I had to solve was due to an evolution of our iac to make the terraform plan more readable and explicit.
Here is the code for role assignment that changed :
module ""my_role_assignment_slot_to_app_config"" {
  for_each = local.map_slot_app_config
  source = ""../../resourceModules/role_assignment""
  scope = each.value.config_id
  role_definition_names = [""Reader"",""App Configuration Data Reader""]
  principal_id = each.value.slot_guid
}

with the following module for role assignments : 

```hcl
resource ""azurerm_role_assignment"" ""my_role_assignment"" {
  for_each              = toset(var.role_definition_names)
  scope                = var.scope
  role_definition_name = each.value
  principal_id         = var.principal_id
}

This code would plan the following, more readable :

But as you can see, the index for the azurerm_role_assignment.my_role_assignment contains spaces.
This was preventing us to terraform import the role assignment (as it has been created manually before the iac was coded) using powershell script in an azureCli task on azure Devops :
      - task: AzureCli@2
        displayName: Runs tfImport.ps1
        condition: and(succeeded(), eq(variables['toUpdate.scriptExists'], 'true'))  # test script presence
        name: tfImport.ps1
        inputs:
          azureSubscription: ${{ parameters.serviceConnection }}
          scriptType: ps
          scriptLocation: InlineScript
          inlineScript: |
            $Env:ARM_CLIENT_ID       = ""$Env:servicePrincipalId""
            $Env:ARM_CLIENT_SECRET   = ""$Env:servicePrincipalKey""
            $Env:ARM_TENANT_ID       = ""$Env:tenantId""
            $Env:ARM_SUBSCRIPTION_ID = az account list --query ""[?isDefault].id"" -o tsv        
            $Env:TF_LOG = ""${{ parameters.TF_LOG }}""
        
            terraform init `
              -migrate-state `
              -backend-config=""resource_group_name=${{ parameters.storageAccountResourceGroup }}""`
              -backend-config=""storage_account_name=${{ parameters.storageAccount}}"" `
              -backend-config=""key=${{ parameters.storageContainer }}"" `
              -backend-config=""container_name=${{ parameters.stateBlobContainer }}""
        
            // runs my tfImport.ps1 script here
            ./tfImport.ps1 
        
          workingDirectory: $(pipeline_artefact_folder_extract)/   
          addSpnToEnvironment: true
          failOnStderr: false  
        continueOnError: true    

The script I used had the following terraform import line,
terraform import  'module.my_role_assignment_slot_to_app_config[\""sit03_z-adf-ftnd-shrd-npd-ew1-cfg01\""].azurerm_role_assignment.my_role_assignment[\""App Configuration Data Reader\""]'  /subscriptions/**********/resourceGroups/*********/providers/Microsoft.AppConfiguration/configurationStores/z-adf-ftnd-shrd-npd-ew1-cfg01/providers/Microsoft.Authorization/roleAssignments/<role_assignment_id>

and so I've had the following error :

(id datas removed)
After some researches, I've found the following link that explains the why and give to me one start of solution :
https://github.com/hashicorp/terraform/issues/25116
But I had to go further and find a way to use my powershell script without the startProcess method.
And as I had also to get my role_assignment resourceId from its PrincipalId (as we can gets the PrincipalId of resources that have the 'App Configuration Data Reader' role on the app_config using the following)
    # role assignment over a specific scope (such as app_config)
    $rsRolesCfg = az role assignment list  --scope /subscriptions/******/resourceGroups/*******/providers/Microsoft.AppConfiguration/configurationStores/******-cfg01 | ConvertFrom-Json
    $myRole = $rsRolesCfg | Where-Object roleDefinitionName -eq 'App Configuration Data Reader'  | Where-Object id -like ""*<app_config_resourceId>""
    ## principalId is the id of the object that get the role over the scope !
    
    # GetsresourceId from PrincipalId / ObjetcId (without the '<>' on body, off course :) ) : 
    $resourceId = (az rest --method POST --url 'https://graph.microsoft.com/v1.0/directoryObjects/getByIds' --headers 'Content-Type=application/json'  --body '{\""ids\"":[\""<PrincipalId>\""]}' | ConvertFrom-Json | Select-Object value).value.alternativeNames[1]

Solution from How to Get Azure AD Object by Object ID Using Azure CLI (thanks a lot !)
I had to test it locally in powershell terminal... So it did not work as expected.
So then, I change a little bit the script and got the solution (next post, as solution from my problem).
",76387955,"So I've had to test my get_ResourceId methods in local on a powershell VSCode terminal, that does not accept the above code (as the paces wher badly interpreted by powershell)
So, after a quick search that explain to me that the ""`"" was the escape character for Powershell, I've tested this that works and give to me the expected resourceId for role_assignment :
$rsRolesCfg = az role assignment list  --scope /subscriptions/************/resourceGroups/******/providers/Microsoft.AppConfiguration/configurationStores/<app_configuration_name> | ConvertFrom-Json

($rsRolesCfg | Where-Object roleDefinitionName -eq 'App Configuration Data Reader') | ForEach-Object {$local=$_.principalId; (az rest --method POST --url 'https://graph.microsoft.com/v1.0/directoryObjects/getByIds' --headers 'Content-Type=application/json'  --body ""{\`""ids\`"":[\`""$local\`""]}"" | ConvertFrom-Json | Select-Object value).value.alternativeNames[1] }

So, the use of ""`"" was the solution for my point, so I've tried to use it on my terraform import script (on first post) and it works fine also :
terraform import ""module.my_role_assignment_slot_to_app_config[\`""sit03_z-adf-ftnd-shrd-npd-ew1-cfg01\`""].azurerm_role_assignment.my_role_assignment[\`""App Configuration Data Reader\`""]""  /subscriptions/*****/resourceGroups/******/providers/Microsoft.AppConfiguration/configurationStores/z-adf-ftnd-shrd-npd-ew1-cfg01/providers/Microsoft.Authorization/roleAssignments/<role_assignment_id>

But I had also to change the yaml task, using pscore rather than ps, like the following :
      - task: AzureCli@2
        displayName: Runs tfImport.ps1
        condition: and(succeeded(), eq(variables['toUpdate.scriptExists'], 'true'))  # test script presence
        name: tfImport.ps1
        inputs:
          azureSubscription: ${{ parameters.serviceConnection }}
          scriptType: **pscore**

So then, the terrform import script was running with success !
I've used the same ""title"" for this stackOverflow question / solution as the one in the GitHub, so them peoples who are lokking for that solution could find easily the solution with the question... At least, I hope so :-P
thanks for reading !
",
76378706,"(WMI) IWbemServices::Release() throws ""Access Denied"" exception when connected to remote machine","In C++, I can't find a way to appropriately terminate a WMI session with a remote server.  Any attempt to release the IWbemServices pointer throws an exception; the TCP connection to the server remains established until the process exits (it's open after the last CoUninitialize call).  This problem (thrown exception) does not occur when connecting to the local machine.
I've looked at a similar question asked here, but the solution from Microsoft (retrieving the IUnknown pointer and releasing it first) didn't solve the issue.
Here's the code (error checking has been omitted for readability):
HRESULT hRes = S_OK;

IWbemLocator* pWbemLocator = NULL;
IWbemServices* pWbemServices = NULL;

// these three pointers are already initialized...
PWCHAR wcUser; // L""theuser""
PWCHAR wcPass; // L""thepassword""
PWCHAR wcAuth; // L""ntlmdomain:THEDOMAIN""

std::wstring wstrNsPath = L""\\\\remoteserver\\ROOT\\CIMV2"";

hRes = CoInitializeEx(NULL, COINIT_MULTITHREADED);

hRes = CoCreateInstance(CLSID_WbemLocator, NULL, CLSCTX_INPROC_SERVER, IID_IWbemLocator, (LPVOID*)&pWbemLocator);

// this returns S_OK :)
hRes = pWbemLocator->ConnectServer((BSTR)wstrNsPath.c_str(),
        (BSTR)wcUser,
        (BSTR)wcPass,
        NULL,
        WBEM_FLAG_CONNECT_USE_MAX_WAIT,
        (BSTR)wcAuth,
        NULL,
        &pWbemServices);

hRes = CoSetProxyBlanket(pWbemServices,
        RPC_C_AUTHN_DEFAULT,
        RPC_C_AUTHZ_DEFAULT,
        COLE_DEFAULT_PRINCIPAL,
        RPC_C_IMP_LEVEL_IMPERSONATE,
        RPC_C_AUTHN_LEVEL_DEFAULT,
        NULL, // domain info already specified in 'wcAuth'
        EOAC_NONE);

// do some queries..

// cleanup
pWbemServices->Release(); // on remote sessions, this throws an exception
pWbemLocator->Release();
CoUninitialize();

The exception is displayed in the debug output (Visual Studio):
onecore\com\combase\dcomrem\call.cxx(1234)\combase.dll!0000ABCDEFABCDEF: (caller: 0000FEDCBAFEDCBA) ReturnHr(1) tid(4321) 80070005 Access is denied.

Is this expected behavior? Should the connection between the client and server not be terminated once the session is released?
I attempted to follow MSDN's advice and added the following after the CoSetProxyBlanket() call in the code above.  It didn't change anything.
IUnknown* pUnknown = NULL;
pWbemServices->QueryInterface(IID_IUnknown, (LPVOID*)&pUnknown);
if (pUnknown)
{
    hRes = CoSetProxyBlanket(pUnknown,
        RPC_C_AUTHN_DEFAULT,
        RPC_C_AUTHZ_DEFAULT,
        COLE_DEFAULT_PRINCIPAL,
        RPC_C_IMP_LEVEL_IMPERSONATE,
        RPC_C_AUTHN_LEVEL_DEFAULT,
        NULL,
        EOAC_NONE);
    pUnknown->Release();
}

Any advice is greatly appreciated!
EDIT So after capturing session packets, it would appear that setting the proxy security with pAuthInfo == NULL causes the request to be made by the current logged on user of the client machine.  It ignores the credentials that I provided when calling ConnectServer.  I'm aware that the COAUTHIDENTITY structure allows you to pass the correct credentials to CoSetProxyBlanket, but I'd like to avoid having to input the domain as a separate variable.  In other words, is there a way that this information can be extracted using the wcAuth when the request is made to a remote server? If so, how could I distinguish local vs. remote requests?
Here is the output from Wireshark that led me to believe this is the problem (see packet 1617):
No.     Time        Source      Destination Protocol Length Info
1615    162.221354  [CLIENT_IP] [SERVER_IP] DCERPC  174 Alter_context: call_id: 8, Fragment: Single, 1 context items: IRemUnknown2 V0.0 (32bit NDR), NTLMSSP_NEGOTIATE
1616    162.228517  [SERVER_IP] [CLIENT_IP] DCERPC  366 Alter_context_resp: call_id: 8, Fragment: Single, max_xmit: 5840 max_recv: 5840, 1 results: Acceptance, NTLMSSP_CHALLENGE
1617    162.229396  [CLIENT_IP] [SERVER_IP] DCERPC  612 AUTH3: call_id: 8, Fragment: Single, NTLMSSP_AUTH, User: .\[client_user]
1618    162.229495  [CLIENT_IP] [SERVER_IP] IRemUnknown2    182 RemRelease request Cnt=1 Refs=5-0
1619    162.235567  [SERVER_IP] [CLIENT_IP] TCP  60 49669 → 59905 [ACK] Seq=1606 Ack=4339 Win=64768 Len=0
1620    162.235567  [SERVER_IP] [CLIENT_IP] DCERPC  86  Fault: call_id: 8, Fragment: Single, Ctx: 0, status: nca_s_fault_access_denied

",76387141,"I was able to resolve the issue.  If you are not using the current logged-on user token, you must specify the pAuthInfo parameter of CoSetProxyBlanket to a valid COAUTHIDENITY structure pointer.  The docs for IWbemLocator::ConnectServer actually state that it's best practice to include the domain in the strUser parameter... and that if you do so, you must pass the authority string as NULL.
One thing to note is that if ConnectServer succeeds, you don't have to go crazy with the sanitizing of the username string for correctness; the login either worked or it didn't (and breaks/throws an exception, depending on how you handle errors).  In other words, just search the string for the domain delimiters ('\\' or '@') and split them appropriately into domain name and username.
",
76383760,"Going through the react-router tutorial and rather than using default or null values when loading an empty object, it kicks up an error","I've been going through react-router's tutorial, and I've been following it to the letter as far as I'm aware. I'm having some issues with the url params in loaders  segment.
The static contact code looks like this
export default function Contact() {
  const contact = {
    first: ""Your"",
    last: ""Name"",
    avatar: ""https://placekitten.com/g/200/200"",
    twitter: ""your_handle"",
    notes: ""Some notes"",
    favorite: true,
  }

And when it loads, it looks like this. That works just fine, however, the tutorial then tells me to change that code so that I use data that's loaded in instead. The code now looks like this
import { Form, useLoaderData } from ""react-router-dom"";
import { getContact } from ""../contacts""

export async function loader({ params }) {
  const contact = await getContact(params.contactid);
  return {contact}
}

export default function Contact() {
  const { contact } = useLoaderData();

According to the tutorial, it should just load in an empty contact that looks like this  but instead, every time I try to open one of the new contacts, it kicks up an error saying
React Router caught the following error during render TypeError: contact is null
The actual line of code this error points to is in the return segment of the contact component, which looks like this
return (
    <div id=""contact"">
      <div>
        <img
          key={contact.avatar}
          src={contact.avatar || null}
        />
      </div>

      <div>
        <h1>
          {contact.first || contact.last ? (
            <>
              {contact.first} {contact.last}
            </>
          ) : (
            <i>No Name</i>
          )}{"" ""}
          <Favorite contact={contact} />
        </h1>

        {contact.twitter && (
          <p>
            <a
              target=""_blank""
              href={`https://twitter.com/${contact.twitter}`}
            >
              {contact.twitter}
            </a>
          </p>
        )}

        {contact.notes && <p>{contact.notes}</p>}

        <div>
          <Form action=""edit"">
            <button type=""submit"">Edit</button>
          </Form>
          <Form
            method=""post""
            action=""destroy""
            onSubmit={(event) => {
              if (
                !confirm(
                  ""Please confirm you want to delete this record.""
                )
              ) {
                event.preventDefault();
              }
            }}
          >
            <button type=""submit"">Delete</button>
          </Form>
        </div>
      </div>
    </div>
  );
}

Pretty much anywhere contacts is called gets an error. So, anyone have any idea what I'm doing wrong here? To my knowledge, I've been following their guide to the letter and it seems like it should be able to handle contacts not having any data, but it's not.
These are the pieces of my code that are supposed to be working together to render a contact, or at least the pertinent parts
The router, this is the main file, the only part missing is the part where it's rendered
import * as React from ""react"";
import * as ReactDOM from ""react-dom/client"";
import {
  createBrowserRouter,
  RouterProvider,
} from ""react-router-dom"";
import ""./index.css"";
import Root, { loader as rootLoader, action as rootAction } from ""./routes/root"";
import ErrorPage from ""./error-page"";
import Contact, { loader as contactLoader } from ""./routes/contact""

const router = createBrowserRouter([
  {
    path: ""/"",
    element: <Root />,
    errorElement: <ErrorPage />,
    loader: rootLoader,
    action: rootAction,
    children: [
      {
        path: ""contacts/:contactID"",
        element: <Contact />,
        loader: contactLoader
      }
    ]
  }

These are the functions in the root file that are called when a new contact is made and when it needs to be displayed
import { Outlet, Link, useLoaderData, Form } from ""react-router-dom""
import { getContacts, createContact } from ""../contacts""

export async function action() {
  const contact = await createContact();
  console.log(""Contact made"")
  return {contact}
}

export async function loader(){
  const contacts = await getContacts();
  return {contacts};
}

This is the createContacts function that gets called when a contact is created, and this is the getContacts function
export async function createContact() {
  await fakeNetwork();
  let id = Math.random().toString(36).substring(2, 9);
  let contact = { id, createdAt: Date.now() };
  let contacts = await getContacts();
  contacts.unshift(contact);
  await set(contacts);
  return contact;
}

export async function getContact(id) {
  await fakeNetwork(`contact:${id}`);
  let contacts = await localforage.getItem(""contacts"");
  let contact = contacts.find(contact => contact.id === id);
  return contact ?? null;
}

This is the contacts.jsx file where things are currently going wrong. When a new contact is made, it's going to be empty, which I imagine is the source of the problem, but there are checks here to deal with that, or at least there are supposed to be.
import { Form, useLoaderData } from ""react-router-dom"";
import { getContact } from ""../contacts""

export async function loader({ params }) {
  const contact = await getContact(params.contactid);
  return { contact }
}

export default function Contact() {
  const { contact } = useLoaderData();

  return (
    <div id=""contact"">
      <div>
        <img
          // these next two lines are where the errors typically start, 
          // although it seems to extend down to any instance where contact
          // gets called.
          key={contact.avatar}
          src={contact.avatar || null}
        />
      </div>

      <div>
        <h1>
          {contact.first || contact.last ? (
            <>
              {contact.first} {contact.last}
            </>
          ) : (
            <i>No Name</i>
          )}{"" ""}
          <Favorite contact={contact} />
        </h1>

        {contact.twitter && (
          <p>
            <a
              target=""_blank""
              href={`https://twitter.com/${contact.twitter}`}
            >
              {contact.twitter}
            </a>
          </p>
        )}

        {contact.notes && <p>{contact.notes}</p>}

        <div>
          <Form action=""edit"">
            <button type=""submit"">Edit</button>
          </Form>
          <Form
            method=""post""
            action=""destroy""
            onSubmit={(event) => {
              if (
                !confirm(
                  ""Please confirm you want to delete this record.""
                )
              ) {
                event.preventDefault();
              }
            }}
          >
            <button type=""submit"">Delete</button>
          </Form>
        </div>
      </div>
    </div>
  );
}

",76387258,"There are some subtle, but detrimental, casing issues in the route path params.
The Contacts component's route path param is declared as contactID.
const router = createBrowserRouter([
  {
    path: ""/"",
    element: <Root />,
    errorElement: <ErrorPage />,
    loader: rootLoader,
    action: rootAction,
    children: [
      {
        path: ""contacts/:contactID"", // <-- ""contactID""
        element: <Contact />,
        loader: contactLoader,
      },
    ],
  },
]);

The contact loader is referencing a contactid path parameter.
export async function loader({ params }) {
  const contact = await getContact(params.contactid); // <-- ""contactid""
  return { contact };
}

As such, the loader function is unable to find a match and returns null to the Contact component. An error is thrown in the UI when attempting to access properties of the null reference.
Any valid Javascript identifier will work as the name of the route path parameter, but they should all be in agreement. Casing matters in variable names in Javascript. The common convention in variable names is to use camelCasing, e.g. contactId.
const router = createBrowserRouter([
  {
    path: ""/"",
    element: <Root />,
    errorElement: <ErrorPage />,
    loader: rootLoader,
    action: rootAction,
    children: [
      {
        path: ""contacts/:contactId"",
        element: <Contact />,
        loader: contactLoader,
      },
    ],
  },
]);

export async function loader({ params }) {
  const contact = await getContact(params.contactId);
  return { contact };
}

",
76387927,Checking the values of JSON response in React Native,"I would like to display either one of two images in a modal based on an API response...unfortunately, the API returns a long string array. I need to be able to determine if the word ""Congratulations"" is in this array. So far, I've tried a couple basic things:
Here is an example API response:
{
    id: 12,
    message: [""1 bonus point!"", ""Congratulations"", ""You have leveled up!""]
}


   console.log(response) // the full response prints to the console, no problem
   console.log(response?.message) //undefined
   console.log(reponse.message) //undefined
   console.log(response[""message""]) //undefined

I want to be able to do something like this:
setSuccess(response[""message""].contains(""Congratulations""))
I'm sure it will be some small syntax thing, but I'm been banging my head against the wall. Any help is appreciated, let me know what I should try!
",76387999,"I would join the array and than call include.


const data = {
    id: 12,
    message: [""1 bonus point!"", ""Congratulations"", ""You have leveled up!""]
}

const containTerm = data.json().message.join().includes('Congratulations')

console.log('Is term in array',  containTerm)



",
76383873,"Creating a ""FootBall Field Chart""","I have a DataFrame consisting of the following columns:
VP-ID,
MotivA_MotivatorA_InnerDriverA_PR,
MotivA_MotivatorA_InnerDriverB_PR,
MotivA_MotivatorB_InnerDriverA_PR,
MotivA_MotivatorB_InnerDriverB_PR,
MotivA_MotivatorC_InnerDriverA_PR,
MotivA_MotivatorC_InnerDriverB_PR,
MotivA_MotivatorD_InnerDriverA_PR,
MotivA_MotivatorD_InnerDriverB_PR,
...
MotivC_MotivatorA_InnerDriverA_PR,
MotivC_MotivatorA_InnerDriverB_PR,
MotivC_MotivatorB_InnerDriverA_PR,
MotivC_MotivatorB_InnerDriverB_PR,
MotivC_MotivatorC_InnerDriverA_PR,
MotivC_MotivatorC_InnerDriverB_PR,
MotivC_MotivatorD_InnerDriverA_PR,
MotivC_MotivatorD_InnerDriverB_PR.

Behind the designations MotivatorA etc. are of course correct terms (column names).
Here, ""PR"" stands for Percentile Rank (0-100).
A graphic represents a motive, which consists of four motivators with two variations, which then have the values from InnerDriverA_PR and InnerDriverB_PR.
The final result should look like this:

Is this a ""Football Field Chart""?
How can I implement this graph with Matplotlib?
Minimal reproducible example:
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns

# Create example data
columns = [
    'MotivA_MotivatorA_InnerDriverA_PR',
    'MotivA_MotivatorA_InnerDriverB_PR',
    'MotivA_MotivatorB_InnerDriverA_PR',
    'MotivA_MotivatorB_InnerDriverB_PR',
    'MotivA_MotivatorC_InnerDriverA_PR',
    'MotivA_MotivatorC_InnerDriverB_PR',
    'MotivA_MotivatorD_InnerDriverA_PR',
    'MotivA_MotivatorD_InnerDriverB_PR',
    'MotivB_MotivatorA_InnerDriverA_PR',
    'MotivB_MotivatorA_InnerDriverB_PR',
    'MotivB_MotivatorB_InnerDriverA_PR',
    'MotivB_MotivatorB_InnerDriverB_PR',
    'MotivB_MotivatorC_InnerDriverA_PR',
    'MotivB_MotivatorC_InnerDriverB_PR',
    'MotivB_MotivatorD_InnerDriverA_PR',
    'MotivB_MotivatorD_InnerDriverB_PR'
]

df = pd.DataFrame(columns=columns)

for i in range(1, 6):  
    df.loc[f'Subject_{i}'] = [random.randint(0, 100) for _ in range(len(columns))]

#────────────────────────────────────────────────

def create_horizontal_bar_chart(df, proband):
    motives = sorted(set(col.split('_')[0] for col in df.columns))

    for motive in motives:
        columns = [col for col in df.columns if col.startswith(motive)]

        data = df.loc[proband, columns].reset_index()

        data['Motivator'] = data['index'].apply(lambda x: x.split('_')[1])
        data['InnerDriver'] = data['index'].apply(lambda x: x.split('_')[2])
        data['Value'] = data[proband]
        data = data.drop(['index', proband], axis=1)

        plt.figure(figsize=(10, 6))
        sns.barplot(x='Value', y='Motivator', hue='InnerDriver', data=data)
        plt.title(f'{proband} - {motive}')
        plt.show()

create_horizontal_bar_chart(df, 'Subject_1')

However, this creates the motivators as extra bars and is still far from how I would want it, as in the example above.
",76389257,"It's a spine chart. The issue with that is it'll not line up your bars. So to do that, you need to get creative:
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

columns = [
    'MotivA_MotivatorA_InnerDriverA_PR',
    'MotivA_MotivatorA_InnerDriverB_PR',
    'MotivA_MotivatorB_InnerDriverA_PR',
    'MotivA_MotivatorB_InnerDriverB_PR',
    'MotivA_MotivatorC_InnerDriverA_PR',
    'MotivA_MotivatorC_InnerDriverB_PR',
    'MotivA_MotivatorD_InnerDriverA_PR',
    'MotivA_MotivatorD_InnerDriverB_PR',
    'MotivB_MotivatorA_InnerDriverA_PR',
    'MotivB_MotivatorA_InnerDriverB_PR',
    'MotivB_MotivatorB_InnerDriverA_PR',
    'MotivB_MotivatorB_InnerDriverB_PR',
    'MotivB_MotivatorC_InnerDriverA_PR',
    'MotivB_MotivatorC_InnerDriverB_PR',
    'MotivB_MotivatorD_InnerDriverA_PR',
    'MotivB_MotivatorD_InnerDriverB_PR'
]

df = pd.DataFrame(columns=columns)

for i in range(1, 6):  # 5 Probanden
    df.loc[f'Subject_{i}'] = [random.randint(0, 100) for _ in range(len(columns))]

def create_horizontal_bar_chart(df, proband):
    motives = sorted(set(col.split('_')[0] for col in df.columns))

    for motive in motives:
        columns = [col for col in df.columns if col.startswith(motive)]

        data = df[df.index == proband].reset_index()

        # Rename the new column to ""Subject""
        data = data.rename(columns = {""index"": ""Subject""})
        
        # Melt the dataframe
        data_melted = data.melt(id_vars=[""Subject""], var_name=""Motiv_Motivator_InnerDriver_PR"", value_name=""PR"")
        
        # Create new columns from the ""Motiv_Motivator_InnerDriver_PR"" column
        data_melted[['Motiv', 'Motivator', 'InnerDriver', '_']] = data_melted['Motiv_Motivator_InnerDriver_PR'].str.split(""_"",expand=True)
        data_melted = data_melted[data_melted['Motiv'] == motive]
        
        # Drop unnecessary columns
        data_melted = data_melted.drop(columns=['Motiv_Motivator_InnerDriver_PR', '_'])
        
        # Reorder the columns
        data_melted = data_melted[['Subject', 'Motiv', 'Motivator', 'InnerDriver', 'PR']]
        
        # Pivot the table
        data_pivot = pd.pivot_table(data_melted, values='PR', index=['Subject', 'Motiv', 'Motivator'],
                            columns='InnerDriver', aggfunc='first').reset_index()

        data_pivot['InnerDriverA'] = -data_pivot['InnerDriverA']
        data_pivot = data_pivot.sort_values('Motivator', ascending=False).reset_index(drop=True)


        fig, ax = plt.subplots(figsize=(10, 8))
        # Stacked bar chart
        data_pivot.plot(kind='barh', x='Motivator', y=['InnerDriverA', 'InnerDriverB'], 
                        ax=ax, stacked=True, color='#5fba7d', alpha=0.5, legend=False)
        
        ax.set_xlabel('PR')
        ax.axvline(0, color='grey', linewidth=4) # Add a vertical line at x=0
        ax.set_xlim(-100, 100)  # set x limit as -100 to 100
        
        # Add horizontal grid lines every 25 units
        ax.set_xticks(range(-100, 101, 25))
        ax.grid(True, axis='x', linestyle='dotted')
        
        # Adjust the x-axis tick labels to display all values as positive
        ax.set_xticklabels([abs(x) for x in ax.get_xticks()], fontsize=16, color='white')
        
        
        # Add y-axis labels
        yticks = np.arange(len(data_pivot))
        yticklabels_left = [f'{motive}   InnerDriverA' for motive in data_pivot['Motivator']]
        yticklabels_right = ['InnerDriverB'] * len(data_pivot)
        ax.set_yticks(yticks)
        ax.set_yticklabels(yticklabels_left, va='center', ha='right', fontsize=14, color='black')
        
        # Calculate y-tick positions for right-side labels
        split = len(data_pivot)
        intervals = np.linspace(0, 1, split + 1)  # Split the number line into specified number of intervals
        yticks_right = (intervals[:-1] + intervals[1:]) / 2  # Compute the midpoints
        
        # Add right-side y-axis labels
        ax2 = ax.twinx()
        ax2.set_yticks(yticks_right)
        ax2.set_yticklabels(yticklabels_right, va='center', ha='left', fontsize=14, color='black')
        

        # Remove x and y tick marks
        ax.tick_params(axis='x', which='both', bottom=False, top=False)
        ax.tick_params(axis='y', which='both', left=False, right=False)
        ax2.tick_params(axis='y', which='both', left=False, right=False)

        # Remove border around the axes
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)
        
        # Remove border around the axes
        ax2.spines['top'].set_visible(False)
        ax2.spines['right'].set_visible(False)
        ax2.spines['bottom'].set_visible(False)
        ax2.spines['left'].set_visible(False)

        
        # Add values inside the bars
        for i, row in data_pivot.iterrows():
            value_a = row['InnerDriverA']
            value_b = row['InnerDriverB']
            ax.text(value_a + 2, i, str(-value_a), va='center', ha='left', color='white', fontsize=18, fontweight='bold')
            ax.text(value_b - 2, i, str(value_b),va='center', ha='right', color='white', fontsize=18, fontweight='bold')


        # Create a rectangle to set the background for bottom x-axis tick labels
        rect = plt.Rectangle((-.05, -0.08), 1.10, 0.08, transform=ax.transAxes, color='grey', clip_on=False)
        ax.add_patch(rect)        
        
        plt.title(f'{proband} - {motive}')
        plt.show()

create_horizontal_bar_chart(df, 'Subject_1')

Output:

and...

",
76383427,AOSP build not working - failed to build some targets,"I am trying to build the AOSP but i always get this error.

In every thread the answer is that its a RAM problem.
I have tried the build with the following commands. Nothing works
m -j32
m -j16
m -j12
m -j8
m -j2
m -j1
I am using a 32GB 14Core Native Linux Workstation.
",76387760,"After adding 20GB of Swap Storage it now works.
",
76389244,Use python schedule module in an efficient manner,"I want to use python scheduler module for an api call.
I have two main tasks.
1-Generate token at interval of 10 mins
2-Call the api using the token generated in previous step
The expected behavior is when a user calls the api it should use the same token for 10 mins and after 10 mins the toekn will be updated in background
Here are the sample piece of code
token=""""(This is a global variable)
def generate_token():
 token=//Logic to generate token//


def apicall():
 postreq=//api call using the token//


def schedule_run():
 schedule.every(10).minutes.do(get_token_api)
 while 1:
        schedule.run_pending()
        time.sleep(1)

if __name__ == ""__main__"":
 schedule_run()
 apicall()
 


When I am running above code the code is getting stuck in the while loop of schedule_run()  and not calling apicall()
Is there any efficient way to handle this?
",76389294,"You are getting stuck in the (infinite) loop inside schedule_run. First define the two scheduled jobs, the run the schedule waiting loop:
token=""""(This is a global variable)
def generate_token():
 token=//Logic to generate token//


def apicall():
 schedule.run_pending() #will update the token if it has not been done in the last 10 minutes
 postreq=//api call using the token//

if __name__ == ""__main__"":
 schedule.every(10).minutes.do(get_token_api)
 apicall()

",
76388900,PostgreSQL - Get value from from the same table,"Get the average traffic from last week data per WEEK number and get the traffic data for last week Traffic(D-7)
For example if date = 5/13/2023, need to output traffic data (Traffic(D-7)) for date = 5/6/2023

I manage to get the Average but no idea how to retrieve the date-7 data and output it altogether
create table a
(
date  varchar(50),
Tname varchar(50),
Week varchar(5),
Traffic float
)

insert into  a values ('5/1/2023', 'ID1', '18', 7.98)
insert into  a values ('5/2/2023', 'ID1', '18', 4.44)
insert into  a values ('5/3/2023', 'ID1', '18', 5.66)
insert into  a values ('5/4/2023', 'ID1', '18', 10.01)
insert into  a values ('5/5/2023', 'ID1', '18', 9.41)
insert into  a values ('5/6/2023', 'ID1', '18', 6.71)
insert into  a values ('5/7/2023', 'ID1', '18', 8.24)
insert into  a values ('5/8/2023', 'ID1', '19', 8.97)
insert into  a values ('5/9/2023', 'ID1', '19', 6.74)
insert into  a values ('5/10/2023', 'ID1', '19', 6.45)
insert into  a values ('5/11/2023', 'ID1', '19', 9.33)
insert into  a values ('5/12/2023', 'ID1', '19', 8.08)
insert into  a values ('5/13/2023', 'ID1', '19', 8.36)


SELECT date, Tname, Week,
AVG(Traffic) OVER(PARTITION BY Week) AS AVTraffic
FROM a
ORDER BY week

http://sqlfiddle.com/#!18/538b7/3
",76389306,"First of all, you need to fix your flaws in your table schema design, and declare:

dates with the ""DATE"" type (instead of VARCHAR(50))
week values with the INT type (instead of VARCHAR(5))
traffic values with the DECIMAL type (instead of FLOAT)

CREATE TABLE tab(
    DATE      DATE,
    Tname     VARCHAR(50),
    Week      INT,
    Traffic   DECIMAL(4,2)
);

Once you've carried it out, you can solve this problem by:

creating a ranking value for each day of the week in your weeks, using EXTRACT on your date
extracting your traffic value from previous week with LAG, by partitioning on your ranking created at previous step, and ordering on the week_number.

WITH cte AS (
    SELECT date, Tname, Week, Traffic,
           ROUND(AVG(Traffic) OVER(PARTITION BY Week), 2) AS AVGTraffic,
           EXTRACT(ISODOW FROM date) - 1                  AS week_day
    FROM tab
)
SELECT date, Tname, Week, 
       LAG(Traffic) OVER(PARTITION BY week_day ORDER BY Week) AS prevweek_traffic,
       AVGTraffic
FROM cte
ORDER BY Week, week_day

And if you realize that you may have holes among your weeks (..., week 17, week 18, week 20, week 21, ...) and specifically want values from the exact previous week (that may be missing), you can add a filter on the LAG function, that checks if week and previous week are consecutive:
...
CASE WHEN LAG(Week) OVER(PARTITION BY week_day ORDER BY Week) = Week-1
     THEN LAG(Traffic) OVER(PARTITION BY week_day ORDER BY Week) 
END
...

(in place of LAG(Traffic) OVER(...) only)
Output:




date
tname
week
prevweek_traffic    avgtraffic




2023-05-01T00:00:00.000Z
ID1
18
null


2023-05-02T00:00:00.000Z
ID1
18
null


2023-05-03T00:00:00.000Z
ID1
18
null


2023-05-04T00:00:00.000Z
ID1
18
null


2023-05-05T00:00:00.000Z
ID1
18
null


2023-05-06T00:00:00.000Z
ID1
18
null


2023-05-07T00:00:00.000Z
ID1
18
null


2023-05-08T00:00:00.000Z
ID1
19
7.98


2023-05-09T00:00:00.000Z
ID1
19
4.44


2023-05-10T00:00:00.000Z
ID1
19
5.66


2023-05-11T00:00:00.000Z
ID1
19
10.01


2023-05-12T00:00:00.000Z
ID1
19
9.41


2023-05-13T00:00:00.000Z
ID1
19
6.71




Check the demo here.
This query allows any kind of holes in your data, if that's a needed requirement.
Note: The last ORDER BY clause is not needed. It's there just for visualization purposes.
",
76382863,Firebase RT Database Query on field value hash equality,"I have a Firebase Realtime Database with users and the email is stored in a child field, for example:
/users/00Vho6lTQke46IqpRv0D5dw6DXs2/email -> user@email.com

I want to query once and get a user based on the MD5 hash of the email address. For example, MD5('user@email.com') -> 'b58c6f14d292556214bd64909bcdb118'.
I don't have the email address of the user, I only have the MD5 hash. Is it possible to query using an MD5 function to transform the email field to MD5 and query on that?
",76388174,"
Is it possible to query using an MD5 function to transform the email field to MD5 and query on that?

No, unless you save the MD5 as a field inside the database. If you do that, you'll be able to query using the MD5. Your schema should look like this:
db
|
--- users
     |
     --- 00Vho6lTQke46IqpRv0D5dw6DXs2
           |
           --- email: ""user@email.com""
           |
           --- md5: ""b58c6f14d292556214bd64909bcdb118""

And the query should look like this:
val query = db.child(""users"").orderByChild(""md5"").equalTo(""b58c6f14d292556214bd64909bcdb118"");

",
76387305,Update + join from two databases,"Help me create an sql query.
There are two databases DB1 and DB2. In each of them there are tables ""users"" and ""cities"" with the names of cities. Users and city names are the same in both databases. But the city ID values are different in the two databases. In the first DB1 database, the ""users"" table contains the city IDs from the ""cities"" table.
I need to update the city data in the second DB2 database. For each ""users"" value from DB1, find the name ""cities"" and update the values in DB2.users. The problem is that the city ID values are different in the two databases. Only the names are the same.

",76388059,"Use an update with join:
update DB2.users set
city = DB2.cities.ID
from DB2.users 
join DB1.users on DB1.users.ID = DB2.users.ID
join DB1.cities on DB1.cities.ID = DB1.users.city
join DB2.cities on DB2.cities.name = DB1.cities.name

The join from DB2.users to DB2.cities goes via DB1's tables, going over using user ID and coming back using city name.
",
76389247,Writing a Nested List of Strings to CSV using CSV helper,"I have a nested list of strings that I would like to write to a CSV file using CSV helper. So, it is a list called Entries and each element has a list of strings called Entry.
My current code is exporting a blank CSV file.
Entry example:
{""Date"", ""param1"", ""param2"", ""param3"", ""param4""}

and the Entries list will have a list of those.
I would like to write to a CSV file with each entry being a new line.
Intended outcome:
""Date"", ""param1"", ""param2"", ""param3"", ""param4""
""Date"", ""param1"", ""param2"", ""param3"", ""param4""
""Date"", ""param1"", ""param2"", ""param3"", ""param4""
""Date"", ""param1"", ""param2"", ""param3"", ""param4""

My current code:
var rawDataList = await _siteRepository.GetRawSiteData(siteGuid);

var Entries = _siteRepository.GetSiteData(rawDataList);
var writer = new StreamWriter(""YodelTesting.csv"");
var csv = new CsvWriter(writer, CultureInfo.InvariantCulture);

foreach(var record in Entries)
{
    csv.WriteRecord(record);
}

Help would be appreciated!
",76389335,"I used the current code to get it working
using (var stream = new MemoryStream())
using (var writer = new StreamWriter(""yodelTesting.csv""))
using (var reader = new StreamReader(stream))
using (var csv = new CsvWriter(writer, CultureInfo.InvariantCulture))
{
    foreach (var record in output)
    {
        foreach (var field in record)
        {
            csv.WriteField(field);

        }
        csv.NextRecord();
    }
    //not sure what this does tbh
    writer.Flush();
    stream.Position = 0;
}

",
76382701,How to deal with 48 or 64bit images in C#,"I've been using the Bitmap-class and it's png-ByteStream to deal with images in my WPF-application, mostly with reading/writing files, getting images from native cam libs and showing them in the UI.
Lately I've got some new requirements and tried to upgrade some of that to deal with 64 bit deep images (or 48 bits, I don't need an alpha channel). In pretty much every operation bitmap is converting my image back to 32bppArgb. I've figured out deepcopying and am currently looking into Image.FromStream(), but needing to deal with this got me wondering:
Is there a way to properly deal with non 32bppArgb-images within a C#-application (including true greyscale)? Or does microsoft just neglect the need for it? I've found some related questions, but mostly with 10 year old hacks, so I'd expect there to be a proper way by now...
Edit: As requested, I'm gonna show some code. It's running on .netStandard2.0 to be used mostly inside .net6.0 apps; with System.Drawing.Common on Version 5.0.2 (I don't think it matters but would be willing to upgrade if that's the case). Imagine nativeImage to be a struct from some cameraLib, so we want a deepcopy to get away from the shared memory:
using Bitmap bitmap = new(nativeImage.Width, nativeImage.Height, nativeImage.stride, PixelFormat.Format64bppArgb, nativeImage.DataPtr);
using Bitmap copy = bitmap.DeepCopy(); //custom function

using MemoryStream stream = new();
copy.Save(stream, ImageFormat.Png);
byte[] byteStream = stream.ToArray(); //Saving this to file shows it's really 64 bit

//These bits are stitched together for testing, normally this byte-Array might get passed around quite a bit
using MemoryStream ms = new(byteStream);
using Image image = Image.FromStream(ms, true); //this suddenly is 32bppArgb

with:
public static Bitmap DeepCopy(this Image original)
{
// this just copies the code of the Bitmap(Image)-constructor but uses the original pixelFromat instead of 32bppArgb
    var result = new Bitmap(original.Width, original.Height, original.PixelFormat);
    using (Graphics g = Graphics.FromImage(result))
    {
        g.Clear(Color.Transparent);
        g.DrawImage(original, 0, 0, original.Width, original.Height);
    }
    return result;
}

",76388249,"I do not think you will have much success with System.Drawing.Bitmap. In my experience these just have poor support high bit depths. I would instead take a look at the corresponding System.Windows.Media classes. I know PngBitmapEncoder/decoder at least support 16 bit grayscale, but I suspect 48/64 bit works fine as well.
For display I would expect that you would want to do your own tone mapping, since I have never seen any builtin method do a particular good job.
If you want a format for interchange I would suggest creating your own. Any image is essentially represented using just a few properties:

Width
Height
Stride - The number of bytes on a row. This must be at least large enough to fit a row of pixels, but may be larger for alignment reasons.
PixelFormat
Pixel Data - This can essentially anything representing binary data. Byte[], a pointer, a stream, Memory<T> etc. The total size of the data should be Height * Stride.

Most image processing libraries should have methods accepting raw image data. You might need to jump thru some hoops to get it to work, like using unsafe code or doing a blockCopy. You also likely need some mapping code to convert between all the various PixelFormat enums.
Libraries typically provide Bitmap access for convenience, but convert this data to some internal format as soon as possible. You should be able to do the same.
",
76387881,Ansible Variable precedence - what are 'role params' & 'include params',"The Ansible documentation has a list of Variable precedence
Some of them are clear to me but I wonder whether anybody coulde kinkdy shed some light on those 2.

20. role (and include_role) params
21. include params

usage, location, syntax.
I am trying to get Variable declaration a little further to the surface inside a little more complex playbook utilizing 2 roles I am currently working on.
concrete ... some values inside task files of a role should rather be declared as variables in a single location.
",76388086,"In a nutshell examples:
role params
(full playbook)
- name: I'm a dummy play
  hosts: localhost

  roles:
    - role: somerole
      vars:
        param1: ""I'm a role param""

Include role params
(task only)
- name: Including role somerole
  ansible.builtin.include_role:
    name: somerole
  vars:
    param1: ""I'm an include role param""

Include params
(task only)
- name: Including a task file
  ansible.builtin.include_tasks: sometasks.yaml
  vars:
    param1: ""I'm an include param""


As a fictive and (most probably bad practice) example: if you include a role passing a parameter and later include a task file in that role passing that same parameter with a different value, the include param will take precedence over the role param.
",
76389245,Why garbage value is not being returned in a recursive call to a function with undefined behavior?,"I came across this code somewhere when practicing basic C questions:
int func(num) {
    if (num > 0) {
        return (num + func(num - 2));
    }
}

int main() {
    printf(""%d\n"", func(5));
    return 0;
}

The code when executed returns 8 as the answer. But I think that the answer should be ""cannot be determined"".
The recursive calls in my head looks like this:
5 + func(3)  => 5 + (3 + func(1))
3 + func(1)  => 3 + (1 + func(-1))
1 + func(-1) => 1 + (?)

For the ? symbol, I think reading the return value for func(-1) is undefined behavior. I'm basing this on the assumption that for func(-1) no explicit int value is being returned. So reading from func(-1) in the expression (1 + func(-1)) should produce some garbage result + 1 in my opinion.
Why is that the code is returning 8 as the answer and not something garbage?
When I explicitly pass a negative number to func and read the result I do get the garbage value, such as in this code;
int main() {
    printf(""%d\n"", (1 + func(-1)));   // returns garbage result and not 0
    return 0;
}

Why is that in the recursive call (1 + func(-1)) is being evaluated to 0 value?
I compiled the code on a 64-bit machine with gcc as gcc myfile.c. Compiling the code as gcc -W -Wall -ansi -pedantic myfile.c gives warning about the func function, but that's not the point. I am unable to figure out how 8 is the answer.
",76389353,"
Why garbage value is not being returned in a recursive call to a function with undefined behavior?

Because ""garbage"" does not mean what you think it means in this context.  In particular, ""garbage"" does not mean ""random"".  (Or if it does, it's more in the sense of xkcd 221.)
Computers are usually deterministic.  You have to work pretty hard to get truly random behavior.  Even a program that contains the worst kind of undefined behavior will quite often return exactly the same strange and indeterminate number every time you run it.
I have used this analogy:

Suppose you go to the store and buy a brand-new garbage can. But it's completely clean! There's no garbage in it at all! It's so clean you could eat out of it! Was this false advertising? Did the store fraudulently sell you a non-garbage can?

See more discussion at these previous questions:
1
2
3
4
5.
(Most of those are talking about the values of uninitialized local variables, not the values of functions that fail to execute a proper return statement, but the arguments are the same.)
",
76387483,SQL to fetch the count between tow date range,"I require to fetch the count of PRODUCT_ID between 00:15:00 and 01:15:00 and subsequentially for any date range.
Example Scripts:-
My DB structure and data is as follows.
CREATE TABLE time1 (cr_date date , product_id number );

insert into time1 values (to_date ('01-JAN-2022 01:00:00', 'DD_MON-YYYY HH:MI:SS') , 12345);
insert into time1 values (to_date ('01-JAN-2022 01:00:00', 'DD_MON-YYYY HH:MI:SS') , 12346);
insert into time1 values (to_date ('01-JAN-2022 01:00:00', 'DD_MON-YYYY HH:MI:SS') , 12347);
insert into time1 values (to_date ('01-JAN-2022 03:30:00', 'DD_MON-YYYY HH:MI:SS') , 42345);
insert into time1 values (to_date ('01-JAN-2022 03:30:00', 'DD_MON-YYYY HH:MI:SS') , 42346);
insert into time1 values (to_date ('01-JAN-2022 03:35:00', 'DD_MON-YYYY HH:MI:SS') , 42347);
insert into time1 values (to_date ('01-JAN-2022 03:40:00', 'DD_MON-YYYY HH:MI:SS') , 42348);
insert into time1 values (to_date ('01-JAN-2022 10:40:00', 'DD_MON-YYYY HH:MI:SS') , 10348);
insert into time1 values (to_date ('01-JAN-2022 10:42:00', 'DD_MON-YYYY HH:MI:SS') , 10349);
insert into time1 values (to_date ('01-JAN-2022 10:43:00', 'DD_MON-YYYY HH:MI:SS') , 11348);

COMMIT;

Output is required to be as below:-
| hours   | count |
|:------  |:------|
|00:15:00 |3|
|01:15:00 |0|
|02:15:00 |0|
|03:15:00 |0|
|04:15:00 |4|
|05:15:00 |0|
|06:15:00 |0|
|07:15:00 |0|
|08:15:00 |0|
|09:15:00 |0|
|10:15:00 |0|
|11:15:00 |3|
|..
|...
|23:15:00 |0|

",76388088,"Sample data you (initially) posted is pretty much useless, there's no time component involved.
In one of my tables, there's a datum column and values look like this:
SQL> SELECT id, TO_CHAR (datum, 'hh24:mi') hrs FROM obr WHERE rownum <= 10;

        ID HRS
---------- -----
     21547 08:41
     21541 08:17
     21563 09:03
     21614 10:46
     21618 11:01
     21620 11:04
     21622 11:05
     21626 11:10
     21629 11:14
     21642 13:35

10 rows selected.

SQL>

This is query which

in fmin CTE creates 24 rows (00:15, 01:15, ... 23:15)
join is done on datum ""rounded"" to previous 15-minute value (whether in the same hour, or in previous hour - depends on minutes)

So:
SQL> WITH
  2     fmin
  3     AS
  4        (    SELECT TRUNC (SYSDATE) + (LEVEL - 1) / 24 + INTERVAL '15' MINUTE c_time
  5               FROM DUAL
  6         CONNECT BY LEVEL <= 24)
  7    SELECT TO_CHAR (f.c_time, 'hh24:mi') c_time, COUNT (z.id) cnt
  8      FROM fmin f
  9           LEFT JOIN obr z
 10              ON TO_CHAR (f.c_time, 'hh24:mi') =
 11                 TO_CHAR (
 12                      TRUNC (datum, 'hh24')
 13                    + CASE
 14                         WHEN TO_NUMBER (TO_CHAR (datum, 'mi')) >= 15
 15                         THEN
 16                            INTERVAL '15' MINUTE
 17                         WHEN TO_NUMBER (TO_CHAR (datum, 'mi')) < 15
 18                         THEN
 19                            INTERVAL '-45' MINUTE
 20                      END,
 21                    'hh24:mi')
 22  GROUP BY TO_CHAR (f.c_time, 'hh24:mi')
 23  ORDER BY 1;

Result:
C_TIM        CNT
----- ----------
00:15          0
01:15          0
02:15          0
03:15          0
04:15          0
05:15          0
06:15          0
07:15          2
08:15         10
09:15          1
10:15         14
11:15          6
12:15         10
13:15         38
14:15          5
15:15          0
16:15          0
17:15          0
18:15          0
19:15          0
20:15          0
21:15          0
22:15          0
23:15          0

24 rows selected.

SQL>

",
76383461,DB2 Group by specific columns,"



ID_1
ID_2
ID_3
LANG




1
11
111
F_lang


1
11
111
null


2
22
222
null




Is it possible to build  a query which returns only these two lines below.
It should be grouped by the first three columns and should take only that line with value of column LANG which is preferably not null, if no value for LANG exists, it should take the line with language null.




ID_1
ID_2
ID_3
LANG




1
11
111
F_lang


2
22
222
null



",76388255,"Does this anwser the question ?
with t1(id_1, id_2, id_3, lang) as (
  VALUES
  ('1', '11', '111', 'F_lang'),
  ('1', '11', '111', NULL),
  ('2', '22', '222', NULL),
  ('3', '33', '333', 'F_lang_3_1'),
  ('3', '33', '333', 'F_lang_3_2'),
  ('3', '33', '333', NULL)
)
select id_1, id_2, id_3, lang from t1 where lang is not null
union all
select id_1, id_2, id_3, null as lang from t1 group by id_1, id_2, id_3 having max(lang) is null
order by id_1, id_2, id_3





ID_1
ID_2
ID_3
LANG




1
11
111
F_lang


2
22
222
null


3
33
333
F_lang_3_1


3
33
333
F_lang_3_2




fiddle
",
76382933,"How to express h(n) which is the ""union"" of the two sequences f(n) & g(n)","I am on thin ice both syntax-wise and English-wise here, apologise if my way of expressing maths here is a rambling of a mad-man.
I have two sequences/""functions"" f(n) and g(n). They are technically not functions, I have them just defined as a sequence of repeating modulus of 9 and 10.
n = 0,1,2,3,...
f(n): nMOD9={0,3,4,6}
g(n): nMOD10={0,3,4,5,8,9}

This means that f(n) is will go 0,3,4,6,9,12,13,15,18,ect. Just repeating the four numbers in the brackets of modulus of 9. 

g(n) will be 0,3,4,5,8,9,10,13,14,15,18,19,20,ect. Repeating the six numbers in brackets of modulus of 10.

Now I wonder, can I express h(n) which is the list of numbers that is both present in f(n) and g(n)? This will be 0,3,4,9,13,15,18,ect. Either as a function or as some nMODx={a,b,c}? Or some other genius way I have not thought about. 

Currently I do a manual check of both list, and I wonder if can be done more elegant.
",76388472,"One possible solution would be to find all values of h(n) up to 90, then h(n) will be those nMOD90, so
h(n): nMOD90={0,3,4,9,13,15,18,24,30,33,39,40,45,48,49,54,58,60,63,69,75,78,84,85}

",
76388049,"""BASE64DecoderStream"" gives error for javax-mail dependency","I recently changed the dependency
<dependency>
    <groupId>javax.mail</groupId>
    <artifactId>mail</artifactId>
    <version>1.4</version>
</dependency>

to
<dependency>
    <groupId>javax.mail</groupId>
    <artifactId>javax.mail-api</artifactId>
    <version>1.6.0</version>
</dependency>

because Java 8 doesn't support 1.4 version as it uses TLS 1.0.
After changing the dependency this code starts giving error.
Error code:
if (p.getContentType().contains(""image/"")) {
    File f = new File(""image"" + new Date().getTime() + "".jpg"");
    DataOutputStream output = new DataOutputStream(
            new BufferedOutputStream(new FileOutputStream(f)));
    
    com.sun.mail.util.BASE64DecoderStream test = (com.sun.mail.util.BASE64DecoderStream) p
            .getContent();
    byte[] buffer = new byte[1024];
    int bytesRead;
    while ((bytesRead = test.read(buffer)) != -1) {
        output.write(buffer, 0, bytesRead);
    }

Eclipse suggestion error:
Multiple markers at this line
    - com.sun.mail.util.BASE64DecoderStream cannot be resolved to a type
    - com.sun.mail.util.BASE64DecoderStream cannot be resolved to a type

",76388104,"You're using the wrong dependency. You have only added the JakartaMail API. You should use the JakartaMail 1.6.x implementation:
<dependency>
    <groupId>com.sun.mail</groupId>
    <artifactId>jakarta.mail</artifactId>
    <version>1.6.7</version>
</dependency>

This dependency does include com.sun.mail.util.BASE64DecoderStream.
As an aside, instead of what you're doing now (using getContent() and casting to an implementation-specific class), you could also use Part.getInputStream():
try (var in = p.getInputStream()) {
    in.transferTo(output);
}

Also, your use of DataOutputStream is suspect, as you don't seem to be using any DataOutputStream specific methods, so using the BufferedOutputStream directly should be sufficient.
",
76389296,Where should pygame.time.Clock().tick() be called in the script,"According to this statement, pygame.time.Clock().tick() should be called at the end of the main loop. However, I couldn't see any differences on my screen display regardless where I executed that method within the loop. Could someone please give some clarification on this? Thanks
",76389365,"The documentation say :

A call to the tick() method of a Clock object in the game loop can
make sure the game runs at the same speed no matter how fast of a
computer it runs on

So it is better to call it at the end of the loop because if you do it in the middle of your display fonction, a part of the element will be refresh before the wainting and a part after.You should call pygame.display.update() before that otherwise you refresh the screen after the ""frame wait time"".
",
76383813,Remote Interpreter DataSpell,"I am trying to run scripts on a remote server with DataSpell, so I am trying to configure a remote interpreter.
I follow these instructions and I get the error
Cannot Save Settings:
SSH Python x.x.x user@IP : Python x.x.x
(path/to/interpreter) can't be used as a workspace interpreter

As a path to the interpreter I have used the .../bin/python file that is created in a virtual environment. I have tried virtual environments created both with conda and venv. I have also found this, but I cannot understand the solution clearly.
Any ideas?
",76388631,"It appears that Dataspell can have different interpreters for its workspace and projects or notebooks that are opened in the workspace.
I right-clicked the project folder in the Project window, and selected a remote interpreter just for the project. This is what the second source that I posted on my OP says. This works fine.
",
76382654,Microsoft Graph .NET SDK on-behalf-of flow returns Access Denied 403 for Sites/Files,"I'm trying to implement on-behalf-of user flow for Microsoft Graph API. I'm using Microsoft Graph .NET SDK and I'm encountering Access Denied (status code 403) for my requests. I'm not sure how to correctly set up permissions for users of my organization in Active Azure Directory. I'm trying to implement SharePoint / OneDrive file operations like list files/download/upload etc. with on-behalf-of flow. I want to be able to access any drive/site files and be able to upload/download/list.
The way my application is designed is like this:

But instead of Desktop WPF app, I have next.js website using next-auth. This is how I define my Azure Active Directory provider to sign-in in my next.js application using next-auth:
providers: [
    AzureADProvider({
      clientId: process.env.AZURE_AD_CLIENT_ID, // NextAuth client id in AAD
      clientSecret: process.env.AZURE_AD_CLIENT_SECRET, // NextAuth client secret value in AAD
      tenantId: process.env.AZURE_AD_TENANT_ID, 
      authorization: {
        params: {
          scope: ""openid email profile offline_access api://NextAuthAPI/.default"",
          prompt: ""consent"",
        }
      },
    }),
  ]

In my AAD I registered my frontend app and web api as NextAuth and NextAuthAPI, generated client secret for both of them. For the NextAuthAPI (webAPI) I created a scope:

and added these permissions:

For the NextAuth (next.js) app I added this permission:

After signing in my next.js frontend application, I'm able to receive an access token with:
""aud"": ""api://NextAuthAPI"",
""scp"": ""access_as_user""
....

Then I provide this access_token to the webAPI and create client like this:
var scopes = new[] { ""https://graph.microsoft.com/.default"" };

var tenantId = ""c0dc-------------------5ce7"";
var clientId = ""087e66-----------------------b5"";
var clientSecret = ""p5Q8Q-----------------------JddpK"";

// using Azure.Identity;
var options = new OnBehalfOfCredentialOptions
{
    AuthorityHost = AzureAuthorityHosts.AzurePublicCloud
};

var oboToken = /* HERE I PASTE ACCESS_TOKEN FROM NEXTJS APP */;

var onBehalfOfCredential = new OnBehalfOfCredential(tenantId, clientId, clientSecret, oboToken, options);

m_GraphClient = new GraphServiceClient(onBehalfOfCredential, scopes);

Now, when I have the m_GraphClient initialized, I'm trying to make some requests. This request works for me:
var response = await m_GraphClient.Me.GetAsync();

But something like this:
var response2 = await m_GraphClient.Sites.GetAllSites.GetAsync();

throws an ODataError exception with status code 403 and message ""Access denied"".
Packages I use:

Microsoft.Graph 5.11.0 (backend)
Azure.Identity 1.9.0 (backend)
Next.JS 13.4.5-canary.2 (frontend)
next-auth 4.22.1 (frontend)

",76388806,"You want to be able to access any drive/site files and be able to upload/download/list. Then on-behalf-of flow isn't suitable for you. OBO flow will generate access token with Delegated API permission, which can't be used to access any others' drive/site files. We need to use access token with Application API permission.
Then in this scenario, we can only use client credential flow so that we can generate access token with Application API permission. First, grant application API permission in Azure portal. For example, we are trying to use this Graph API.

Then in your API application, your code should look like this which used ClientSecretCredential instead of your OnBehalfOfCredential:
var scopes = new[] { ""https://graph.microsoft.com/.default"" };
var tenantId = ""tenant_id"";
var clientId = ""client_id"";
var clientSecret = ""client_secret"";
var clientSecretCredential = new ClientSecretCredential(
                            tenantId, clientId, clientSecret);
var graphClient = new GraphServiceClient(clientSecretCredential, scopes);

",
76387995,Gitlab CI Deploy Token cant push to repo,"I am trying to copy some files from Firebase into a Gitlab repo.
Using my personal SSH credentials, I am able to do this.
I'd like to use deploy tokens. I generated a fresh token and gave it all the permissions I can. However, when I run the CI pipeline I get an ""fatal: Authentication failed""
",76388110,"A deploy token is simply not made for that. As per the documentation a deploy token is not capable to write to a GitLab repository. Here is what you can do with a deploy token:

Clone Git repositories.
Pull from and push to a GitLab container registry.
Pull from and push to a GitLab package registry.

What may be a better fit for your use case is a so called Deploy Key. If the deploy key has read-write permissions you should be able to solve your issues.
",
76389214,Replace one button with another when countdown timer ends,"I have a simple countdown timer which is launched on the click of the .btn-submit-a button. I tried to make it so that after the timer ends, button A is replaced by button B (.btn-submit-b), but, unfortunately, nothing comes out. How can I achieve this? I will be glad for any help.


jQuery(function($){
  $('.btn-submit-a').on('click', doCount);
});

function doCount() {
var timeleft = 5;
var downloadTimer = setInterval(function(){
  if(timeleft <= 0){
    clearInterval(downloadTimer);
    document.getElementById(""countdown"").innerHTML = ""Time is Up"";
  } else {
    document.getElementById(""countdown"").innerHTML = timeleft + ""<span class='remain'>seconds remain</span>"";
  }
  timeleft -= 1;
}, 1000);
};
.btn-submit-b {
  display: none;
}
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<button type=""submit"" class=""btn-submit-a"">Button A</button>
<button type=""submit"" class=""btn-submit-b"">Button B</button>

<div id=""countdown""></div>



",76389384,"You had a weird combination between jquery and native code. You can do that ofcourse but I would recommend to stick to either jQuery or native where possible. Therefore I changed some code to jQuery functions.
This said you can hide and show elements with the hide() and show() jQuery functions as you can see in the example below.
If you want to toggle them instead you can use toggle()


jQuery(function($) {
  $('.btn-submit-a').on('click', doCount);
});

function doCount() {
  var timeleft = 5;
  var downloadTimer = setInterval(function() {
    if (timeleft <= 0) {
      clearInterval(downloadTimer);
      $(""#countdown"").html(""Time is Up"");
      $('.btn-submit-a').hide();
      $('.btn-submit-b').show();
    } else {
      $(""#countdown"").html(timeleft + ""<span class='remain'>seconds remain</span>"");
    }
    timeleft -= 1;
  }, 1000);
};
.btn-submit-b {
  display: none;
}
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<button type=""submit"" class=""btn-submit-a"">Button A</button>
<button type=""submit"" class=""btn-submit-b"">Button B</button>

<div id=""countdown""></div>



",
76389116,Clob Issues- Splitting a clob column to multiple rows,"I have a table (description of table):




Name
Type




KEYVALUE
VARCHAR2(100)


TEXT
CLOB




Example




Keyvalue
Text




101
Customer Input 05/15/2023 07:20:20 My name is ABX +++ Private Notes What is you name+++Customer Input 04/30/2023 19:40:58 I have issue related to water purifier purchased on Jan 23 +++ Public Notes 04/30/2023 18:19:18 +++Customer Input 04/30/2023 Requesting to send a technicial, we could not bring them up due to the same issue that was looked into in ticket 20092. We dont know if this is the same issue as the previous ticket, but need to know the reason Language Preference: English


102
Customer Input 05/15/2023 07:20:20 20424596 Reference to the above ticket+++Customer Input 04/30/2023 19:40:58 Plesae replace the item as this is a faulty one +++ Public Notes 04/30/2023 18:19:18 +++Customer Input 04/30/2023 17:54:54 Shared the faulty machine pics for quick action Problem Context: When was the issue first observed? - 4/30, 1AM or so Were there any recent changes or maintenance performed? - Language Preference: English




I am basically splitting the entire phase to multiple rows by the word ""customer Input"".
Something like below:
SELECT distinct keyvalue, level pos, trim(regexp_substr(text, 'Customer Input[^+++]*', 1, level))  x
  FROM 
  (
    SELECT 101 as keyvalue,'Customer Input 05/15/2023 07:20:20 My name is ABX +++ Private Notes What is you name+++Customer Input 04/30/2023 19:40:58 I have issue related to water purifier purchased on Jan 23 
    +++ Public Notes 04/30/2023 18:19:18 +++Customer Input 04/30/2023 Requesting to send a technicial, we could not bring them up due to the same issue that was looked into in ticket 20092. We dont know if this is the same issue as the previous ticket, but need to know the reason Language Preference: English| '
    as text from dual
    union all 
    SELECT 102 as keyvalue,' Customer Input 05/15/2023 07:20:20 20424596 Reference to the above ticket+++Customer Input 04/30/2023 19:40:58 Plesae replace the item as this is a faulty one 
    +++ Public Notes 04/30/2023 18:19:18 +++Customer Input 04/30/2023 17:54:54 Shared the faulty machine pics for quick action Problem Context: When was the issue first observed? - 4/30, 1AM or so Were there any recent changes 
    or maintenance performed? - Language Preference: English| '
    as text from dual  

  ) t
CONNECT BY instr(text, 'Customer Input', 1, level - 1) > 0
order by keyvalue;





keyvalue
pos
text




101
1
Customer Input 05/15/2023 07:20:20 My name is ABX


101
2
Customer Input 04/30/2023 19:40:58 I have issue related to water purifier purchased on Jan 23


101
3
Customer Input 04/30/2023 Requesting to send a technicial, we could not bring them up due to the same issue that was looked into in ticket 20092. We dont know if this is the same issue as the previous ticket, but need to know the reason Language Preference: English


101
4



102
1
Customer Input 05/15/2023 07:20:20 20424596 Reference to the above ticket


102
2
Customer Input 04/30/2023 19:40:58 Please replace the item as this is a faulty one


102
3
Customer Input 04/30/2023 17:54:54 Shared the faulty machine pics for quick action Problem Context: When was the issue first observed? - 4/30, 1AM or so Were there any recent changes or maintenance performed? - Language Preference: English


102
4





This is working fine since the text column is of character datatype.
But when I am running this below query (on the actual column which is of clob datatype)
SELECT distinct keyvalue, level pos, trim(regexp_substr(customer_input_info, 'Customer Input[^+++]*', 1, level)) str
  FROM (select 101 as keyvalue,to_clob('Customer Input 05/15/2023 07:20:20 20424596 Reference to the above ticket+++Customer Input 04/30/2023 19:40:58 Plesae replace the item as this is a faulty one 
    +++ Public Notes 04/30/2023 18:19:18 +++Customer Input 04/30/2023 17:54:54 Shared the faulty machine pics for quick action Problem Context: When was the issue first observed? - 4/30, 1AM or so Were there any recent changes 
    or maintenance performed? - Language Preference: English| ') as customer_input_info from dual) t
CONNECT BY instr(customer_input_info, 'Customer Input', 1, level - 1) > 0
order by 1

I am getting below error
ORA-00932: inconsistent datatypes: expected - got CLOB
00932. 00000 -  ""inconsistent datatypes: expected %s got %s""
*Cause:
*Action:
Error at Line: 44 Column: 38.
I can't make changes to inner sql query as the source table is of clob data type. What changes should I make to outer query.
",76389387,"The code you posted will get that error if the source text is a CLOB, whatever the length. The problem isn't the length itself, it's that each split row value is also a CLOB, and you can't use distinct with CLOBs.
The use of distinct is often a sign that there's a deeper problem that's just covering up. Without it you do get duplicates, but that's a well-known issue with connect-by queries against multiple source rows, and will get progressively worse with more rows.
You need to limit the connect-by to the same source row, which is simple assuming keyvalue is unique; but you also need to introduce a non-deterministic function call to prevent it ballooning the results, for example:
CONNECT BY instr(text, 'Customer Input', 1, level - 1) > 0
AND keyvalue = PRIOR keyvalue
AND PRIOR dbms_random.value IS NOT NULL

fiddle
You might finder it easier to understand and maintain if you switch to using recursive subquery factoring instead of a hierarchical query:
WITH r (keyvalue, text, pos, x) as (
  SELECT keyvalue, text, 1, trim(regexp_substr(text, 'Customer Input[^+++]*', 1, 1))
  FROM t
  UNION ALL
  SELECT keyvalue, text, pos + 1, trim(regexp_substr(text, 'Customer Input[^+++]*', 1, pos + 1))
  FROM r
  WHERE instr(text, 'Customer Input', 1, pos) > 0
)
SELECT keyvalue, pos, x
FROM r
order by keyvalue, pos;





KEYVALUE
POS
X




101
1
Customer Input 05/15/2023 07:20:20 My name is ABX


101
2
Customer Input 04/30/2023 19:40:58 I have issue related to water purifier purchased on Jan 23 


101
3
Customer Input 04/30/2023 Requesting to send a technicial, we could not bring them up due to the same issue that was looked into in ticket 20092. We dont know if this is the same issue as the previous ticket, but need to know the reason Language Preference: English|


101
4



102
1
Customer Input 05/15/2023 07:20:20 20424596 Reference to the above ticket


102
2
Customer Input 04/30/2023 19:40:58 Plesae replace the item as this is a faulty one 


102
3
Customer Input 04/30/2023 17:54:54 Shared the faulty machine pics for quick action Problem Context: When was the issue first observed? - 4/30, 1AM or so Were there any recent changes     or maintenance performed? - Language Preference: English|


102
4





fiddle
I've left it with the same stop condition, which generates a final null entry. You may want to revisit that, for either approach.
",
76383547,How can I get Ansible client IP from target host?,"On an isolated network (without internet access to do public IP address lookups), I want to run a playbook from a controller against a number of target hosts where one of the tasks is to download a file via HTTP/HTTPS from the controller without hard-coding the controller IP as part of the task. E.g.
Controller: 192.168.0.5
Target 1: 192.168.0.10
Target 2: 192.168.0.11
Target 3: 192.168.0.12

The controller can have different IPs configured via DHCP, and there could be multiple network interfaces listed in ansible_all_ipv4_addresses (some of which may not be available to the target hosts) so it may not be straight forward to determine which network interface the target hosts should use from ansible_facts on localhost without exploring the idea of looping through them with a timeout until the file has been downloaded. It seems as though the most robust way to determine the public IP of the controller (assuming the web server is listening on 0.0.0.0) would be to determine the originating IP of the established connection (192.168.0.5) from the target host - is there a way to do this?
The motivation for downloading the file from the controller rather than sending it to remote hosts is that some of the target hosts are running Windows and the win_copy module is incredibly slow via WinRM so the Ansible documentation includes the following note:

Because win_copy runs over WinRM, it is not a very efficient transfer
mechanism. If sending large files consider hosting them on a web
service and using ansible.windows.win_get_url instead.

",76389112,"Limited test on my machine which has a single ip and with a single target. But I don't see why it would not work in your scenario.
Given the following inventories/default/hosts.yml
all:
  hosts:
    target1:
      ansible_host: 192.168.0.10
    target2:
      ansible_host: 192.168.0.11
    target3:
      ansible_host: 192.168.0.12

The following test playbook should do what you expect. Replace the dummy debug task with get_url/uri to initiate the download.
Notes:

this playbook infers you have access to the ip command line tool on the controller.
I took for granted that the controller IP used to connect to target is the one that the target has access to in the other direction. If this isn't the case then the below will not work in your situation.

---
- hosts: all
  gather_facts: false

  tasks:
    - name: Check route on controller for each target destination
      ansible.builtin.command: ip route get {{ ansible_host }}
      register: route_cmd
      delegate_to: localhost

    - name: Register the controller outgoing ip for each target
      ansible.builtin.set_fact:
        controller_ip: ""{{ route_cmd.stdout_lines[0] | regex_replace('^.* src (\\d*(\\.\\d*){3}).*$', '\\1') }}""

    - name: Show result
      ansible.builtin.debug:
        msg: ""I would connect from target {{ inventory_hostname }} ({{ ansible_host }}) 
          to controller using ip {{ controller_ip }}""

",
76387851,How can I split an array of objects and extract specific keys while adding 0s to fill up to a specific length?,"My array of objects looks like this:
[{""data"": [5, 2, 7, 2, 4, 2], ""date"": {""begin"": ""03.07."", ""beginYear"": ""2023"", ""end"": ""10.07."", ""endYear"": ""2023"", ""timestamp"": 1688335200000}}, {""data"": [5, 2, 7, 2, 4, 2], ""date"": {""begin"": ""26.06."", ""beginYear"": ""2023"", ""end"": ""03.07."", ""endYear"": ""2023"", ""timestamp"": 1687730400000}}, {""data"": [5, 2, 7, 2, 4, 2], ""date"": {""begin"": ""19.06."", ""beginYear"": ""2023"", ""end"": ""26.06."", ""endYear"": ""2023"", ""timestamp"": 1687125600000}}, {""data"": [5, 2, 7, 2, 4, 2], ""date"": {""begin"": ""12.06."", ""beginYear"": ""2023"", ""end"": ""19.06."", ""endYear"": ""2023"", ""timestamp"": 1686520800000}}, {""data"": [5, 2, 7, 2, 4, 2], ""date"": {""begin"": ""05.06."", ""beginYear"": ""2023"", ""end"": ""12.06."", ""endYear"": ""2023"", ""timestamp"": 1685916000000}}, {""data"": [5, 2, 7, 2, 4, 2], ""date"": {""begin"": ""29.05."", ""beginYear"": ""2023"", ""end"": ""05.06."", ""endYear"": ""2023"", ""timestamp"": 1685311200000}}]

I want to map through the array, checking if the data key contains more than 4 numbers and if so, I want to extract the first 4 numbers (each data key is the same in all objects) and put it into a new object key ""data"" and the date key should look like the first item of the original array. The rest of the data key (in this example [4,2] should go into a new array that will be filled up with 0s until the data's length is 4. The date key of this should contain the date in 4 weeks starting from the first date.
I want to modify it so the result will be like this:
[{
    data: [ 5, 2, 7, 2 ],
    date: {
      begin: '29.05.',
      beginYear: '2023',
      end: '05.06.',
      endYear: '2023',
      timestamp: 1685311200000
    }
  },
{
    data: [4, 2, 0, 0 ],
    date: {
      begin: '26.06.',
      beginYear: '2023',
      end: '03.07.',
      endYear: '2023',
      timestamp: 1687730400000
    }
  }
]

The logic should also applicable if the data key contains more numbers
I tried it several times but somehow it doesn't as planned.
const fourStack = []

    const firstFourItems = result.slice(0, 4)
    const restItems = result.slice(4)

    const firstItem = firstFourItems[0]
    const newData = firstFourItems.map((item) => item.data.slice(0, 4))
    const newObj = {
      data: newData,
      date: {
        begin: firstItem.date.begin,
        beginYear: firstItem.date.beginYear,
        end: firstItem.date.end,
        endYear: firstItem.date.endYear,
        timestamp: firstItem.date.timestamp
      }
    }

    fourStack.push(newObj)

    restItems.forEach((item) => {
      fourStack.push({
        data: item.data.slice(0, 4),
        date: {
          begin: newObj.date.begin,
          beginYear: newObj.date.beginYear,
          end: newObj.date.end,
          endYear: newObj.date.endYear,
          timestamp: newObj.date.timestamp
        }
      })
    })

It gives me, not the result I want. Instead, it gives me this:
[{""data"": [[Array], [Array], [Array], [Array]], ""date"": {""begin"": ""03.07."", ""beginYear"": ""2023"", ""end"": ""10.07."", ""endYear"": ""2023"", ""timestamp"": 1688335200000}}, {""data"": [5, 2, 7, 2], ""date"": {""begin"": ""03.07."", ""beginYear"": ""2023"", ""end"": ""10.07."", ""endYear"": ""2023"", ""timestamp"": 1688335200000}}, {""data"": [5, 2, 7, 2], ""date"": {""begin"": ""03.07."", ""beginYear"": ""2023"", ""end"": ""10.07."", ""endYear"": ""2023"", ""timestamp"": 1688335200000}}]

",76388128,"

let arr = [{
  ""data"": [5, 2, 7, 2, 4, 2],
  ""date"": {
    ""begin"": ""03.07."",
    ""beginYear"": ""2023"",
    ""end"": ""10.07."",
    ""endYear"": ""2023"",
    ""timestamp"": 1688335200000
  }
}, {
  ""data"": [5, 2, 7, 2, 4, 2],
  ""date"": {
    ""begin"": ""26.06."",
    ""beginYear"": ""2023"",
    ""end"": ""03.07."",
    ""endYear"": ""2023"",
    ""timestamp"": 1687730400000
  }
}, {
  ""data"": [5, 2],
  ""date"": {
    ""begin"": ""19.06."",
    ""beginYear"": ""2023"",
    ""end"": ""26.06."",
    ""endYear"": ""2023"",
    ""timestamp"": 1687125600000
  }
}, {
  ""data"": [5, 2, 7, 2, 4, 2],
  ""date"": {
    ""begin"": ""12.06."",
    ""beginYear"": ""2023"",
    ""end"": ""19.06."",
    ""endYear"": ""2023"",
    ""timestamp"": 1686520800000
  }
}, {
  ""data"": [5, 2, 7, 2, 4, 2],
  ""date"": {
    ""begin"": ""05.06."",
    ""beginYear"": ""2023"",
    ""end"": ""12.06."",
    ""endYear"": ""2023"",
    ""timestamp"": 1685916000000
  }
}, {
  ""data"": [5],
  ""date"": {
    ""begin"": ""29.05."",
    ""beginYear"": ""2023"",
    ""end"": ""05.06."",
    ""endYear"": ""2023"",
    ""timestamp"": 1685311200000
  }
}];

arr.map((e) => {
  if (e.data.length > 4) {
    e.data = e.data.slice(0, 4);
  } else {
    let length = 4 - e.data.length;
    for (let i = 0; i < length; i++) {
      e.data.push(0);
    }
  }

})
console.log(arr);



",
76382277,How to create/update key/value pair in JFrog Artifactory with PHP and cURL?,"I am using PHP 8.2 to read our JFrog Artifactory API and this works fine. Now I am in the need to either create or update some of the properties on an artifact - so either create it if it does not exists, and update it if it does exists.
For example I can read all properties for a specific artifactory with this code:
<?PHP

// The full URL for the specific artifact and its ""properties""
$api = ""https://mysrv/api/storage/TestProduct/TestComponent/1.0.0/?properties"";

$ch = curl_init($api);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, false);
curl_setopt($ch, CURLOPT_HTTPHEADER, array(
    ""X-JFrog-Art-Api: "". $artifactoryKey,
));

// Execute the request to retrieve existing properties
$response = curl_exec($ch);
echo ""<pre>"";
var_dump($response);

?>

This will dump something like this:
string(123) ""{
    ""properties"" : {
        ""ComponentName"" : [ ""TestComponent"" ],
        ""ContactEmail"" : [ ""me@nowhere.com"" ],
        ""ContactName"" : [ ""John Doe"" ],
        ""VersionNumber"" : [ ""1.0.0"" ]
    },
    ""uri"" : ""https://mysrv/api/storage/TestProduct/TestComponent/1.0.0""
}""

Now I want to create a new key/value pair inside the properties.
For example if I want to create a new key named MyKey with a value of False then I would like to see this result:
string(123) ""{
    ""properties"" : {
        ""ComponentName"" : [ ""TestComponent"" ],
        ""ContactEmail"" : [ ""me@nowhere.com"" ],
        ""ContactName"" : [ ""John Doe"" ],
        ""VersionNumber"" : [ ""1.0.0"" ]
        ""MyKey"" : [ ""False"" ]
    },
    ""uri"" : ""https://mysrv/api/storage/TestProduct/TestComponent/1.0.0""
}""

I have of course tried various solutions like:
// Define the new key/value pair
$newProperty = array(
    'key' => ""MyKey"",
    'value' => ""False""
);

// Prepare the JSON payload
$payload = json_encode(array(""properties"" => array($newProperty)));

// Initialize cURL to update the properties
$ch = curl_init($api);
curl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'PUT');
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, false);
curl_setopt($ch, CURLOPT_HTTPHEADER, array(
    'Content-Type: application/json',
    'X-JFrog-Art-Api: ' . $artifactoryKey
));
curl_setopt($ch, CURLOPT_POSTFIELDS, $payload);

// Execute the request to update the properties
$response = curl_exec($ch);
echo ""<pre>"";
var_dump($response);

... and various tweaks to this but I cannot get it to work. Doing the above solution will give me this error:
string(98) ""{
    ""errors"" : [ {
        ""status"" : 400,
        ""message"" : ""Properties value cannot be empty.""
    } ]
}""

My account should have anough access to write/modify (I am not the admin but have been told so) but the error message does not look like it is related to permissions either, so any help on this would be appreciated - I expect this may be a simple fix as I have not much experience with handling Artifactory stuff :-)
",76389186,"Actually I did figure out this myself and it was a stupid/simple mistake. I easily solved this after finding the right help page - I should probably have invested more time in finding this in the first place :-)
Let me show the solution first:
<?PHP

// Full URL for specific artifact and the property to create/update
$api = ""https://mysrv/api/storage/TestProduct/TestComponent/1.0.0/?properties=MyKey=False"";

// Initialize cURL to create/update the properties
$ch = curl_init($api);
curl_setopt($ch, CURLOPT_CUSTOMREQUEST, ""PUT"");
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);
curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, false);
curl_setopt($ch, CURLOPT_HTTPHEADER, array(
    ""Content-Type: application/json"",
    ""X-JFrog-Art-Api: "". $artifactoryKey
));

// Execute the cURL request
$response = curl_exec($ch);

// Dump the result
echo ""<pre>"";
var_dump($response);

// Check if the request was successful
$httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);
var_dump($httpCode);

// Close the cURL session
curl_close($ch);

?>

This code will result in a HTTP Status 204 (No Content) which equals a success and it will create the new propery named MyKey with the value of False. The cURL PUT command will work for both creating it or updating the value.
Found the help for it here, https://jfrog.com/help/r/jfrog-rest-apis/set-item-properties
",
76388109,How to export a PyTorch model for HuggingFace?,"I have been training my custom Image classification model on the PyTorch transformers library to deploy to hugging face however, I cannot figure out how to export the model in the correct format for HuggingFace with its respective config.json file.
I'm new to PyTorch and AI so any help would be greatly appreciated
train.py
from tqdm import tqdm

best_accuracy = 0

# Train the model for a number of epochs
for epoch in range(20):
    # Create a progress bar for this epoch
    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{20}')
    
    # Loop over each batch of data
    for X_batch, y_batch in pbar:
        # Move the batch of data to the device
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)

        # Zero the gradients...
        # Define an optimizer...
        
        # Update the progress bar
        pbar.set_postfix({'Loss': loss.item()})
    
    # Evaluate the model on the validation set
    model.eval()
    correct = 0
    total = 0
    val_loss = 0
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            # Move the batch of data to the device
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            
            # Compute the model's predictions for this batch of data
            y_pred = model(X_batch)
            
            # Compute the loss
            loss = criterion(y_pred, y_batch)
            val_loss += loss.item()
            
            # Compute the number of correct predictions
            _, predicted = torch.max(y_pred.data, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()
    
    val_loss /= len(test_loader)
    accuracy = correct / total
    
    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')
    
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        torch.save(model.state_dict(), 'best_model.pth')
    
    model.train()

",76388143,"You are using HuggingFace Transformers, you can use:
model.save_pretrained(""FOLDER_NAME_HERE"")

After you saved the model, the folder will contain the pytorch_model.bin along with config JSONs.
",
76389395,AttributeError: module 'numpy' has no attribute 'long',"I am trying to find 9 raise to power 19 using numpy.
I am using numpy 1.24.3
This is the code I am trying:
import numpy as np
np.long(9**19)

This is the error I am getting:
AttributeError: module 'numpy' has no attribute 'long'

",76389396,"Sadly, numpy.long was deprecated in numpy 1.20 and it is removed in numpy 1.24
If you wan the result you have to try numpy.longlong
import numpy as np
np.longlong(9**19)

#output
1350851717672992089

https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
",
76390641,403 for the new pages in the cloudfront distribution,"I have a static web site in the s3 bucket behind the cloudfront distribution.
The bucket serves the static site, and the origin is bound to the web site endpoint.
I see a couple of pages if they were added before the distribution
However, when I upload some new html files, I receive 403 for them.
How should I fix this issue?
Bucket policy:
{
    ""Version"": ""2008-10-17"",
    ""Id"": ""PolicyForCloudFrontPrivateContent"",
    ""Statement"": [
        {
            ""Sid"": ""AllowCloudFrontServicePrincipal"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""Service"": ""cloudfront.amazonaws.com""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:PutObject""
            ],
            ""Resource"": ""arn:aws:s3:::a-test-upload/*"",
            ""Condition"": {
                ""StringEquals"": {
                    ""AWS:SourceArn"": ""arn:aws:cloudfront::xxx:distribution/yyy""
                }
            }
        }
    ]
}

",76390728,"Try to invalidate cloudfront cache. Go to cloud front distribution and click on invalidation enter ""/*"". Click on create invalidation.
If you are trying to access object publicly, then provide public access to s3 bucket objects.
",
76387987,Get the end_time in a query using the start time of the next sorted column,"Imagine I have the following SQL table:
| id | price | start_time |
---------------------------
|  1 |  0.1  | 2023-01-01 |
|  2 |  0.3  | 2023-03-01 |
|  3 |  0.2  | 2023-02-01 |

But then I want to query the prices in that table in a way that I can also get the end time as the start time of the next in time column. So, as an example, if I want to query all the entries in the table I would get something like this:
| id | price | start_time | end_time   |
----------------------------------------
|  1 |  0.1  | 2023-01-01 | 2023-02-01 | // end_time = start_time of the next entry
|  3 |  0.2  | 2023-02-01 | 2023-03-01 |
|  2 |  0.3  | 2023-03-01 |            |

But, I would also like to query that table with others filters, as an example, all entries whose prices are lower than 0.25, then I expect:
| id | price | start_time | end_time   |
----------------------------------------
|  1 |  0.1  | 2023-01-01 | 2023-02-01 |
|  3 |  0.2  | 2023-02-01 | 2023-03-01 | end_time = start_time of entry with id 2

So even that the entry with id 2 is filtered out, its start_time is still used as end_time of one of the entries.
Is this possible to achieve with one single query? I am bit lost on how to solve this approach without doing multiple queries.
",76388168,"This can be done using the window function lead() to get the which follows the current row :
select *, lead(start_time) over (order by start_time) as end_time
from mytable

With where clause it can be :
select * from (
  select *, lead(start_time) over (order by start_time) as end_time
  from mytable
) as s
where price < 0.25

Demo here
",
76388994,Next.js 13.4 and NextAuth Type Error: 'AuthOptions' is not assignable to type 'never',"I'm currently working on a Next.js 13.4 project and trying to set up NextAuth using the app/ router. However, I'm encountering a type error that I can't seem to resolve.
Here's my route.ts file:
import NextAuth, { AuthOptions } from ""next-auth"";
import DiscordProvider from ""next-auth/providers/discord"";

export const authOptions: AuthOptions = {
  providers: [
    DiscordProvider({
      clientId: process.env.CLIENT_ID as string,
      clientSecret: process.env.CLIENT_SECRET as string,
    }),
  ],
  session: {
    strategy: ""jwt"",
  },
  secret: process.env.NEXTAUTH_SECRET,
}

const handler = NextAuth(authOptions);

export { handler as GET, handler as POST }

And here's the error message when running 'npm run build':
- info Linting and checking validity of types ...Failed to compile.

.next/types/app/api/auth/[...nextauth]/route.ts:8:13
Type error: Type 'OmitWithTag<typeof import(""C:/Users/Luk/Documents/Workspace/zerotwo-dash/src/app/api/auth/[...nextauth]/route""), ""GET"" | ""POST"" | ""HEAD"" | ""OPTIONS"" | ""PUT"" | ""DELETE"" | ""PATCH"" | ""config"" | ... 6 more ... | ""runtime"", """">' does not satisfy the constraint '{ [x: string]: never; }'.
  Property 'authOptions' is incompatible with index signature.
    Type 'AuthOptions' is not assignable to type 'never'.

   6 |
   7 | // Check that the entry is a valid entry
>  8 | checkFields<Diff<{
     |             ^
   9 |   GET?: Function
  10 |   HEAD?: Function
  11 |   OPTIONS?: Function

I really have no idea whats happening here... when looking up the 'AuthOptions' on t he github page from nextauth i see nothing wrong with my code..
I would appreciate any insights or suggestions on how to resolve this issue. Thanks in advance!
",76389403,"Okay i solved it myself. For anyone having the same issue, i created a new file @/utils/authOptions.ts:
import { NextAuthOptions } from ""next-auth"";
import DiscordProvider from ""next-auth/providers/discord"";

export const authOptions: NextAuthOptions = {
    providers: [
        DiscordProvider({
            clientId: process.env.CLIENT_ID as string,
            clientSecret: process.env.CLIENT_SECRET as string,
        }),
    ],
    session: {
        strategy: ""jwt"",
    },
    secret: process.env.NEXTAUTH_SECRET,
}

and used it in @/api/auth/[...nextauth]/route.ts:
import { authOptions } from ""@/utils/authOptions"";
import NextAuth from ""next-auth/next"";

const handler = NextAuth(authOptions);

export { handler as GET, handler as POST };

I changed the imports a little (import NextAuth from ""next-auth/next"")
I dont know why this works tbh, but it does... even changing the imports around in my route.ts as it was before wont fix it. Only if its seperated like that...
",
76390749,is defined but never used no-unused-var,"I'm New to React, and I'm trying to learn about React Component right now. but when I create nameList.js, add data in there, and export to app.js, it does not show anything in the browser. I read some answers in Stackoverflow and tried it. but not show anything.
App.js
import './App.css';
import nameList from './Components/nameList';

function App() {
  return (
    <div className=""App"">
      <div className=""App"" >
            <nameList/>
        </div>
    </div>
  );
}

export default App;


nameList.js in Component Folder

import React from 'react';

function nameList() {
    return (
    <div>
       <h1>Name List</h1>
        <ul>
            <li>Stu1</li>
            <li>Stu2</li>
            <li>Stu3</li>
        </ul>
    </div>
  )
}

export default nameList;

I need to get data in nameList.js into App.js
",76390796,"Ensure that nameList starts with a capital letter so that React knows it's a component and not an HTML element.
import React from 'react';

function NameList() {
    return (
    <div>
       <h1>Name List</h1>
        <ul>
            <li>Stu1</li>
            <li>Stu2</li>
            <li>Stu3</li>
        </ul>
    </div>
  )
}

export default NameList;

App.js
import './App.css';
import NameList from './Components/nameList';

function App() {
  return (
    <div className=""App"">
      <div className=""App"" >
            <NameList/>
        </div>
    </div>
  );
}

export default App;

https://react.dev/learn/your-first-component#what-the-browser-sees
",
76387775,useState() not updated when state is used in a loop,"This code is where I modified the data, wherein I update the ""checked"" to whether ""true"" or ""false"". and this code works because when I console.log() it it updates the state.
    setRolesData((prevState) => {
        const newState = Array.from(prevState);
        newState[targetCb][""checked""] = e.target.checked;
        return newState;
    });
    console.log(rolesData);

Image of the result:

This code is where I loop thru the data that is updated, but the problem is it doesnt return the same data as the log data if tried for the second time.
    // get all checked roles
    const roleArr: any = [];
    rolesData.map((item, index) => {

        console.log(item.checked + "" ^ "" + item.name);

    });

Image of the Result:

",76388177,"In React, components have a lifecycle that is based around the idea of preserving state. When you update a value in a component we effectively trigger a re-render of the component. So if you have a variable at the top of your component with a certain value and try to update this value inside a function, you'll find that this value gets reset. React is replacing the component including the JavaScript variables inside it.
In order to preserve any variables we use useState to persist the state of the component between re-renders. However, in a function, using useState does not save the value to state immediately. useState is an instruction that is sent to React that tells it that when the component re-renders, we need to save that value. If you try to access the value in state before your re-render has begun you will be using the ""previous"" value instead. In your case, your component will not re-render until the calling function has been completed.
So how do you listen for changes?
UseEffect is a special hook that takes an array of dependencies as ""listeners"". When we use useEffect and give it a value in state as a dependency, it's telling it to run the code inside the useEffect if there is a mutation to that value.
useEffect(() => {

    rolesData.map((item, index) => {
      console.log(item.checked + "" ^ "" + item.name);
    });

}, [rolesData]);

This code will run every time myState is changed, including on component initialisation. So we can use useEffect to process logic that requires the newly updated stateful values.
",
76389309,How to capture words with letters separated by a consistent symbol in Python regex?,"I am trying to write a Python regex pattern that will allow me to capture words in a given text that have letters separated by the same symbol or space.
For example, in the text ""This is s u p e r and s.u.p.e.r and s👌u👌p👌e👌r and s!u.p!e.r"", my goal is to extract the words ""s u p e r"", ""s.u.p.e.r"", and s👌u👌p👌e👌r. However, I want to exclude ""s!u.p!e.r"" because it does not have the same consistent separating symbol within the word.
I'm currently using the following:
x=""This is s u p e r and s.u.p.e.r and s👌u👌p👌e👌r and s!u.p!e.r""


pattern = r""(?:\b\w[^\w\d]){2,}""

re.findall(pattern, x)


['s u p e r ', 's.u.p.e.r ', 's👌u👌p👌e👌r ', 's!u.p!e.']

I'm just curious if it's possible to exclude the cases that do not have the same symbol.
",76389422,"You may consider using
pattern = r""(?<!\S)\w(?=(\W))(?:\1\w)+(?!\S)""
results = [m.group() for m in re.finditer(pattern, x)]

See the Python demo and the regex demo.
import re
x=""This is s u p e r and s.u.p.e.r and s👌u👌p👌e👌r and s!u.p!e.r""
pattern = r""(?<!\S)\w(?=(\W))(?:\1\w)+(?!\S)""
print([m.group() for m in re.finditer(pattern, x)])
# => ['s u p e r', 's.u.p.e.r', 's👌u👌p👌e👌r']

Pattern details

(?<!\S) - left-hand whitespace boundary
\w - a word char
(?=(\W)) - a positive lookahead that requires the next char to e a non-word char capturing it into Group 1 (\1)
(?:\1\w)+ - one or more repetitions of the same char as captured in Group 1 and then a single word char
(?!\S) - right-hand whitespace boundary

",
76390683,How to transform Microsoft Graph API query with expand=extensions to SDK code,"I am transforming from Graph API to Graph SDK. How can I transform this API call?
https://graph.microsoft.com/v1.0/users/XXX/calendarView?startDateTime=YYY&endDateTime=ZZZ&$expand=extensions($filter=id eq 'NAME')

Expand part is missing. How should I add Expand = ??? or do it somehow with Filter or Select?
var ret = await graphClient.Users[XXX].CalendarView.GetAsync((requestConfiguration) =>
            {
                requestConfiguration.QueryParameters.StartDateTime = YYY;
                requestConfiguration.QueryParameters.EndDateTime = ZZZ;
            });

Thank you for help.
",76390801,"Thank you @user2250152 . I made a typo. Solution:
requestConfiguration.QueryParameters.Expand = new string[] { ""extensions($filter=id+eq+'NAME')"" };
",
76387285,Unable to add horizontal ListView.builder while using overlapping_panels 0.0.3,"
I am new to Flutter development
The horizontal list stops scrolling but is able To Scroll while using Axis.vertical.
What is expected is Once All the content of List Scrolls then only go to the next Slider.
Problem = only displays the list of items that we can visible on a screen unable to scroll horizontally
lib-link https://pub.dev/packages/overlapping_panels
Code
body: OverlappingPanels(
      right: Builder(
          builder: (context)  {
            return Text(""right"");
          }
      ),
      main: Builder(
        builder: (context) {
          var items = [""item1"",""Item2"", ""Item3"", ""Item4"", ""Item5"", ""Item6"", ""Item7""];
          return Container(
              width: double.infinity,
              height: 200,
              color: Colors.blue,
              child: Scrollbar(
                  child: ListView.builder(
                      scrollDirection: Axis.horizontal,
                      itemCount: items.length,
                      itemBuilder: (BuildContext context , int index) {
                        return Container(
                          width: 150,
                          margin: const EdgeInsets.all(8),
                          child: Center(
                            child: Text(
                              items[index],
                              style: const TextStyle(
                                  color: Colors.black,
                                  fontSize: 18
                              ),
                            ),
                          ),
                        );
                      })
              )
          );

        },
      ),
      onSideChange: (side) {

        setState(() {
          if (side == RevealSide.main) {
            // hide something
          } else if (side == RevealSide.left) {
            // show something
          }
        });
      },
    )

",76388205,"I Looked at your code. And also looked at the Library of OverlappingPanels.
The thing is, if you wrap you page with Overlapping Panels, It wrap you whole Screen with a Gesture Detector and it listens to a gesture to swipe from right to left.
If you are new, I would try something else. Otherwise you can copy their library and make it to your own class 'my_overlapoing_panels.dart like:
library overlapping_panels;

import 'package:flutter/material.dart';
import 'dart:core';

const double bleedWidth = 20;

/// Display sections
enum RevealSide { left, right, main }

/// Widget to display three view panels with the [MyOverlappingPanels.main] being
/// in the center, [MyOverlappingPanels.left] and [MyOverlappingPanels.right] also
/// revealing from their respective sides. Just like you will see in the
/// Discord mobile app's navigation.
class MyOverlappingPanels extends StatefulWidget {
  /// The left panel
  final Widget? left;

  /// The main panel
  final Widget main;

  /// The right panel
  final Widget? right;

  /// The offset to use to keep the main panel visible when the left or right
  /// panel is revealed.
  final double restWidth;

  final bool allowSidePanel;

  /// A callback to notify when a panel reveal has completed.
  final ValueChanged<RevealSide>? onSideChange;

  const MyOverlappingPanels({
    this.left,
    required this.main,
    this.right,
    this.restWidth = 40,
    this.onSideChange,
    this.allowSidePanel = true,
    Key? key,
  }) : super(key: key);

  static MyOverlappingPanelsState? of(BuildContext context) {
    return context.findAncestorStateOfType<MyOverlappingPanelsState>();
  }

  @override
  State<StatefulWidget> createState() {
    return MyOverlappingPanelsState();
  }
}

class MyOverlappingPanelsState extends State<MyOverlappingPanels> with TickerProviderStateMixin {
  AnimationController? controller;
  double translate = 0;

  double _calculateGoal(double width, int multiplier) {
    return (multiplier * width) + (-multiplier * widget.restWidth);
  }

  void _onApplyTranslation() {
    final mediaWidth = MediaQuery.of(context).size.width;

    final animationController = AnimationController(vsync: this, duration: const Duration(milliseconds: 200));

    animationController.addStatusListener((status) {
      if (status == AnimationStatus.completed) {
        if (widget.onSideChange != null) {
          widget.onSideChange!(translate == 0 ? RevealSide.main : (translate > 0 ? RevealSide.left : RevealSide.right));
        }
        animationController.dispose();
      }
    });

    if (translate.abs() >= mediaWidth / 2) {
      final multiplier = (translate > 0 ? 1 : -1);
      final goal = _calculateGoal(mediaWidth, multiplier);
      final Tween<double> tween = Tween(begin: translate, end: goal);

      final animation = tween.animate(animationController);

      animation.addListener(() {
        setState(() {
          translate = animation.value;
        });
      });
    } else {
      final animation = Tween<double>(begin: translate, end: 0).animate(animationController);

      animation.addListener(() {
        setState(() {
          translate = animation.value;
        });
      });
    }

    animationController.forward();
  }

  void reveal(RevealSide direction) {
    // can only reveal when showing main
    if (translate != 0) {
      return;
    }

    final mediaWidth = MediaQuery.of(context).size.width;

    final multiplier = (direction == RevealSide.left ? 1 : -1);
    final goal = _calculateGoal(mediaWidth, multiplier);

    final animationController = AnimationController(vsync: this, duration: const Duration(milliseconds: 200));

    animationController.addStatusListener((status) {
      if (status == AnimationStatus.completed) {
        _onApplyTranslation();
        animationController.dispose();
      }
    });

    final animation = Tween<double>(begin: translate, end: goal).animate(animationController);

    animation.addListener(() {
      setState(() {
        translate = animation.value;
      });
    });

    animationController.forward();
  }

  void onTranslate(double delta) {
    setState(() {
      final translate = this.translate + delta;
      if (translate < 0 && widget.right != null || translate > 0 && widget.left != null) {
        this.translate = translate;
      }
    });
  }

  @override
  Widget build(BuildContext context) {
    return Stack(children: [
      Offstage(
        offstage: translate < 0,
        child: widget.left,
      ),
      Offstage(
        offstage: translate > 0,
        child: widget.right,
      ),
      Transform.translate(
        offset: Offset(translate, 0),
        child: widget.main,
      ),
      widget.allowSidePanel
          ? GestureDetector(
              behavior: HitTestBehavior.translucent,
              onHorizontalDragUpdate: (details) {
                onTranslate(details.delta.dx);
              },
              onHorizontalDragEnd: (details) {
                _onApplyTranslation();
              },
            )
          : SizedBox(),
    ]);
  }
}


Now you can use also the variable  'allowSidePanel' in your code. And If you update your code to:
class TestScreen extends StatefulWidget {
  const TestScreen({super.key});

  @override
  State<TestScreen> createState() => _TestScreenState();
}

class _TestScreenState extends State<TestScreen> {
  ScrollController controller = ScrollController();

  bool allowScroll = false;

  @override
  void initState() {
    super.initState();

    // Setup the listener.
    controller.addListener(() {
      if (controller.position.atEdge) {
        bool atBegin = controller.position.pixels == 0;
        if (atBegin) {
          /// here you can later allow left panel later
        } else {
          /// here allow sidepannel
          setState(() {
            allowScroll = true;
          });
        }
      }
    });
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: MyOverlappingPanels(
        allowSidePanel: allowScroll,
        right: Builder(builder: (context) {
          return Text(""right"");
        }),
        main: Builder(
          builder: (context) {
            var items = [""item1"", ""Item2"", ""Item3"", ""Item4"", ""Item5"", ""Item6"", ""Item7""];
            return Container(
                width: double.infinity,
                height: 200,
                color: Colors.blue,
                child: ListView.builder(
                    controller: controller,
                    scrollDirection: Axis.horizontal,
                    itemCount: items.length,
                    itemBuilder: (BuildContext context, int index) {
                      return Container(
                        width: 150,
                        margin: const EdgeInsets.all(8),
                        child: Container(
                          padding: EdgeInsets.all(8),
                          color: Colors.red,
                          child: Center(
                            child: Text(
                              items[index],
                              style: const TextStyle(color: Colors.black, fontSize: 18),
                            ),
                          ),
                        ),
                      );
                    }));
          },
        ),
        onSideChange: (side) {
          setState(() {
            if (side == RevealSide.main) {
              /// here deaktivate ssidepannel again
              allowScroll = false;
            } else if (side == RevealSide.left) {
              // show something
            }
          });
        },
      ),
    );
  }
}

this will work.
",
76390731,Parsing multiple date string languages and formats,"I'm parsing a list of emails in a text file and I need to parse dates in the email headers. The dates are in a multitude of formats and languages:
sexta-feira, 26 de agosto de 2022 16:41
viernes, 26 de agosto de 2022 19:24
2022/08/26 13:30:56
26 de agosto de 2022 13:32:49 BRT

Mostly portuguese, spanish, italian and english.
What would be the best aproach? I have tried Babel but the date parsing is very basic. For now I only have access to the text files exported from Outlook not the smpt sources.
",76390810,"The dateparser package provides modules to parse localized dates in most string formats.
The following snippet successfully retrieves all dates in the given example:
import dateparser

text_dates = [
    ""sexta-feira, 26 de agosto de 2022 16:41"",
    ""viernes, 26 de agosto de 2022 19:24"",
    ""2022/08/26 13:30:56"",
    ""26 de agosto de 2022 13:32:49 BRT"",
]

datetimes = [dateparser.parse(line) for line in text_dates]

print(datetimes)

>>> [datetime.datetime(2022, 8, 26, 16, 41),
 datetime.datetime(2022, 8, 26, 19, 24),
 datetime.datetime(2022, 8, 26, 13, 30, 56),
 datetime.datetime(2022, 8, 26, 13, 32, 49, tzinfo=<StaticTzInfo 'BRT'>)]

",
76389259,"Window function, order by clause, between operator","Consider the MWE below
WITH samp AS (
    SELECT '2023-01-01' AS day, 1 AS spent UNION ALL
    SELECT '2023-01-02' AS day, 2 AS spent UNION ALL 
    SELECT '2023-01-03' AS day, 3 AS spent
) 
SELECT day, 
       spent 
     , ARRAY_AGG(spent) OVER(ORDER BY day BETWEEN '2023-01-02' AND '2023-01-03') ss
FROM samp 
ORDER BY day

I cannot figure out what the order by clause is doing here. I'd expect to restrict the entries to those of the selected dates, but dates outside it also have a contribution? E.g., outcome of the above




day
spent
ss




'2023-01-01'
1
[1]


'2023-01-02'
2
[1,2,3]


'2023-01-03'
3
[1,2,3]



",76389434,"The clause
day between '2023-01-02' and '2023-01-03'

is a boolean expression, and will only evaluate to two possible values, true or false (1 or 0).  Therefore, your window function array_agg(spent) will compute using an order where dates other than 2023-01-02 and 2023-01-03 will be ordered first, followed by these dates next.
Here is your updated output showing the ordering logic:
     day         spent         ss      order (day between ...)
'2023-01-01'       1           [1]       0
'2023-01-02'       2         [1,2,3]     1
'2023-01-03'       3         [1,2,3]     1

",
76389304,How do I print the user and umask for all running processes?,"The username is in column one and PID in column two of ps gaux, so I have:
ps gaux | awk '{print $2;}' | while read line ; do grep -i umask /proc/$line/status ; done

but is there a way to print the username as well?
",76389435,"I hope this helps
ps gaux | awk '{printf $1 "" "" ; system(""grep Umask /proc/""$2""/status | tr -dc [:digit:]""); printf ""\n""}'

Explanation:

get output from ps
print first column (username) and space
run the grep and remove everything except the actual umask, which is a number (awk does not print the command output, it gets just printed directly from the subshells stdout)
print a new line

",
76383190,Django - INTEGRITY ERROR on column that no longer exists,"I am getting this error:
IntegrityError at /register/
null value in column ""total_daily_mission_progress"" violates not-null constraint
DETAIL:  Failing row contains (363, 0, 374, free, 0, null, unranked, 0, , odifj@gmail.com, 0, f, [], 0, {}, {}, t, null, null, null, null, null, {}, null, null, No phone number set, This user has not set a description yet., /static/images/profilepictures/defaultavatar.png, {}, {}, {}, {}, {}, 0).

However, the column total_daily_mission_progress no longer exists in my UserDetail model. I deleted it a while ago and migrated. However, this issue comes up every time I try to create a new UserDetail model.
Why is this occuring? I don't have the total_daily_mission_progress anywhere in my code. And how can I fix it?
EDIT:
Here is my output after running python3 manage.py showmigrations --verbosity 2
 [X] 0001_initial (applied at 2022-12-29 19:39:10)
 [X] 0002_logentry_remove_auto_add (applied at 2022-12-29 19:39:10)
 [X] 0003_logentry_add_action_flag_choices (applied at 2022-12-29 19:39:10)
auth
 [X] 0001_initial (applied at 2022-12-29 19:39:09)
 [X] 0002_alter_permission_name_max_length (applied at 2022-12-29 19:39:11)
 [X] 0003_alter_user_email_max_length (applied at 2022-12-29 19:39:11)
 [X] 0004_alter_user_username_opts (applied at 2022-12-29 19:39:11)
 [X] 0005_alter_user_last_login_null (applied at 2022-12-29 19:39:12)
 [X] 0006_require_contenttypes_0002 (applied at 2022-12-29 19:39:12)
 [X] 0007_alter_validators_add_error_messages (applied at 2022-12-29 19:39:12)
 [X] 0008_alter_user_username_max_length (applied at 2022-12-29 19:39:12)
 [X] 0009_alter_user_last_name_max_length (applied at 2022-12-29 19:39:13)
 [X] 0010_alter_group_name_max_length (applied at 2022-12-29 19:39:13)
 [X] 0011_update_proxy_permissions (applied at 2022-12-29 19:39:13)
 [X] 0012_alter_user_first_name_max_length (applied at 2022-12-29 19:39:13)
codera_main
 [X] 0001_initial (applied at 2022-12-29 19:39:14)
 [X] 0002_achievement_reward_alter_achievement_rarity (applied at 2022-12-29 19:39:15)
 [X] 0003_achievement_num (applied at 2022-12-29 19:39:15)
 [X] 0004_alter_achievement_name_alter_achievement_num (applied at 2022-12-29 19:39:15)
 [X] 0005_alter_achievement_description_alter_achievement_name (applied at 2022-12-29 19:39:16)
 [X] 0006_userdetail (applied at 2022-12-29 19:39:17)
 [X] 0007_userdetail_skilllevel (applied at 2022-12-29 19:39:17)
 [X] 0008_userdetail_plan (applied at 2022-12-29 19:39:17)
 [X] 0009_userdetail_friends_alter_userdetail_skilllevel (applied at 2022-12-29 19:39:17)
 [X] 0010_alter_userdetail_friends (applied at 2022-12-29 19:39:18)
 [X] 0011_alter_userdetail_friends (applied at 2022-12-29 19:39:18)
 [X] 0012_alter_userdetail_friends (applied at 2022-12-29 19:39:18)
 [X] 0013_userstat (applied at 2022-12-29 19:39:19)
 [X] 0014_usercode (applied at 2022-12-29 19:39:19)
 [X] 0015_userreward (applied at 2022-12-29 19:39:20)
 [X] 0016_remove_userstat_ribbons_remove_userstat_shards_and_more (applied at 2022-12-29 19:39:20)
 [X] 0017_alter_userdetail_totallearningtime (applied at 2022-12-29 19:39:21)
 [X] 0018_alter_userdetail_totallearningtime (applied at 2022-12-29 19:39:21)
 [X] 0019_alter_userdetail_totallearningtime (applied at 2022-12-29 19:39:22)
 [X] 0020_alter_userdetail_totallearningtime (applied at 2022-12-29 19:39:22)
 [X] 0021_userstat_last_login (applied at 2022-12-29 19:39:23)
 [X] 0022_userstat_monthly_streaks (applied at 2022-12-29 19:39:23)
 [X] 0023_userdetail_user_spent (applied at 2022-12-29 19:39:23)
 [X] 0024_remove_userdetail_user_spent_and_more (applied at 2022-12-29 19:39:24)
 [X] 0025_alter_userdetail_totallearningtime (applied at 2022-12-29 19:39:24)
 [X] 0026_alter_usercode_levelscompleted (applied at 2022-12-29 19:39:25)
 [X] 0027_remove_achievement_rarity_remove_achievement_reward_and_more (applied at 2022-12-29 19:39:25)
 [X] 0028_achievement_achive_date (applied at 2022-12-29 19:39:25)
 [X] 0029_alter_achievement_num (applied at 2022-12-29 19:39:26)
 [X] 0030_rename_name_achievement_title (applied at 2022-12-29 19:39:26)
 [X] 0031_delete_userreward (applied at 2022-12-29 19:39:26)
 [X] 0032_userdetail_profilepicture (applied at 2022-12-29 19:39:27)
 [X] 0033_alter_userdetail_profilepicture (applied at 2022-12-29 19:39:27)
 [X] 0034_alter_userdetail_profilepicture (applied at 2022-12-29 19:39:27)
 [X] 0035_alter_userdetail_profilepicture (applied at 2022-12-29 19:39:27)
 [X] 0036_userstat_badges (applied at 2022-12-29 19:39:28)
 [X] 0037_remove_userstat_badges_userreward (applied at 2022-12-29 19:39:28)
 [X] 0038_remove_userdetail_profilepicture (applied at 2023-01-16 15:38:38)
 [X] 0039_remove_userdetail_friends (applied at 2023-01-16 15:40:34)
 [X] 0040_emailverified (applied at 2023-02-18 02:18:32)
 [X] 0041_remove_usercode_levelscompleted_and_more (applied at 2023-02-18 02:18:33)
 [X] 0042_userdetail_isverified (applied at 2023-02-18 02:18:33)
 [X] 0043_remove_usercode_user_remove_userreward_user_and_more (applied at 2023-02-18 02:40:26)
 [X] 0044_alter_userdetail_levelscompleted (applied at 2023-02-18 02:40:28)
 [X] 0045_userdetail_xp_alter_userdetail_league (applied at 2023-02-18 02:40:28)
 [X] 0046_avatar (applied at 2023-02-18 02:40:29)
 [X] 0047_alter_avatar_moves_unlocked (applied at 2023-02-18 02:40:29)
 [X] 0048_alter_avatar_moves_unlocked (applied at 2023-02-18 02:40:29)
 [X] 0049_userdetail_yesturday_diamonds (applied at 2023-02-18 02:40:29)
 [X] 0050_remove_userdetail_yesturday_diamonds_and_more (applied at 2023-02-18 02:40:30)
 [X] 0051_alter_userdetail_dimaond_progress (applied at 2023-02-18 02:40:30)
 [X] 0052_rename_dimaond_progress_userdetail_diamond_progress (applied at 2023-02-18 02:40:30)
 [X] 0053_userdetail_medal_progress (applied at 2023-02-18 02:40:30)
 [X] 0054_alter_userdetail_totallearningtime_badge (applied at 2023-02-18 02:40:31)
 [X] 0055_badge_image (applied at 2023-02-18 02:40:32)
 [X] 0056_badge_popup_image (applied at 2023-02-18 02:40:32)
 [X] 0057_alter_badge_badge_date_and_more (applied at 2023-02-18 02:40:32)
 [X] 0058_alter_badge_badge_date (applied at 2023-02-18 02:40:33)
 [X] 0059_alter_badge_badge_date (applied at 2023-02-18 02:40:33)
 [X] 0060_alter_achievement_user_alter_badge_user (applied at 2023-02-18 02:40:34)
 [X] 0061_userdetail_iscurrentlyactive (applied at 2023-02-18 02:40:34)
 [X] 0062_avatar_accuracy_avatar_speed_alter_avatar_character_and_more (applied at 2023-02-18 02:40:35)
 [X] 0063_avatar_rank (applied at 2023-02-18 02:40:35)
 [X] 0064_alter_avatar_moves_unlocked_alter_avatar_rank (applied at 2023-02-18 02:40:35)
 [X] 0065_alter_avatar_moves_unlocked (applied at 2023-02-18 02:40:36)
 [X] 0066_avatar_strenght (applied at 2023-02-18 02:40:36)
 [X] 0067_rename_strenght_avatar_strength (applied at 2023-02-18 02:40:37)
 [X] 0068_userdetail_plan_purchase_date (applied at 2023-02-18 02:40:37)
 [X] 0069_userdetail_checkout_id (applied at 2023-02-18 02:40:37)
 [X] 0070_alter_avatar_accuracy_alter_avatar_energy_and_more (applied at 2023-02-20 17:13:05)
 [X] 0071_guideemail (applied at 2023-02-22 23:52:59)
 [X] 0072_dailymission_userdetail_todays_daily_mission_and_more (applied at 2023-04-20 22:14:25)
 [X] 0074_alter_userdetail_totallearningtime (applied at 2023-04-24 13:54:24)
 [X] 0075_chest (applied at 2023-04-24 17:10:00)
 [X] 0076_userdetail_friends_list (applied at 2023-05-01 14:30:12)
 [X] 0077_friendrequest (applied at 2023-05-01 14:37:47)
 [X] 0078_friendrequest_date_sent (applied at 2023-05-01 22:54:00)
 [X] 0079_alter_friendrequest_date_sent (applied at 2023-05-01 23:00:54)
 [X] 0080_alter_friendrequest_date_sent (applied at 2023-05-02 01:07:36)
 [X] 0081_alter_userdetail_friends_list (applied at 2023-05-02 23:57:12)
 [X] 0082_remove_userdetail_friends_list (applied at 2023-05-02 23:59:09)
 [X] 0083_userdetail_friends_list (applied at 2023-05-02 23:59:27)
 [X] 0084_userdetail_first_name_userdetail_last_name_and_more (applied at 2023-05-09 20:39:01)
 [X] 0085_userdetail_description (applied at 2023-05-09 20:43:14)
 [X] 0086_userdetail_profile_picture (applied at 2023-05-09 21:41:22)
 [X] 0087_alter_userdetail_phone_number (applied at 2023-05-09 22:48:31)
 [X] 0088_alter_userdetail_profile_picture (applied at 2023-05-09 22:59:01)
 [X] 0089_remove_userdetail_profile_picture (applied at 2023-05-09 23:00:03)
 [X] 0090_userdetail_profile_picture (applied at 2023-05-09 23:00:13)
 [X] 0091_alter_userdetail_profile_picture (applied at 2023-05-09 23:08:33)
 [X] 0092_remove_userdetail_profile_picture (applied at 2023-05-09 23:09:17)
 [X] 0093_userdetail_profile_picture (applied at 2023-05-09 23:09:55)
 [X] 0094_remove_userdetail_profile_picture (applied at 2023-05-10 00:31:07)
 [X] 0095_userdetail_profile_picture (applied at 2023-05-10 00:31:20)
 [X] 0096_remove_userdetail_profile_picture (applied at 2023-05-10 00:34:04)
 [X] 0097_userdetail_profile_picture (applied at 2023-05-10 00:34:17)
 [X] 0098_remove_userdetail_profile_picture (applied at 2023-05-10 00:37:46)
 [X] 0099_userdetail_profile_picture (applied at 2023-05-10 00:41:59)
 [X] 0100_alter_userdetail_profile_picture (applied at 2023-05-10 00:46:03)
 [X] 0101_alter_userdetail_profile_picture (applied at 2023-05-10 00:48:49)
 [X] 0102_remove_userdetail_profile_picture (applied at 2023-05-10 00:49:22)
 [X] 0103_userdetail_profile_picture (applied at 2023-05-10 00:50:02)
 [X] 0104_alter_userdetail_profile_picture (applied at 2023-05-10 14:32:59)
 [X] 0105_alter_userdetail_league (applied at 2023-05-21 23:31:23)
 [X] 0106_userdetail_xp_progress_and_more (applied at 2023-05-28 23:28:41)
 [X] 0107_remove_userdetail_todays_daily_mission_completed_and_more (applied at 2023-05-29 16:14:45)
 [X] 0108_remove_userdetail_todays_daily_mission_and_more (applied at 2023-05-29 16:14:45)
 [X] 0109_userdetail_todaysdailymission_and_more (applied at 2023-06-01 14:39:33)
 [X] 0110_rename_todaysdailymissioncompelted_userdetail_todaysdailymissioncompleted (applied at 2023-06-01 14:39:33)
 [X] 0111_mainmission_alter_dailymission_num_and_more (applied at 2023-06-01 14:39:34)
contenttypes
 [X] 0001_initial (applied at 2022-12-29 19:39:05)
 [X] 0002_remove_content_type_name (applied at 2022-12-29 19:39:11)
sessions
 [X] 0001_initial (applied at 2022-12-29 19:39:29)

",76390828,"Ended up figuring it out, ended up using python3 manage.py dbshell and runnning the following command:
ALTER TABLE_NAME FROM USERMODAL
",
76387798,How to change color scheme for static methods in IntelliJ IDEA / Android Studio,"How to disable italics on static methods called inside AndroidStudio IDE?

I know, it's a personal preference question but is there a way to disable italics? and only keep the colour coding?
",76388214,"Settings/Preferences (on macOS) | Editor | Color Scheme | Java | Methods | Static method

Additional trick: to easily find the corresponding color scheme settings do the following:

Put the cursor at the needed element you need to change
Press Shift twice
Type Just to Colors and Fonts and press Enter. Then select the corresponding option

",
76389341,common/unified regex for a set of pattern,"I am trying to do some text processing and was interested to know if I can have a common/unified regex for a certain pattern. The pattern of interest is strings that ends with {string}_{i} where i is a number, on the second column of test.csv. Once the regex is matched, I wish to replace it with {string}[i].
For now the python script works as expected for the strings for which I explicitly mention the regex pattern. I want to have a more generic regex pattern that will match all the strings that have {string}_{i} instead of writing a regex for all the patterns (which is not scalable).
input test.csv
bom_a14 , COMP_NUM_0
bom_a17 , COMP_NUM_2
bom_a27 , COMP_NUM_11
bom_a35 , FUNC_1V8_OLED_OUT_7
bom_a38 , FUNC_1V8_OLED_OUT_9
bom_a39 , FUNC_1V8_OLED_OUT_10
bom_a46 , CAP_4
bom_a47 , CAP_3
bom_a48 , CAP_6

test.py
import csv
import re

# Match the values in the first column of the second file with the first file's data
with open('test.csv', 'r') as file2:
    reader = csv.reader(file2)
    for row in reader:
        row_1=row[1]
        # for matching COMP_NUM_{X}
        match_data = re.match(r'([A-Z]+)_([A-Z]+)_(\d+)',row_1.strip())
        # for matching FUNC_1V8_OLED_OUT_{X}
        match_data2 = re.match(r'([A-Z]+)_([A-Z0-9]+)_([A-Z]+)_([A-Z]+)_(\d+)',row_1.strip())
        # if match found, reformat the data
        if match_data:
            new_row_1 = match_data.group(1) +'_'+ match_data.group(2)+ '[' + match_data.group(3) + ']'
        elif match_data2:
            new_row_1 = match_data2.group(1) +'_'+ match_data2.group(2)+ '_'+ match_data2.group(3)+'_'+ match_data2.group(4)+'[' + match_data2.group(5) + ']'
        else:
            new_row_1 = row_1
        print new_row_1


output
COMP_NUM[0]
COMP_NUM[2]
COMP_NUM[11]
FUNC_1V8_OLED_OUT[7]
FUNC_1V8_OLED_OUT[9]
FUNC_1V8_OLED_OUT[10]
 CAP_4
 CAP_3
 CAP_6

expected output
COMP_NUM[0]
COMP_NUM[2]
COMP_NUM[11]
FUNC_1V8_OLED_OUT[7]
FUNC_1V8_OLED_OUT[9]
FUNC_1V8_OLED_OUT[10]
CAP[4]
CAP[3]
CAP[6]

",76389488,"I would use sub with a single generic pattern :
with open(""test.csv"", ""r"") as file2:
    for row in csv.reader(file2):

        s = re.sub(r""(.+)_(\d+)$"", r""\1[\2]"", row[-1].strip())

        print(s)


Regex : [demo]

Output :
COMP_NUM[0]
COMP_NUM[2]
COMP_NUM[11]
FUNC_1V8_OLED_OUT[7]
FUNC_1V8_OLED_OUT[9]
FUNC_1V8_OLED_OUT[10]
CAP[4]
CAP[3]
CAP[6]

",
76387936,How to use array variable in my sql statement in plpgsql,"total nube in PL/PGSQL. I would like to define an array of strings and then used that in my SELECT... WHERE In statement but can't seem to get it to work, help appreciated.
DO $$
DECLARE 
    testArray varchar[] := array['john','lisa'];    
    ids integer[];
BEGIN
    ids = array(select id from tableA where name in (testArray));
    -- this works
    ids = array(select id from tableA where name in ('john','lisa'));
END $$;

",76388264,"You can use any near of testarray.
DO $$
DECLARE 
    testArray varchar[] := array['john','lisa'];    
    ids integer[];
BEGIN
    ids = array(select id from tableA where name = any(testArray));
    -- this works
    ids = array(select id from tableA where name in ('john','lisa'));
END $$;

",
76390640,Adding 3D masked arrays results in TypeError: 'numpy.bool_' object is not iterable,"I have two 3D masked arrays (netCDF4 files output from climate model) that I want to add together. I followed this thread and got the following (simplified) code out of it:
import numpy as np
from netCDF4 import Dataset
from operator import and_
from numpy.ma.core import MaskedArray

with Dataset(dir + 'V10.nc') as file_V10:
    with Dataset(dir + 'U10.nc') as file_U10:
        raw_V10 = file_V10.variables['V10'][744 : 9503, :, :] ** 2
        raw_U10 = file_U10.variables['U10'][744 : 9503, :, :] ** 2                                                                                                                                                                                                               
        10m_raw_squared = MaskedArray(raw_V10[:].data + raw_U10[:].data, mask=list(map(and_,raw_V10.mask, raw_U10.mask)))

However, I get the error message:
Traceback (most recent call last):
  File ""code.py"", line 92, in <module>
    10m_raw_squared = MaskedArray(raw_V10[:].data + raw_U10[:].data, mask=list(map(and_,raw_V10.mask, raw_U10.mask)))    
TypeError: 'numpy.bool_' object is not iterable

If I try changing the mask from boolean to string (in order to make it iterable) by adding mask.astype('str'), I get this error message:
Traceback (most recent call last):
  File ""code.py"", line 92, in <module>
    10m_raw_squared = MaskedArray(raw_V10[:].data + raw_U10[:].data, mask=list(map(and_,raw_V10.mask.astype('str'),raw_U10.mask.astype('str'))))    
TypeError: unsupported operand type(s) for &: 'str' and 'str'

I have also tried to add the arrays together using a for-loop, but somehow couldn't get that to work without losing a dimension and the majority of the array elements of the data.
How can I add my two datasets together?
Edit: I called for the class of the dataset and got the following output:
<class 'numpy.ma.core.MaskedArray'>

",76390875,"You can use np.logical_and to create the mask.
with Dataset(dir + 'V10.nc') as file_V10:
    with Dataset(dir + 'U10.nc') as file_U10:
        raw_V10 = file_V10.variables['V10'][744 : 9503, :, :] ** 2
        raw_U10 = file_U10.variables['U10'][744 : 9503, :, :] ** 2
        mask = np.logical_and(raw_V10.mask, raw_U10.mask)
        10m_raw_squared = MaskedArray(raw_V10[:].data + raw_U10[:].data, mask=mask)

",
76389299,Select the data and generate the SELECT for each day from date to date?,"I have this SELECT statement in SQL Server:
select testname as 'Test', 
       tests_morning , 
       tests_evening ,
       Date
from  labtests_hajj 
left join departments_statistics on departments_statistics.test_id = labtests_hajj.testid 
inner join departments on labtests_hajj.dept_id = departments.dept_id

The output now like this:
Test      tests_morning         tests_evening Date 
CBC       null                  null          null 
CALCIUM   null                  null          null   
SODIUMN   null                  null          null   

How can I get the output as script and add the date also depends on date range for example I need the output for 3 days to be like this :
Test      tests_morning      tests_evening   Date 
CBC       null                  null     01/06/2023 
CALCIUM   null                  null     01/06/2023  
SODIUMN   null                  null     01/06/2023

CBC       null                  null     02/06/2023 
CALCIUM   null                  null     02/06/2023  
SODIUMN   null                  null     02/06/2023     
                                           
CBC       null                  null     03/06/2023 
CALCIUM   null                  null     03/06/2023  
SODIUMN   null                  null     03/06/2023  

How can I do the select and put 2 dates and show the output like thie?
",76389493,"There are have two ways

use DimDate or calendar table
you create a date table yourself with CTE

With DimDate
declare @StartData date='2023-02-01'
declare @EndData date='2023-02-06'
;with _t as 
(
    select testname as 'Test', 
           tests_morning , 
           tests_evening ,
           Date
    from  labtests_hajj 
    left join departments_statistics 
                     on departments_statistics.test_id = labtests_hajj.testid 
    inner join departments on labtests_hajj.dept_id = departments.dept_id

)
select 
             a.Test
            ,a.tests_morning
            ,a.tests_evening
            ,s.Date_ as  Date
from _t 
cross apply (select * from DimDate where date_ between  @StartData and @EndData )s

If your date range is consecutive, you can create the desired data by setting the start and end dates with CTE.
declare @StartData date='2023-02-01'
declare @EndData date='2023-02-06'
;WITH List
as
(
    SELECT @StartData as Date_
    UNION ALL
    SELECT DATEADD(day, 1, Date_) as 'MonthStart'
    FROM List
    where Date_<= @EndData
),
_t as 
(
    select testname as 'Test', 
           tests_morning , 
           tests_evening ,
           Date
    from  labtests_hajj 
    left join departments_statistics on departments_statistics.test_id = labtests_hajj.testid 
    inner join departments on labtests_hajj.dept_id = departments.dept_id
)
select 
              a.Test
            ,a.tests_morning
            ,a.tests_evening
            ,s.Date_ as  Date
from _t  a
cross apply (select * from List)s


",
76389280,sed doesn't replace a string when this is at the end of the line,"Why this works
sed -n '242p' /usr/local/lib/python3.6/site-packages/keras/models.py

        model_config = json.loads(model_config.decode('utf-8'))


sed -i ""242s/.decode('utf-8')//"" /usr/local/lib/python3.6/site-packages/keras/models.py

sed -n '242p' /usr/local/lib/python3.6/site-packages/keras/models.py
        model_config = json.loads(model_config)


and this doesn´t
sed -n '3328p' /usr/local/lib/python3.6/site-packages/keras/engine/topology.py
        original_keras_version = f.attrs['keras_version'].decode('utf8')

sed -i ""3328s/.decode('utf-8')//""  /usr/local/lib/python3.6/site-packages/keras/engine/topology.py

sed -n '3328p' /usr/local/lib/python3.6/site-packages/keras/engine/topology.py
        original_keras_version = f.attrs['keras_version'].decode('utf8')

Why I cannot delete the second .decode('utf8'). It is because it is at the end of the string.
I could another approach but I would like to be consistent with the code. I don't get any errors so I don't know what to do although I have been working around and looking for the answer on Internet.
",76389522,"There's an important one character difference:
utf8
utf-8

",
76388209,Cannot Mock but can Mock?,"I am trying to build a mock factory, like this:
public Mock<T> CreateMock<T>(SomeParams someParams) where T : IMyInterface
{
    Mock<T> result = new Mock<T>();
    ...
}

However, I am getting the compiler error CS0452: The type 'T' must be a reference type in order to use it as parameter 'T' in the generic type or method 'Mock'.
Yet, when I tried this, it worked just fine:
public Mock<IMyInterface> CreateMock(SomeParams someParams)
{
    Mock<IMyInterface> result = new Mock<IMyInterface>();
    ...
}

I don't understand, why I am getting the compiler error, when the code is functionally the same? Is there any simple way to workaround this? I would like to avoid the second approach, as it would require considerable changes in our testing infrastructure.
",76388265,"
I don't understand, why I am getting the compiler error, when the code is functionally the same?

Consider this case:
public struct Awkward : IMyInterface
{
}
...
Mock<Awkward> mock = MockFactory.CreateMock<Awkward>();

That satisfies the constraint you've put on CreateMock - but Awkward is a value type. Mock<T> requires T to be a reference type.
You need to constrain the T type parameter in your method to be a reference type that implements the interface:
public Mock<T> CreateMock<T>(SomeParams someParams) where T : class, IMyInterface

",
76390665,Does an error existe on the next code? it shows error on line 7,"import pandas as pd
from sklearn.cluster import KMeans

dataset = pd.read_csv(""smogon.csv"")
dataset.drop([""url"", 'texto'], axis=1, inplace=True)
km = KMeans(n_clusters=1, n_init='auto')
cluster = km.fit_predict(dataset[[""moves""]])
dataset[""Grupo""] = cluster
print(dataset)

it shows (cluster = km.fit_predict(dataset[[""moves""]])
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
also it shows ( could not convert string to float)
im doing a project with kmeans and pandas and when im trying to do the clustering with the csv file it shows an error but when i do it with another csv file, it doesnt show any problem running it.
",76390920,"Try to use LabelEncoder to convert your column moves as numeric:
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder

dataset = pd.read_csv(""smogon.csv"")
dataset.drop([""url"", 'texto'], axis=1, inplace=True)

# It does not make sense to have only one cluster...
# There is many documentation to find the best number of clusters (elbow method, ...)
km = KMeans(n_clusters=3, n_init='auto')
lbe = LabelEncoder()
dataset['moves_num'] = lbe.fit_transform(dataset['moves'])

cluster = km.fit_predict(dataset[['moves_num']])
dataset[""Grupo""] = cluster
print(dataset)

",
76380543,Detecting page refresh c#,"I have created a document upload site with asp.net core web app's, and I have encountered a small bug but I'm not sure how to fix it.
On my site, you first create a 'file' like so:

It then appears in a list like so:

And when you press upload attachment, it passes the id from the previous table to ensure it uploads to the correct file.

The code behind the upload page is as below, and the error is if you press upload before choosing a file, it does a page refresh to display an error and then the ID passed through before has been lost, so myInv ends up being null.
using FarmersPortal.Data;
using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Mvc.RazorPages;
using File = FarmersPortal.Data.File;

namespace FarmersPortal.Pages.Admin
{
    [Authorize(Roles =""Admin"")]
    public class UploadModel : PageModel
    {
        private readonly filedbContext _context;

        public UploadModel(filedbContext context)
        {

            _context = context;
        }
        public int? myID { get; set; }

        [BindProperty]
        public IFormFile file { get; set; }

        [BindProperty]
        public int? ID { get; set; }
        public void OnGet(int? id)
        {
            myID = id;
        }

        [BindProperty]
        public File File { get; set; }
        public async Task<IActionResult> OnPostAsync()
        {
            if (file != null)
            {
                if (file.Length > 0 && file.Length < 300000)
                {
                    var myInv = _context.Files.FirstOrDefault(x => x.Id == ID);
                    var date = DateTime.Today;

                    using (var target = new MemoryStream())
                    {
                        file.CopyTo(target);
                        myInv.UploadDate = date;
                        myInv.Attachment = target.ToArray();
                    }
                    if (myInv == null)
                    {
                        return NotFound();
                    }
                    else
                    {
                        File = myInv;
                    }
                    _context.Files.Update(myInv);
                    await _context.SaveChangesAsync();
                }

            }

            if (File.FileType == ""Purchase Order"")
            {
                return RedirectToPage(""./PurchaseOrders"");
            }
            else if (File.FileType == ""Remittance"")
            {
                return RedirectToPage(""./Remittance"");
            }
            else if (File.FileType == ""Haulage Self Bill"")
            {
                return RedirectToPage(""./HaulageSelfBill"");
            }
            else if (File.FileType == ""Growers Return"")
            {
                return RedirectToPage(""./GrowersReturn"");
            }
            return Page();
        }
    }
}

I am not sure how to work my way around this, any ideas?
@page
@model FarmersPortal.Pages.Admin.UploadModel
@{
}

<h1 style=""color:white"">Upload File</h1>

<style>
    body {
        background-image: url(""http://10.48.1.215/PORTAL/hero-range-1.jpg"");
        height: 100%;
        background-position: center;
        background-repeat: no-repeat;
        background-size: cover;
    }
</style>

<hr />
<div class=""row"">
    <div class=""col-md-4"">
        <form method=""post"" enctype=""multipart/form-data"">
            <div class=""form-group"">
                <div class=""col-md-10"">
                    <p style=""color:white"">Upload file</p>
                    <input type=""hidden"" asp-for=""@Model.ID"" value=""@Model.myID"" />
                    <input asp-for=""file"" class=""form-control"" accept="".pdf"" type=""file"" />
                    <span asp-validation-for=""file"" class=""text-white""></span>
                </div>
            </div>
            <div class=""form-group"">
                <div class=""col-md-10"">
                    <input class=""btn btn-success"" type=""submit"" value=""Upload"" />
                </div>
            </div>
        </form>
    </div>
</div>

<div>
    <a asp-page=""Index"">Back to List</a>
</div>

Above is the front end code.
",76388271,"You can custom the error message
[BindProperty]
[Required(ErrorMessage =""You must select a file before upload this form"")]
public IFormFile file { get; set; }

And you also need to add Jquery validation library to your view:
@section Scripts {
    @{
        await Html.RenderPartialAsync(""_ValidationScriptsPartial"");
    }
}

Then when user don't select any file and click the upload button, View will show error message and stop uploading the form.

",
76389514,Regex to detect deformed words,"I need to detect if a string contains a specific word like ""Hello"".

Hello -> yes
HhhheeeEEEElllLLLLoooOOOO -> Yes
hell0 -> yes
h   e l    l o / h. e .l .L . o -> Yes
h@@$$eee///LLL!!!ooo -> yes

My attempt:
let string = ""h@@$$eee///LLL!!!ooo"";

if (string.match(/\bhello\b/i)) {
      console.log(""Yes"");
} else {
      console.log(""No"");
}

",76389555,"You can use .* between the chars, [o0] for o and zero and the flag i for case-insensitive matches:


[
  ""h@@$$eee///LLL!!!ooo"",
  ""Hello"",
  ""HhhheeeEEEElllLLLLoooOOOO"",
  ""hell0"",
  ""h   e l    l o / h. e .l .L . o"",
  ""h@@$$eee///LLL!!!ooo""
].forEach(string => {
  if (string.match(/h.*e.*l.*l.*[o0]/i)) {
    console.log(""Yes"");
  } else {
    console.log(""No"");
  }
});



You can use a dynamic regular expression:


const input = ""Hello"";
const re = new RegExp(input.toLowerCase().split('').map(c => c === 'o' ? '[o0]' : c).join('.*'), 'i');

[
  ""h@@$$eee///LLL!!!ooo"",
  ""Hello"",
  ""HhhheeeEEEElllLLLLoooOOOO"",
  ""hell0"",
  ""h   e l    l o / h. e .l .L . o"",
  ""h@@$$eee///LLL!!!ooo""
].forEach(string => {
  if (string.match(re)) {
    console.log(""Yes"");
  } else {
    console.log(""No"");
  }
});



I used an additional toLowerCase to simplify the replacement of o => [o0].
",
76389515,Retrieve every value between an alphanumeric range in R using ifelse,"This is my df:
df <- data.frame(id=as.integer(c(1:6)),
                 code=as.character(c(""C410"", ""D486"", ""D485"", ""D501"", ""D600"", ""D899"")))
df
  id code
1  1 C410
2  2 D486
3  3 D485
4  4 D501
5  5 D600
6  6 D899

I want to attribute causes to each id depending on the range they fall into in column 2. For this, I use a ifelse statement:
    df$cause <- ifelse(df$code >= ""C00"" & df$code <= ""D48"", ""cause 1"", 
                       ifelse(df$code >= ""D50"" & df$code <= ""D89"", ""cause 2"", NA)) 

Issue: the algorithm does not capture values above the end of each range (until the maximum possible value)
df
  id code   cause
1  1 C410 cause 1
2  2 D486    <NA>
3  3 D485    <NA>
4  4 D501 cause 2
5  5 D600 cause 2
6  6 D899    <NA>

Desired output:
 df
      id code   cause
    1  1 C410 cause 1
    2  2 D486 cause 1
    3  3 D485 cause 1  
    4  4 D501 cause 2
    5  5 D600 cause 2
    6  6 D899 cause 2

",76389565,"Yo need to add a third digit:
df$cause <- ifelse(df$code >= ""C000"" & df$code <= ""D489"", ""cause 1"", 
                   ifelse(df$code >= ""D500"" & df$code <= ""D899"", ""cause 2"", NA))


> df
  id code   cause
1  1 C410 cause 1
2  2 D486 cause 1
3  3 D485 cause 1
4  4 D501 cause 2
5  5 D600 cause 2
6  6 D899 cause 2

",
76390791,"When using join in a string, I get too many characters in the string","I am trying to join an underscore into a string, but when running the code I get 4 times the character i used
str = potetoBox

for indx in range (len(str)):
    if str[indx].isupper():
        #split before indx and input an underscore
        str = ''.join((str[:indx],'_',str[indx:]))

print(str)

I have tried to change characters and change the order of the code. I expected to get only one underscore.
",76390956,"Another solution without re that doesn't build a list:
result = """"
for c in my_str:
    result += ""_"" * c.isupper() + c

",
76389352,How to copy from byte[] to new byte[] with some changed element item?,"I have byte[] like this, the length of byte[] is 16, and I want to change some value of an item and add this to a new byte[]. For example, the value of byte[7] is 13, I want to change this value with a new value. After that, add into a new byte, the value of byte[7] will be changed plus one more unit.


",76389617,"You should perform the operations the other way round. First copy then change the value in the copied data. This way the original will stay unchanged.
Efficient:
var original = new byte[]{1,2,3};
var result = new byte[original.Length];

Array.Copy(original, result, original.Length); //first make a copy
result[2] = 42; //then set your new value
//result is now [1, 2, 42]

Simple with LINQ:
//do not forget using System.Linq;
var original = new byte[]{1,2,3};

var result = original.ToArray(); //first copy
result[2] = 42; //then set your new value
//result is now [1, 2, 42]

For small data there will be no difference in performance. For large arrays the direct copy should perform a bit better. See this answer for further alternatives regarding the copy part.
",
76388142,I am trying to remove an item from my mongoDb object which is an object with an array of nested objects. But this code is not working,"I want to delete the object where the name is ""Sleep"".
The code I am using:
const listName = ""Holiday;
  const item = new Item({
    name: ""Sleep""
  });
  User.updateOne({username : req.user.username}, {$pull:{""lists.$[updateList].items"" : item}}, {
    ""arrayFilters"": [
      {""updateList.name"" : listName}
    ]
  }).exec().then(function(){
    console.log(""Deleted successfully"");
    res.redirect(""/list-""+listName);
  })

The mongodb object:
  ""_id"" : ObjectId(""64797ebc9e84ed9d8be3ea54""),
  ""username"" : ""aryan@gmail.com"",
  ""lists"" : [
      {
          ""name"" : ""Holiday"",
          ""items"" : [
              {
                  ""name"" : ""Welcome to your todo-list!"",
                  ""_id"" : ObjectId(""647988267f3ddfc2982f7d77"")
              },
              {
                  ""name"" : ""Click + to add another item."",
                  ""_id"" : ObjectId(""647988267f3ddfc2982f7d78"")
              },
              {
                  ""name"" : ""<-- Click this to delete an item."",
                  ""_id"" : ObjectId(""647988267f3ddfc2982f7d79"")
              },
              {
                  ""name"" : ""Sleep"",
                  ""_id"" : ObjectId(""64799279c3da415dc4ce7574"")
              },
              {
                  ""name"" : ""WakeUp"",
                  ""_id"" : ObjectId(""6479930e6d49e494aad1dffa"")
              }
          ],
          ""_id"" : ObjectId(""647988357f3ddfc2982f7d85"")
      }
]
}

It seems there is some problem with the updateOne and $pull: attributes but I can't figure out what.
",76388285,"Try with
const listName = 'Holiday';
User.updateOne(
  { username: req.user.username, 'lists.name': listName },
  { $pull: { 'lists.$[].items': { name: 'Sleep ' } } }
)
  .exec()
  .then(function () {
    console.log('Deleted successfully');
    res.redirect('/list-' + listName);
  });

",
76390964,questions is undefined in useEffect,"I'm trying to display all the questions from this data set however it's only working sometimes and then other times I receive 'questions is undefined'.
Why am I receiving this error?
const [questions, setQuestions] = useState<any>();
const [question, setQuestion] = useState<string>()
const [answers, setAnswers] = useState<[]>()

useEffect(() => {
    fetch(""/environment_questions"")
      .then((response) => response.json())
      .then((data) => setQuestions(data));
    }, []);
  
    useEffect(() => {
      if (questions.length > 0) {
        for (let i = 0, l = questions.length; i < l; i++) {
          setQuestion(questions[i].question)
          setAnswers(questions[i].answers)
        }
      }
    }, [questions])
return (
<p>{JSON.stringify(question)}</p>
      <p>{JSON.stringify(answers)}</p>
)
}

Data I'm trying to access:
[{""id"":1,""question"":""..."":[{""id"":1,""answer"":""...""},{""id"":2,""answer"":""...""}]},{""id"":2,""question"":""..."",""answers"":[{""id"":1,""answer"":""""},{""id"":2,""answer"":""""},{""id"":3,""answer"":""""}]} ...}]
",76391015,"import React, { useEffect, useState } from 'react'

const Component = () => {
  const [questions, setQuestions] = useState<any>()
  const [question, setQuestion] = useState<string>()
  const [answers, setAnswers] = useState<[]>()

  useEffect(() => {
    fetch('/...')
      .then(response => response.json())
      .then(data => setQuestions(data))
  }, [])

  useEffect(() => {
    if (questions.length > 0) {
      for (let i = 0, l = questions.length; i < l; i++) {
        setQuestion(questions[i].question)
        setAnswers(questions[i].answers)
      }
    }
  }, [questions])

  return (
    <>
      <p>{JSON.stringify(question)}</p>
      <p>{JSON.stringify(answers)}</p>
    </>
  )
}

export default Component

",
76389510,Add a Column with a string value based on other column values R,"I want to add one a column called ""Opt-Numbers"" to my data frame with the following values Opt-CMM and Opt-MM based on Numbers column. If the value in Numbers column are greater or equal to 4 then it should add Opt-CMM in the same row of that value or if it is less than 4 then add Opt-MM in the same row. I am also showing an example in below df.
Given DF.




S.NO
Numbers




P1
2


P2
5


P3
2


P4
2


P5
3


P6
4




Required DF




S.NO
Numbers
Opt-Numbers




P1
2
Opt-MM


P2
5
Opt-CMM


P3
2
Opt-MM


P4
2
Opt-MM


P5
3
Opt-MM


P6
4
Opt-CMM



",76389660,"We can do this with dplyr and case_when ()
library(dplyr)

#example data
data <- structure(list(S.NO = c(""P1"", ""P2"", ""P3"", ""P4"", ""P5"", ""P6""), Numbers = c(2, 5, 2, 2, 3, 4)), class = ""data.frame"", row.names = c(NA, -6L))

create new column with filters
data <- data %>%
          mutate(Opt-Numbers = case_when(
                    Numbers >= 4 ~ ""Opt-CMM"",
                    Numbers < 4 ~ ""Opt-MM""
          ))

data

",
76387972,How can I use clip-path to achieve a text effect on my SVG in the browser?,"I want to use clip-path to implement effect like: svg with text clip-path,  but open in browser it just display an empty
svg.  I wrap the path with <g> tag, then it failed.
<svg xmlns=""http://www.w3.org/2000/svg"" width=""32"" height=""32"" viewBox=""0 0 32 32"" fill=""none"" style=""
    width: 200px;
    height:  200px;
"">
    <defs>
        <clipPath id=""text-path"">
            <g clip-rule=""evenodd"" fill-rule=""evenodd"">
                <path d=""M0 0H14V2H2V13H22V10H24V13H32V27H24V32H0V0ZM2 30V27H22V30H2Z""/>
                <path d=""M16 0L24 8H16V0Z""/>
                <text x=""7.5"" y=""23"">PDF</text>
            </g>
        </clipPath>
    </defs>
    <g clip-path=""url(#text-path)"" fill=""red"">
        <path d=""M0 0H14V2H2V13H22V10H24V13H32V27H24V32H0V0ZM2 30V27H22V30H2Z""/>
        <path d=""M16 0L24 8H16V0Z""/>
    </g>

    <style>
        text {dominant-baseline: hanging;font-size: 8px;font-weight: bold;}
    </style>
</svg>

it should be displayed in browser.
",76388288,"In your example (above) you may remove the wrapping group around the 2 paths and the text and it would work.
 <clipPath id=""text-path"">
        <!-- <g clip-rule=""evenodd"" fill-rule=""evenodd""> -->
            <path d=""M0 0H14V2H2V13H22V10H24V13H32V27H24V32H0V0ZM2 30V27H22V30H2Z""/>
            <path d=""M16 0L24 8H16V0Z""/>
            <text x=""7.5"" y=""23"">PDF</text>
        <!-- </g> -->
 </clipPath>

However you won't see the text.
If what you need is to see the text as a hole in the shapes you will need a mask where the text is black and the 2 paths are white. In the case of the mask  you can use a group if you think you need it.


<svg xmlns=""http://www.w3.org/2000/svg"" width=""32"" height=""32"" viewBox=""0 0 32 32"" style=""
    width: 200px;
    height:  200px;
    background:silver"">
  <mask id=""m"">
  <g fill=""white"">
    <path d=""M0 0H14V2H2V13H22V10H24V13H32V27H24V32H0V0ZM2 30V27H22V30H2Z"" />
    <path d=""M16 0L24 8H16V0Z"" />
  </g>
    <text x=""7.5"" y=""23"" fill=""black"">PDF</text>
  </mask>

  <rect width=""32"" height=""32"" mask=""url(#m)"" fill=""red"" />

  <style>
    text {
      dominant-baseline: hanging;
      font-size: 8px;
      font-weight: bold;
    }
  </style>

</svg>



",
76390686,Vuejs. is it possible to pass a function returned from another function to event handler?,"<template>
  <v-list-item v-for=""(category, i) in categories"" :key=""i"">
    <v-item-group multiple @update:model-value=""selectedChanged(category)"">
       <v-item></v-item>
    </v-item-group>
  </v-list-item>
</template>

<script>
  function selectedChanged(category) {
    return function(items) {
      console.log(`select ${items} from ${category}`);
    }
  }
</script>

I hope that, in function selectedChanged, I can know which category was selected. But it doesn't work. Vue just called selectedChanged with parameter category.
The reason why I want to do this is that, if I define selectedChanged as follows:
function selectedChanged(items) {
  console.log(items);
}

I don't know which category was selected.
How to impelement function selectedChanged so that I can know which category was selected?
",76391032,"Like Estus said in the comments, it looks like what you want can probably be achieved by explicitly passing $event as one of your event handler arguments. (Here is a link to the relevant Vue 3 documentation on $event.) The code would look something like this:
<template>
  <v-list-item v-for=""(category, i) in categories"" :key=""i"">
    <v-item-group multiple @update:model-value=""selectedChanged(category, $event)"">
       <v-item></v-item>
    </v-item-group>
  </v-list-item>
</template>

<script>
  function selectedChanged(category, items) {
    console.log(`select ${items} from ${category}`);
  }
</script>

Of course, this solution assumes that the items you're looking for are emitted by the update:model-value call in (what I assume to be) Vuetify. You might need to do some destructuring or refactoring to get precisely what you want.
",
76389651,OR-TOOLS Job Shop Scheduling - stop when find first feasible solution,"I am trying to implement Job Shop Scheduling with CP-SAT, but I have more than 20000 tasks to schedule and finding optimal solution will take too much time. I'm using solver time limit, but sometimes it gives me feasible solution in assumed time and sometimes not. Could anyone show how to limit solver to find the first feasible solution?
https://developers.google.com/optimization/scheduling/job_shop
I know that I should use SolveWithSolutionCallback, but I don't know how.
",76389684,"set the parameter: stop_after_first_solution to true.
See the definition.
",
76387879,Is it possible to exclude certain fields using JSONATA?,"Using JSONATA, is it possible to exclude certain fields that are nested in a deep structure without using object construction? For example, with the following object
{
    ""collection"": [
        {
            ""id"": ""ABC"",            
            ""learningunit"": {
                ""metadata"": {
                    ""show"": true,
                    ""unitType"": {
                        ""code"": ""U"",                        
                        ""value"": ""Unit""
                    }                   
                }
            }
        },
        {
            ""id"": ""UYE"",            
            ""learningunit"": {
                ""metadata"": {
                    ""show"": false,
                    ""unitType"": {
                        ""code"": ""C"",                        
                        ""value"": ""COURSE""
                    }                   
                }
            }
        }
    ]
}

can we exclude the field ""show"" and ""value"" in order to get the following result.
{
    ""collection"": [
        {
            ""id"": ""ABC"",            
            ""learningunit"": {
                ""metadata"": {                   
                    ""unitType"": {
                        ""code"": ""U""                     
                    }                   
                }
            }
        },
        {
            ""id"": ""UYE"",            
            ""learningunit"": {
                ""metadata"": {                   
                    ""unitType"": {
                        ""code"": ""C""                     
                    }                   
                }
            }
        }
    ]
}

FYI, the following object construction expression does the job but it is cumbersome to write if the object is complex.
{""collection"":collection.
    {
    ""id"": id,
    ""learningunit"": learningunit.
        { 
            ""metadata"": metadata. 
            {
                ""unitType"": unitType.
                {
                    ""code"": code
                }
            }
        }
    }
} 

",76388302,"You can make use of the transform operator and remove all 'value' and 'show' fields from the nested structure:
$$ ~> | ** | {}, ['show', 'value'] |

See it on the live Stedi playground: https://stedi.link/Usc1tpg
Note that if you need to clear those on a specific path only, you can also do it more surgically:
$$
  ~> | *.learningunit.metadata | {}, ['show'] |
  ~> | *.learningunit.metadata.unitType | {}, ['value'] |

Playground: https://stedi.link/Bh8cUiM
",
76391013,How to count number of consecutive Trues in a numpy array along a given axis?,"I have for example the following np array
array([[ True, False, False],
       [False,  True,  True],
       [ True,  True,  True],
       [False,  True,  True],
       [False,  True,  True],
       [ True, False, False],
       [ True,  True, False],
       [False, False,  True]])

I want to count the number of consecutive Trues along the columns, so in the above example, the first column contains 3 blocks of consecutive Trues, the second column contains 2 blocks, and the third contains 2 blocks. The output should then be
array([3, 2, 2])

I know I can do loops for each columns, like in this answer for a one-dimensional array, but what is a numpy-way for doing this on a 2-d array?
",76391051,"Use boolean arithmetic to identify the True that are not followed by a True (using slicing and pad), then sum the True per column:
out = ((a != np.pad(a[1:], ((0,1), (0,0)), constant_values=False))
       & a).sum(axis=0)

Or:
out = (a & ~np.pad(a[1:], ((0,1), (0,0)), constant_values=False)).sum(axis=0)

Output:
array([3, 2, 2])

",
76390971,Does std::set work correctly with a type that compares as std::partial_ordering?,"I came across this when defaulting the three-way-comparison operator (spaceship operator).
Let's consider this small example:
#include <set>
#include <limits>

struct A {
    float x;
    auto operator<=>(A const& other) const = default; // will be std::partial_ordering
};

int main()
{
    std::set<A>{ A{.x = std::numeric_limits<float>::quiet_NaN()}, A{.x = 1.f} };
}

Here we have a struct A that has one member of type float and defaults the <=> operator. Due tot he float member, the return type of the <=> operator will be std::partial_ordering. Partial ordering allows the category std::partial_ordering::unordered for elements that cannot be put into a specific order relative to other elements. For float this would affect nan for example. Any comparison of a float to nan will yield false.
Now, containers like std::set order their elements for being able to do binary search. For this they basically do a < comparison. Wouldn't this be broken for any type defining a partial ordering? In the example above, I can create a set of A and it compiles just fine. This seems like a pitfall to me because I assume that as soon as an element is inserted into the set that yields std::partial_ordering::unordered, the ordering inside the set would basically be broken, and various operations on the set would just cease to work correctly. Is my assumption correct?
Now, I know that it is possible in C++ to actually compare floats using a strong ordering. For this the function std::strong_order exists. So to fix the code, I could manually implement the three-way-comparison operator like this:
struct A {
    float x;
    auto operator<=>(A const& other) const{
        return std::strong_order(x, other.x);
    }
};

For this exmaple this is easy. But for larger structs/classes we are basically back to having to manually write comparisons that check member by member. Is there any way to achieve this comparison behaviour while still being able to default the spaceship operator? (without writing a wrapper class for float that offers strong order comparison)
tl;dr: To my understanding, set::set requires a strict weak ordering to work correctly. Still, I can construct a set using an element type that only offers a partial_ordering. This seems prone to causing bugs.
The example code is here, in case you're interested: https://godbolt.org/z/q4a95czTr
",76391062,"
Wouldn't this be broken for any type defining a partial ordering?

No, not necessarily. I'm going to use float as the canonical example of a type with partial ordering, but the argument here applies to any partially ordered type.
std::set<float>, for instance, is not an inherently broken type. std::set<float>{1.f, 2.f, 3.f} will do exactly what you want, for instance. Indeed, it will do what you want for any floats you put into it... as long as they are not NaN.
There are really two approaches to this problem:

you could put it in the type system, requiring that <=> yields at least weak_ordering, thus rejecting std::set<float>
you could put it in the value system, and say it's undefined behavior if you have a partial ordering that compares as unordered. With <=>, if a <=> b is valid and yields a partial_ordering, you can assert that it's not unordered. That is, an operation is defined on a domain of values - not necessarily on every possible value in the type - and an algorithm is valid as long as the provided values are elements of the domain on which the operation is defined.

Rust does it the former way (which requires manually providing a comparison operation if you want to do something like sort a Vec<f32> - where that comparison probably just panics in the unordered case), and C++ has always done it the latter way - std::set<float> and sorting a std::vector<float> have always been valid in general, just with the precondition that NaN is not present in those containers (i.e. NaN is outside of the domain of the comparison).
I'm not sure that one approach is necessary better than the other. Rust's approach requires you to explicitly handle the unordered case, which is more tedious if you just don't have NaNs. In some sense, the C++ approach is more bug prone - since the unordered case won't be typically handled by simply aborting the program (although nothing stops you from writing an equivalently-buggy comparison in the Rust model).
",
76389516,PHP form not loading password match,"I'm trying to build a Sign In / Sign Up web server with PHP, which fills a mysql table.
The code is running kind of well, but when I tried to implement a control for the password to be the same, it seems like it skips the password match code and goes directly to the login form.
Here's the code:
<?php

require_once('config.php');

$email = $connessione->real_escape_string($_POST['email']);
$username = $connessione->real_escape_string($_POST['username']);
$password = $connessione->real_escape_string($_POST['password']);
$hashed_password = password_hash($password, PASSWORD_DEFAULT);


// Controlla se l'email esiste già nel database
$query = ""SELECT * FROM utenti WHERE email = '$email'"";
$result = $connessione->query($query);

if ($result->num_rows > 0) {
    echo ""L'email esiste già nel database. Si prega di utilizzare un'altra email."";
} else {
    // L'email non esiste nel database, procedi con la creazione dell'account


$passwordMatchError = '';

if(isset($_POST['password']) && isset($_POST['password1'])) {
    $password = $_POST['password'];
    $password1 = $_POST['password1'];

    if($password === $password1) {

        header(""Location: login.html"");
        exit();
      
    } else {
        $passwordMatchError = 'Le password non corrispondono.'; 
    }
}


    $sql = ""INSERT INTO utenti (email, username, password) VALUES ('$email', '$username', '$hashed_password')"";
    if ($connessione->query($sql) === true) {
        // Registrazione avvenuta con successo, reindirizza l'utente alla pagina desiderata
        header(""Location: login.html"");
        exit();
    } else {
        echo ""Errore durante la registrazione utente: "" . $connessione->error;
    }
}

$connessione->close();

?>


<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Document</title>
    <link rel=""stylesheet"" href=""styles.css"">
</head>

<body>
    <div>
        <form action=""register.php"" method=""POST"">
            <h2>Registrati</h2>
            <label for=""email"">Email</label>
            <input type=""text"" name=""email"" id=""email"" placeholder=""Inserisci la mail"" required>
           
            <label for=""username"">Username</label>
            <input type=""text"" name=""username"" id=""username"" placeholder=""Inserisci username"" required>
            
            <label for=""password"">Password</label>
            <input type=""password"" name=""password"" id=""password"" placeholder=""Inserisci la password"" required>
    
            <label for=""password1"">Conferma Password</label>
            <input type=""password"" name=""password1"" id=""password1"" placeholder=""Conferma password"" required>
    
            <input type=""submit"" value=""Invia"">
    
            <?php if (isset($error)) : ?>
                <p><?php echo $error; ?></p>
            <?php endif; ?>
    
            <p>Hai già un account? <a href=""login.html"">Accedi</a></p>
        </form>
    </div>
</body>
</html>

So far I tried to implement the register.php directly to the ""index.php"", but nothing worked. I was expecting the echo to show up when the user submits the form with two non-matching passwords, but when i click on submit it brings me directly to the login page, skipping the error. I think it brings the user directly to the login despite the password aren't matching or maybe it's skipping the pass control.
",76389696,"What you're supposed to do to keep it efficient is to check if the passwords match first or not, then to check the existence of the email address, and from that point do your logic, so by that your code should look something like this:
if (isset($_POST['password']) && isset($_POST['password1'])) {
    $password = $_POST['password'];
    $password1 = $_POST['password1'];

    if ($password === $password1) {
        $query = ""SELECT email ...."";
        $result = $connessione->query($query);

        if ($result->num_rows > 0) {
            echo ""L'email ...."";
        } else {
            $sql = ""INSERT ...."";
            if ($connessione->query($sql) === true) {
                ....
            }
        }
    }
}

other than that, important notes:

$password = $_POST['password']; overwrites your $password = $connessione->real_escape_string($_POST['password']);, name your variables wisely.
I highly recommend using PHP PDO to handle databases when you're working with databases, or MySQLi if you know what you're doing.
Revise your execution plan so you don't to run queries that might not be used because of a false condition.

",
76387495,resolve write problem when write to Cassandra replica set,"By design we can write to any node in the Cassandra replica set. For situation my replica set has 2 node. When I make a write operation to node A but the node is unavailable. Do I have catch exception then re-write to node B manually ?
On mongodb, their Driver have ""Retry-able Writes"" to auto write to another node if primary node is down. Does Cassandra have this feature ?
",76388317,"When you write to Cassandra you specify the consistency level you wish to write wish - ranging from ANY which provides no guarantees, up to ALL which requests that all replicas in all DCs acknowledge back to the co-ordinator.
This write is sent to a single node - based on your load balancing policy - that node acts as the co-ordinator for the whole operation, and will return a single response of success / exception- your application does not have to itself individually send the write to multiple nodes, it just sends to 1 node (any node can be used) who co-ordinates the write to the replicas.
In a normal scenario of using local_quorum for a write with a very normal replication factor of 3 then as long as the co-ordinator has 2 of the 3 nodes providing acknowledgement of the write, the application will not get any exception - even if the 3rd node fails to write the data.
There is a retry policy available on the driver - which can allow for a retry in the event of a timeout, you should ensure though that the operation is idempotent when using this. (for example, appending an item to a list, retrying could result in the item being on the list twice on one of the replicas).
With your particular replication factor being 2 - you are currently in a position where you are lack consistency guarantees, or resilience.

one / local_one - only guarantees one of the nodes got the write. (Both are likely to get it but there is no guarantee provided)
quorum / local_quorum - requires both nodes acknowledge, so you have no ability to handle a node failure.

This is because the quorum of 2 is 2 - if you used 3 nodes with an RF=3, then local_quorum requires 2 of the 3, which would allow a node to be down while providing a stronger guarantee on consistency.
",
76391056,What a vertical bar means in a PropertyGroup .csproj file,"I have a PropertyGroup like this:
<PropertyGroup Condition=""'$(Configuration)|$(Platform)'=='Debug|x86'"">
  <DefineConstants>$(DefineConstants)TRACE;X86</DefineConstants>
</PropertyGroup>

I want to know what the vertical bar (|) means when it is placed between parameters in the Condition attribute.
",76391077,"It doesn't mean anything - it's just a way of separating the configuration and platform.
Note that these aren't really parameters in a condition attribute - they're just property values which are replaced with the MSBuild property. | is unlikely to be part of a configuration or platform name, so it's a good choice as a separator.
You could have a condition of ""'$(Configuration)/$(Platform)'=='Debug/x86'"" just as easily, for example. | is just more conventional.
",
76389600,How do I move UIView to the back?,"I have a prototype cell with some labels and image views, all added thru Storyboard. I am trying to add a color background view behind these elements (sort of like a bubble view behind the text that is commonly used in messengers). Trying to add this bubble view through Storyboard became a nightmare because of constraints' conflicts. So I decided to add it programmatically right in the prototypeCell class:
let backgroundBubble = UIView()

override func awakeFromNib() {
    super.awakeFromNib()
    backgroundBubble.backgroundColor = .yellow
    backgroundBubble.translatesAutoresizingMaskIntoConstraints = false
    addSubview(backgroundBubble)
    let constraints = [
        backgroundBubble.topAnchor.constraint(equalTo: Label.topAnchor, constant: 0),
        backgroundBubble.leadingAnchor.constraint(equalTo: Label.leadingAnchor, constant: 0),
        backgroundBubble.bottomAnchor.constraint(equalTo: Label.bottomAnchor, constant: 0),
        backgroundBubble.trailingAnchor.constraint(equalTo: Label.trailingAnchor, constant: 0)
    ]
    NSLayoutConstraint.activate(constraints)
}

The problem is that now the bubble view is sitting in front of the content:

I tried to move it back with:
sendSubviewToBack(backgroundBubble)

but then the yellow bubbleView disappears altogether.
How do I move the bubbleView behind the content of the prototype cell?
",76389698,"The content of a cell should basically always be added to its contentView. In this case, you may use contentView.insertSubview(backgroundBubble, at: 0) to add it as the first view in the hierarchy stack
",
76388105,Efficient algorithm to calculate the most right non-zero digit of a number's factorial in Python,"Calculate the most right non-zero digit of n factorial efficiently
I want to calculate the right most digit of a given number's factorial and print it. What I've done so far is:
import math
n = int(input())
fact = math.factorial(n)
print(str(fact).rstrip('0')[-1])

but I still get time limits and I look for faster solutions.
It's worth noting that I must use python to solve this problem.
Also, I shall point out that n is from 1 to 65536, the time limit is 0.5 seconds and I have 256 megabytes of memory.
",76388318,"There is a neat recursive formula you can use: let D(n) be the last non-zero digit in n!

If n<10, use a lookup table
If the second last digit of n is odd, D(n) = 4 * D(n//5) * D(unit digit of n)
If the second last digit of n is even, D(n) = 6 * D(n//5) * D(Unit digit of n)

See this math stackexchange post for a proof.
Translating it into code:
def last_nonzero_factorial_digit(n):
    lookup = [1, 1, 2, 6, 4, 2, 2, 4, 2, 8]
    if n < 10:
        return lookup[n]

    if ((n // 10) % 10) % 2 == 0:
        return (6 * last_nonzero_factorial_digit(n // 5) * lookup[n % 10]) % 10
    else:
        return (4 * last_nonzero_factorial_digit(n // 5) * lookup[n % 10]) % 10

On my laptop, this version runs ~14,000 times faster on a 5-digit number.
",
76390824,Python Cerberus - Validating Schema with this Example,"I am using Cerberus to validate dataframes schema. Using this sample data and code below, the  if-else statement should ""data structure is valid"", however it returns that the ""data structure is not valid"". Any insight would be appreciated.
import pandas as pd
from cerberus import Validator

df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'city': ['New York', 'Paris', 'London']
})

data = df.to_dict()

schema = {
    'name': {'type': 'string'},
    'age': {'type': 'integer', 'min': 18},
    'city': {'type': 'string'}
}

validator = Validator(schema)
is_valid = validator.validate(data)

if is_valid:
    print(""Data structure is valid!"")
else:
    print(""Data structure is not valid."")
    print(validator.errors)

Which results:
>>> Data structure is not valid.
>>> {'age': ['must be of integer type'], 'city': ['must be of string type'], 'name': ['must be of string type']}

",76391078,"It is failing because df.to_dict() returns dictionary with values ad dictionaries
data = df.to_dict()
print(data)
>>> {'name': {0: 'Alice', 1: 'Bob', 2: 'Charlie'}, 'age': {0: 25, 1: 30, 2: 35}, 'city': {0: 'New York', 1: 'Paris', 2: 'London'}}

If you want your schema to validate this data you need to change it to:
schema = {
    ""name"": {""type"": ""dict"", ""valuesrules"": {""type"": ""string""}},
    ""age"": {""type"": ""dict"", ""valuesrules"": {""type"": ""integer"", ""min"": 18}},
    ""city"": {""type"": ""dict"", ""valuesrules"": {""type"": ""string""}},
}

",
76388981,WebScraping - Parsel: XPath in python,"I am trying to scrape medium.com
I am using the following code:
from parsel import Selector
def get_trending(html):
    selector = Selector(text=html)

    text = selector.xpath(""//*[@id='root']/div/div[4]/div[1]/div/div/div/div[2]/div/div[1]/div/div/div[2]/div[2]/a/div/h2"")
    # h2 = text.xpath(""//h2/text()"")
    print(text)

response = requests.get(""https://medium.com"")
opponents = get_trending(response.text)
opponents

For some reason, it is giving an empty list as a result. I tried it with another h2 and I got the result I wanted. What might be the problem?
Here is the screenshot when I try the xpath in the inspect tool it is working as shown in the image below.

",76389717,"Here is a way to get those elements you're looking for using BeautifulSoup instead of Parsel -- you can adapt the code to Parsel if you are so inclined, and you can edit the class content as well to suit your usecase:
from bs4 import BeautifulSoup as bs
import requests

headers= {
    'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'
}

r = requests.get(""https://medium.com/"", headers=headers)
soup = bs(r.text, 'html.parser')
elements = soup.select('h2[class^=""by j""]')
for x in elements:
    print(x.text)

Result in terminal:
No One Really Tells You The Hardest Part Of Getting Older
A good day with Jeff
4 Secrets of Emotionally Stable People
Thought experiment in the National Library of Thailand
Unnatural Keys
Business Model Generation & Playing to Win
On the nature of elegance
It’s Time to Re-Design How We Think
Report from a Relationship
The Secret to Good Photography
A good day with Jeff
How I Turned My Company’s Docs into a Searchable Database with OpenAI
Thought experiment in the National Library of Thailand
What We’re Reading: Always Take a Peek at the Comments
A Contrarian Bet Against Generative AI: The Value of Being Uniquely Human
Why an Engineering Manager Should Not Review Code

BeautifulSoup documentation can be found here.
",
76387742,How to enable authenticator MFA for cognitio user pool created using AWS Node js API?,"I am trying to create a AWS Cogntio user pool using AWS Node js SDK in a lambda function. Our use case requires multi factor authentication and enabling authenticator apps for MFA. Looks like the cognito console UI provides an option to specify this but the API does not provide a way to enable authenticator apps for MFA.
Screenshot of Cognito user pool console UI showing option to enable Authenticator apps for MFA
Tried adding
""SoftwareTokenMfaConfiguration"": {
    ""Enabled"": true
  }


field in the params to cognitoIdentityServiceProvider.createUserPool
API but the response indicates ""Error creating User Pool:
UnexpectedParameter: Unexpected key 'SoftwareTokenMfaConfiguration'
found in params""

So my code snippet looks like below:
async function createCognitoUserPool(poolName) {
  const cognitoIdentityServiceProvider =
    new AWS.CognitoIdentityServiceProvider();

  // Define the parameters for creating the user pool
  const params = {
    PoolName: poolName,
    Policies: {
      PasswordPolicy: {
        MinimumLength: 8,
        RequireUppercase: true,
        RequireLowercase: true,
        RequireNumbers: true,
        RequireSymbols: true,
      },
    },
    MfaConfiguration: ""ON"",
    SoftwareTokenMfaConfiguration: {
      Enabled: true,
    },
    AutoVerifiedAttributes: [""phone_number""],
    SmsConfiguration: JSON.parse(
      process.env.COGNITO_USER_POOL_SMS_CONFIGURATION
    ),
  };

  try {
    // Call the CreateUserPool method
    const response = await cognitoIdentityServiceProvider
      .createUserPool(params)
      .promise();

    console.log(""User Pool created successfully:"", response.UserPool.Id);
    return response.UserPool.Id;
  } catch (error) {
    console.error(""Error creating User Pool:"", error);
    throw error;
  }
}

Looking forward to find a solution for this. Thanks!
",76388326,"You can use SetUserPoolMfaConfig method after creating the user pool.
From TOTP software token MFA page.

You can activate TOTP MFA for your user pool in the Amazon Cognito console, or you can use Amazon Cognito API operations. At the user pool level, you can call SetUserPoolMfaConfig to configure MFA and enable TOTP MFA.

Here are the links of API references from AWS documentation.

SetUserPoolMfaConfig
setUserPoolMfaConfig(params = {}, callback) ⇒ AWS.Request

",
76390765,Count 2 different date fields in a single query,"I have 2 different date fields, 1 when the case was created and the other when the case completed. I am trying to count total cases and total completions in say 2023.
Whenever I run my query and set the where field, both columns return the same figure.
SELECT COUNT(*) AS cases,
       COUNT(IF(YEAR(tbl_lead.CompDate) = '2023', 1, NULL)) AS Completed,
       tbl_lead.LeadSource
FROM tbl_lead
WHERE YEAR(tbl_lead.CompDate) = '2023'
GROUP BY tbl_lead.LeadSource

I guess what I'm trying to do is count all records for 2023 as cases, then count how many of those have completed in 2023. Is it impossible?
It should output as:




Cases
Completed
LeadSource




1000
500
Google


2000
700
Facebook




Whereas it currently outputs:




Cases
Completed
LeadSource




500
500
Google


700
700
Facebook




Thank you
",76391096,"Your current query only refers to CompDate but in your question you refer to two different date columns. Is this what you are looking for:
SELECT SUM(CreateDate >= '2023-01-01' AND CreateDate < '2024-01-01') AS cases,
       SUM(CompDate >= '2023-01-01' AND CompDate < '2024-01-01') AS Completed,
       LeadSource
FROM tbl_lead
WHERE (CreateDate >= '2023-01-01' AND CreateDate < '2024-01-01')
   OR (CompDate >= '2023-01-01' AND CompDate < '2024-01-01')
GROUP BY LeadSource

",
76389613,R: Append matrix rows if condition is met,"I loop through a matrix and I would like to append a matrix row to another empty matrix if a specific condition is met. How do I do this without getting problems with the different indexes?
I had this code, but my dataset is very large, so I get problems in the implementation
for (i in 1:length(matrix1)) {
    if ((substr(matrix1[i,1],6,7) == '02') == TRUE) {
        for (j in 1:nrow(matrix2)) {
            matrix2[j,] <- matrix1[i,]
        }
    }
}

Is there a more efficient solution?
dput(matrix1[1]) is c(""271"", ""269"", ""274"", ""278"", ""293"", ""270"", ""274"", ""274"", ""275"",  ""271"", ""2018-01-03_0445"")
nrow(matrix1) is 400000
",76389735,"You can simply extract the rows satisfying your condition:
matrix2 <- matrix1[substr(matrix1[, 1], 6, 7) == '02', ]

",
76391004,Error finding a string in another string in Python,"I'm trying to return links that have the word ""attach"" in a list using Python on a web page. But every method I use to find that word in the link gives almost the same error.
This is my code and the error is received from line 16:
from bs4 import BeautifulSoup

# pip install requests
import requests

def list_image_links(url):
    # Send a GET request to the URL
    response = requests.get(url)

    soup = BeautifulSoup(response.content, ""html.parser"")
    image_links = []
    for image_links in soup.find_all('a'):
        href = image_links.get('href')
#if attach is in href
        word = 'attach'
        if word in href:
            image_links.append(href)
    
    print(image_links)
    return image_links

list_image_links('https://forum.ubuntu.ir/index.php?topic=211.0')


Error:
Traceback (most recent call last):
  File ""d:\...\main.py"", line 22, in <module>
    list_image_links('https://forum.ubuntu.ir/index.php?topic=211.0')
  File ""d:\...\main.py"", line 16, in list_image_links
    if word in href:
       ^^^^^^^^^^^^
TypeError: argument of type 'NoneType' is not iterable

",76391129,"In my perspective, the error you're encountering is due to the fact that some of the href values returned by link.get('href') may be None, and you cannot iterate over a None value. To avoid this error, you can add a condition to check if href is not None before checking if the word is in it. Here's the modified code:
from bs4 import BeautifulSoup
import requests

def list_image_links(url):
    response = requests.get(url)

    soup = BeautifulSoup(response.content, ""html.parser"")
    image_links = []
    for image_links in soup.find_all('a'):
        href = image_links.get('href')
        word = 'attach'
        
        if href and word in href:
            image_links.append(href)
    
# rest of the code...

The only change I made was adding an additional condition to the if clause, specifically if href and word in href:. I think this modification will resolve the issues.
",
76387594,Vue - show loader while computed property being computed,"EDIT - Found that sorting is quick, real issue is performance of rendering huge list, so already answered
pls explain to me, why this does nothing:
I have array of thousands of items, there is a button for sorting them by some prop (changes ""sortBy"" prop. Sorting items takes almost 2 seconds, at least that how long after the click does the list change. During computing (until new list displayed) i want to display some ""Loading"" element. Im not aware, byt maybe Vue has some app-wide state to tell something is being recomputed?
<div v-if=""loading"">Wait pliz</div>
<div @click=""sortBy='ctg'"">SortByCtg</div>
<div v-for=""item in sortedRows"">{{item.ctg}} , {{item.name}} .... </div>

and the computed prop fn:
data: function(){ 
   return { 
    'sortby': 'name', 
    'sortbyDir': 1, 
    'loading': false,
    'rows': [ {'name':'abc','ctg':'def'}, ...] 
  }
},
computed: {
    sortedRows: function(){
     this.loading = true; //  <<< should show element
     var sortby = this.sortby;
     var sortbyDir = this.sortbyDir;
     var sorted = this.rows;
     sorted = this.rows.sort(function(a, b) { 
      return sortbyDir * a[sortby].localeCompare(b[sortby]); 
     });
    this.loading = false; //  <<< hide element
    return sorted;
  }
},
...

but the ""loading"" element never shows. May it be sort is quick, and what is taking the time is the nodes generation itself? Then can i anyhow show the loader? Maybe somehow use next tick? I tried but with no result.
",76388332,"Sorting is quick (few miliseconds),
what really takes time is rendering the long list
",
76389468,How to subtract values between months for each group and each year separately in R,"i have such data example
mydata=structure(list(month_id = c(201206L, 201206L, 201207L, 201207L, 
201306L, 201306L, 201307L, 201307L, 201406L, 201406L, 201407L, 
201407L, 201506L, 201506L, 201507L, 201507L, 201606L, 201606L, 
201607L, 201607L, 201706L, 201706L, 201707L, 201707L), MDM_Key = c(1L, 
2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 
2L, 1L, 2L, 1L, 2L, 1L, 2L), sale_count = c(6978517L, 13957034L, 
6148636L, 12297272L, 6147466L, 12294932L, 6050044L, 12100088L, 
8127548L, 16255096L, 3341936L, 6683872L, 8995701L, 17991402L, 
6803563L, 13607126L, 7098546L, 14197092L, 7855146L, 15710292L, 
5575453L, 11150906L, 7543770L, 15087540L)), class = ""data.frame"", row.names = c(NA, 
-24L))

I need for each group (mdm_key) to get the difference between the months (07-06) by sale_count Variable. The data contains histories by year. e.g. 06(june) and 07(jule) monthes for 2012,
06 and 07 for 2013 and so on...
For each year and each mdm_key I need to subtract the sale_count from month 7 to month 6.
So that the desired result for each year and each group mdm_key looks something like this
   year MDM_Key sale_count
1  2012       1    -829881
2  2013       1     -97422
3  2014       1   -4785612
4  2015       1   -2192138
5  2016       1     756600
6  2017       1    1968317
7  2012       2   -1659762
8  2013       2    -194844
9  2014       2   -9571224
10 2015       2   -4384276
11 2016       2    1513200
12 2017       2    3936634

What is the easiest way to do such a subtraction?
Thanks for your any help.
",76389744,"Does this work?
library(tidyverse)
mydata %>%
  # Filter on months 6 and 7
  filter(str_sub(month_id, -2) %in% c(""07"", ""06"")) %>%
  # Sort by Key + yearmonth
  arrange(MDM_Key, month_id) %>%
  # Group by Key
  group_by(MDM_Key) %>%
  # Calculate difference between sales
  mutate(sale_diff = sale_count - lag(sale_count)) %>%
  # Return unique values for each Key + year
  filter(str_sub(month_id, -2) == ""07"") %>%
  mutate(year = as.integer(str_sub(month_id, 1, 4))) %>%
  select(year, MDM_Key, sale_count = sale_diff)
# A tibble: 12 x 3
# Groups:   MDM_Key [2]
    year MDM_Key sale_count
   <int>   <int>      <int>
 1  2012       1    -829881
 2  2013       1     -97422
 3  2014       1   -4785612
 4  2015       1   -2192138
 5  2016       1     756600
 6  2017       1    1968317
 7  2012       2   -1659762
 8  2013       2    -194844
 9  2014       2   -9571224
10  2015       2   -4384276
11  2016       2    1513200
12  2017       2    3936634

",
76391017,How can I split a column in pandas,"I have a data frame with a column that contains string and digit,
Prod_nbr| prod_name
5   Natural chip companyseasalt175g
66  cC Nacho cheese 172g
61  Smiths Crinkle cut chips chicken135g

My desired output is
Prod_nbr|pack|prod_name
5          175g  Natural chip....
66         172g  cC Nacho cheese..
61         135g   Smiths Crinkle...

I tried the code below but I didn't get my desired output, I got

df['pack'] = df['prod_name'].str.extract(r'\d+\s*(\w{,5})\b').fillna('')[0]

",76391136,"I would make a custom function to solve the parsing of the field, then apply it by row to the whole DataFrame. I prefer this way because most of the time you will find some unexpected string in the data, and using a function helps you with tweaking the output when needed.
Here is a quick example.
def parse(row):
    s = row.prod_name
    matches = re.findall('\d+g', s)
    if matches:
        if len(matches) == 1:
            return matches[0] #if you have a single match
        else:
            return 'parsing error' #if you have multiple unexpected matches
    return np.nan #no matches


df['pack'] = df.apply(parse, axis=1)

",
76389471,Angular retaining input value after form reset,"I have a reactive form with two fields.First is custom input using ControlValueAccessor, and last is just regular HTML input.
Problem is, after performing form.reset(), the value of custom input is retained event its value in reactive form is null already.

As you can see in image, the first time I input and clear the values, it is working well.
But, as second time and onwards, the input value is STILL retained in custom input component. While, the normal HTML input is cleared and working well regardless of how many times I click Clear Input. Can you help me, please? Did I miss to put something?
Files:

app.component.ts/html: where the form lives
custom-input.component.ts/html: custom input component

Here is the form:
ts file
import { Component } from '@angular/core';
import { FormBuilder, FormGroup } from '@angular/forms';

@Component({
  selector: 'my-app',
  templateUrl: './app.component.html',
})
export class AppComponent {
  form: FormGroup;

  constructor(private fb: FormBuilder) {
    this.form = this.fb.group({
      firstName: [{ value: '', disabled: false }],
      lastName: [{ value: '', disabled: false }],
    });
  }

  clearInput() {
    this.form.reset();
  }
}

html file:
<form [formGroup]=""form"">
  <app-custom-input formControlName=""firstName""></app-custom-input>
  <input formControlName=""lastName"" placeholder=""Last name"" />
  <button (click)=""clearInput()"">Clear Input</button>
</form>

<br />
<pre>{{ form.value | json }}</pre>

Here is the custom input file:
ts file:
import { Component, forwardRef } from '@angular/core';
import { ControlValueAccessor, NG_VALUE_ACCESSOR } from '@angular/forms';

@Component({
  selector: 'app-custom-input',
  templateUrl: './custom-input.component.html',
  providers: [
    {
      provide: NG_VALUE_ACCESSOR,
      useExisting: forwardRef(() => CustomInputComponent),
      multi: true,
    },
  ],
})
export class CustomInputComponent implements ControlValueAccessor {
  value: string;
  changed: (value: any) => void;
  touched: () => void;
  isDisabled: boolean;

  writeValue(value: string): void {
    this.value = value;
  }

  registerOnChange(fn: any): void {
    this.changed = fn;
  }

  registerOnTouched(fn: any): void {
    this.touched = fn;
  }

  setDisabledState(isDisabled: boolean): void {
    this.isDisabled = isDisabled;
  }

  onChange(event: Event): void {
    const value: string = (<HTMLInputElement>event.target).value;

    this.changed(value);
  }
}

html file:
<input
  placeholder=""First name""
  [disabled]=""isDisabled""
  [value]=""value""
  (input)=""onChange($event)""
  (blur)=""touched()""
/>

Full working code is here
",76389757,"Ok, you doing things well, but, you have to research a little bit deeper about this problem. If you go through HTML specification, you may find that the value attribute for the input html element is just an initial value. And that's why you get only first change if you push the reset button (actually you assign value there and writeValue method invokes).
So the solutions are several, the simplest and relative to your code style is to get the reference to the input and assign value manually:
custom-input.component.html
<input
  placeholder=""First name""
  [disabled]=""isDisabled""
  (input)=""onChange($event)""
  (blur)=""touched()""
  #inputRef
/>


custom-input.component.ts
export class CustomInputComponent implements ControlValueAccessor {
  @ViewChild('inputRef')
  inputRef: ElementRef<HTMLInputElement>;
  changed: (value: any) => void;
  touched: () => void;
  isDisabled: boolean;

  writeValue(value: string): void {
    if (this.inputRef) {
      this.inputRef.nativeElement.value = value;
    }
  }

  ...

Another solution is to use ngModel and then it can work with value property binded to the input.
",
76388239,Why does my 2 pointers approach to solve the 3 sum problem on LeetCode throw an index out of bounds error?,"class Solution {
    public List<List<Integer>> threeSum(int[] nums) {
        Arrays.sort(nums);
        List<List<Integer>> ans = new ArrayList<>();     
        for(int i=0;i<=nums.length-3;i++){
            if(i==0 || nums[i]!=nums[i-1]){
                int L=i+1;
                int R = nums.length-1;
                int target = 0-(nums[i]);
                while(L<R){ 
                    if(nums[L]+nums[R]==target){
                        List<Integer> temp = new ArrayList<>();
                        temp.add(nums[i]);
                        temp.add(nums[L]);
                        temp.add(nums[R]);
                        ans.add(temp);
                        while(L<nums.length-1 && nums[L]==nums[L+1]) L++;
                        while(R>0 && nums[R]==nums[R-1]) R--;
                        L++;
                        R--;
                    }else if(nums[L]+nums[R]<target){
                        L+=1;
                    }else{
                        R+=1;
                    }
                }    
            }
        }
    return ans;    
    }
# }


trying to solve the 3 sum problem of leetcode with 2 pointers approach . I am getting java.lang.ArrayIndexOutOfBoundsException: Index 6 out of bounds for length 6 at
if(nums[L]+nums[R]==target){
",76388358,"A error in the condition for increasing the R pointer is probably what is generating the ArrayIndexOutOfBoundsException in your code. R-=1 should be used instead of R+=1 to move the pointer to the left.
The updated code is as follows:
class Solution {
    public List<List<Integer>> threeSum(int[] nums) {
        Arrays.sort(nums);
        List<List<Integer>> ans = new ArrayList<>();
        for (int i = 0; i <= nums.length - 3; i++) {
            if (i == 0 || nums[i] != nums[i - 1]) {
                int L = i + 1;
                int R = nums.length - 1;
                int target = 0 - nums[i];
                while (L < R) {
                    if (nums[L] + nums[R] == target) {
                        List<Integer> temp = new ArrayList<>();
                        temp.add(nums[i]);
                        temp.add(nums[L]);
                        temp.add(nums[R]);
                        ans.add(temp);
                        while (L < nums.length - 1 && nums[L] == nums[L + 1]) L++;
                        while (R > 0 && nums[R] == nums[R - 1]) R--;
                        L++;
                        R--;
                    } else if (nums[L] + nums[R] < target) {
                        L += 1;
                    } else {
                        R -= 1;  // Fix the typo here
                    }
                }
            }
        }
        return ans;
    }
}

",
76391053,g++ Flag variable not seen by make,"Folks, I need some help in understanding what's happening, I do not see how to pass an option to g++ compiler in a makefile. If you can just try to reproduce my stuff and get me some info, it would be valuable (for me).
Makefile content:
MYCPPFLAGS := what a surprise

%.o: %.cpp %.h
    g++ $(MYCPPFLAGS) -c $< -o $@

dumpy:
    echo $(MYCPPFLAGS)

It's not that kind of magic, isn't it???
Now I have a c++11 file named dessiner.cpp, if you need it I'll post it, but let's skip it for the moment. Please have a look at what I get in the terminal:
grenx08{lc177705}hw-m2-cpp-07-dessiner: make dumpy
echo what a surprise
what a surprise
grenx08{lc177705}hw-m2-cpp-07-dessiner: make dessiner.o
g++    -c -o dessiner.o dessiner.cpp
dessiner.cpp:18:13: warning: override controls (override/final) only available with -std=c++11 or -std=gnu++11 [enabled by default]

[... 
a series of warnings/ errors because I did not use std=c++11 option] 

make: *** [dessiner.o] Error 1


So my question is : why can't I see anything about ""what a surprise"" in the g++ command-line, but just four blank spaces???? Is make smarter than me (LoL, of course!!!) and erasing stupid options passed to g++??? The real issue is that in this case, I cannot even give the good options...
Please find hereby the sware versions, quite old stuff due to a constrained working environment. Any hint appreciated.
grenx08{lc177705}hw-m2-cpp-07-dessiner: make -v
GNU Make 3.82
Built for x86_64-redhat-linux-gnu
Copyright (C) 2010  Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
grenx08{lc177705}hw-m2-cpp-07-dessiner: g++ --version
g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

Edit 1:
grenx08{lc177705}hw-m2-cpp-07-dessiner: make -d dessiner.o
GNU Make 3.82
Built for x86_64-redhat-linux-gnu
[ tons of output ]
  No need to remake target `dessiner.cpp'.
 Finished prerequisites of target file `dessiner.o'.
Must remake target `dessiner.o'.
Invoking builtin recipe to update target `dessiner.o'.
g++    -c -o dessiner.o dessiner.cpp
[ other irrelevant output ]

grenx08{lc177705}hw-m2-cpp-07-dessiner: make -r dessiner.o
make: *** No rule to make target `dessiner.o'.  Stop.

So it looks as my rule definition has a flaw? I have no .h file...
",76391162,"This recipe:
%.o: %.cpp %.h
        g++ $(MYCPPFLAGS) -c $< -o $@

tells make ""hey make, if you're trying to build a target that matches the pattern %.o, and if you have prerequisites that match the patterns %.cpp and %.h, then you can use this recipe to build the target"".
Note carefully: ALL the prerequisites MUST exist, or there must be rules telling make how to build them, otherwise this rule does not match.
In your case, you have a file dessiner.cpp but you do not have a file dessiner.h, and so make cannot create both the prerequisites, and so your rule doesn't match and make ignores it.
Then make looks in its built-in set of rules and it sees a %.o : %.cpp pattern rule there, and since that prerequisite exists, it matches and make runs that recipe.  That recipe doesn't know anything about your variable MYCPPFLAGS so it's not used.
",
76389248,Same height of columns for bootstrap,"It's probably simple, but I could not figure out how to fix it.
I use Bootstrap5.
The full code is here :
https://www.codeply.com/p/BywuhLNUXy
Seems codeply has some problems...
So I put it also on jsfiddle
https://jsfiddle.net/paul_z/y7a6dnkf/1/
Mainly it's contact directory page.
The code structure is
<div class=""container"">
<div class=""row gy-5"">

    <div class=""col-lg-6"">
        <div class=""card m-b-30"">
            <div class=""card-body py-5"">
                <div class=""row"">
                    <div class=""col-lg-4 text-center"">            
                    <img src=""/static/logo.png"" alt=""logo"" >            
                    </div>
                <div class=""col-lg-8"">
                    <h5 class=""card-text mb-0""><i class=""fa-solid fa-person""></i> user 1</h5>
                    <p class=""card-text"">CDD IT Informatique</p>
                    <hr class=""dropdown-divider"">                
                    <p class=""card-text""><i class=""fa-sharp fa-solid fa-building""></i> ###</p>        
                    <p class=""card-text""><i class=""fa-solid fa-envelope""></i> mail</p>        
                    <p class=""card-text""><i class=""fa-solid fa-phone""></i> phone</p>
               </div>
                </div>
              </div>
        </div>
    </div>
    
    ...

On large screens it's shown two cards by column.
The problem is that cards sometimes different height.
For example for user 2 his role change : ""Enseignant Chercheur Astrophysique Hautes Energies"" instead ""CDD IT Informatique"" so this line take two lines instead of one line. And so the card for user 2 has height different to others cards.
How could I fix it ?
I don't know the cards size, so I could not fix it implicitly (and in any case it's probably not a good idea).
P.S.: I have also the same problem if logos have different sizes. Despite playing with img-fluid, widht, max-widht, max-height etc some logos change the height of card. But I think at first I have to fix the simple height problem.
",76389777,"The col elements already have the same height - Bootstrap v5's grid implementation based on flexbox already sees to that.
All you need to do is make the cards take 100% height of their parent:
.card { height: 100%; }

",
76389727,google.script.run.function() returning null,"On a google sheet, I have a sidebar form allowing user to add infos in another sheet.
I have to give an ID to each line so when a user add one, generate a new id.
example :
the sidebar form has two inputs : criterion number (int) and criterion (text). when a user add these inputs, I want to put them in a sheet with 3 columns : A = ID, B = criterion number and C = criterion.
To generate the ID, I created a creeID() function on the server side (this function checks the last ID in the sheet and generate the next ID, works well when tested) and I have to call it when the user add a  line (""ajouter"" button) in order to get a tab with this format [ID, criterion number, criterion] and to push this tab in the sheet with appendRow()
I thought I could use google script run with .withSuccessHandler() like said here and I tried something based on @Tanaike's help and this video.
Everything seems to work fine except that google.script.run.withSuccessHandler(data => {alert(""preuve ajoutée"")}).creeID() returns null
here is my html file :
<!DOCTYPE html>
<html>
  <head>
    <base target=""_top"">
    <style>
      ...
    </style>
  </head>
  <body>
    <p>Indicateur</p>
    <input type=""number"" name=""indicateur"" id=""indic"" value="""" min = ""1"" max = ""32"">
    <p>Preuve</p>
    <input type=""text"" name=""preuve"" id=""preuve"">
    <br>
    <input type=""button"" value=""Ajouter"" onclick=""ajouter()"">
    <span class=""annuler"" onclick=""google.script.host.close()"">Annuler</span>

    <script>
      function ajouter() {
        const inputs = document.querySelectorAll('#indic, #preuve');
        let tab = [google.script.run.withSuccessHandler(data => {alert(""preuve ajoutée"")}).creeID()];
        

        // Récupération des valeurs
        for (const input of inputs) {
          tab.push(input.value);
        }

        // Si tous les champs sont vides
        if (tab.join('') == '') {
          alert('Le formulaire est vide !');
          return;
        }

        // Vider les champs
        inputs.forEach(input => input.value = '');

        // Envoi dans la feuille
        google.script.run.ajouterLigne(tab);
      }
  
    </script>
  </body>
</html>

and the javascript code :
function ajouterLigne(tab) {
  const PREUVES = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(""PREUVES"");
  console.log(tab)
  PREUVES.appendRow(tab);
}

function creeID() {
  const SHEET = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(""PREUVES"");
  let lastRow = SHEET.getLastRow();
  let lastIdRange = SHEET.getRange(lastRow, 1);
  let lastId = lastIdRange.getValue(); 
  let newId;

  if (lastId == ""ID"") {
    newId = 1;
  } else {
    newId = lastId + 1;
  };

  return(newId)
}

",76389822,"In the case of your script, google.script.run.creeID() returns no value. I think that this is the reason for your current issue. From maybe I should use .withSuccessHandler() like said here but I don't know how., how about using withSuccessHandler() as follows?
In this case, please modify ajouter() of your Javascript as follows.
Modified script:
function ajouter() {
  const inputs = document.querySelectorAll('input[type=""text""]');
  google.script.run.withSuccessHandler(tab => {
    tab = [tab];
    for (const input of inputs) {
      tab.push(input.value);
    }
    if (tab.join('') == '') {
      alert('Le formulaire est vide !');
      return;
    }
    inputs.forEach(input => input.value = '');
    google.script.run.ajouterLigne(tab);
  }).creeID();
}


In this modification, when a button is clicked, a value of tab is retrieved and the values of tab and the inputted values are appended to the Spreadsheet.

Reference:

withSuccessHandler(function)

",
76388314,"Why does my Python function return 'None' instead of ('Paul', 29, 1.75)?","I am following a tutorial
when he runs the code it gives  (""Paul', 29, 1.75) but when I run it it gives me None
I tried
# -----Part 2------
# name,age,height
persons = [
    (""Alice"", 25, 1.6),
    (""Brian"", 35, 1.8),
    (""Paul"", 29, 1.75),
    (""Martin"", 32, 1.7)
]

def get_infos(name, l):
    for i in l:
        if i[0] == name:
            return i
        return None
    
infos = get_infos(""Paul"", persons)
print(infos)

and got:
None
but when he runed it he got:
('Paul', 29, 1.75)
",76388360,"It's an identation problem, always in the first iteration will return None, try by identing the return None one less tab, like:

def get_infos(name, l):
    for i in l:
        if i[0] == name:
            return i
    return None


",
76390825,How to group states into regions when there is no region table,"My data set has states as a column but no region column to group the states by. I would like to group the states into standard census bureau regions to get the count of employee IDs by region:

Select COUNT(DISTINCT Empl_ID) AS Employee_Count,
STATE
FROM Employee_Table
GROUP BY STATE

I tried exporting the query and then sorting in Excel but the dataset is too large for Excel.
",76391171,"Create a table based on the data provided in the image:
create table region_state (region varchar(30), state varchar(30));
insert into region_state values
('Northen Region', 'New Jersey'),
('Northen Region', 'New York'),
('Midwest Region', 'Illinois')
...;

Now we can use a join query to get the region for each employee state as the following:
Select COUNT(DISTINCT E.Empl_ID) AS Employee_Count,
       R.region
FROM Employee_Table E JOIN region_state R
ON E.state = R.state
GROUP BY R.region

If there are no duplicates in the Empl_ID column (and that is the typical case, i.e. Empl_ID is the primary key of the table) you would use COUNT(E.Empl_ID), with no need for DISTINCT.
",
76389271,Round decimals up to multiples of 5 with BigDecimal,"I want to round BigDecimal decimal part according to the following rules:

1710.10 becomes 1710.10
1710.11 becomes 1710.15
1710.15 becomes 1710.15
1710.16 becomes 1710.20

I tried this way new BigDecimal(""1710.11"").setScale(2, RoundingMode.HALF_UP)), expecting to get 1710.15 but I get 1710.11. I also tried with Apache Math Utils and Decimal Format but no way to achieve this.
",76389823,"Your misunderstanding
First of all setting the scale on a BigDecimal only returns a BigDecimal with a different value if your current BigDecimal has a higher scale than you specified.
For example
new BigDecimal(""1710.113"").setScale(2, RoundingMode.HALF_UP)

returns a new BigDecimal equal to new BigDecimal(""1710.11"")
However, if you already have a scale of 2 or lower in this example, the new BigDecimal stays the same (read: equal).
HALF_UP simply means that a BigDecimal ending on a 5 will result in the higher value, e.g.
new BigDecimal(""1710.115"").setScale(2, RoundingMode.HALF_UP)

returns a result equal to new BigDecimal(""1710.12"")
Now to your question
Since your way of ""rounding"" doesn't actually change the scale, I doubt there is an already existing function you could use. However, doing it by hand is actually quite simple:
public class QuickMathz {

    public static void main(String[] args) {
        System.out.println(roundToNext5(new BigDecimal(""1710.10""), 2));
        System.out.println(roundToNext5(new BigDecimal(""1710.11""), 2));
        System.out.println(roundToNext5(new BigDecimal(""1710.15""), 2));
        System.out.println(roundToNext5(new BigDecimal(""1710.16""), 2));
        System.out.println(roundToNext5(new BigDecimal(""1710.1135""), 2));
        System.out.println(roundToNext5(new BigDecimal(""1710.1635""), 2));
        System.out.println(roundToNext5(new BigDecimal(""1710.1675""), 2));
        System.out.println();
        System.out.println(roundToNext5(new BigDecimal(""1710.10""), 3));
        System.out.println(roundToNext5(new BigDecimal(""1710.11""), 3));
        System.out.println(roundToNext5(new BigDecimal(""1710.15""), 3));
        System.out.println(roundToNext5(new BigDecimal(""1710.16""), 3));
        System.out.println(roundToNext5(new BigDecimal(""1710.1135""), 3));
        System.out.println(roundToNext5(new BigDecimal(""1710.1635""), 3));
        System.out.println(roundToNext5(new BigDecimal(""1710.1675""), 3));
    }

    public static BigDecimal roundToNext5(BigDecimal bigDecimal, int scale) {
        // Get the last digit we need to decide if we have to round to 0, 5 or 10
        int lastDigit = bigDecimal
                .movePointRight(scale)
                .remainder(BigDecimal.TEN).intValue();

        // Setting the Scale to scale - 1 to remove one more digit than we need
        // and then increase the scale to what we want
        BigDecimal result = bigDecimal
                .setScale(scale - 1, RoundingMode.DOWN)
                .setScale(scale, RoundingMode.UNNECESSARY);

        if (lastDigit == 0) {
            // Last digit is a 0 upscaling adds a 0
            return result;
        } else if (lastDigit <= 5) {
            // rounding up to 5
            return result.add(new BigDecimal(""5"").movePointLeft(scale));
        } else {
            // rounding up to 10
            return result.add(new BigDecimal(""1"").movePointLeft(scale - 1));
        }
    }
}

This class yields the output of
1710.10
1710.15
1710.15
1710.20
1710.15
1710.20
1710.20

1710.100
1710.110
1710.150
1710.160
1710.115
1710.165
1710.170

(It's neither optimized nor checked for negative values, so don't use in critical environments)
",
76389621,List is not updating properly after deleting an item in flutter,"I am working with dynamic list which add and delete items in the list. but when i delete a specific index with removeAt() method it always delete the last item, below is all code
import 'package:bizzsmart_web/constants/constants.dart';
import 'package:bizzsmart_web/model/expense_item_model.dart';
import 'package:flutter/cupertino.dart';
import 'package:flutter/material.dart';

class TestPage extends StatefulWidget {
  const TestPage({Key? key}) : super(key: key);

  @override
  State<TestPage> createState() => _TestPageState();
}

class _TestPageState extends State<TestPage> {
  List<ExpenseItemModel> expenses =
      List.generate(1, (index) => ExpenseItemModel());

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: Column(
        children: [
          sbh(150),
          Expanded(
            child: ListView.builder(
              itemCount: expenses.length,
              itemBuilder: (c, i) {
                return Column(
                  children: [
                    Row(
                      children: [
                        IconButton(
                          onPressed: () {
                            if (i == expenses.length - 1 ||
                                (expenses.length == 1)) return;
                            expenses.removeAt(i);
                            setState(() {});
                          },
                          icon: const Icon(CupertinoIcons.delete),
                        ),
                        Expanded(
                          child: TextField(
                            onChanged: (val) {
                              if (expenses.length - 1 == i) {
                                expenses.add(ExpenseItemModel());
                              }
                              setState(() {});
                            },
                            decoration: InputDecoration(
                              border: border,
                              enabledBorder: border,
                              focusedBorder: border,
                            ),
                          ),
                        ),
                        sbw(50),
                      ],
                    ),
                    sbh(10),
                  ],
                );
              },
            ),
          ),
        ],
      ),
    );
  }

  final border = OutlineInputBorder(
    borderRadius: BorderRadius.circular(8),
  );
}


and Dart pad link  :  https://dartpad.dev/?id=9f87f484c1dc201b4d0c10d504eb7d1b
Somebody please help.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.
",76389825,"The problem is that you don't keep track on which TextField corresponds with which expense. To solve it you need to define controllers that you can give the TextFields. Like this for example:
class _TestPageState extends State<TestPage> {
  List<ExpenseItemModel> expenses = [ExpenseItemModel()];
  List<TextEditingController> controllers = [TextEditingController()];

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: Column(
        children: [
          SizedBox(height: 150),
          Expanded(
            child: ListView.builder(
              itemCount: expenses.length,
              itemBuilder: (c, i) {
                return Column(
                  children: [
                    Row(
                      children: [
                        IconButton(
                          onPressed: () {
                            if (i == expenses.length - 1 ||
                                (expenses.length == 1)) return;
                            expenses.removeAt(i);
                            controllers.removeAt(i);
                            setState(() {});
                          },
                          icon: Icon(CupertinoIcons.delete),
                        ),
                        Expanded(
                          child: TextField(
                            controller: controllers[i],
                            onChanged: (val) {
                              if (expenses.length - 1 == i) {
                                expenses.add(ExpenseItemModel());
                                controllers.add(TextEditingController());
                              }
                              setState(() {});
                            },
                            decoration: InputDecoration(
                              border: border,
                              enabledBorder: border,
                              focusedBorder: border,
                            ),
                          ),
                        ),
                        SizedBox(width: 50),
                      ],
                    ),
                    SizedBox(height: 10),
                  ],
                );
              },
            ),
          ),
        ],
      ),
    );
  }

  final border = OutlineInputBorder(
    borderRadius: BorderRadius.circular(8),
  );
}

Note I also changed the way you initialize expenses it a more concise way.
Instead of having two lists like this you might even want to consider making the TextEditingController part of your ExpenseItemModel and then do something like controller: expenses[i].controller,
",
76391151,"how program to join 2 different tables based on which one has the highest number of rows, with tidyverse?","How program to join 2 different tables based on which one has the highest number of rows, with tidyverse? Now, the total_number_views_ndinstict has only 8 but in the future this may have more rows than the second total_number_views_unique_na which currently has 10 rows. I need both columns in the joined table.
Here is the first table:
> total_number_views_ndinstict
    # A tibble: 8 × 2
      app_name                    n_distinct_users
      <chr>                                  <int>
    1 animals_to_groups                          2
    2 cage_randomiser                            5
    3 combo_cor                                  1
    4 crispr_screen_viz                         21
    5 dep_map_bem                                4
    6 growth_rate_explorer                       3
    7 moprospector                               2
    8 translatability_single_gene               17

And the second table is
> total_number_views_unique_na
# A tibble: 10 × 2
   app_name                    users_na
   <chr>                          <int>
 1 animals_to_groups                 21
 2 cage_randomiser                   14
 3 combo_cor                         14
 4 crispr_screen_viz                  1
 5 dep_map_bem                        0
 6 dtp_browser_prod                   6
 7 flat                              81
 8 growth_rate_explorer              48
 9 moprospector                       0
10 translatability_single_gene        2

Can someone help?
",76391228,"A full join will keep theh values of both tables
library(dplyr)
full_join(total_number_views_ndinstict, total_number_views_unique_na)

",
76388303,How can I get the primary key of a selected option from a dataframe-based selectInput in Shiny?,"I am displaying a selectInput in my shiny app and it is getting its choices from a dataframe (which is coming from a table in the database).
this table (dataframe) has primary key and title in it. I want to show my users the title, but when they choose an option, I want to get the Id of the selected option to use in my code.
I managed to get the selected ""value"" but I couldn't find how to get the ""id"" from the selected option.
below is a simple example of how my code is looking with test objects:


library(DT)
library(tidyverse)
library(shiny)




test_id <- c(1, 2, 3)
test_title <- c(""a"", ""b"", ""c"")

test_df <- data.frame(test_id, test_title)
test_df <- column_to_rownames(test_df, var = ""test_id"")


ui <- fluidPage(
  selectInput(""test"", ""test"", choices = test_df),
  
  textOutput(""selectedItem"")
  
)

server <- function(input, output, session) {
  
  observeEvent(input$test, {
    output$selectedItem <- renderText(input$test)
  })
  
  
    
}

shinyApp(ui, server)


does anyone have a solution to get the ""id"" of the selected option?
I have tried this
output$selectedItem <- renderText(input$test)

but it is returning the value and not the Id
",76388370,"You could pass a named vector or list to the choices argument.
From the docs (?selectInput):

If elements of the list are named, then that name — rather than the value — is displayed to the user.

library(tidyverse)
library(shiny)

test_id <- c(1, 2, 3)
test_title <- c(""a"", ""b"", ""c"")

test_df <- data.frame(test_id, test_title)
choices <- setNames(test_df$test_id, test_df$test_title)

ui <- fluidPage(
  selectInput(""test"", ""test"", choices = choices),
  textOutput(""selectedItem"")
)

server <- function(input, output, session) {
  observeEvent(input$test, {
    output$selectedItem <- renderText(input$test)
  })
}

shinyApp(ui, server)
#> 
#> Listening on http://127.0.0.1:5936


",
76389806,I'm getting an error: Effect callbacks are synchronous to prevent race conditions. I'm having trouble formatting it in the way it wants,"I'm getting this error back though this same format seems to work in examples.
    const response = await axios.get('http://localhost:5000/get-tasks')

    const dataObject = response.data
    

    const arrayOfKeys = Object.keys(dataObject)
    const arrayOfData = Object.keys(dataObject).map((key) => dataObject[key])

    console.log(arrayOfKeys)
    console.log(arrayOfData)
  }, [])```

",76389878,"The useEffect function arg can't be asynchronous, but you can call an async function inside it.
useEffect(() => {
  async function getTasks() {
    const response = await axios.get('http://localhost:5000/get-tasks')

    const dataObject = response.data

    const arrayOfKeys = Object.keys(dataObject)
    const arrayOfData = Object.keys(dataObject).map((key) => dataObject[key])

    console.log(arrayOfKeys)
    console.log(arrayOfData)
  };

  getTasks();
}, []);

",
76391180,How to fix this postgresql error `FATAL: password authentication failed for user ?,"I'm trying to setup a simple server using IntelliJ Community, Spring Boot and PostgreSQL.
I was following an online tutorial and downloaded the code from this github as an initial template. It should be a template generated by this website: https://start.spring.io/.
Bellow is the error log. How do I fix it?
2023-06-02T11:30:49.064-03:00  WARN 12340 --- [  restartedMain] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'flywayInitializer' defined in class path resource [org/springframework/boot/autoconfigure/flyway/FlywayAutoConfiguration$FlywayConfiguration.class]: Unable to obtain connection from database: FATAL: password authentication failed for user ""fernandakipper""
----------------------------------------------------------------------------------------------------------
SQL State  : 28P01
Error Code : 0
Message    : FATAL: password authentication failed for user ""fernandakipper""

2023-06-02T11:30:49.071-03:00  INFO 12340 --- [  restartedMain] o.apache.catalina.core.StandardService   : Stopping service [Tomcat]
2023-06-02T11:30:49.095-03:00  INFO 12340 --- [  restartedMain] .s.b.a.l.ConditionEvaluationReportLogger : 

Error starting ApplicationContext. To display the condition evaluation report re-run your application with 'debug' enabled.
2023-06-02T11:30:49.140-03:00 ERROR 12340 --- [  restartedMain] o.s.boot.SpringApplication               : Application run failed

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'flywayInitializer' defined in class path resource [org/springframework/boot/autoconfigure/flyway/FlywayAutoConfiguration$FlywayConfiguration.class]: Unable to obtain connection from database: FATAL: password authentication failed for user ""fernandakipper""
----------------------------------------------------------------------------------------------------------
SQL State  : 28P01
Error Code : 0
Message    : FATAL: password authentication failed for user ""fernandakipper""

    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1770) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:598) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:520) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:326) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:324) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200) ~[spring-beans-6.0.9.jar:6.0.9]
    at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1156) ~[spring-context-6.0.9.jar:6.0.9]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:931) ~[spring-context-6.0.9.jar:6.0.9]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:608) ~[spring-context-6.0.9.jar:6.0.9]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.1.0.jar:3.1.0]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:733) ~[spring-boot-3.1.0.jar:3.1.0]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:435) ~[spring-boot-3.1.0.jar:3.1.0]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:311) ~[spring-boot-3.1.0.jar:3.1.0]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1305) ~[spring-boot-3.1.0.jar:3.1.0]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1294) ~[spring-boot-3.1.0.jar:3.1.0]
    at com.example.crud.CrudApplication.main(CrudApplication.java:10) ~[classes/:na]
    at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:578) ~[na:na]
    at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) ~[spring-boot-devtools-3.1.0.jar:3.1.0]
Caused by: org.flywaydb.core.internal.exception.FlywaySqlException: Unable to obtain connection from database: FATAL: password authentication failed for user ""fernandakipper""
----------------------------------------------------------------------------------------------------------
SQL State  : 28P01
Error Code : 0
Message    : FATAL: password authentication failed for user ""fernandakipper""

",76391248,"You have to install postgresql in your local and change the properties as per your db configuration. What they had provided is dummy. You have to put the actuals to make it work as expected
",
76388093,Change this SPARQL query,"I have a query ""change the number of publications on artificial intelligence over time."" Can you help me change this query not only about artificial intelligence, but also about subclasses of artificial intelligence?
SELECT ?year (COUNT(DISTINCT ?item) AS ?count)
WHERE {
  ?item wdt:P31 wd:Q13442814 ;
        wdt:P921 wd:Q11660 ;
        wdt:P577 ?date .
  BIND(YEAR(?date) AS ?year)
  SERVICE wikibase:label { bd:serviceParam wikibase:language ""en"". }
}
 GROUP BY ?year
 ORDER BY ?year

",76388374,"You have just to replace wdt:P921 with wdt:P921/wdt:P279* (see here for the new query).
wdt:P921/wdt:P279* is called property path. Such expressions are used for concatenating more properties in a single path which must comply with a certain structure. In this case, we are saying that the object of wdt:P921 can be the subject of wdt:P279 (* stands for ""zero or more times"") and the final object of such optional (chain of) wdt:P279 must be wd:Q11660.
Check out SPARQL 1.1 Query Language § 9 Property Paths for deepening this topic.
",
76391217,Group table by month,"data = pd.read_csv(""MLs_Unit_per_day.csv"",index_col='Date')



            X1      X2  X3  X4  X5      X6
Date                        
01/01/2023  13  0   20  5   24  14
02/01/2023  13  0   20  5   24  15
03/01/2023  11  0   20  6   24  15
04/01/2023  12  0   20  6   22  16
05/01/2023  11  0   20  6   22  16
... ... ... ... ... ... ...
29/05/2023  13  1   23  6   22  17
30/05/2023  13  1   23  6   22  18
31/05/2023  13  1   23  7   23  18
01/06/2023  12  1   23  7   24  19
02/06/2023  13  0   23  8   24  19

I'm trying to group by month ans Sum X1, X2, etc for the hole month. how can I do?
thank you
I tried before without making Date as index with this
data.groupby(data['Date'].dt.strftime('%B'))['RO1'].sum().sort_values()

but it show me wrong time format and then wrong months
",76391290,"You need to convert the date to datetime, then create a new column for month and finally use groupby
data = pd.read_csv(""MLs_Unit_per_day.csv"")
data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')

# Create a separate column for year and month
data['YearMonth'] = data['Date'].dt.to_period('M')

monthly_data = data.groupby('YearMonth').sum()

print(monthly_data)

",
76388339,R data.table lost rows after order,"I have a data.table
> dput(data.summary)
structure(list(summary_type = c(""0"", ""1"", ""2""), count = structure(list(
    62234441L, 5119L, 821770L), .internal.selfref = <pointer: 0x557538f028c0>)), row.names = c(NA, 
-3L), class = c(""data.table"", ""data.frame""), .internal.selfref = <pointer: 0x557538f028c0>)

data.summary
   summary_type    count
1:            0 62234441
2:            1     5119
3:            2   821770

Its a data.table with 3 row, I want to sort it by count
data.summary[order(count)]

But after this, there are only have 1 row in data.table
data.summary[order(count)]
   summary_type    count
1:            0 62234441

",76388399,"For some reason, your count column is a list.
library(data.table)
df <- structure(list(summary_type = c(""0"", ""1"", ""2""), count = structure(list(
  62234441L, 5119L, 821770L))), row.names = c(NA, -3L), class = c(""data.table"", ""data.frame""))

df[count]
#Error in `[.data.table`(df, count, ) : 
#  count is not found in calling scope but it is a column of type list.  

Use unlist:
df[order(unlist(count))]

#    summary_type    count
# 1:            1     5119
# 2:            2   821770
# 3:            0 62234441

",
76389918,InvalidProgramException in TypedBinding when running on iPad,"My MAUI app runs fine on the iOS Simulator in Debug mode, but when I run it on an iPad in Release mode, I get the following cryptic error when opening a certain screen:
System.Reflection.TargetInvocationException: Arg_TargetInvocationException
 ---> System.InvalidProgramException
       at Microsoft.Maui.Controls.Internals.TypedBinding`2[[OIL.ISA.ViewModels.SettingsViewModel, OIL.ISA, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null],[System.String, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].ApplyCore(Object sourceObject, BindableObject target, BindableProperty property, Boolean fromTarget)
       at Microsoft.Maui.Controls.Internals.TypedBinding`2[[OIL.ISA.ViewModels.SettingsViewModel, OIL.ISA, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null],[System.String, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].Apply(Object context, BindableObject bindObj, BindableProperty targetProperty, Boolean fromBindingContextChanged)
   at Microsoft.Maui.Controls.BindableObject.ApplyBindings(Boolean skipBindingContext, Boolean fromBindingContextChanged)
       at Microsoft.Maui.Controls.BindableObject.SetInheritedBindingContext(BindableObject bindable, Object value)
   at Microsoft.Maui.Controls.Element.SetChildInheritedBindingContext(Element child, Object context)
       at Microsoft.Maui.Controls.Element.<OnBindingContextChanged>b__82_0(BindableObject child, Object bc)
   at Microsoft.Maui.Controls.BindableObjectExtensions.PropagateBindingContext[Element](BindableObject self, IEnumerable`1 children, Action`2 setChildBindingContext)
   at Microsoft.Maui.Controls.Element.OnBindingContextChanged()
       at Microsoft.Maui.Controls.VisualElement.OnBindingContextChanged()
   at Microsoft.Maui.Controls.View.OnBindingContextChanged()
   at Microsoft.Maui.Controls.BindableObject.SetInheritedBindingContext(BindableObject bindable, Object value)
       at Microsoft.Maui.Controls.Element.SetChildInheritedBindingContext(Element child, Object context)
   at Microsoft.Maui.Controls.Element.<OnBindingContextChanged>b__82_0(BindableObject child, Object bc)
       at Microsoft.Maui.Controls.BindableObjectExtensions.PropagateBindingContext[Element](BindableObject self, IEnumerable`1 children, Action`2 setChildBindingContext)
   at Microsoft.Maui.Controls.Element.OnBindingContextChanged()
   at Microsoft.Maui.Controls.VisualElement.OnBindingContextChanged()
       at Microsoft.Maui.Controls.View.OnBindingContextChanged()
   at Microsoft.Maui.Controls.BindableObject.SetInheritedBindingContext(BindableObject bindable, Object value)
   at Microsoft.Maui.Controls.Element.SetChildInheritedBindingContext(Element child, Object context)
       at Microsoft.Maui.Controls.TemplatedPage.SetChildInheritedBindingContext(Element child, Object context)
   at Microsoft.Maui.Controls.Element.<OnBindingContextChanged>b__82_0(BindableObject child, Object bc)
       at Microsoft.Maui.Controls.BindableObjectExtensions.PropagateBindingContext[Element](BindableObject self, IEnumerable`1 children, Action`2 setChildBindingContext)
   at Microsoft.Maui.Controls.Element.OnBindingContextChanged()
   at Microsoft.Maui.Controls.VisualElement.OnBindingContextChanged()
       at Microsoft.Maui.Controls.Page.OnBindingContextChanged()
   at Microsoft.Maui.Controls.ContentPage.OnBindingContextChanged()
   at Microsoft.Maui.Controls.BindableObject.BindingContextPropertyChanged(BindableObject bindable, Object oldvalue, Object newvalue)
       at Microsoft.Maui.Controls.BindableObject.SetValueActual(BindableProperty property, BindablePropertyContext context, Object value, Boolean currentlyApplying, SetValueFlags attributes, Boolean silent)
       at Microsoft.Maui.Controls.BindableObject.SetValueCore(BindableProperty property, Object value, SetValueFlags attributes, SetValuePrivateFlags privateAttributes)
   at Microsoft.Maui.Controls.BindableObject.SetValue(BindableProperty property, Object value, Boolean fromStyle, Boolean checkAccess)
       at Microsoft.Maui.Controls.BindableObject.SetValue(BindableProperty property, Object value)
   at Microsoft.Maui.Controls.BindableObject.set_BindingContext(Object value)
   at OIL.ISA.Views.SettingsPage..ctor()
       at System.Reflection.RuntimeConstructorInfo.InternalInvoke(Object , Object[] , Boolean )
   Exception_EndOfInnerExceptionStack
   at ObjCRuntime.Runtime.ThrowException(IntPtr )
   at UIKit.UIApplication.UIApplicationMain(Int32 , String[] , IntPtr , IntPtr )
       at UIKit.UIApplication.Main(String[] , Type , Type )
   at OIL.ISA.Platforms.iOS.Program.Main(String[] args)
    Unhandled managed exception: Arg_TargetInvocationException (System.Reflection.TargetInvocationException)
   at ObjCRuntime.Runtime.ThrowException(IntPtr )
   at UIKit.UIApplication.UIApplicationMain(Int32 , String[] , IntPtr , IntPtr )
   at UIKit.UIApplication.Main(String[] , Type , Type )
   at OIL.ISA.Platforms.iOS.Program.Main(String[] args)
 (System.InvalidProgramException)
   at Microsoft.Maui.Controls.Internals.TypedBinding`2[[OIL.ISA.ViewModels.SettingsViewModel, OIL.ISA, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null],[System.String, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].ApplyCore(Object sourceObject, BindableObject target, BindableProperty property, Boolean fromTarget)
   at Microsoft.Maui.Controls.Internals.TypedBinding`2[[OIL.ISA.ViewModels.SettingsViewModel, OIL.ISA, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null],[System.String, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]].<…>

Searching for MAUI and InvalidProgramException does not give meaningful results. What's the cause of this exception?

As requested, here are some debugging details. The view model is like this:
public class SettingsViewModel : MainMenuPageViewModel {
    public SettingsViewModel(ViewModelServices services, IPushNotificationsService pushNotificationsService, IDatabaseUploader databaseUploader, ISecureStorage secureStorage) : base(services) {
        _pushNotificationsService = pushNotificationsService;
        _databaseUploader = databaseUploader;
        _databaseUploader.OnProgressChanged += OnDatabaseUploadProgressChanged;
        _secureStorage = secureStorage;

        Title = BaseApp.Settings;
        PushNotificationsEnabled = pushNotificationsService.DeviceToken != null;
        PushNotificationsEnabledMessage = PushNotificationsEnabled ? BaseApp.PushNotificationsEnabled : BaseApp.PushNotificationsNotEnabled;
        Logout = new Command(DoLogout);
        SendDatabase = new Command(UploadDatabase);
        ShowPushNotificationDetails = new Command(ShowNotificationDetails);

        using var context = new Context();
        UserInformation = BaseApp.LoggedInAs + "" "" + LocalSettings.GetUsername(context);
        UpdateSettingsInformation();
        Settings.OnRefresh += (sender, e) => UpdateSettingsInformation();
    }

    private void UpdateSettingsInformation() {
        StoreInformation = Settings.Store == null ? """" : Settings.Store.Number + "" - "" + Settings.Store.Name;
        OnPropertyChanged(nameof(StoreInformation));
    }

    private void OnDatabaseUploadProgressChanged(object sender, ProgressChangedEventArgs e) {
        Progress = e.ProgressPercentage / 100f;
    }

    private readonly IPushNotificationsService _pushNotificationsService;
    private readonly IDatabaseUploader _databaseUploader;
    private readonly ISecureStorage _secureStorage;

    public string UserInformation { get; }
    public string StoreInformation { get; private set; }
    public bool PushNotificationsEnabled { get; }
    public string PushNotificationsEnabledMessage { get; }
    public string AppVersion => AppInfo.VersionString;

    public ICommand Logout { get; }
    public ICommand SendDatabase { get; }
    public ICommand ShowPushNotificationDetails { get; }

    ...
}

and the page itself
<?xml version=""1.0"" encoding=""UTF-8"" ?>
<ContentPage
    xmlns=""http://schemas.microsoft.com/dotnet/2021/maui""
    xmlns:x=""http://schemas.microsoft.com/winfx/2009/xaml""
    xmlns:controls=""clr-namespace:OIL.ISA.Controls""
    xmlns:vm=""clr-namespace:OIL.ISA.ViewModels""
    xmlns:r=""clr-namespace:OIL.ISA.Resources""
    x:DataType=""vm:SettingsViewModel""
    x:Class=""OIL.ISA.Views.SettingsPage""
    Title=""{Binding Title}""
    Shell.NavBarIsVisible=""{Binding NavBarIsVisible}"">
    <ContentPage.Content>
        <AbsoluteLayout HorizontalOptions=""FillAndExpand"" VerticalOptions=""FillAndExpand"">
            <Image AbsoluteLayout.LayoutFlags=""All"" AbsoluteLayout.LayoutBounds=""0,0,1,1""
                   Source=""background.jpg"" Aspect=""AspectFill"" />
            <StackLayout AbsoluteLayout.LayoutFlags=""PositionProportional"" AbsoluteLayout.LayoutBounds="".5,0"" WidthRequest=""320""
                         BackgroundColor=""#D0FFFFFF"" Spacing=""16"" Padding=""16"" Margin=""0,16"">
                <Label Text=""{Binding UserInformation}"" HorizontalOptions=""Center"" />
                <Label Text=""{Binding StoreInformation}"" HorizontalOptions=""Center"" />
                <Button Style=""{StaticResource SettingsButton}""
                        Text=""{x:Static r:BaseApp.LogoutButton}""
                        Command=""{Binding Logout}"" />
                <Button Style=""{StaticResource SettingsButton}""
                        Text=""{x:Static r:BaseApp.SendDatabaseButton}""
                        Command=""{Binding SendDatabase}"" />
                <StackLayout Orientation=""Horizontal"">
                    <StackLayout.GestureRecognizers>    
                        <TapGestureRecognizer Command=""{Binding ShowPushNotificationDetails}"" />
                    </StackLayout.GestureRecognizers>
                    <CheckBox IsChecked=""{Binding PushNotificationsEnabled}"" VerticalOptions=""Center"" IsEnabled=""False"" />
                    <Label Text=""{Binding PushNotificationsEnabledMessage}"" VerticalOptions=""Center"" />
                </StackLayout>
                <Label Text=""{Binding Path=AppVersion, StringFormat={x:Static r:BaseApp.Version}}""
                       HorizontalOptions=""Center"" />
            </StackLayout>
            <controls:BlockingActivityIndicator
                AbsoluteLayout.LayoutFlags=""All"" AbsoluteLayout.LayoutBounds=""0,0,1,1""
                IsVisible=""{Binding IsBusy}""
                Message=""{Binding BusyMessage}""
                HasProgress=""{Binding HasProgress}""
                Progress=""{Binding Progress}"" />
        </AbsoluteLayout>
    </ContentPage.Content>
</ContentPage>

(but as you can see, I already found the answer, I just need to document it somewhere in case someone else runs into it. Which, believe it or not, happened to myself, two months later, in another view model in the application. This is now the third hit in Google, I hope it can be useful this way)
",76389919,"It turned out that my colleague used Resharper to analyze the code, and it had detected that one of the ViewModel properties:
public string AppVersion => AppInfo.VersionString;

could be changed into a static property:
public static string AppVersion => AppInfo.VersionString;

This did not cause any problems on Android or in the iOS Simulator, but was discovered only much later when testing a distribution build on an iPad. Making it an instance property again fixed the error.
",
76388166,Invalid Syntax on community.mysql.mysql_user with Ansible AWX,"I am trying to run a playbook through AWX where I want to create a mariadb user with permissions. The playbook works nicely when run from ansible on my local machine. However, when I try to run the job on AWX, I get the following error :

""module_stdout"": ""Traceback (most recent call last):\r\n  File ""/home/ansible/.ansible/tmp/ansible-tmp-1685691063.0199862-220-69171919718447/AnsiballZ_mysql_user.py"", line 107, in \r\n    _ansiballz_main()\r\n  File ""/home/ansible/.ansible/tmp/ansible-tmp-1685691063.0199862-220-69171919718447/AnsiballZ_mysql_user.py"", line 99, in _ansiballz_main\r\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\r\n  File ""/home/ansible/.ansible/tmp/ansible-tmp-1685691063.0199862-220-69171919718447/AnsiballZ_mysql_user.py"", line 48, in invoke_module\r\n    run_name='main', alter_sys=True)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 205, in run_module\r\n    return _run_module_code(code, init_globals, run_name, mod_spec)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/tmp/ansible_community.mysql.mysql_user_payload_r3e02o87/ansible_community.mysql.mysql_user_payload.zip/ansible_collections/community/mysql/plugins/modules/mysql_user.py"", line 352, in \r\n  File """", line 969, in _find_and_load\r\n  File """", line 958, in _find_and_load_unlocked\r\n  File """", line 664, in _load_unlocked\r\n  File """", line 634, in _load_backward_compatible\r\n  File ""/tmp/ansible_community.mysql.mysql_user_payload_r3e02o87/ansible_community.mysql.mysql_user_payload.zip/ansible_collections/community/mysql/plugins/module_utils/mysql.py"", line 22, in \r\n  File ""/usr/local/lib/python3.5/dist-packages/pymysql/init.py"", line 59, in \r\n    from . import connections  # noqa: E402\r\n  File ""/usr/local/lib/python3.5/dist-packages/pymysql/connections.py"", line 206\r\n    ):\r\n    ^\r\nSyntaxError: invalid syntax\r\n""

Here's the task for creating the user :
- name : Create mariadb user
  community.mysql.mysql_user:
    login_user: root
    login_password: ""{{ root_password }}""
    name: user
    password: ""{{ mariadb_user_pw }}""
    priv: ""*.*:PROCESS,SELECT,CREATE,CREATE TABLESPACE,INSERT""
    state: present

",76388426,"Ensure the hosts have the correct package installed.
Include this in your playbook:
- name: ensure mysql package for the mysql api is installed
  pip:
    name: pymysql
    executable: pip3

--
you should check your awx container.
either by logging in to the container itself, and check stuff manually, or you can do this w ansible as well.
- shell: locate ansible-galaxy
  delegate_to: localhost
  register: check_galaxy

- debug:
    msg: ""{{ check_galaxy.stdout }}""
  delegate_to: localhost

Then, once you have the output, simply check the installed roles and it versions
- shell: ansible-galaxy collection list
  delegate_to: localhost
  register: check_collection

- debug:
    msg: ""{{ check_collection }}""
  delegate_to: localhost

",
76390844,The difference between UserDetailsService and the class that implements it,"Does it make sense to create your own class that implements the UserDetailsService interface? In many examples, I see how the UserDetailsService is embedded in the security configuration class, and not the class that implements it. What's the difference?
And why, when I enter the wrong username and password, I don't get the error message that I registered in PersonDetailsService? ""Bad credentials"" is coming out. I'm a beginner and just started learning spring security
@Configuration
@EnableWebSecurity
public class SecurityConfig {
    private final PersonDetailsService personDetailsService;

    @Autowired
    public SecurityConfig(PersonDetailsService personDetailsService) {
        this.personDetailsService = personDetailsService;
    }

    @Autowired
    void configure(AuthenticationManagerBuilder builder) throws Exception {
        builder.userDetailsService(personDetailsService);
    }

    @Bean
    public PasswordEncoder getPasswordEncoder() {
        return NoOpPasswordEncoder.getInstance();
    }

}


@Service
public class PersonDetailsService implements UserDetailsService {
    private final PeopleRepository peopleRepository;

    @Autowired
    public PersonDetailsService(PeopleRepository peopleRepository) {
        this.peopleRepository = peopleRepository;
    }

    @Override
    public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {
        Optional<Person> person = peopleRepository.findByUsername(username);

        if (person.isEmpty())
            throw new UsernameNotFoundException(""User not found"");

        return new PersonDetails(person.get());
    }
}


",76391301,"For your first question you spring security can authenticate multiple ways such as in memory and DB . in UserdetailsService and loadUserByUsername method you can customize how to authenticate users . and if you want DB mode you have multiple tables is DB and you can tell spring for finding username and password which table and columns must be check . for your second question put break point on your if statement and check what is person found . I think if it doesn't return username you have to use this if :
if(person.get()==null)
   throw new UsernameNotFoundException(""User not found"");

",
76389703,User Document ID is the same as User ID in Firestore,"I try to store users inside collection in Firestore as documents and want to make every User ID is same as Document ID..
I tried but it still gives me another ID for the user:
Future<void> userSetupDone() async {
CollectionReference users = FirebaseFirestore.instance.collection('Users');

final docUser = FirebaseFirestore.instance.collection('Users').doc();

FirebaseAuth auth = FirebaseAuth.instance;
String? uid = docUser.id;
// String? uid = auth.currentUser?.uid.toString();
String? email = auth.currentUser?.email.toString();
String? phone = auth.currentUser?.phoneNumber.toString();
String? displayName = auth.currentUser?.displayName.toString();

users.add({'Uid': uid, ""Email"": email, ""Phone"": phone, ""Name"": displayName});
return;
}


",76389923,"You have to update uid immediately after adding data to the database. Because you can not get id before adding that data from the Firestore Database.
You will get it by following code:
 Future<void> userSetupDone() async {
    CollectionReference users = FirebaseFirestore.instance.collection('Users');
    
    final docUser = FirebaseFirestore.instance.collection('Users').doc();
    
    FirebaseAuth auth = FirebaseAuth.instance;

    String? email = auth.currentUser?.email.toString();
    String? phone = auth.currentUser?.phoneNumber.toString();
    String? displayName = auth.currentUser?.displayName.toString();
    
    DocumentReference reference= await users.add({'Uid': '', ""Email"": email, ""Phone"": phone, ""Name"": displayName});
    await reference.update({""Uid"": reference.id});
    return;
}

",
76381567,How can I resolve the 'Could not find a valid Docker environment' error when using Testcontainers with Quarkus?,"I try to use Testcontainers (v1.18.0) for tests in a Quarkus (v2.16.4.Final, java: Amazon Corretto 17) application. When I start it up I get the following stack trace:
2023-06-01 13:11:10,326 INFO  [org.tes.uti.ImageNameSubstitutor] (main) Image name substitution will be performed by: DefaultImageNameSubstitutor (composite of 'ConfigurationFileImageNameSubstitutor' and 'PrefixingImageNameSubstitutor')
2023-06-01 13:11:10,502 INFO  [org.tes.doc.DockerClientProviderStrategy] (pool-3-thread-1) Loaded org.testcontainers.dockerclient.NpipeSocketClientProviderStrategy from ~/.testcontainers.properties, will try it first
2023-06-01 13:11:10,599 INFO  [org.tes.doc.DockerMachineClientProviderStrategy] (pool-3-thread-1) docker-machine executable was not found on PATH ([C:\Python311\Scripts\, C:\Python311\, C:\Program Files (x86)\Microsoft SDKs\Azure\CLI2\wbin, C:\WINDOWS\system32, C:\WINDOWS, C:\WINDOWS\System32\Wbem, C:\WINDOWS\System32\WindowsPowerShell\v1.0\, C:\WINDOWS\System32\OpenSSH\, C:\ProgramData\chocolatey\bin, C:\Program Files\Azure Data Studio\bin, C:\Program Files\Microsoft\Azure Functions Core Tools\, C:\Program Files\PuTTY\, C:\Program Files\TortoiseGit\bin, C:\Program Files\Java\jdk17.0.4_8\bin, C:\Program Files\Java\scripts, C:\Program Files\nodejs\, C:\Program Files\dotnet\, C:\Program Files\TortoiseSVN\bin, C:\Program Files\Git\cmd, C:\Program Files\Maven\apache-maven-3.8.6\bin, , C:\Program Files\Docker\Docker\resources\bin, C:\Users\***\AppData\Local\Microsoft\WindowsApps, C:\Program Files\Azure Data Studio\bin, C:\Users\***\AppData\Local\Programs\Microsoft VS Code\bin, C:\Users\***\AppData\Roaming\npm, C:\Program Files\Java\jdk11.0.16_8\bin, C:\Users\***\.dotnet\tools, C:\Users\***\AppData\Local\JetBrains\Toolbox\scripts, C:\Users\***\.azure-kubelogin, C:\Users\***\AppData\Local\Microsoft\WindowsApps, C:\Program Files\Docker\Docker\resources\bin])
2023-06-01 13:11:10,601 ERROR [org.tes.doc.DockerClientProviderStrategy] (pool-3-thread-1) Could not find a valid Docker environment. Please check configuration. Attempted configurations were:
    NpipeSocketClientProviderStrategy: failed with exception NoClassDefFoundError (com/github/dockerjava/api/command/LoadImageAsyncCmd). Root cause ClassNotFoundException (com.github.dockerjava.api.command.LoadImageAsyncCmd)As no valid configuration was found, execution cannot continue.
See https://www.testcontainers.org/on_failure.html for more details.

java.lang.RuntimeException: java.lang.reflect.InvocationTargetException

    at io.quarkus.test.junit.QuarkusTestExtension.throwBootFailureException(QuarkusTestExtension.java:625)
    at io.quarkus.test.junit.QuarkusTestExtension.interceptTestClassConstructor(QuarkusTestExtension.java:696)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:73)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:62)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:363)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:310)
    at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:286)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:278)
    at java.base/java.util.Optional.orElseGet(Optional.java:364)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:277)
    at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$prepare$0(TestMethodTestDescriptor.java:105)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:104)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.prepare(TestMethodTestDescriptor.java:68)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$prepare$2(NodeTestTask.java:123)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.prepare(NodeTestTask.java:123)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:90)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:57)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at io.quarkus.test.junit.QuarkusTestExtension.doJavaStart(QuarkusTestExtension.java:237)
    at io.quarkus.test.junit.QuarkusTestExtension.ensureStarted(QuarkusTestExtension.java:592)
    at io.quarkus.test.junit.QuarkusTestExtension.beforeAll(QuarkusTestExtension.java:640)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$12(ClassBasedTestDescriptor.java:395)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:395)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:211)
    at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:84)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
    ... 36 more
Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Unable to start Quarkus test resource class com.***.locref.DatabaseResource
    at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:315)
    at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:320)
    at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1807)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.RuntimeException: Unable to start Quarkus test resource class com.***.locref.DatabaseResource
    at io.quarkus.test.common.TestResourceManager$TestResourceEntryRunnable.run(TestResourceManager.java:487)
    at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
    ... 3 more
Caused by: java.lang.IllegalStateException: Could not find a valid Docker environment. Please see logs and check configuration
    at org.testcontainers.dockerclient.DockerClientProviderStrategy.lambda$getFirstValidStrategy$7(DockerClientProviderStrategy.java:256)
    at java.base/java.util.Optional.orElseThrow(Optional.java:403)
    at org.testcontainers.dockerclient.DockerClientProviderStrategy.getFirstValidStrategy(DockerClientProviderStrategy.java:247)
    at org.testcontainers.DockerClientFactory.getOrInitializeStrategy(DockerClientFactory.java:150)
    at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:186)
    at org.testcontainers.DockerClientFactory$1.getDockerClient(DockerClientFactory.java:104)
    at com.github.dockerjava.api.DockerClientDelegate.authConfig(DockerClientDelegate.java:108)
    at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:321)
    at com.***.locref.DatabaseResource.start(DatabaseResource.java:28)
    at io.quarkus.test.common.TestResourceManager$TestResourceEntryRunnable.run(TestResourceManager.java:481)
    ... 4 more

I have a class that implements QuarkusTestResourceLifecycleManager and starts a PostgreSQLContainer. I add it to a Test class annotated with @QuarkusTest and @QuarkusTestResource.
I used a very similar setup in a plain Java application without any problems. Problems only arise when Quarkus appears in the loop. I researched all exceptions related to the docker runtime and the NpipeSocketClientProviderStrategy but was not able to find a solution.
",76388484,"The versions of Quarkus and Testcontainers were incompatible. Resolving the version of Testcontainers via quarkus-bom in maven dependencyManagement resulted in 1.17.6 which works as expected.
",
76391249,Is there a way to maintain C++ array formatting when using VScode's default formatter upon saving?,"When using the default VScode formatter for c++. I want to keep format on save but I don't want the array to be changed this drastically.
It changes an array from this
    const int level[] =
    {
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3,
        0, 1, 0, 0, 2, 0, 3, 3, 3, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 3, 3, 3, 0, 0, 0, 1, 1, 1, 2, 0, 0,
        0, 0, 1, 0, 3, 0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 0,
        2, 0, 1, 0, 3, 0, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 3, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1,
    };


to this
    const int level[] = {
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        1,
        0,
        1,
        1,
        1,
        1,
        1,
        1,
        ...

    };

I want to keep format on save but I don't want the array to be changed this drastically.
",76391358,"Not sure if you are trying to keep using the default formatting engine for some reason, I can't find an option to change this behavior using the default, but this behavior is not present when setting the option  C_Cpp:Formatting (Extensions > C/C++ > Formatting > C_Cpp: Formatting) to vcFormat. If you just want format on save to not elongate your arrays Id use that engine option.
",
76389050,Cleaning data in R by using a reference date,"Consider the following dataset
mydata<-data.frame(id = c(""R007"", ""R008"", ""R008"", ""R009"", ""R009""),
                   statenumber= c(1, 2, 3, 4, 5),
                   startdate = c(20080101, 20080101, 20120301,20120101, 20121001),
                   enddate = c(20121201, 20120301, 20121201, 20121001, 20121201))

#if necessary we can write the dates as dates
mydata$startdate<-as.Date(as.character(mydata$startdate), ""%Y%m%d"")
mydata$enddate<-as.Date(as.character(mydata$enddate), ""%Y%m%d"")

The explanation of the dataset is as follows. A person with id number R007 lives during 2008 and 2012 at the same address. Person R008 lives during 2008 and 2012 at two addresses (state number). So for the years 2008-2011, he lives at address 2, and for the year 2012 he lives at address 3. Person R009 is only available in the file since 2012. During the first year of 2012 he lives at address 4 and in the last two months he lives at address 5.
Now I want to rearrange this dataset such that I have one address line per person per year. I would like to do this by making use of a reference month (say June). In other words, if a person lives in June at a particular address, I assign that address to him for the entire year. The dataset would then look as follows
endresult<-data.frame(id = c(""R007"", ""R007"",""R007"",""R007"",""R007"",
                             ""R008"", ""R008"", ""R008"", ""R008"",""R008"", ""R009""),
                      statenumber = c(1,1,1,1,1,2,2,2,2,3,4),
                      year = c(2008, 2009, 2010, 2011, 2012,
                               2008,2009,2010,2011,2012, 2012))

Does anyone know how to get to this endresult dataset? I appreciate any help.
",76389929,"Two basic approaches: (1) do a yearly sequence, conditioning on whether the start is before and if the end is after June (06), used in the base R approach; (2) do a monthly sequence, conditioning (group-filtering) each year on whether ""06"" is in one of the months. They both work, which you choose depends on comfort/preference.
base R
Since this example uses ""sequence by year"", I'll use the fact that POSIXlt (notice the l, as opposed to the more common POSIXct) is really a list with elements for year, month, etc:
dput(as.POSIXlt(Sys.time()))
# structure(list(sec = 21.7977600097656, min = 30L, hour = 8L, 
#     mday = 2L, mon = 5L, year = 123L, wday = 5L, yday = 152L, 
#     isdst = 1L, zone = ""EDT"", gmtoff = -14400L), class = c(""POSIXlt"", 
# ""POSIXt""), tzone = c("""", ""EST"", ""EDT""))

With that, we can easily $-index the list and compare the month as a value. NOTE: ?POSIXlt shows that mon is ‘mon’ 0-11: months after the first of the year, meaning that June is 5 in 0-based months. (Ergo the use of 5 below.)
fun <- function(sd, ed) {
  sdlt <- as.POSIXlt(sd)
  edlt <- as.POSIXlt(ed)
  if (sdlt$mon > 5) sdlt$year <- sdlt$year+1
  if (edlt$mon < 5) edlt$year <- edlt$year-1
  if (sdlt <= edlt) unique(format(seq.Date(as.Date(sdlt), as.Date(edlt), by = ""year""), format=""%Y""))
}
years <- Map(fun, mydata$startdate, mydata$enddate)
str(years)
# List of 5
#  $ : chr [1:5] ""2008"" ""2009"" ""2010"" ""2011"" ...
#  $ : chr [1:4] ""2008"" ""2009"" ""2010"" ""2011""
#  $ : chr ""2012""
#  $ : chr ""2012""
#  $ : NULL
out <- data.frame(
  id = rep(mydata$id, lengths(years)), 
  statenumber = rep(mydata$statenumber, lengths(years)), 
  year = unlist(years))
out
#      id statenumber year
# 1  R007           1 2008
# 2  R007           1 2009
# 3  R007           1 2010
# 4  R007           1 2011
# 5  R007           1 2012
# 6  R008           2 2008
# 7  R008           2 2009
# 8  R008           2 2010
# 9  R008           2 2011
# 10 R008           3 2012
# 11 R009           4 2012

If there's a chance to have duplicates (repeated states for an id that gap within the same year), then you can use unique(out).
Map just ""zips"" the data together. The individual calls to fun unrolled would look like this:
list(
  fun(mydata$startdate[1], mydata$enddate[1]),
  fun(mydata$startdate[2], mydata$enddate[2]),
  fun(mydata$startdate[3], mydata$enddate[3]),
  fun(mydata$startdate[4], mydata$enddate[4]),
  fun(mydata$startdate[5], mydata$enddate[5])
)

The inclusion of ed on the end of c(seq.Date(..), ed) is to guard against the fact that seq may not include the year of the enddate. In this case, it ensures that R008 in state 2 sees 2012.
dplyr
In this (and the data.table) section, we'll use the monthly sequence instead, using format=""%m"" as the month. Dissimilar from POSIXlt above (June is 5), reading ?%strptime for the %-codes defines %m as ‘%m’ Month as decimal number (01-12), so June is back to ""06"".
library(dplyr)
mydata %>%
  rowwise() %>%
  summarize(
    id, statenumber, 
    dates = seq(startdate, enddate, by = ""month""), 
    year = format(dates, format = ""%Y"")) %>%
  group_by(id, statenumber, year) %>%
  filter(any(format(dates, format = ""%m"") == ""06"")) %>%
  distinct(id, statenumber, year) %>%
  ungroup()
# # A tibble: 11 × 3
#    id    statenumber year 
#    <chr>       <dbl> <chr>
#  1 R007            1 2008 
#  2 R007            1 2009 
#  3 R007            1 2010 
#  4 R007            1 2011 
#  5 R007            1 2012 
#  6 R008            2 2008 
#  7 R008            2 2009 
#  8 R008            2 2010 
#  9 R008            2 2011 
# 10 R008            3 2012 
# 11 R009            4 2012 

I generally try to avoid rowwise when able, but this problem does need to be executed one row at a time (which is effectively what Map is doing in the base R solution above).
data.table
library(data.table)
as.data.table(mydata)[, .(id, statenumber, dates = seq(startdate, enddate, by = ""month"")), by = .(seq(nrow(mydata)))
  ][, year := format(dates, format=""%Y"")
  ][, .SD[any(format(dates, format=""%m"") == ""06""),], by = .(id, statenumber, year)
  ][, c(""seq"", ""dates"") := NULL
  ][, unique(.SD)]
#         id statenumber   year
#     <char>       <num> <char>
#  1:   R007           1   2008
#  2:   R007           1   2009
#  3:   R007           1   2010
#  4:   R007           1   2011
#  5:   R007           1   2012
#  6:   R008           2   2008
#  7:   R008           2   2009
#  8:   R008           2   2010
#  9:   R008           2   2011
# 10:   R008           3   2012
# 11:   R009           4   2012

",
76388250,Repeat a formula in google sheet within a single cell using different values,"So to summarize what i want:
Let's say i have
   A         B
1 ""Tommy""    ""1,2,3""
2 ""Berry""    ""3,4,5""
3 ""Hank""     ""1,4,5""
4
5
6 ""1""        
7 ""5""

I would like B6 to show ""Tommy Hank"" and B7 to show ""Berry Hank""
If have managed to create this formula;
=IF(ISERROR(MATCH($A6,SPLIT(B1, "","" , 1,1),0)),"""",A1)

This formula will give me back the value ""Tommy"" but not Hank.
I want to list all corresponding A values for every B cell that contains the value ""1"".
I tried some things with arrayformula like =IF(ISERROR(MATCH($A6,SPLIT(B1:B3, "","" , 1,1),0)),"""",A1:A3) but i don't really understand what i'm doing here.
",76388539,"I was going to suggest
=ArrayFormula(
   textjoin("" "",,
     query(
       {A$1:A$3,"",""&B$1:B$3&"",""},
       ""select Col1 
        where Col2 contains'""&"",""&A6&"",""&""'""
     )
   )
 )


",
76391142,FutureProvider does not return data to UI,"I'm new using Riverpod and its providers. I have 2 almost identical futureProviders but one of them does not return data to UI even though API returns data.
Here is UI part

class ProductDetailSimilarProductsWidget extends ConsumerWidget {
  const ProductDetailSimilarProductsWidget(this.product, {super.key});

  final ProductDetailModel product;

  @override
  Widget build(BuildContext context, WidgetRef ref) {
    var value = ref.watch(getSimilarProductsFutureProvider([product.Id, product.KategoriId]));
    return Container(
      height: MediaQuery.of(context).size.height * 0.5,
      width: MediaQuery.of(context).size.width,
      child: value.when(
        data: (data) {
          print(""data length:${data.length}"");
          return ListView.builder(
            itemCount: data.length,
            shrinkWrap: true,
            scrollDirection: Axis.horizontal,
            itemBuilder: (context, index) {
              return Padding(
                padding: EdgeInsets.only(left: AppTheme.mediumPadding, right: AppTheme.mediumPadding),
                child: ProductGridContainerWidget(product: data[index]),
              );
            },
          );
        },
        error: (err, trace) {
          return Text(err.toString());
        },
        loading: () => AppTheme.spinkit,
      ),
    );
  }
}

Here is futureProvider object

final getSimilarProductsFutureProvider = FutureProvider.family<List<Products>, List<dynamic>>((ref, value) async {
  var result = await ProductDetailData().getSimilarProducts(productId: value[0], categoryId: value[1]);
  print(""result: ${result.length}"");
  return result;
});

Finally, the method does API call
    var service = ApiService();
    var sharedPreferencesManager = SharedPreferencesManager();
    await sharedPreferencesManager.init();
    var langId = await sharedPreferencesManager.getLanguageId();
    var countryId = await sharedPreferencesManager.getCountryId();
    var result =
        await service.getRequest(subUrl: ""${ApiRoutes.getSimilarProducts}?productId=$productId&langId=$langId&countryId=$countryId&categoryId=$categoryId"");
    List<Products> products = [];
    var elements = json.decode(result.body);
    await Future.forEach(elements, (element) => products.add(Products.fromMap(element as Map<String, dynamic>)));
    return products;

Inside provider object, result.length variable prints 10 and it is expected. But inside the UI part, widget stucks on loading part and does not print data.length variable.
Is there something wrong in my codes? Also API works fine and successfully returns data.
",76391393,"Your mistake is probably using the list as an argument for .family.

Ideally, the parameter should either be a primitive (bool/int/double/String), a constant (providers), or an immutable object that overrides == and hashCode.

Try using Records or create an immutable object with the two required fields.
",
76389061,Non-Repeating Random Numbers for Multiple Variables,"I am trying to create a function that applies random values to a range of parameters that are used in another function where the random sequence does not repeat.
The reason: used for random hyper parameter tuning and to cut down on processing by not repeating sequence.
Example Code:

num_evals = 2500

parameters = {
        'n_parameter_1': range(2,100),
        'n_parameter_2': range(1,20),
        'n_parameter_3': range(2,150),
              }  

for i in range(num_evals):
        n_parameter_1 = random.choice(parameters['n_parameter_1'])
        n_parameter_2 = random.choice(parameters['n_parameter_2'])
        n_parameter_3 = random.choice(parameters['n_parameter_3'])


The results then get populated to a file with the random parameters used to generate.
I need help with the second bit.
As this function runs over time, you start getting sequences that look like this:
n_parameter_1 = 54, n_parameter_2 = 15, n_parameter_3 = 120
n_parameter_1 = 10, n_parameter_2 = 12, n_parameter_3 = 89
n_parameter_1 = 54, n_parameter_2 = 15, n_parameter_3 = 120
I want to avoid the last sequence by either capturing the sequences in a list or dataframe etc. (already saving the sequences) that the function checks and then generates a new/different sequence if there is a duplicate sequence in the list.
Thanks for the help in advance.
",76389938,"You can store sequences in a set and then check if a sequence is already in the set.
num_evals = 2500
i = 0

parameters = {
        'n_parameter_1': range(2,100),
        'n_parameter_2': range(1,20),
        'n_parameter_3': range(2,150),
              }  

sequences = set()

while i < num_evals:
        n_parameter_1 = random.choice(parameters['n_parameter_1'])
        n_parameter_2 = random.choice(parameters['n_parameter_2'])
        n_parameter_3 = random.choice(parameters['n_parameter_3'])
        sequence = (n_parameter_1, n_parameter_2, n_parameter_3)
        if sequence not in sequences:
            sequences.add(sequence)
            i += 1

The for loop has been changed to a while loop to allow for the conditional increment.
",
76389663,Algorithm to list all combinations from a table where data is present or NULL,"I have an Excel file (which can optionally be loaded into a database, and into an array of arrays of course) with values such as:




A
B
C




NULL
NULL
zxy


xyz
xzy
NULL


xyz
xzy
xyy


yzy
yyx
yxy


NULL
NULL
xyx


xyz
NULL
yxx




and so on.
There are thousands of values.
Is there any known algorithm to come up with all possible combinations of rows where values are not ""NULL""?
For example for the table above the result would be:




A
B
C
Number of occurrences




NULL
NULL
*
2


*
*
NULL
1


*
*
*
2


*
NULL
*
1




I feel like it is a typical task, but cannot find the algorithm anywhere. Would appreciate your help a lot.
",76389956,"If you are tempted to use pandas :
#pip install pandas
import pandas as pd

df = pd.read_excel(""file.xlsx"")

out = (
    df.replace("".+"", ""*"", regex=True).fillna(""NULL"")
        .groupby(list(df), group_keys=False, sort=False)
        .size().reset_index(name=""Number of occurrences"")
)

Output :
print(out)
    
      A     B     C  Number of occurrences
0  NULL  NULL     *                      2
1     *     *  NULL                      1
2     *     *     *                      2
3     *  NULL     *                      1

",
76388396,Unable to extract id from json response in jmeter,"My am calling POST on an API. The response has
{
  ""id"" : ""1234”,
---
}

I have following regular expression extractor:

However in the DELETE HTTP Request on API/${id}, I am getting API/NOT FOUND.
Please suggest whats the issue with my regular expression ?
",76388560,"JSON is not a regular language hence using regular expressions for it is not the best idea
Consider using JSON Extractor instead, the relevant JSONPath expression is just id
Demo:

",
76391063,ESLint error: Failed to load 'eslint-plugin-jsdoc' in IntelliJ on Ubuntu - troubleshooting tips?,"I am struggling with ESLint not being able to load in IntelliJ 2023.1 on Ubuntu 22.04.
It seems like there are several syntax errors in the libraries, but I just could not belive that.
What could I do?
Best regards
Peter
PS: error in detail:


If I correct this specific one (yes, I should not overwrite libraries, especially when I'm not an expert on that stuff), a new syntax error comes again.
",76391398,"What Node.js version do you have? Looks like it doesn't support the nullish coalescing operator; please make sure that the interpreter set up in Settings | Languages & Frameworks | Node.js is v. 14 or higher, see https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Nullish_coalescing#browser_compatibility
",
76384879,Can't access localhost sites anymore,"No access to localhost sites anymore.
I have IIS 6.0 running on a Windows 7 64 bit PC where I work on local websites.
I have 4 sites that were working until yesterday and now give me a 404 error when I try to access. I have tried IE, Chrome, Brave and Firefox.
One of the sites I'm working on for our karate club is a login for weekend courses where the member's card is checked by a webcam in a box to allow them to log in. It was working (they all were), but I needed this site to be HTTPS as the javascripts for the camera must be downloaded using this protocol. The location of the course may not have internet access so these must be loaded from the localhost site.
I was messing about trying creating self signed certificates for localhost which I managed to do, but i could not get the site to work using https. I then removed the certificates but the sites would not then access normally (http).
I cleared the cache of each browser and have reset IIS and restarted the pc. I also went through and disabled the world wide services, restarted the PC and restarted WWS.
Nothing has helped.
Any advice/pointers would be a great help.
Thanks in Advance,
Andy
",76389984,"I solved the problem myself by doing a system restore to the point just before i installed the IIS toolkit. My sites are now back to working using http again.
I'm guessing that it was something in the toolkit that caused the problem.
Thanks for the advice anyway.
Regards,
Andy
",
76388415,Why is data.posts undefined when accessing using subscriber method in redux?,"How can I access complete state in redux?
const { createStore } = require(""redux"");

const initialState = {
  posts: [
    { id: 1, title: ""Post one"" },
    { id: 2, title: ""Post two"" },
  ],
};

My Posts Reducer
const PostsReducer = (state = initialState, action) => {
  switch (action.type) {
    case ""add_post"":
      return {
        ...state,
        posts: [...state.posts, action.payload],
      };

    case ""delete_post"":
      return {
        ...state,
        posts: state.posts.filter((post) => post.id !== action.payload),
      };

    case ""get_all_posts"":
      return state.posts;

    default:
      return state;
  }
};

const store = createStore(PostsReducer);

My Action Creators:
const GetAllPosts = () => {
  return {
    type: ""get_all_posts"",
  };
};

const AddPost = (payload) => {
  return { type: ""add_post"", payload };
};

const removePost = (payload) => {
  return { type: ""delete_post"", payload };
};

Accessing Store data:
store.subscribe(() => {
  const data = store.getState();
  console.log(data.posts);
});

store.dispatch(GetAllPosts());

While I am console logging data.posts it returns undefined.
I am accessing using subscriber method:
store.subscribe(() => {
  const data = store.getState();
  console.log(data.posts);
});

store.dispatch(GetAllPosts());

Note:
I am Using Redux(4.2.1)
By using GetAllPosts(), I am able to access data, but I cannot get data.posts in console.
",76388571,"The get_all_posts case is returning the state.posts array directly instead of setting a state object with a posts property.
case ""get_all_posts"":
  return state.posts; // <-- just the posts array

Update get_all_posts case to return the correct state invariant, likely the initial state value with all the posts included.
Example:
const initialState = {
  posts: [
    { id: 1, title: ""Post one"" },
    { id: 2, title: ""Post two"" },
  ],
};

const PostsReducer = (state = initialState, action) => {
  switch (action.type) {
    case ""add_post"":
      return {
        ...state,
        posts: [...state.posts, action.payload],
      };

    case ""delete_post"":
      return {
        ...state,
        posts: state.posts.filter((post) => post.id !== action.payload),
      };

    case ""get_all_posts"":
      return { ...initialState };

    default:
      return state;
  }
};

",
76390677,Why does my reactjs project on hostinger give a 404 error when opening a new tab or refreshing the page?,"I have deployed reactjs project on hostinger where i open any new tab or refresh the page it give error 404
i have used latest version of react router dom
Index.js
<Browserrouter>
<App/>
<Browserrouter/>


App.js

<Routes>
<Route path=""/"" element={<Home/>}/>
        <Route path=""/about-us"" element={<Aboutus/>}/>
        <Route path=""/contact-us"" element={<Contactus/>}/>
        <Route path=""/career"" element={<Career/>}/>
        <Route path=""/work"" element={<Work/>}/> 
        <Route path=""/services"" element={<ServicesMain/>}/>

<Routes/>

",76391409,"The issue is not with the React but your hosting config. You need to add rewrite rules by adding .htaccess file inside your 'public' folder with the following code.
<IfModule mod_rewrite.c>
  RewriteEngine On
  RewriteBase /
  RewriteRule ^index\.html$ - [L]
  RewriteCond %{REQUEST_FILENAME} !-f
  RewriteCond %{REQUEST_FILENAME} !-d
  RewriteCond %{REQUEST_FILENAME} !-l
  RewriteRule . /index.html [L]
</IfModule>

",
76389787,Passing variables to a variable in bash,"I'm writing custom logging for the bash:
#!/usr/bin/env bash

set -e +x

SCRIPT_LOG_NAME='[MYSCRIPT]'
SCRIPT_LOG_DATETIME_FORMAT=""+%Y-%m-%d %H:%M:%S""
SCRIPT_LOG_LEVEL_INFO='INFO'
SCRIPT_LOG_PATTERN=""${SCRIPT_LOG_NAME} $(date ""${SCRIPT_LOG_DATETIME_FORMAT}"") $1 : $2""

log_general() {
  ...
}

log_info() {
  log_general ""$SCRIPT_LOG_LEVEL_INFO"" ""$1""
}

And I want to pass variables $SCRIPT_LOG_LEVEL_INFO and ""$1"" as parameters to the function log_general(). And use this parameters in the $SCRIPT_LOG_PATTERN to achive output: 
[MYSCRIPT] 2023-01-01 10:10:10 INFO : Something logged
I tried something like:
log_general() {
  echo -e ""$SCRIPT_LOG_PATTERN"" ""$1"" ""$2""
}

Or
SCRIPT_LOG_LEVEL=''
SCRIPT_LOG_PATTERN=""${SCRIPT_LOG_NAME} $(date ""${SCRIPT_LOG_DATETIME_FORMAT}"") ${SCRIPT_LOG_LEVEL} : ""

log_general() {
  SCRIPT_LOG_LEVEL=$SCRIPT_LOG_LEVEL_INFO
  echo -e ""${SCRIPT_LOG_PATTERN}${1}""
}

log_info() {
  log_general ""$1""
}

But I could not achieve the desired result. Can you help me, please?
",76389989,"If you'd like to use ${SCRIPT_LOG_PATTERN} as a global variable and later on substitute the ${1} and ${2} you would have to escape the $ sign and then evaluate the string later on in log_general.
This would look something like this:
#!/bin/bash

set -e +x

SCRIPT_LOG_NAME='[MYSCRIPT]'
SCRIPT_LOG_DATETIME_FORMAT=""+%Y-%m-%d %H:%M:%S""
SCRIPT_LOG_LEVEL_INFO='INFO'
SCRIPT_LOG_PATTERN=""${SCRIPT_LOG_NAME} $(date ""${SCRIPT_LOG_DATETIME_FORMAT}"") \$1 : \$2""

log_general() {
        eval echo ""${SCRIPT_LOG_PATTERN}""
}

log_info() {
        log_general ""${SCRIPT_LOG_LEVEL_INFO}"" ""${1}""
}

Usage:
$ source logging_test.sh
$ log_info ""my custom message""
[MYSCRIPT] 2023-06-02 14:03:27 INFO : my custom message


I'd also recommend escaping the date command since otherwise the date will be parsed once, here is what I mean:
$ log_info ""asdads""; sleep 5; log_info ""asdsad""
[MYSCRIPT] 2023-06-02 14:06:28 INFO : asdads
[MYSCRIPT] 2023-06-02 14:06:28 INFO : asdsad

As you can see the time does not change even with 5 seconds of sleep.
To fix this escape the date as well like this:
SCRIPT_LOG_PATTERN=""${SCRIPT_LOG_NAME} \$(date '${SCRIPT_LOG_DATETIME_FORMAT}') \$1 : \$2""

Output now:
$ log_info ""asdads""; sleep 5; log_info ""asdsad""
[MYSCRIPT] 2023-06-02 14:09:19 INFO : asdads
[MYSCRIPT] 2023-06-02 14:09:25 INFO : asdsad

",
76388237,"Is there an equivalent of Vue's `v-show` in Angular so that the component is hidden, but still in DOM?","There are some situations where it's better to keep a hidden component in DOM. In Vue there the v-show in addition to the v-if. But in Angular I didn't find an equivalent? So the best solution is to use s.th. like [ngStyle]=""{ 'display': showMe ? 'block' : 'none' }""?
EDIT: I also could set the hidden attribute, which should have a very similar effect, e.g. [hidden]=""!showMe"".
Is there actually a reason why Vue.js did include v-show instead of simply using :hidden?
",76388580,"v-show is simply sugar syntax for adding style=""display: none"". While I'm not aware of anything like that built into angular, it's trivial to build your own directive that will achieve the same result (code not tested):
@Directive({
  selector: 'appHidden',
})
class HiddenDirective {
 @Input('appHidden')
 isHidden: boolean = false;

 @HostBinding('style.display')
 get displayStyle() {
   return this.isHidden ? 'none' : undefined;
 }
}

Usage:
<div [appHidden]=""shouldBeHidden""></div>

",
76390978,Spring Boot custom constraint validator with multiple values,"I want to add 2 values for my custom constraint validator because I have 2 feature flags:
 @JsonProperty(value = ""name"")
    @BlockedWithoutEnabledFeatureFlag(feature = FeatureFlag.AAA, values = {""aaa"", ""bbb""})
    @BlockedWithoutEnabledFeatureFlag(feature = FeatureFlag.BBB, values = {""ccc"", ""ddd""})
    private String parameter;

where:
@Constraint(validatedBy = {BlockedWithoutEnabledFeatureFlagValidator.class})
@Target({FIELD, PARAMETER})
@Retention(RetentionPolicy.RUNTIME)
@ReportAsSingleViolation
public @interface BlockedWithoutEnabledFeatureFlag {

    String message() default ""{validation.constraints.BlockedWithoutEnabledFeatureFlag.message}"";

    Class<?>[] groups() default {};

    Class<? extends Payload>[] payload() default {};

    FeatureFlag feature();

    String[] values() default {};
}

and implementation:
public class BlockedWithoutEnabledFeatureFlagValidator implements ConstraintValidator<BlockedWithoutEnabledFeatureFlag, Object> {
    private final FeatureFlagService featureFlagService;

    private List<String> blocked;
    private FeatureFlag feature;

    @Override
    public void initialize(BlockedWithoutEnabledFeatureFlag constraintAnnotation) {
        blocked = Arrays.asList(constraintAnnotation.values());
        feature = constraintAnnotation.feature();
    }

@Override
    public boolean isValid(Object value, ConstraintValidatorContext context) {


But now I get compilation error ""Duplicate annotation"". How to do that?
",76391418,"You should make your BlockedWithoutEnabledFeatureFlag repeatable. To do this:

Create a new ""top-level"" annotation:

@Target({ ElementType.FIELD })
@Retention(RetentionPolicy.RUNTIME)
public @interface RepeatableBlockedWithoutEnabledFeatureFlag {
    BlockedWithoutEnabledFeatureFlag[] value();
}


Add @Repeatable to your BlockedWithoutEnabledFeatureFlag annotation

...
@Repeatable(RepeatableBlockedWithoutEnabledFeatureFlag.class)
public @interface BlockedWithoutEnabledFeatureFlag {
...

Then you should be able to add more than one @BlockedWithoutEnabledFeatureFlag annotation.
",
76388242,Typescript/Angular: Wait for sequentially delayed async tasks to be completed,"Even though there's multiple similar questions here on StackOverflow, I haven't been able to solve this, please bear with me:
In an Angular 16 application I have a functionality to batch rename hundreds of users. Thus, I have an array of RenameUser objects, each one having an async method .executeRename() performing an API call to rename a user. To not overwhelm the API, these should be triggered with a 200ms delay between them. Once all users have been renamed, I would like to run other tasks.
Summarized: Trigger a bunch of async operations with a 200ms delay between them, do something when all are complete.
I have managed to delay the calls, and I do not care how long a single call takes to finish, but I need to know when all tasks have finished.
This is the solution I came up with, I would run the triggerRenameProcess() method to start the process.
async triggerRenameProcess(users: RenameUser[]) {
  try {
    await this.renameUsersWithDelay(users, 200);
    console.log('All users renamed');
  } catch (error) {
    console.error('Exception while renaming users: ', error);
  }
}

renameUsersWithDelay(users: RenameUser[], delay: number): Promise<void> {
  return new Promise<void>((resolve) => {
    let currentIndex = 0;
    const executeNext = () => {
      if (currentIndex < users.length) {
        const user = users[currentIndex];
        user.executeRename().then(() => {
          currentIndex++;
          setTimeout(executeNext, delay);
        });
      } else {
        resolve();
      }
    };
  });
}

I kind of understand what the problem is, the renameUsersWithDelay() returns a Promise that can be awaited, but that will happen once all requests have been triggered - it is not waiting for the longest running one to complete. I can make it wait for every single one and run them sequentially, but that's not what it is supposed to do...
I did some research and found this question or this one, but they seem not to address the same issue - and frankly, I am not too familiar with JavaScript - I feel more at home in .NET where I just would use something like .WaitAll() (and probably use a more sophisticated throttling pattern), but I'm not sure how to achieve the same thing in TypeScript.
",76388621,"Whenever you've got an asynchronous data flow manipulation that isn't trivial, you can assume that observables will help you a lot.
Here's the feature you're trying to achieve with observables:
function renameUsersWithDelay(
  users: RenameUser[],
  delayMs: number = 200
): Observable<any> {
  const usersWithDelay = users.map((user, i) =>
    of(user).pipe(delay(delayMs * i), switchMap(executeRename))
  );

  return forkJoin(usersWithDelay);
}

Here's how it works:

we loop on the users array with a map to create a new array out of it
for each user, we put the user into an observable using of
we then apply a delay relative to the index we're at so they're all shifted
once the delay resolves, we use a switchMap to make make the http call, which will effectively subscribe to the observable returned by executeRename
because observables are cold by nature, nothing that we just created here is triggered yet. Which is fantastic because it lets you build up all your instructions without triggering them straight away
finally, we use forkJoin to subscribe to each observables that we just created

I've created a live demo with mocked data so you can see it behaves as expected.
I wanted to give you exactly what you asked for, which is what I did so far. But my advice would be to go for a different approach where you still don't DDOS your backend while being more efficient, by having a limited pool of running calls. For example you could say, up until there's nothing left to proceed, I want to have 3 ongoing rename calls and as soon as 1 finishes in that current pool, another one is taken from the queue and ran straight away. This is IMO more efficient.
Here's how you could implemented this:
const renameUsersWithDelay = (users: RenameUser[]): Observable<any> =>
  from(users.map((user) => of(user).pipe(switchMap(executeRename)))).pipe(
    mergeAll(3)
  );

Live demo
",
76389697,clean data in r from image,"I am trying to scan a text from an Ocr and clean it, I got a character that is divided to few lines, however I would like to have the text in similar to the way it is in the image
the code :
heraclitus<-""greek.png""
library(tidyverse)
library(tesseract)
library(magick)

image_greek<-image_read(heraclitus)

image_greek<-image_greek %>% image_scale(""600"") %>% 
  image_crop(""600x400+220+150"") %>% 
  image_convert(type = 'Grayscale') %>% 
  image_contrast(sharpen = 1) %>% 
  image_write(format=""jpg"")

heraclitus_sentences<-magick::image_read(image_greek)%>% 
  ocr() %>% str_split(""\n"")

As you can see from the output, I have white spaces and sentences that are divided to two lines. I would like to have it in a vector or a list, that each element will be a sentence


",76390000,"You need to split on \n\n (not \n) then replace the middle \n values:
magick::image_read(image_greek) %>% 
  ocr() %>% 
  str_split(""\n\n"") %>%
  unlist() %>%
  str_replace_all(""\n"", "" "")

Output:
[1] ""© Much learning does not teach understanding.""                                                       
[2] ""© The road up and the road down is one and the same.""                                                
[3] ""© Our envy always lasts longer than the happiness of those we envy.""                                 
[4] ""© No man ever steps in the same river twice, for it's not the same river and he's not the same man. ""

",
76388633,Join values of two string lists to new string list and add characters in VB.NET using LINQ,"In VB.NET i would like to create a list of string from the values of two given lists of string.
E.g.
        Dim varNames As New List(Of String)({""var1"", ""var2""})
        Dim varValues As New List(Of String)({""value1"", ""value2""})
        Dim finalList As List(Of String) = .....
        'Output should be:
        'finalList(0) -> ""[var1] = 'value1'""
        'finalList(1) -> ""[var2] = 'value2'""

I would like to do this in on line of code using LINQ. (I know how to do this using for loops)
",76388649,"Use Zip to ""concat"" two lists via index:
Dim finalList As List(Of String) = varNames.
    Zip(varValues, Function(s1, s2) $""[{s1}] = '{s2}'"").
    ToList()

",
76388555,Why is my Python code not inserting data into PostgreSQL database from a JSON file?,"I'm trying desing and implement database model in python from a json file but my python code seems like is not inserting data into table and i'm not sure where is the problem in my code
import json
import psycopg2


class Policy:
    def __init__(self, type, name, rule_exists):
        self.type = type
        self.name = name
        self.rule_exists = rule_exists


# load json data from the file
with open(""demo_data.json"", 'r') as file:
    policy_data = json.load(file)
    first_level = policy_data[""uniconfig-topology:configuration""]
    second_level = first_level[""Cisco-IOS-XR-infra-policymgr-cfg:policy-manager""]
    third_level = second_level[""policy-maps""]
    forth_level = third_level[""policy-map""]

table_of_policies = []
for item in forth_level:
    if ""policy-map"" in item:
    # forth_level = item
        type = forth_level['type']
        name = forth_level['name']
        rule_exists = 'policy-map-rule' in forth_level
        policy_map = Policy(type, name, rule_exists)
        table_of_policies.append(policy_map)
    #print(forth_level)


conn = psycopg2.connect(
    host=""localhost"",
    database=""postgres"",
    user=""postgres"",
    password=""postgres"",
    port=""5432""
)


cursor = conn.cursor()
cursor.execute(""DROP TABLE IF EXISTS policy_map"")
cursor.execute('''create table policy_map
( type VARCHAR(255),
  name VARCHAR(255),
  rule_exists BOOLEAN)
''')

for policy_map in table_of_policies:
    insert_data = ""INSERT INTO policy_map (type, name, rule_exists) VALUES (%s, %s, %s)""
    cursor.execute(insert_data, (policy_map.type,
                   policy_map.name, policy_map.rule_exists))


conn.commit()


I tried several different approach for dict in json file to be sure im on right level of dict but it never append any data to my list. Would like to know whats wrong with my code
",76388668,"type is a built-in python function that returns the type of a variable. Overwriting it might cause your code to work unexpectedly.
You can check the documentation , avoid using build-in function names for variables.
Regarding table column naming you can refer to this question.
Apart from these you're looping over forth_level but not using the item:
type = forth_level['type']
name = forth_level['name']

I would expect this code to continue like:
    if ""policy-map"" in item:
    type = item['type']
    name = item['name']
    rule_exists = 'policy-map-rule' in item
    policy_map = Policy(type, name, rule_exists)
    table_of_policies.append(policy_map)

But without seeing the JSON I cannot tell for sure.
",
76391208,Undefined variable $patients,"this is my [tag:doctor.write_medical.blade.php]. i try to get patient name from patient database that i created in migration file and fill in some form so that i can save the medical records with patient_id as foreign key. but when i try to open the page, it shows Undefined variable $patients. i dont know why it become like this it shows error at
 @foreach($patients as $patient)

<!DOCTYPE html>
<html lang=""en"">
<head>
  @include('doctor.css')
</head>
<body>

<header>
    <div class=""topbar"">
      <div class=""container"">
        <div class=""row"">
          <div class=""col-sm-8 text-sm"">
            <div class=""site-info"">
              <a href=""#""><span class=""mai-call text-primary""></span> 0105739057</a>
              <span class=""divider"">|</span>
              <a href=""#""><span class=""mai-mail text-primary""></span> afiqaqil@gmail.com</a>
            </div>
          </div>
          <div class=""col-sm-4 text-right text-sm"">
            <div class=""social-mini-button"">
              <a href=""#""><span class=""mai-logo-facebook-f""></span></a>
              <a href=""#""><span class=""mai-logo-twitter""></span></a>
              <a href=""#""><span class=""mai-logo-dribbble""></span></a>
              <a href=""#""><span class=""mai-logo-instagram""></span></a>
            </div>
          </div>
        </div> <!-- .row -->
      </div> <!-- .container -->
    </div> <!-- .topbar -->

    <nav class=""navbar navbar-expand-lg navbar-light shadow-sm"">
      <div class=""container"">
        <a class=""navbar-brand"" href=""#""><span class=""text-primary"">MyHealthCare</span>System</a>


        <div class=""collapse navbar-collapse"" id=""navbarSupport"">
          <ul class=""navbar-nav ml-auto"">
            <li class=""nav-item"">
              <a class=""nav-link"" href=""#"">Request Access</a>
            </li>
            <li class=""nav-item active"">
              <a class=""nav-link"" href=""index.html"">Home</a>
            </li>
            <li class=""nav-item"">
              <a class=""nav-link"" href=""about.html"">About Us</a>
            </li>
            <li class=""nav-item"">
              <a class=""nav-link"" href=""contact.html"">Contact</a>
            </li>

            @if(Route::has('login'))
            @auth

            <x-app-layout>
                </x-app-layout>

            @else
            <li class=""nav-item"">
              <a class=""btn btn-primary ml-lg-3"" href=""{{route('login')}}"">Login</a>
            </li>
            <li class=""nav-item"">
              <a class=""btn btn-primary ml-lg-3"" href=""{{route('register')}}"">Register</a>
            </li>
            @endauth
            @endif
          </ul>
        </div> <!-- .navbar-collapse -->
      </div> <!-- .container -->
    </nav>
  </header>

<div class=""container-fluid page-body-wrapper"">
  <div class=""row"">
    <div class=""col-md-2"">
      <div class=""sidebar-sticky"">
        <ul class=""nav flex-column"">
          <li class=""nav-item active"">
            <a class=""nav-link"" href=""{{route('doctor.home')}}"">
              <i class=""mdi mdi-account-search""></i>
              Add Patient
            </a>
          </li>
          <li class=""nav-item"">
            <a class=""nav-link"" href=""{{ route('write_medical') }}"">
              <i class=""mdi mdi-download""></i>
              Write Medical Description
            </a>
          </li>
          <li class=""nav-item menu-items"">
            <a class=""nav-link"" data-toggle=""collapse"" href=""#doctors"" aria-expanded=""false"" aria-controls=""doctors"">
              <span class=""menu-icon"">
                <i class=""mdi mdi-doctor""></i>
              </span>
              <span class=""menu-title"">View Patients</span>
              <i class=""menu-arrow""></i>
            </a>
            <div class=""collapse"" id=""doctors"">
              <ul class=""nav flex-column sub-menu"">
              <li class=""nav-item"">
                <a class=""nav-link"" href=""{{ route('patient_1') }}"">Patient 1</a>
                </li>

                <li class=""nav-item"">
                  <a class=""nav-link"" href=""{{ route('add_doctor_view') }}"">Patient 2</a>
                </li>
              </ul>
            </div>
          </li>
          <li class=""nav-item"">
            <a class=""nav-link"" href=""#"">
              <i class=""mdi mdi-chat""></i>
              Chat with Patients
            </a>
          </li>
        </ul>
      </div>
    </div>

    <div class=""col-md-10"">
      <div class=""content-wrapper"">
        <div class=""container-fluid"">
          <div class=""row"">
            <div class=""col-md-12"">
              <div class=""form-container"">
              @if (session('success'))
        <div class=""alert alert-success"">
            {{ session('success') }}
        </div>
    @endif
                  <form action=""{{ route('save_medical') }}"" method=""POST"">
                  @csrf

                  <div class=""form-group"">
                      <label for=""patient"">Patient</label>
                      <select class=""form-control"" id=""patients"" name=""patients"" required>
                          @foreach($patients as $patient)
                              <option value=""{{ $patient->patient_id }}"">{{ $patient->patient_name }}</option>
                          @endforeach
                      </select>
                  </div>

                  <div class=""form-group"">
                    <label for=""date"">Date</label>
                    <input type=""text"" class=""form-control"" id=""date"" value=""{{ date('Y-m-d') }}"" readonly>
                  </div>
                  <div class=""form-group"">
                    <label for=""symptoms"">Symptoms</label>
                    <textarea class=""form-control"" id=""symptoms"" name=""symptoms"" rows=""4"" required></textarea>
                  </div>
                  <div class=""form-group"">
                    <label for=""treatment"">Simplified Treatment</label>
                    <textarea class=""form-control"" id=""treatment"" name=""treatment"" rows=""4"" required></textarea>
                  </div>
                  <div class=""form-group"">
                    <label for=""medication"">Medication</label>
                    <textarea class=""form-control"" id=""medication"" name=""medication"" rows=""4"" required></textarea>
                  </div>
                  <button type=""submit"" class=""btn btn-primary"">Submit</button>
                </form>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<footer class=""page-footer"">
    <div class=""container"">
      <div class=""row px-md-3"">
        <div class=""col-sm-6 col-lg-3 py-3"">
          <h5>Company</h5>
          <ul class=""footer-menu"">
            <li><a href=""#"">About Us</a></li>
            <li><a href=""#"">Career</a></li>
            <li><a href=""#"">Editorial Team</a></li>
            <li><a href=""#"">Protection</a></li>
          </ul>
        </div>
        <div class=""col-sm-6 col-lg-3 py-3"">
          <h5>More</h5>
          <ul class=""footer-menu"">
            <li><a href=""#"">Terms & Condition</a></li>
            <li><a href=""#"">Privacy</a></li>
            <li><a href=""#"">Advertise</a></li>
            <li><a href=""#"">Join as Doctors</a></li>
          </ul>
        </div>
        <div class=""col-sm-6 col-lg-3 py-3"">
          <h5>Our partner</h5>
          <ul class=""footer-menu"">
            <li><a href=""#"">One-Fitness</a></li>
            <li><a href=""#"">One-Drugs</a></li>
            <li><a href=""#"">One-Live</a></li>
          </ul>
        </div>
        <div class=""col-sm-6 col-lg-3 py-3"">
          <h5>Contact</h5>
          <p class=""footer-link mt-2"">351 Willow Street Franklin, MA 02038</p>
          <a href=""#"" class=""footer-link"">701-573-7582</a>
          <a href=""#"" class=""footer-link"">healthcare@temporary.net</a>

          <h5 class=""mt-3"">Social Media</h5>
          <div class=""footer-sosmed mt-3"">
            <a href=""#"" target=""_blank""><span class=""mai-logo-facebook-f""></span></a>
            <a href=""#"" target=""_blank""><span class=""mai-logo-twitter""></span></a>
            <a href=""#"" target=""_blank""><span class=""mai-logo-google-plus-g""></span></a>
            <a href=""#"" target=""_blank""><span class=""mai-logo-instagram""></span></a>
            <a href=""#"" target=""_blank""><span class=""mai-logo-linkedin""></span></a>
          </div>
        </div>
      </div>

      <hr>

      <p id=""copyright"">Copyright &copy; 2020 <a href=""https://macodeid.com/"" target=""_blank"">MACode ID</a>. All right reserved</p>
    </div>
  </footer>

@include('doctor.script')
  
</body>
</html>


this is my route web.php
<?php

use Illuminate\Support\Facades\Route;
use App\Http\Controllers\HomeController;
use App\Http\Controllers\AdminController;
use App\Http\Controllers\RegisterController;
use App\Http\Controllers\Admin\DataRequestorController;
use App\Http\Controllers\DoctorController;
use App\Http\Controllers\PatientController;
use App\Http\Controllers\AccessRequestController;
use App\Http\Controllers\WriteMedicalController;




Route::get('access-request/create', [AccessRequestController::class, 'create'])->name('access-request.create');
Route::post('access-request', [AccessRequestController::class, 'store'])->name('access-request.store');

Route::get('/get-doctors-count', [DoctorController::class, 'getDoctorsCount'])->name('get_doctors_count');
Route::post('/add-patient', [PatientController::class, 'addPatient'])->name('add.patient');

Route::get('/write-medical', [WriteMedicalController::class, 'writeMedical'])->name('write_medical');
Route::post('/save-medical', [WriteMedicalController::class, 'saveMedical'])->name('save_medical');

Route::get('/admin/data-requestors/{requestor}/accept', [DataRequestorController::class, 'accept'])->name('admin.data-requestors.accept');
Route::get('/admin/data-requestors/{requestor}/decline', [DataRequestorController::class, 'decline'])->name('admin.data-requestors.decline');
Route::get('/admin/data-requestors/{requestor}', [DataRequestorController::class, 'show'])->name('admin.data-requestors.show');
Route::get('/admin/data-requestors', [DataRequestorController::class, 'index'])->name('admin.data-requestors.index');

Route::get('/', [HomeController::class, 'index']);
Route::get('/home', [HomeController::class, 'redirect'])->name('home.redirect');
Route::get('/doctor/home', [HomeController::class, 'doctorHome'])->name('doctor.home');
Route::get('/user/home', [HomeController::class, 'userHome'])->name('user.home');

Route::middleware([
    'auth:sanctum',
    config('jetstream.auth_session'),
    'verified'
])->group(function () {
    Route::get('/dashboard', function () {
        return view('dashboard');
    })->name('dashboard');
});

Route::get('/write-medical', function () {
    return view('doctor.write_medical');
})->name('write_medical');

Route::get('/patient_1', function () {
    return view('doctor.patient_1');
})->name('patient_1');

Route::get('/add_doctor_view', [AdminController::class, 'addview'])->name('add_doctor_view');
Route::post('/register_doctor', [AdminController::class, 'registerDoctor'])->name('register.doctor');
Route::get('/viewDoctors', [AdminController::class, 'viewDoctors'])->name('viewDoctors');
Route::delete('/delete_doctor/{doctor}', [AdminController::class, 'deleteDoctor'])->name('admin.doctors.delete');

and this is my writemedicalcontroller
<?php

namespace App\Http\Controllers;

use Illuminate\Http\Request;
use App\MedicalDescription;
use App\Models\Patient;
use App\Models\Doctor;

class WriteMedicalController extends Controller
{
    public function writeMedical()
    {
        $patients = Patient::all();

        return view('doctor.write_medical',  compact('patients'));
    }

    public function saveMedical(Request $request)
    {
        $validatedData = $request->validate([
            'patient' => 'required|exists:patients,patient_id',
            'symptoms' => 'required',
            'treatment' => 'required',
            'medication' => 'required',
        ]);        

        $medicalDescription = new MedicalDescription();
        $medicalDescription->patient_id = $validatedData['patient'];
        $medicalDescription->date = date('Y-m-d');
        $medicalDescription->symptoms = $validatedData['symptoms'];
        $medicalDescription->treatment = $validatedData['treatment'];
        $medicalDescription->medication = $validatedData['medication'];
        $medicalDescription->save();

        return redirect()->back()->with('success', 'Medical description saved successfully.');
    }
}

this is my medicaldescription model
",76391517,"Your route web.php file has two definitions of the route /write-medical, so Laravel is using the last definition:
Route::get('/write-medical', function () {
    return view('doctor.write_medical');
})->name('write_medical');

Remove that and I think it will work as you expect because your first definition is calling the controller WriteMedicalController::writeMedical as you expect.
",
76388273,Repeating and looping API calls in Karate framework,"I am new to Karate framework and have written the following working code to run few sample API tests.
Feature: Sample API Tests

  Background:
    * def request_headers = {apikey: ""#(apikey)"", Content-Type: 'application/json'}
    * configure headers = request_headers

  Scenario: Create Customer

    * def requestPayload = read('classpath:payload/customerdetails.json')
    * def RandomDataGenerator = Java.type('utils.RandomDataGenerator')

    Given url testurl+'/customers'
    And request requestPayload
    * requestPayload.firstName = RandomDataGenerator.generateRandomFirstName()
    * requestPayload.lastName = RandomDataGenerator.generateRandomLastName()
    * requestPayload.mobileNumber = RandomDataGenerator.generateRandomMobileNumber()
    * requestPayload.emailAddress = RandomDataGenerator.generateRandomEmailAddress()
    When method post
    Then status 200

    And def customerId = response.id
    
    # Create An Account for the customer
    * def createAccountPayload = read('classpath:payload/accountdetails.json')

    Given url testurl+'/accounts/createCreditAccount'
    And request createAccountPayload
    * createAccountPayload.customerId = customerId
    When method post
    Then status 200

    And def accountId = response.id

  # Perform a Transaction
    * def debitTransactionPayload = read('classpath:payload/transaction.json')

    Given url testurl+'/accounts/createDebitTransaction'
    And request debitTransactionPayload
    * debitTransactionPayload.accountId = accountId
    When method post
    Then status 200
    

Now I want to run a test where I would:

Create 100 customers (and in turn 100 accounts).
Do 500 transactions (5 transactions each for a given customer).

How do I achieve this?
I tried using repeat and looping options, but I believe I am missing something basic. Can someone please help ?
",76388683,"Here is a Karate test that uses a JS function as a data-source to create 10 JSON payloads. You should be easily able to modify this to do what you want:
Feature:

  @setup
  Scenario:
    * def generator = 
    """"""
    function(i){ 
        if (i == 10) return null; 
        return { name: `cat${i}`, age: i };
    }
    """"""

  Scenario Outline:
    * print __row

    Examples:
        | karate.setup().generator |

How this works is explained here: https://github.com/karatelabs/karate#json-function-data-source
Also refer: https://stackoverflow.com/a/75394445/143475
",
76389982,POST request to /api/notes/addnote endpoint returning 404 Not Found error,"I'm encountering an issue when making a POST request to the /api/notes/addnote endpoint. The server responds with a 404 Not Found error. I have checked the server-side code and verified that the endpoint is correctly defined. Here are the details of my setup:
Server-side:
Framework/Language: Express.js
Code snippet handling the /api/notes/addnote endpoint:
router.post('/addnote', fetchUser, [
    body('tittle', 'Tittle cannot be blank').exists(),
    body('description', 'Description must be atleast 5 characters').isLength({ min: 5 })
], async (req, res) => {
    // If there are any errors in the validation of an express request, return status 400 and the errors
    const errors = validationResult(req);
    if (!errors.isEmpty()) {
        return res.status(400).json({ errors: errors.array() });
    } 

    // Creating a new note in the database and saving it
    const { tittle, description, tag } = req.body;
    const note = await Notes.create({
        tittle,
        description,
        tag,
        user: req.id
    });

    // Sending the note as a response
    res.json(note);
});

module.exports = router;

main.js file code:
const connectToMongo = require('./Db');
const express = require('express');

connectToMongo();

const app = express()
const port = 3000

app.use(express.json());

app.use('/api/notes/addnote', require('./Routes/notes'));

app.listen(port, () => {
  console.log(`Example app listening on port ${port}`)
})

Client-side:
Code snippet for making the POST request:

The server is running and accessible at localhost:3000
I have successfully tested other endpoints on the same server, and they are working correctly.
I have confirmed the URL for the POST request: http://localhost:3000/api/notes/addnote.
I have already attempted the following troubleshooting steps:

Double-checked the server-side code and endpoint spelling.

Tested the POST request using different HTTP clients (e.g., Postman) with the same result.

Reviewed the server logs for any error messages related to the /api/notes/addnote endpoint.


Despite these efforts, I'm still receiving a 404 Not Found error. Any guidance or suggestions on how to resolve this issue would be greatly appreciated. Thank you!
",76390015,"The URL while setting the Router to the appliction is the same as you have mentioned.
app.use('/api/notes', require('./Routes/notes'));

The problem here is:
app.use('/api/notes/addnote', require('./Routes/notes'));
router.post('/addnote');

Now the route works for /api/notes/addnote/addnote.
",
76389927,"Android RequestMultiplePermissions(): one of the permission's status is never true, even after allowing it","I want to ask for the SEND_SMS and READ_SMS runtime permissions, if they are not given. This is my code:
private ActivityResultLauncher<String[]> mRequestPermissionsLauncher;
String[] PERMISSIONS = {
    Manifest.permission.SEND_SMS,
    Manifest.permission.READ_SMS
};
@Override
protected void onCreate(Bundle savedInstanceState) {

    mRequestPermissionsLauncher = registerForActivityResult(new ActivityResultContracts.RequestMultiplePermissions(), result -> {
         Log.d(""DEBUG"",result.toString());
    });

    if (!hasPermissions(this, PERMISSIONS)) {
         if(shouldShowRequestPermissionRationale(Manifest.permission.SEND_SMS) || shouldShowRequestPermissionRationale(Manifest.permission.READ_SMS)){
              new AlertDialog.Builder(this)
                   .setTitle(""Permission Needed"")
                   .setMessage(""Please press OK to allow the permissions"")
                   .setCancelable(false)
                   .setPositiveButton(""OK"", (dialog, which) -> mRequestPermissionsLauncher.launch(PERMISSIONS)).create().show();
         }
         else{
             mRequestPermissionsLauncher.launch(PERMISSIONS);
         }
    }
}
public static boolean hasPermissions(Context context, String... permissions) {
    if (context != null && permissions != null) {
        for (String permission : permissions) {
            if (ActivityCompat.checkSelfPermission(context, permission) != PackageManager.PERMISSION_GRANTED) {
                return false;
            }
         }
    }
    return true;
}


After I allow the permissions and debug the result, it keeps showing:
{android.permission.SEND_SMS=true, android.permission.READ_SMS=false}

Why is my READ_SMS permission false?
EDIT: Never mind, the issue is resolved. I forgot to declare the READ_SMS permission in the manifest file. Silly me :p
",76390042,"You can't and that's by design - to prevent abuse.
You're better of following the official guidelines.
The only way to request permissions again, is to navigate the user to app info, where the user has to manually allow it:
fun launchPermissionSettings(context: Context) {
    Intent().apply {
        action = Settings.ACTION_APPLICATION_DETAILS_SETTINGS
        data = Uri.fromParts(""package"", context.packageName, null)

        addFlags(Intent.FLAG_ACTIVITY_NEW_TASK)
        addFlags(Intent.FLAG_ACTIVITY_NO_HISTORY)
        addFlags(Intent.FLAG_ACTIVITY_EXCLUDE_FROM_RECENTS)

        context.startActivity(this)
    }
}

static void launchPermissionSettings(Context context) {
    Intent intent = new Intent();
    intent.setAction(Settings.ACTION_APPLICATION_DETAILS_SETTINGS);
    intent.setData(Uri.fromParts(""package"", context.getPackageName(), null));

    intent.addFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
    intent.addFlags(Intent.FLAG_ACTIVITY_NO_HISTORY);
    intent.addFlags(Intent.FLAG_ACTIVITY_EXCLUDE_FROM_RECENTS);

    context.startActivity(intent);
}

You're also using a deprecated way of requesting permissions. Consider using ActivityResultContracts.RequestMultiplePermissions():
class MyFragment: Fragment() {
    private lateinit var mRequestPermissionsLauncher: ActivityResultLauncher<Array<String>>

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        mRequestPermissionsLauncher = registerForActivityResult(ActivityResultContracts.RequestMultiplePermissions()) { result: Map<String, Boolean> ->
            //Handle result here
        }

        //Request permissions like this:
        mRequestPermissionsLauncher.launch(arrayOf(
            Manifest.permission.SEND_SMS,
            Manifest.permission.READ_SMS,
        ))
    }
}

class MyFragment extends Fragment {
    private ActivityResultLauncher<String[]> mRequestPermissionsLauncher;

    @Override
    public void onCreate(@Nullable Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        mRequestPermissionsLauncher = registerForActivityResult(new ActivityResultContracts.RequestMultiplePermissions(), result -> {
            //Handle activity result here
        });

        //Request permissions like this:
        String[] PERMISSIONS = {
            Manifest.permission.SEND_SMS,
            Manifest.permission.READ_SMS,
        };
        mRequestPermissionsLauncher.launch(PERMISSIONS);
    }
}

",
76390997,Not able to submit message to Azure service bus Topic using MassTransit,"I am trying to submit a message to a Azure Service Bus Topic and it keeps complaining about
MassTransit.ConfigurationException: The message type Contracts.ClassName entity name was already evaluated: Contracts/ClassName
I have checked my registration and can't seem to figure out what is missing?
Here is my code for submitting a message to a Topic
var bus = Bus.Factory.CreateUsingAzureServiceBus(cfg =>
        {
            cfg.Message<ClassName>(x =>
            {
                x.SetEntityName(""topicName"");
            });
        });
        
try
   {
       await bus.Publish(new ClassName
       {
            Status = status,
            User = user,                
            });
            _logger.LogInformation($""Topic Message Sent Successfully."");
        }
        catch (Exception ex)
        {
            _logger.LogError($""Failed to Send Message!\n{ex.Message} to Topic"");
        }

Registration
services.AddMassTransit(x =>
{
     x.AddConsumer<TestConsumer>();
     x.UsingAzureServiceBus((ctx, cfg) =>
     {
           cfg.ClearMessageDeserializers();
           cfg.UseRawJsonSerializer();

           cfg.Host(_busConfiguration.ServiceBus.ConnectionString);

           cfg.ReceiveEndpoint(_busConfiguration.ServiceBus.QueueName, ec =>
           {
                 ec.ConfigureConsumeTopology = false;
                 ctx.ConfigureConsumer<TestConsumer>(ec);
            });
        });
     });                    

",76391529,"You should configure everything at once, the separate bus configuration is a really bad idea.
services.AddMassTransit(x =>
{
  x.AddConsumer<TestConsumer>();

  x.UsingAzureServiceBus((ctx, cfg) =>
  {
    cfg.Message<ClassName>(x =>
    {
      x.SetEntityName(""topicName"");
    });

    cfg.UseRawJsonSerializer(isDefault: true);

    cfg.Host(_busConfiguration.ServiceBus.ConnectionString);

    cfg.ReceiveEndpoint(_busConfiguration.ServiceBus.QueueName, ec =>
    {
          ec.ConfigureConsumeTopology = false;
          ec.ConfigureConsumer<TestConsumer>(ctx);
    });
  });
});


",
76389933,monkey patch without the need to intitalise,"I would like to money patch a function without the requirement to initialize using pd.DataFrame.function = function
For example, if I have a dataframe df and I call df.unique() I do not have to run pd.DataFrame.unique = unique prior to running as this appears to be built in. Is there a way I can do the same for my own functions without the need to initialise every time?
",76390050,"Patching an existing class, e.g. pd.DataFrame, so that all instances have a patched method is not trivial, IMO.
What about simply subclassing pd.DataFrame?
import pandas as pd


class MyDataFrame(pd.DataFrame):
    def my_func(self):
        print(""yey"")

df = MyDataFrame()
df.my_func()

# Outputs:
# yey

",
76388645,Trying to sort a selectListItem alphabetically not by ID,"I have the below code that populates a selectListItem with information from my database.
I am trying to sort this by name rather than by ID like how it is in the database.
I tried adding Vendors.OrderBy(x => x.VendorName); but it didn't seem to do anything at all, not sure if I have it in the wrong place or something.
public IList<Data.Vendor> Vendors { get; set; }
public void OnGet()
        {
             List<SelectListItem> test = new List<SelectListItem>();
             Vendors = _context.Vendors.ToList();
                         foreach (var item in Vendors)
            //foreach item In the vendors list
            {
                test.Add(new SelectListItem { Text = item.VendorName, Value = item.Id.ToString() });
                //Add the current item i3n the loop to the selectListItem
            }
            ViewData[""demo""] = test;
            Vendors.OrderBy(x => x.VendorName);
}

This is the front end code:
@page
@model PurchaseOrdersModel
@{
    ViewData[""Title""] = ""Home page"";
}
<style>
    body {
        background-image: url(""http://10.48.1.215/PORTAL/hero-range-1.jpg"");
        height: 100%;
        background-position: center;
        background-repeat: no-repeat;
        background-size: cover;
    }

    #myInput {
        /*background-image: url('/css/searchicon.png'); /* Add a search icon to input */
        /*        background-position: 10px 12px; /* Position the search icon */
        /*        background-repeat: no-repeat; /* Do not repeat the icon image **/
        width: 100%; /* Full-width */
        font-size: 16px; /* Increase font-size */
        /*padding: 12px 20px 12px 40px; /* Add some padding */
        border: 1px solid #ddd; /* Add a grey border */
        margin-bottom: 12px; /* Add some space below the input */
    }

</style>
@*
<select asp-for=""Files.VendorId.Value"" asp-items=""@((List<SelectListItem>)ViewData[""demo""])""></select>*@


<h1 style=""color: white"">Purchase Orders</h1>
<p>
    <a asp-page=""Create"" style=""color:white"">Create File</a>
</p>
<div class=""row"">
    <div class=""col-sm-12"">
        <div class=""form-check-inline pull-right"">
            <label style=""color:white"">Search</label>
            <input type=""text"" class=""form-control"" id=""myInput"" placeholder=""Search Purchase Orders..."" onkeyup=""myFunction()"" />
        </div>
        <select asp-items=""@((List<SelectListItem>)ViewData[""test""])"" id=""vendor"" onchange=""myFunctionThree()"">
            <option value="""" disabled selected>Select a Vendor</option>
        </select>
        <input class=""btn btn-light"" value=""Clear All Filters"" onclick=""history.go(0)"" style=""float: right"">
        <table class=""table"" style=""background-color: white"" id=""myTable"">
            <thead>
                <tr class=""header"">
                    <th>
                        PO No.
                    </th>
                    @*            <th>
                    @Html.DisplayNameFor(model => model.Files[0].Date)
                    </th>
                    <th>
                    @Html.DisplayNameFor(model => model.Files[0].Value)
                    </th>*@
                    <th>
                        Haulier
                    </th>
                    <th>
                        Comments
                    </th>
                    <th>
                        Vendor
                    </th>
                    <th>
                        Upload
                    </th>
                    <th>
                        Date Uploaded
                    </th>
                    <th>Download</th>
                    <th>Delete Attachment</th>
                    <th>Notify</th>
                    <th>Sent</th>
                    <th>

                    </th>
                </tr>
            </thead>
            <tbody>

                @foreach (var item in Model.Files)
                {
                    if (item.FileType == ""Purchase Order"")
                    {
                        <tr>
                            <td>
                                @Html.DisplayFor(modelItem => item.Number)
                            </td>

                            @*<td>
                    @Html.DisplayFor(modelItem => item.FileType)
                    </td>*@
                            <td>
                                @Html.DisplayFor(modelItem => item.Haulier)
                            </td>
                            <td>
                                @Html.DisplayFor(modelItem => item.Comments)
                            </td>
                            <td>
                                @Html.DisplayFor(modeItem => item.Vendor.VendorName)
                            </td>
                            <td>
                                <a asp-page=""Upload"" asp-route-id=""@item.Id"">Upload File Attachment</a>
                            </td>
                            <td>
                                @Html.DisplayFor(modelItem => item.UploadDate)
                            </td>
                            <td>
                                @if (item.Attachment != null)
                                {
                                    <form asp-page-handler=""Download"" method=""post"" asp-route-id=""@item.Id"">
                                        <input type=""submit"" class=""btn btn-dark"" value=""Download"">
                                    </form>
                                }
                            </td>
                            <td>
                                @if (item.Attachment != null)
                                {
                                    <form asp-page-handler=""Delete"" method=""post"" asp-route-id=""@item.Id"">
                                        <input type=""submit"" class=""btn btn-danger"" value=""Delete Attachment"">
                                    </form>
                                }
                            </td>
                            <td>
                                @if (item.Attachment != null)
                                {
                                    <form asp-page-handler=""Email"" method=""post"" asp-route-id=""@item.Id"">
                                        <input type=""submit"" class=""btn btn-danger"" value=""Notify Vendor"">
                                    </form>
                                }
                            </td>
                            <td>
                                @Html.DisplayFor(modelItem => item.EmailSent)
                            </td>
                            <td>
                                <a asp-page=""/Admin/Details"" asp-route-id=""@item.Id"">Details</a>
                            </td>
                        </tr>
                    }

                }
            </tbody>
        </table>
    </div>
</div>


<script>

    //function for search bar
    function myFunction() {
        // Declare variables
        var input, filter, table, tr, td, i, txtValue, dropdownValue, dropdownInput, dtd;
        input = document.getElementById(""myInput"");
        filter = input.value.toUpperCase();
        table = document.getElementById(""myTable"");
        tr = table.getElementsByTagName(""tr"");
        dropdownInput = document.getElementById(""vendor"");
        dropdownValue = dropdownInput.value.toUpperCase();

        // Loop through all table rows, and hide those who don't match the search query
        for (i = 0; i < tr.length; i++) {
            td = tr[i].getElementsByTagName(""td"")[0];
            dtd = tr[i].getElementsByTagName(""td"")[3];
            if (td) {
                    txtValue = td.textContent || td.innerText;
                    dropTxtValue = dtd.textContent || dtd.innerText;
                    if(dropdownValue == """"){
                        if (txtValue.toUpperCase().indexOf(filter) > -1) {
                            tr[i].style.display = """";
                        } else {
                            tr[i].style.display = ""none"";
                        }
                    }
                    else{
                        if((txtValue.toUpperCase().indexOf(filter) > -1) && (dropTxtValue.toUpperCase().indexOf(dropdownValue) > -1)){
                            tr[i].style.display = """";
                        }else{
                            tr[i].style.display = ""none"";
                        }
                    }
            }
        }
    }

    //function for dropdown menu
    function myFunctionThree()
    {
        // Declare variables
        var input, filter, table, tr, td, i, txtValue;
        input = document.getElementById(""vendor"");
        filter = input.value.toUpperCase();
        table = document.getElementById(""myTable"");
        tr = table.getElementsByTagName(""tr"");

        // Loop through all table rows, and hide those who don't match the search query
        for (i = 0; i < tr.length; i++) {
            td = tr[i].getElementsByTagName(""td"")[3];
            if (td) {
                txtValue = td.textContent || td.innerText;
                if (txtValue.toUpperCase().indexOf(filter) > -1) {
                    tr[i].style.display = """";
                } else {
                    tr[i].style.display = ""none"";
                }
            }
        }
    }

</script>

I want to sort it A-Z by Vendor Name instead of by the Vendor ID.
",76388706,"OrderBy creates IEnumerable that ""executes"" when used in foreach or when .ToList() is invoked. Try this:
Vendors = _context.Vendors.OrderBy(x => x.VendorName).ToList();

",
76390761,Pass a value to another component using Services when a change in value (dropdown selection) happens in the parent component - Angular,"I'm trying to pass a value (year_value) from a parent component to a child component using services, whenever there is a change in the value (dropdown selection).  Also I'm using Angular routing (<router-outlet>) for navigation to child component, since I will be adding more charts later which are separate components.
app.component.html
<mat-toolbar class=""toolbar2"">
        <div>
            <button mat-button (click)=""getChartType('bar')"" [routerLink]=""'/bar-chart'"" matTooltip=""Bar Chart"">
                <img src=""assets/images/column.svg"">
            </button>
        </div>
        <div>
          <mat-form-field>
            <mat-label>Select year</mat-label>
            <mat-select placeholder=""{{initialValue}}"" (selectionChange)=""selectByYear($event)""
                [(ngModel)]=""year_value"">
                <mat-option *ngFor=""let year of years"" [value]=""year.value"">
                    {{year.viewValue}}
                </mat-option>
            </mat-select>
          </mat-form-field>
        </div>
    </mat-toolbar>

    <mat-divider></mat-divider>

    <div class=""container-fluid"">
        <div class=""col1""></div>
        <div class=""col2"">
          <div class=""col2-row1 charts-area"">
            <router-outlet>
              <div class=""card"" id=""container""></div>
            </router-outlet>
          </div>
          <div class=""col2-row2""></div>
        </div>
    </div>

<mat-select> is the dropdown and when selecting a value, the selectByYear updates the year property.
app.component.ts
  export class AppComponent implements OnInit {

  constructor(private commonActions: CommonActionsService) {}

  beData = this.commonActions.getData();

  canvas: any;
  ctx: any;
  header: any;
  xValues: string[] = [];
  dataValues: any[] = [];
  yValues: number[] = [];
  year: string = '';
  initialValue: any;
  year_value;
  years: any[] = [];
  bgColors: string[] = [];
  buttons = ['Status', 'Auditor', 'ISOScheme'];
  numbers = ['Audits'];
  chart_yfield: string[] = [];
  chart_xfield: string[] = [];
  chartType;
  activity: any;
  xData: any;
  label: any;
  options: any;
  currentYear: any;

  ngOnInit() {
    this.selectYear();
    this.year_value = 2022;
    this.chart_xfield.push('Status');
    this.chart_yfield.push('Audits');
    const index: number = this.buttons.indexOf('Status');
    if (index !== -1) {
      this.buttons.splice(index, 1);
    }
    this.numbers = []
    this.header = this.chart_xfield[0]
    this.getChartType('bar');
  }

  selectYear() {
    this.currentYear = 2022;
    let earliestYear = this.currentYear - 5;
    while (this.currentYear >= earliestYear) {
      let dateOption = {
        ""viewValue"": this.currentYear + ' ' + '-' + ' ' + (this.currentYear + 1),
        ""value"": this.currentYear
      };
      this.years.push(dateOption);
      this.currentYear -= 1;
    }
    this.initialValue = this.years[0].viewValue;
    this.selectByYear(this.years[0]);
  }

  selectByYear(event) {
    this.year = event.value;
    this.getChartType(this.chartType);

    this.commonActions.setYearValue(this.year);
  }

  getChartType(type: string) {
    if (this.chart_yfield.length != 0 && this.header != '') {
      if (!this.year) {
        return;
      }
      this.chartType = type;
      this.isHighChart = true;
      this.dataValues = [];
      this.xValues = [];
      this.yValues = [];
      this.lineData = [];
      this.beData[this.year][this.header].forEach((entry) => {
        this.dataValues.push([entry.name, entry.value]);
      });
      this.beData[this.year][this.header].forEach((entry) => {
        this.xValues.push(entry.name);
      });
      this.beData[this.year][this.header].forEach((entry) => {
        this.yValues.push(entry.value);
      });
      this.beData[this.year][this.header].forEach((entry) => {
        this.bgColors.push(entry.color);
      });
      console.log(this.xValues, this.yValues, this.header, this.year);

    }
  }

}

So inside the selectByYear() I'm setting the year. After the value of year gets updated I'm updating the same in the CommonActionsService also.
common-actions.service.ts
export class CommonActionsService {

  year_value: string;

  constructor() { }

  setYearValue(year: string) {
    this.year_value = year;
  }

  getYearValue() {
    return this.year_value;
  }

}

Now in the bar-chart component, I call the get method. But nothing is displaying in the console.
bar-chart.component.ts
export class BarChartComponent implements OnInit {

  year_value: string;

  constructor(private commonActions: CommonActionsService) {}

  ngOnInit() {
    this.year_value = this.commonActions.getYearValue();
    console.log(""Year Value from Bar Chart: "", this.year_value);
  }

}

I want to get the value of this.year inside the selectByYear() in app.component.ts, inside bar-chart.component.ts whenever the year updates.
Or is there any other way other than using services?
I'm adding my full code here - https://stackblitz.com/edit/stackblitz-starters-k2touo?file=src%2Fmain.ts , but I'm not able to compile it online.
",76391534,"Using services is the right way to share information accross component if they're not parent-child.
Here it is a matter of concurrency: you are calling getYearValue before it has been setted in the other component (both event are being called OnInit, and maybe one component is being initialized before the other).
My suggestion is to use Subject and Observablein order to create streams so that they'll always be up to date.
Here there is a simple example of how you could structure it (I couldn't directly use your code since it was not compiling)
",
76389485,How to Print numbers in sequential manner using TPL multiple tasks,"What is the problem with the following code?
I am trying to print numbers in sequence. One Task is for printing odd numbers and the other task is for printing even numbers.
Could somebody help me how to introduce the synchronization, so that I can get sequenced numbers from 0-10?
Code :
    using System.Threading.Tasks;

Task.Run(() =>{

        var printNumbers = new PrintNumbers();


        for(var i =0; i<10; i++)
        {                
                if(i % 2 == 0)
                    Console.WriteLine(i);                
        }
})


Task.Run(() =>{

        var printNumbers = new PrintNumbers();

        for(var i =0; i<10; i++)
        {                
                if(i % 2 != 0)
                    Console.WriteLine(i);                
        }
    }
})



 oddTask.Start();
evenTask.Start();

Task.WhenAll(new Task[] { oddTask, evenTask});

public class PrintNumbers
{
    private static object seqNumbers = new object();    

    public void PrintOddNumber(int number)
    {
        lock(seqNumbers)
        {
            Console.WriteLine(number);
        }
    }

    public void PrintEvenNumber(int number)
    {
        lock(seqNumbers)
        {
            Console.WriteLine(number);
        }
    }
}

Note: I am using Visual Studio Code.
",76390066,"
What is the problem with the following code?

object seqNumbers = new object();
Task.Run((seqNumbers) =>{   // 1. Task.Run does not allow to pass arguments
        for(var i =0; i<10; i++)
        {
            lock(seqNumbers)  // 2. Using `lock` in here is problematic : 
                              //    you cannot enforce a toggle with a single one.
            {
                if(i % 2 == 0)
                    Console.WriteLine(i);
            }
        }
})
.Wait(); // 3. This will block until the Task is completed

Task.Run((seqNumbers) =>{
    lock(seqNumbers)
    {
        for(var i =0; i<10; i++)
        {
            lock(seqNumbers)
            {
                if(i % 2 != 0)
                    Console.WriteLine(i);
            }
        }
    }
})
.Wait();

So, you see: The locking here is actually futile, because both Tasks run in sequence to each other. If you want Tasks to run in parallel (potentially), you need to start both and then await both.
If you need sequential output without utilizing intermediate data storage of results, then you have to do something awkward like this:
using System;
using System.Threading;
using System.Threading.Tasks;
                    
public class Program
{
    public static async Task Main()
    {
        var oddsem = new SemaphoreSlim(1,1);
        var evensem = new SemaphoreSlim(0,1);
        
        await Task.WhenAll(
            Task.Run(async () => {
                for(int i = 0; i < 10; i++){
                    await oddsem.WaitAsync();
                    try{
                        if( i % 2 == 0 ) Console.WriteLine(""A: {0}"",i);
                    }
                    finally{evensem.Release();}
                }
            }), 
            Task.Run(async () => {
                for(int i = 0; i < 10; i++){
                    await evensem.WaitAsync();
                    try{
                        if( i % 2 == 1 ) Console.WriteLine(""B: {0}"",i);
                    }
                    finally{oddsem.Release();}
                }
            })
        );
    }
}

Output:

A: 0
B: 1
A: 2
B: 3
A: 4
B: 5
A: 6
B: 7
A: 8
B: 9

See Fiddle.
Which defeats all the benefits of making this parallel in the first place.
",
76388235,Multi-Factor Authentication : FirebaseError: Firebase: Error (auth/code-expired),"I've tried integrating Firebase multi-factor authentication to your web app and have followed their documentation:https://firebase.google.com/docs/auth/web/multi-factor
Everything works fine until i get to sms verification. After i get a code and instantly type it in i keep getting FirebaseError: Firebase: Error (auth/code-expired) and i can't figure out why. Keep in mind this is my firste time working with firebase.
Here is my code.(I Change the number to xxxxxxx on purpose for showing you my code). Thanks for your help and help in advanced
    signInWithEmailAndPassword(auth, email, password)
  .then((userCredential) => {
    console.log('Sign in Success');
    navigate(""/Search"");
  })
  .catch(function (error) {
    if (error.code === 'auth/multi-factor-auth-required') {
      console.log('MFA required');
      const resolver = getMultiFactorResolver(auth, error);
      const selectedIndex = 0;
      if (resolver.hints[selectedIndex].factorId === PhoneMultiFactorGenerator.FACTOR_ID) {
        console.log(resolver.hints[selectedIndex].factorId);
        const phoneNumber = ""XXXXXXXX"";
        console.log(resolver.session);
        const phoneInfoOptions = {
          multiFactorHint: resolver.hints[selectedIndex],
          session: resolver.session
        };

        const phoneAuthProvider = new PhoneAuthProvider(auth);

        console.log(phoneAuthProvider);
        // Create recaptchaVerifier instance
        const recaptchaVerifier = new RecaptchaVerifier(
          ""recaptcha-container"",
          {
            size: ""normal"",
            callback: function (response) {
              // reCAPTCHA resolved, continue with phone verification
              phoneAuthProvider.verifyPhoneNumber(phoneNumber, recaptchaVerifier, {
                // Set the code expiration duration to 60 seconds
                codeTime: 60000
              })
                .then(function (verificationId) {
                  // Ask user for the SMS verification code. Then:
                  const verificationCode = prompt(""Enter the verification code"");
                  const cred = PhoneAuthProvider.credential(verificationId, verificationCode);
                  console.log(cred)
                  const multiFactorAssertion = PhoneMultiFactorGenerator.assertion(cred);
                  // Complete sign-in.
                  return resolver.resolveSignIn(multiFactorAssertion);
                })
                .then(function (userCredential) {
                    navigate(""/Search"");
                })
                .catch(function (error) {
                  console.log(error)
                });
            },
            ""expired-callback"": function () {
              // Response expired. Ask user to solve reCAPTCHA again.
              // ...
            },
          },
          auth
        );

        recaptchaVerifier.render().then(function (widgetId) {
          window.recaptchaWidgetId = widgetId;
        });
      }
    }
  });

",76388723,"Try changing phoneInfoOptions to phoneNumber in:
phoneAuthProvider.verifyPhoneNumber(phoneInfoOptions, recaptchaVerifier, {
// Set the code expiration duration to 60 seconds
codeTime: 60000
})

",
76391385,Is Compile-time Polymorphism in C++ Templates Possible?,"Consider the following code snippet:
#include <iostream>
#include <typeinfo>


template<typename T>
class TypePrinter{
    public:
    void print_type() {
        std::cout << typeid(T).name() << std::endl;
    }
}; // end class TypePrinter


class Base{
    public:
    TypePrinter<Base> createTypePrinter(){ // TODO replace ""Base"" with the class of the instance.
        return TypePrinter<Base>();
    }
}; // end Base

class Derived: public Base{
};

int main() {
    // Create a derived instance and try to create a TypePrinter for it
    Derived d;
    auto tp = d.createTypePrinter();
    tp.print_type(); // prints ""4Base"".
    return 0;
} // end main

In the example above, I have

a class with a typename template parameter (TypePrinter) and
another class that instantiate it, using its type as a parameter (Base)

How can the code above be rewritten so that the inherited derived_instance.createTypePrinter() method can create a TypePrinter<Derived> rather than a TypePrinter<Base>?
In other words, what modifications could allow d.createTypePrinter().print_type() to print ""Derived""?
",76391547,"As noted in the comments, this can be implemented with the Curiously Recurring Template Pattern.
#include <iostream>
#include <typeinfo>


template<typename T>
class TypePrinter{
    public:
    void print_type() {
        std::cout << typeid(T).name() << std::endl;
    }
}; // end class TypePrinter

template<typename T> // Base is now a template class. 
                     // Subclasses set T to their type
class Base{
    public:
    TypePrinter<T> createTypePrinter(){
        return TypePrinter<T>();
    }
}; // end Base

class Derived: public Base<Derived>{
};

int main() {
    // Create a derived instance and try to create a TypePrinter for it
    Derived d;
    auto tp = d.createTypePrinter();
    tp.print_type(); // Now this prints ""7Derived""
    return 0;
} // end main


",
76388608,NuGet Compatible vs Computed Framework (Xamarin and .NET 6),"We are currently looking at migrating existing Xamarin.Android and Xamarin.iOS applications to .NET 6 (we aren't at this stage going to use MAUI).
A big part of this process is understanding which NuGet packages we need to update or replace.
We're just trying to understand the difference between a package being compatible with a framework, and a framework being an additionally computed framework.
We can see for some of the packages we are using that net6.0-ios and net6.0-android are additionally computed frameworks rather than being compatible. What does this mean in terms of whether or not a net6.0-ios or net6.0-android app could consume the package?
",76388726,"It really depends.
Lets say you are trying to consume a package that only targets netstandard1.0 or above. That package will be totally fine to consume.
However, if you are attempting to consume a package that has multiple targets, lets say

netstandard1.0
Xamarin.iOS
monoandroid13.0

That package will only partially work. The netstandard stuff will definitely work. monoandroid13.0 will most likely work. However, since there are breaking changes between Xamarin.iOS and net6.0-ios then that part won't work.
Most preferably, for packages that target a specific platform, that should at least be net6.0-android or net6.0-ios, otherwise you will encounter issues.
",
76391254,"Safety of using and approach to ""floating"" promises","In the MDN Docs it explains,

Important: Always return results, otherwise callbacks won't catch the result of a previous promise (with arrow functions, () => x is short for () => { return x; }). If the previous handler started a promise but did not return it, there's no way to track its settlement anymore, and the promise is said to be ""floating"".

With this little json file:
{
  ""ingredients"": ""flour,sugar,butter,chocolate""
}

const listOfIngredients = [];
console.clear();
function doSomething() {
  fetch('http://127.0.0.1:4000/test.json')
  .then((res) => res.json())
  .then((data) => {
    data.ingredients.split(',').forEach(i => listOfIngredients.push(i)); 
  // doesn't return
  })
  .then((nothing) => {
    console.log(nothing); // undefined, of course
    console.log(listOfIngredients);
    return ""and coconut"";
  })
  .then((something) => {
    listOfIngredients.push(something);
    console.log(listOfIngredients);
  });
}
doSomething();

Result:
undefined
['flour', 'sugar', 'butter', 'chocolate']
['flour', 'sugar', 'butter', 'chocolate', 'and coconut']

So, I think that the result of the success callback, (data) => {...} is (said to be) ""floating"", but that since in this case, the promise returns immediately, the chained (nothing) => {...} and (something) => {...} callbacks succeed.
If I update the (nothing) callback to contain another fetch, that fetch does need to be returned ((nothing) => { return fetch...  otherwise we get ['flour', 'sugar', 'butter', 'chocolate', undefined]. Makes sense.
So, in promise chains, do you only need to return from the callbacks for promise objects that handle asynchronous operations?
",76391581,"
So, in promise chains, do you only need to return from the callbacks for promise objects that handle asynchronous operations?

You are correct. From the mdn doc you provided:

If the previous handler started a promise but did not return it, there's no way to track its settlement anymore, and the promise is said to be ""floating"".

Notice in this one:
.then((data) => {
    data.ingredients.split(',').forEach(i => listOfIngredients.push(i)); 
  // doesn't return
  })

you aren't starting a promise so there is no issue.
Floating promises have nothing to do with returning the data but rather returning when the promise resolves you could do something like this for instance:
.then(async res => {
    window.myGlobal = await res.json();
}).then(nothing => /* ... continue */ undefined);

even though we aren't returning the data directly the function provided only resolves when res.json() has finished so we don't have any hanging promises.
Incidentally your example does have a hanging promise: the doSomething function. the doSomething function should be returning the fetch call as in:
function doSomething() {
  return fetch('http://127.0.0.1:4000/test.json')
  /* ... */
}

Note that if you didn't want to return the fetch for some reason you could still avoid the hanging promise by using async/await like:
async function doSomething() {
  await fetch('http://127.0.0.1:4000/test.json')
  /* ... */
}

this is functionally equivalent to the following however:
function doSomething() {
  return fetch('http://127.0.0.1:4000/test.json')
      /* ... your existing .then chain goes here */
      .then(() => undefined)
  }
}

Also note that as mentioned in the comments you are probably doing something really bad if you are writing code this way (not returning the results of asyncronous operations but still wanting to do things after they complete)
",
76388587,How to change the separator of a C# TextBox when I double click to select a word?,"When I use a C# TextBox as an input box, I found it hard to quickly select a word by doubleclicking on then text, because it sometimes selects brackets and other characters.
For example, when I double click on var3, it selects  and what I want is .
I tried adding doubleclick callback, but it seems be handled after default handler, and I don't know where user actually clicked, this also messes up the dragging operation after doubleclick to select multiple words.
Is there a easy way to redefine the separator chars for word selection?
",76388739,"You can handle the DoubleClick event and modify the SelectionStart and SelectionLength properties accordingly. Trim the delimiters at the start and end:
private static readonly char[] Delimiters = { ' ', '(', ')', ',', ';', '.', '-', ':', '[', ']' }; // and so on...

private void textBox1_DoubleClick(object sender, EventArgs e)
{
    TextBox txt = (TextBox)sender;
    string selectedText = txt.SelectedText;
    int selectStartOld = txt.SelectionStart;
    int selectLengthOld = txt.SelectionLength;
    int selectStartNew = selectStartOld;
    int selectLengthNew = selectLengthOld;
    bool inText = false;

    for (int i = 0; i < selectedText.Length; i++)
    {
        if (!inText && Delimiters.Contains(selectedText[i]))
        {
            // TrimStart
            selectStartNew++;
            selectLengthNew--;
        }
        else if (inText && Delimiters.Contains(selectedText[i]))
        {
            // TrimEnd
            selectLengthNew--;
        }
        else
        {
            inText = true;
        }
    }

    txt.SelectionStart = selectStartNew;
    txt.SelectionLength = selectLengthNew;
} 

",
76389849,Pandas drop_duplicates with a tolerance value for duplicates,"What I have is two Pandas dataframes of coordinates in xyz-format. One of these contains points that should be masked in the other one, but the values are slightly offset from each other, meaning a direct match with drop_duplicates is not possible. My idea was to round the values to the nearest significant number, but this also does not always work, since if some values are rounded to different numbers, they won't match and won't be removed. For example, if one point lies at x = 149 and another at x = 151, rounding them to the nearest hundred gives different values. My code looks something like this:
import pandas as pd
import numpy as np
df_test_1 = pd.DataFrame(np.array([[123, 449, 756.102], [406, 523, 543.089], [140, 856, 657.24], [151, 242, 124.42]]), columns = ['x', 'y', 'z'])

df_test_2 = pd.DataFrame(np.array([[123, 451, 756.099], [404, 521, 543.090], [139, 859, 657.23], [633, 176, 875.76]]), columns = ['x', 'y', 'z'])

df_test_3 = pd.concat([df_test_1, df_test_2])

df_test_3['xr'] = df_test_3.x.round(-2)
df_test_3['yr'] = df_test_3.y.round(-2)
df_test_3['zr'] = df_test_3.z.round(1)

df_test_3 = df_test_3.drop_duplicates(subset=['xr', 'yr', 'zr'], keep=False)

What I want is to remove duplicates if the columns 'xr' and 'yr' are duplicates +-100 and 'zr' duplicates +-0.1. For example, if two coordinates are rounded to (100, 300, 756.2) and (200, 400, 756.1), they should be considered duplicates and should be removed. Any ideas are appreciated, thanks!
",76390106,"You can numpy broadcasting:
# Convert to numpy
vals1 = df_test_1.values
vals2 = df_test_2.values

# Remove from df_test_1
arr1 = np.abs(vals1 - vals2[:, None])
msk1 = ~np.any(np.all(arr1 < [100, 100, 0.1], axis=2), axis=1)

# Remove from df_test_2
arr2 = np.abs(vals2 - vals1[:, None])
msk2 = ~np.any(np.all(arr1 < [100, 100, 0.1], axis=2), axis=1)

out = pd.concat([df_test_1[msk1], df_test_2[msk2]], ignore_index=True)

Output:
>>> out
       x      y       z
0  151.0  242.0  124.42
1  633.0  176.0  875.76

Comment of @James

This removes left vs right and right vs left, but not duplicates within left vs left or right vs right.

In this case:
df_test_3 = pd.concat([df_test_1, df_test_2])

arr = df_test_3.values
msk = np.abs(arr - arr[:, None]) < [100, 100, 0.1]
out = df_test_3[np.sum(np.all(msk, axis=2), axis=1) == 1]
print(out)

# Output
       x      y       z
3  151.0  242.0  124.42
3  633.0  176.0  875.76

",
76391597,Publishing a .Net Core 6 Windows Application Will Not Run,"I thought the idea of .Net Core was that I did not have to install a Run-Time, it was included in the published code. However, when I run my App nothing happens. The Application Event Log says...""Message: You must install .NET to run this application."" When I look for .NET Core Runtimes, I only find SDK's. Where are the run-times? What is the published ""Runtimes"" folder for?
",76391645,".NET Core is just a smaller runtime, mostly split into separate libraries, but it certainly DOES require a runtime, just like normal .NET programs. At build time you could bundle it in the exe or you can depend on an external installed library, but ultimately it's still a dependency, much like with the real .NET Framework.
You can find the runtimes in the Microsoft website. Ror example, here is the .NET Core 6 runtimes download webpage, containing the SDKs, ASP libraries and the standalone runtime.
https://dotnet.microsoft.com/es-es/download/dotnet/6.0
",
76388686,getting a problem where the radio button selections are getting reset to the first radio option,"I am a beigner.And i dont know why the gender radio input is getting reset when i click anywhere on the screen below the rdio buttons section.Can anyone help me with this.This below is my code⬇️⬇️⬇️⬇️⬇️
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Document</title>
</head>
<body>
    <h2>Sign Up Form</h2>
    <br><br>
    <form method=GET action=""./pages mine/thank you.html"">
        <input type=""text"" placeholder=""First name""><br><br>
        
        <input type=""text"" placeholder=""Last name""><br><br>
    
        <input type=""email"" placeholder=""Last name""><br><br>
    
        <input type=""password"" placeholder=""Last name"" minlength=""5""><br><br>
<!--gender selection-->
        <label><h3>Select Gender<h3></label><br>
        
        <label for=""male"">male</label>
        <input type=""radio""  value=""male"" id=""male"" name=""gender"" required/><br>
        
        <label for=""female"">female</label>
        <input type=""radio"" value=""female"" id=""female"" name=""gender"" required/><br>
        
        <label for=""other"">other</label>
        <input type=""radio"" value=""other"" id=""other"" name=""gender""  required/><br><br>
<!--when i click on the screen anywhr after this the gender selection is getting reset-->        
        <label for=""nation"">Country</label>
        <select id=""nation"" >
            
            <option value=""india"" name=""nation"">INDIA</option>
            <option value=""usa"" name=""nation"">USA</option>
            <option value=""other"" name=""nation"">OTHER</option>
        </select><br><br>
        <p>Write about yourself below</p><br>
        <input type=""textbox"" placeholder=""type here""><br>
        <input type=""submit"" placeholder=""Sign in"">
    </form>
    
</body>
</html>

i was trying the make radio buttons for gender selection but the selection is getting reset on click any where on the screen after the radio buttons section.
",76388767,"There is a lot of things going wrong in this code.
But for your radio button issue, i think this is caused by <label><h3>Select Gender<h3></label><br>. Your <h3>node is not closed. It should be :
<h3>Select Gender</h3>
Also you don't need a labelaround that.
Here is a refactored version of your code :
https://jsfiddle.net/dzt8ja6n/
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Document</title>
</head>
<body>
    <h2>Sign Up Form</h2>
    <br><br>
    <form method=""GET"" action=""./pages mine/thank you.html"">
        <input type=""text"" placeholder=""First name""><br><br>
        
        <input type=""text"" placeholder=""Last name""><br><br>
    
        <input type=""email"" placeholder=""Email""><br><br>
    
        <input type=""password"" placeholder=""Password"" minlength=""5""><br><br>

        <!-- Gender selection -->
        <label for=""gender""><h3>Select Gender</h3></label><br>
        
        <label for=""male"">Male</label>
        <input type=""radio"" value=""male"" id=""male"" name=""gender"" required><br>
        
        <label for=""female"">Female</label>
        <input type=""radio"" value=""female"" id=""female"" name=""gender"" required><br>
        
        <label for=""other"">Other</label>
        <input type=""radio"" value=""other"" id=""other"" name=""gender"" required><br><br>
        
        <label for=""nation"">Country</label>
        <select id=""nation"" name=""nation"">
            <option value=""india"">INDIA</option>
            <option value=""usa"">USA</option>
            <option value=""other"">OTHER</option>
        </select><br><br>
        
        <p>Write about yourself below</p><br>
        <textarea placeholder=""Type here""></textarea><br>
        
        <input type=""submit"" value=""Sign in"">
    </form>
</body>
</html>

",
76388597,  issue when in html attribute,"Code first.
<div data-a=""a b"" data-b=""a&nbsp;b"" id=""test""></div>
<div data-a=""a<b"" data-b=""a&lt;b"" id=""test2""></div>

var test1= document.getElementById('test');

var test1_a= test.getAttribute('data-a');
var test1_b=test.getAttribute('data-b');

// data-a=""a b"" data-b=""a&nbsp;b""
console.log('1:',test1_a===test1_b); // got false;



var test2= document.getElementById('test2');

var test2_a= test2.getAttribute('data-a');
var test2_b=test2.getAttribute('data-b');
// data-a=""a<b"" data-b=""a&lt;b""
console.log('2:',test2_a===test2_b); // got true;


Question: why &nbsp; and &lt; are different in html attribute?
Online run able example.
https://codepen.io/qinbx/pen/eYPqBGQ
",76388771,"In your case, the problem comes from the data-a attribute, not the HTML Entities.
The 'space' character is different from the 'non-breaking space' character.
See this list of HTML entities and note that the space character can be written using its entity number &#32;.
I updated your example using the 'non breaking space' character (alt + 255) :


var test1= document.getElementById('test');

var test1_a= test.getAttribute('data-a');
var test1_b=test.getAttribute('data-b');

console.log('1:',test1_a===test1_b);

var test2= document.getElementById('test2');

var test2_a= test2.getAttribute('data-a');
var test2_b=test2.getAttribute('data-b');

console.log('2:',test2_a===test2_b);
<!-- a space b -->
<div data-a=""a b"" data-b=""a&#32;b"" id=""test""></div>
<!-- a non-breaking space b (alt + 255) -->
<div data-a=""a b"" data-b=""a&nbsp;b"" id=""test2""></div>



",
76390067,How do I align prefixIcon with text on textFormField?,"I have the following code for my textFormField:
 Container(
                  alignment: Alignment.centerLeft,
                  height: media.height * 0.12,
                  decoration: txtFieldBoxDecoration,
                  child: TextFormField(
                    textAlignVertical: TextAlignVertical.center,
                    keyboardType: TextInputType.emailAddress,
                    style: const TextStyle(
                      color: Colors.white,
                      fontSize: 26,
                    ),
                    decoration: InputDecoration(
                      border: InputBorder.none,
                      contentPadding: const EdgeInsets.only(top: 14),
                      prefixIcon: const Icon(
                        Icons.email,
                        color: Colors.white,
                        size: 28,
                      ),
                      hintText: 'Insira seu E-mail',
                      hintStyle: GoogleFonts.openSans(
                          color: Colors.white54, fontSize: 26),
                    ),
                    onChanged: (value) {},
                  ),
                ),

And I'm getting the following result: 
How do I align the prefix Icon and the hintText like this: 
",76390118,"For align the prefix Icon and the hintText,
You have to remove below code from InputDecoration
contentPadding: const EdgeInsets.only(top: 14),
",
76391041,How to write a Makefile to test multiple Unittest targets?,"I am trying to write a generic Makefile that can run multiple different unit tests based on a second term in the make command.
Basically I would like to write something like:
make test target1 # runs unit tests for target 1
make test target2 # runs unit tests for target 2
      :
      :
      :
make test all     # runs all unit tests

but I can't find any documentation on how to do this. What I have right now is:
.PHONY: clean test
test-target1:
        pytest --cov-report term-missing --cov=a .\target1\
test-target2:
        pytest --cov-report term-missing --cov=b .\target2\
test-all:
        ### I don't know what to put here ###

but this syntax requires you to use make test-target1 with the hyphen included.
EDIT:
Based on some really good advice I now have the updated Makefile:
a: # ...
    pytest --cov-report term-missing --cov=a.\UnitTest\a

.PHONY: test
test : $(RUN_ARGS)
    @echo ""Running unittests for $(RUN_ARGS)...""

but running make test a returns
Running unittests for ...
make: 'a' is up to date.

and if I change a to a1 then make test a1 will run the actual unittests. Why is the a RUN_ARGS not allowed?
For reference, the structure of the code is:
src/a
src/b
src/UnitTest
src/UnitTest/a
src/UnitTest/b

",76391662,"Thanks to all the help I received, I was able to solve both parts of my question by creating the following Makefile:
all: a b c

a: 
    pytest --cov-report term-missing --cov=a .\UnitTest\a\

b: 
    pytest --cov-report term-missing --cov=b .\UnitTest\b\

c: 
    pytest --cov-report term-missing --cov=c .\UnitTest\c\

.PHONY: test a b c

test : $(RUN_ARGS)
    @echo ""Running unittests...""

",
76388755,pd.to_datetime with multiple format,"I need to get a timestamp out of multiple time-formats like:
df['date'] = [29.05.2023 01:00:00, 
              29.05.2023, 
              28.05.2023 23:00:00]

At 00:00:00 on every day the time is missing and only the date has been logged.
I need the timestamp to look like this:
df['date'] = [29.05.2023 01:00:00, 
              29.05.2023 00:00:00, 
              28.05.2023 23:00:00]

",76388787,"Use to_datetime with dayfirst=True parameter, for final ouput DD.MM.YYYY HH:MM:SS use Series.dt.strftime:
df = pd.DataFrame({'date':['29.05.2023 01:00:00', 
                           '29.05.2023', 
                           '28.05.2023 23:00:00',
                           '4.10.2023']})


df['date'] = pd.to_datetime(df['date'], dayfirst=True).dt.strftime('%d.%m.%Y %H:%M:%S')
print (df)
                  date
0  29.05.2023 01:00:00
1  29.05.2023 00:00:00
2  28.05.2023 23:00:00
3  04.10.2023 00:00:00

",
76390099,The elasticsearch html_strip character filter doesn't work as expected with query_search query,"The behaviour of html_strip character filter looks different with the search query using ""query_string"" and ""match"" query.

Created index with ""description"" field using my_analyzer having html_strip char filter

{
  ""settings"": {
    ""analysis"": {
      ""analyzer"": {
        ""my_analyzer"": {
          ""tokenizer"": ""standard"",
          ""char_filter"": [
            ""html_strip""
          ]
        }
      }
    }
  },
  ""mappings"": {
    ""properties"": {
      ""description"": {
        ""type"": ""text"",
        ""analyzer"": ""my_analyzer""
      }
    }
  }
}


Indexed as below with sample html content

PUT test_index/_doc/001
{
  ""description"": ""<html><body>Hello world</body></html>""
}

document created.

search using <html> :


GET /test_index/_search
{
  ""query"": {
    ""query_string"": {
      ""query"": ""<html>""
    }
  }
}

Expecting no hits as the html contents would be stripped out internally when using html_strip character filter as per the document (https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-htmlstrip-charfilter.html) but got the result,
result:
{
    ""took"": 2,
    ""timed_out"": false,
    ""_shards"": {
        ""total"": 1,
        ""successful"": 1,
        ""skipped"": 0,
        ""failed"": 0
    },
    ""hits"": {
        ""total"": {
            ""value"": 1,
            ""relation"": ""eq""
        },
        ""max_score"": 0.2876821,
        ""hits"": [
            {
                ""_index"": ""test_index"",
                ""_type"": ""_doc"",
                ""_id"": ""001"",
                ""_score"": 0.2876821,
                ""fields"": {
                    ""description"": [
                        ""<html><body>Hello world</body></html>""
                    ]
                }
            }
        ]
    }
}

Returns the record (ideally it should not).

Let's search using ""match"" query:

When match query is used it could not find a record when searched using <html> or <body>. whereas it returns record when searched using ""Hello"" which seems to be the right behaviour of html_strip char filter.
GET /test_index/_search
{
    ""query"": {
    ""match"": {
      ""description"": ""<html>""
    }
  },
  ""fields"": [
    ""description""
  ],
   ""_source"": false
}

Result:
{
    ""took"": 2,
    ""timed_out"": false,
    ""_shards"": {
        ""total"": 1,
        ""successful"": 1,
        ""skipped"": 0,
        ""failed"": 0
    },
    ""hits"": {
        ""total"": {
            ""value"": 0,
            ""relation"": ""eq""
        },
        ""max_score"": null,
        ""hits"": []
    }
}

the above query doesn't return any result and looks like the html_strip character filter is working fine with ""match"" query.
Any suggestions why html_strip character filter is not working as expected with ""query_string"" query and working with ""match"" query?
Am I missing anything here?
",76390137,"You didn't define any field in your query. By default, it hits all fields include description, and description.keyword.
When you trying with match query you are defining the fields :).
Update query like the following
GET /test_html_strip/_search
{
  ""query"": {
    ""query_string"": {
      ""query"": ""<html>"",
      ""fields"": [""description""]
    }
  }
}

",
76391406,How to extract blob data from websocket response,"I'm trying to get market data from bingx websocket API but once my connection stablished i recieve blob data and i dont know how can i read candlestick data from that blob response with javascript does any one know how can i extract data?
here is my Js code :
var ws = new WebSocket('wss://open-ws-swap.bingbon.pro/ws');

    ws.onopen = (event) => {
        const msg = {
            id: ""548"",
            reqType:""sub"",
            dataType: ""market.kline.BTC-USDT.1min"",
        }
        ws.send(JSON.stringify(msg));
        console.log('connection stablished!')
    }


    // Listen for messages
    ws.onmessage = function(event) {
        console.log(event.data);
    }

    ws.onclose = function(event) {
        console.log(event)
    };

    ws.onerror = function(error) {
        console.log(error);
    };

And here is the result in console:

I tried this answer bot not working:
text
",76391673,"From the docs ->

All response data from Websocket server are compressed into GZIP
format. Clients have to decompress them for further use

So using this information we can use DecompressionStream -> https://developer.mozilla.org/en-US/docs/Web/API/DecompressionStream  to uncompress the data, and then use TextDecoder to convert that into to text, finally JSON.parse to convert into an object.
Finally you will want to watch for the Ping message, as the API will disconnect if you don't send a Pong.
eg.


var ws = new WebSocket('wss://open-ws-swap.bingbon.pro/ws');


async function decompressBlob(blob) {
  const ds = new DecompressionStream(""gzip"");
  const decompressedStream = blob.stream().pipeThrough(ds);
  return await new Response(decompressedStream).blob();
}




ws.onopen = (event) => {
   const msg = {
      id: ""548"",
      reqType:""sub"",
      dataType: ""market.kline.BTC-USDT.1min"",
   }
   ws.send(JSON.stringify(msg));
   console.log('connection stablished!')
}

// Listen for messages
ws.onmessage = async function(event) {
   const data = await decompressBlob(event.data);
   const txt = new TextDecoder().decode(await data.arrayBuffer());
   if (txt === 'Ping') {
     //this keeps the api open
     ws.send('Pong');
   } else {
     console.log(JSON.parse(txt));
   }
}

ws.onclose = function(event) {
   console.log(event)
};

ws.onerror = function(error) {
   console.log(error);
};



",
76389592,Rspec confused when I run the tests on the rspec with the --pattern and --exclude-patternn flags,"I need to run all the tests from the ""spec/workers/**"" folder, but I don't want to run the tests in a specific folder that is inside the ""spec/workers/"". I'm using the --pattern flags to say what I want and --exclude--pattern to say what I don't want to run. and it still does what is described in --exclude-pattern.
Eg:
bundle exec rspec --pattern ""spec/workers/**/*_spec.rb"" --exclude-pattern ""spec/workers/study_workers/*_spec.rb""

",76390142,"This is not a bug but expected behavior.
When there are two filters defined, one to include a specific files and another to exclude the same specific files, then excluding pattern has a lower priority and therefore the file is included.
Please see the discussion in this GitHub issue.
",
76388276,How can I make flutter 2 finger zoom?,"There is a datatable in a page in Flutter, I want to zoom in and out of this table or page with two fingers. Can anyone offer a solution ?
My main goal here is just to zoom in and out. Is there a widget for this? I couldn't find a way, what can I do? Can you help me ?
There is a video in the link
I have specified all my table codes below. You can review and have details
here are my codes
ListView(children: <Widget>[
              SizedBox(
                width: double.infinity,
                child: Padding(
                  padding: EdgeInsets.fromLTRB(size.width * 0.03,
                      size.width * 0.1, size.width * 0.03, size.width * 0.00),
                  child: TextField(
                    cursorColor: BaseData.vadenHomeSiyah,
                    controller: txtPaletNo,
                    // focusNode: unitCodeCtrlFocusNode,
                    textAlign: TextAlign.center,
                    decoration: InputDecoration(
                        focusedBorder: OutlineInputBorder(
                          borderSide: const BorderSide(
                              color: Color.fromARGB(255, 0, 0, 0), width: 2.0),
                          borderRadius: BorderRadius.circular(25.0),
                        ),
                        border: UnderlineInputBorder(),
                        labelText: 'Palet No',
                        labelStyle: TextStyle(
                            color: Colors.black, fontSize: size.width * 0.05),
                        filled: true,
                        isDense: true,
                        fillColor: Colors.white.withOpacity(0.2)),

                    //işlemler
                    onSubmitted: (value) {
                      setState(() {
                        paletRaporYuklendimi = true;
                      });
                      seciliPaletRaporGetir();
                    },
                    onChanged: (text) {},
                  ),
                  // TextField(
                  //   controller: searchController,
                  //   decoration: const InputDecoration(
                  //     hintText: 'Ara...',
                  //     border: OutlineInputBorder(),
                  //   ),
                  //   onChanged: _onSearchTextChanged,
                  // ),
                ),
              ),
              Padding(
                padding: EdgeInsets.fromLTRB(size.width * 0.00,
                    size.width * 0.08, size.width * 0.00, size.width * 0.00),
                child: SizedBox(
                  width: MediaQuery.of(context).size.width,
                  child: FittedBox(
                    child: paletRaporYuklendimi
                        ? Center(
                            child: Padding(
                              padding: EdgeInsets.all(size.width * 0.2),
                              child: const CircularProgressIndicator(
                                color: Colors.black,
                              ),
                            ),
                          )
                        : DataTable(
                            dataRowHeight: size.width * 0.4,
                            columns: const <DataColumn>[
                              DataColumn(
                                label: Text(
                                  'Palet No',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                              DataColumn(
                                label: Text(
                                  'Paket No',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                              DataColumn(
                                label: Text(
                                  'Tarih',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                              DataColumn(
                                label: Text(
                                  'Hareket Tip',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                              DataColumn(
                                label: Text(
                                  'Hücre Kodu',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                              DataColumn(
                                label: Text(
                                  'Hareket Turu',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                              DataColumn(
                                label: Text(
                                  'Cari Isim',
                                  style: TextStyle(
                                      fontWeight: FontWeight.w900,
                                      fontSize: 30),
                                ),
                              ),
                            ],
                            rows: List.generate(filterList.length, (index) {
                              final item = filterList[index];
                              return DataRow(
                                cells: [
                                  DataCell(Text(
                                    item.PALETNO ?? """",
                                    style: const TextStyle(
                                        fontWeight: FontWeight.w900,
                                        fontSize: 30),
                                  )),
                                  DataCell(Text(
                                    item.PAKETNO.toString(),
                                    style: const TextStyle(
                                        fontWeight: FontWeight.w900,
                                        fontSize: 30),
                                  )),
                                  DataCell(Text(
                                    item.TARIH ?? """",
                                    style: const TextStyle(
                                        fontWeight: FontWeight.w900,
                                        fontSize: 30),
                                  )),
                                  DataCell(item.HAREKETTIPI == ""G""
                                      ? Text(
                                          item.HAREKETTIPI ?? """",
                                          style: const TextStyle(
                                              fontWeight: FontWeight.w900,
                                              fontSize: 30,
                                              color: Colors.green),
                                        )
                                      : Text(
                                          item.HAREKETTIPI ?? """",
                                          style: const TextStyle(
                                              fontWeight: FontWeight.w900,
                                              fontSize: 30,
                                              color: Colors.red),
                                        )),
                                  DataCell(Text(
                                    item.HUCREKODU ?? """",
                                    style: const TextStyle(
                                        fontWeight: FontWeight.w900,
                                        fontSize: 30),
                                  )),
                                  DataCell(Text(
                                    item.HAREKETTURU ?? """",
                                    style: const TextStyle(
                                        fontWeight: FontWeight.w900,
                                        fontSize: 30),
                                  )),
                                  DataCell(Text(
                                    item.CARIISIM ?? """",
                                    style: const TextStyle(
                                        fontWeight: FontWeight.w900,`your text`
                                        fontSize: 30),
                                  )),
                                ],
                              );
                            }),
                          ),
                  ),
                ),
              ),
               if (filterList.length > 0)
                  Padding(
                    padding:  EdgeInsets.all(size.width * 0.03),
                    child:  Text(
                     ""Listelenen Kayıt Sayısı: "" +
                          filterList.length.toString(),
                      textAlign: TextAlign.right,style: TextStyle(color: Colors.green,fontWeight: FontWeight.bold),
                    ),
                  )
                else
                  Text("""")
            ]),
             

",76388832,"Use InteractiveViewer and wrap the widget you want to zoom in or out, you can specify the max and min zoom with fields minScale and maxScale, and, what is also very nice, you can use a TransformationController, like this one below, for example, and apply it to your InteractiveViewer Widget and enable also zooming in and out by controls:
final zoomTransformationController = TransformationController();

 void _zoomIn(){
   zoomTransformationController.value.scale(2);
 }
 void _zoomOut(){
   zoomTransformationController.value.scale(0.9);
 }


class InteractiveViewerExampleApp extends StatelessWidget {
  const InteractiveViewerExampleApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        body: const InteractiveViewerExample(),
      ),
    );
  }
}

class InteractiveViewerExample extends StatelessWidget {
  const InteractiveViewerExample({super.key});

  @override
  Widget build(BuildContext context) {
    return Center(
      child: InteractiveViewer(
        boundaryMargin: const EdgeInsets.all(20.0),
        minScale: 0.1,
        maxScale: 2,
 transformationController: zoomTransformationController,
        child: Container(
          ...
        ),
      ),
    );
  }
}

",
76391704,to_timesamp converting month to different month,"Any idea why snowflake to_timestamp is converting February month to January?
    SELECT to_timestamp(to_char('2022-02-02 08:01:29 AM'),'YYYY-MM-DD HH12:MM:SS AM');
    -- 2022-01-02 08:00:29.000

",76391757,"MM represent month when you convert to char , you should use MI for minute in snowflake, I assume the issue comes from there :
SELECT to_timestamp(to_char('2022-02-02 08:01:29 AM','YYYY-MM-DD HH12:MI:SS AM'));

",
76389885,Parent variable in Vue 3 not updated when using update-emit on prop,"I've the following very simple Vue 3 component:
<template lang=""pug"">
b-modal(v-model=""dshow"")
  template(v-slot:default) test
</template>

<script>
export default {
  props: {
    show: {
      type: Boolean,
      default: false
    }
  },
  data() {
    return {
      dshow: false
    }
  },
  watch: {
    show: function () {
      this.dshow = this.show
    },
    dshow: function () {
      this.$emit('update:show', this.dshow)
    }
  }
}
</script>

In my parent component, I'm using this as following:
import demo from 'mymodal.vue'

<template lang=""pug"">
button.btn.btn-primary.hstack.gap-2(@click=""showModal= true"") Show Modal!
demo(:show.sync=""showModal"")
</template>

<script>
export default {
  components: {
    demo
  },
  data() {
    return {
      showModal: false
    }
  }
}
</script>

Now, when I click on my button in the parent, the modal appears and when I close / hide the dialog, the callback for my watched variable dshow is also called. But unfortunately, my parent component isn't informed about the update. So when I click on my button next time, nothing happens, because the showModal hasn't been updated. this.$emit('update:show', this.dshow) is emitted, but the parent value isn't updated.
Any idea on this? I run completely out of ideas on this :-(
",76390146,"In Vue 3, the .sync modifier has been removed in favor of arguments to v-model (see migration guide)
So
:show.sync=""showModal""

now has to be
v-model:show=""showModal""

",
76388667,Enable clear-text local traffic on iOS not working in .NET MAUI,"I'm trying to connect to http in my .net MAUI app and I disabled the Apple Transport Security (ATS) following Microsoft documentation (from here) by adding below configuration in the info.plist file:
<key>NSAppTransportSecurity</key>    
<dict>
    <key>NSAllowsLocalNetworking</key>
    <true/>
</dict>

However, I'm still getting below error message when attempting to communicate with the API using http:
[0:] An error occurred: 'Error Domain=NSURLErrorDomain Code=-1022 ""The resource could not be loaded because the App Transport Security policy requires the use of a secure connection."" UserInfo={NSLocalizedDescription=The resource could not be loaded because the App Transport Security policy requires the use of a secure connection., NSErrorFailingURLStringKey=http://restapi.adequateshop.com/api/Tourist, NSErrorFailingURLKey=http://restapi.adequateshop.com/api/Tourist, _NSURLErrorRelatedURLSessionTaskErrorKey=(
    ""LocalDataTask <27DB8D6A-2EB6-41FE-953E-6FA2BFDBEDDC>.<1>""
), _NSURLErrorFailingURLSessionTaskErrorKey=LocalDataTask <27DB8D6A-2EB6-41FE-953E-6FA2BFDBEDDC>.<1>, NSUnderlyingError=0x280b9b330 {Error Domain=kCFErrorDomainCFNetwork Code=-1022 ""(null)""}}'. Callstack: ''

Any idea how to resolve this? Thank you in advance.
",76388836,"The key that is specified in that documentation page only works for local addresses like IP addresses or addresses that end in .local as you can see in the Apple documentation.
From your error message it seems that you are trying to reach a regular web address. In that case the key you need is: NSAllowsArbitraryLoads so that it opts out of ATS altogether.
Your full entry will then look like this:
<key>NSAppTransportSecurity</key>    
<dict>
    <key>NSAllowsArbitraryLoads</key>
    <true/>
</dict>

However, I would not recommend doing this. Please try to get your endpoints on HTTPS. It should not be expensive anymore nowadays and this way your users and you will be at risk.
If you really can't add HTTPS, try to limit the unsafe connections made for your app to a single domain. How that is done can be found here in the Apple documentation.
Basically, your info.plist entry should now look something like this:
<key>NSAppTransportSecurity</key>
<dict>
  <key>NSAllowsArbitraryLoads</key>
  <false/>
  <key>NSExceptionDomains</key>
  <dict>
    <key>adequateshop.com</key>
    <dict>
      <key>NSIncludesSubdomains</key>
      <true/>
      <key>NSExceptionAllowsInsecureHTTPLoads</key>
      <true/>
    </dict>
  </dict>
</dict>

But again, I would recommend fixing the server to be secure above all.
",
76391598,How to extract specific records from a JSON file that all have the same value in a specified field with python?,"I have a json file that has over 1000 records in it all similar to the one below:
[{""id"":0,""occupation"":""teacher"",""county"":""Santa Rosa"",""grade"":""3rd"",""workID"":""147767""},

I want to extract all the records that have a certain occupation, in this case I want to return the records where the occupations are teacher.
This is what I have tried:
specified_occ = {}
for o in data:
    specified_occ[o[""occupation""]] = o
specOcc = (specified_occ['teacher'])  

but it only returns the first instance rather than all of them that match.
My dataset has 50 records where the occupation is 'teacher' I want every record returned.
",76391769,"You can use a list comprehension to filter over your data and only select the needed elements. In general a list comprehension looks like this
filtered_list = [element for element in original_list if element == requirement]

Here you only select the elements from the original list if the elements match some requirement.
For this specific problem you would use
filtered_data = [person for person in data if person['occupation'] == 'teacher']

This is syntactically equivalent to
speccified_o = []
for o in data:
    if o['occupation'] == 'teacher':
        speccified_o.append(o)

",
76389800,Why is my C++ string erase-remove-idiom implementation causing an infinite loop with conditional statements?,"index is going out of bound it's not triggering while(i<str.length()) break; i tried to implement this line differently
string delStr(string s, int sIdx, int nChar) {
  string ans = """";
  int i = 0;
  int n = s.length()-1;
  while(n--){
    
    if(i==sIdx){
      i += nChar;
    }

    if(n<0) break;
    ans.push_back(s[i]);
    i++;
  }
}


Trying to implement string erase method.
",76390151,"You should be sure to post a minimal, reproducible example when you ask a question. This will help people provide an answer.
If I understand the function correctly, I believe it is supposed to return a new std::string with the characters from sIdx to sIdx + nChar removed. If that is the case, there is a logic error that I can't quite follow. You can compare the posted code to the sample below and maybe that will help you correct it.
In the sample code, I show two different solutions. The first uses the standard library algorithms and the second just iterates over the characters similar to the posted code. The advantage of using the algorithms is that they are well-tested for correctness, have efficient implementations, and serve as a common language among developers.
Sample Code
#include <iostream>

using std::cout, std::endl;

std::string delete_substr(const std::string& str, int sIdx, int nChar) {
    std::string answer;
    std::copy(str.begin(), str.begin() + sIdx, std::back_inserter(answer));
    std::copy(str.begin() + sIdx + nChar, str.end(), std::back_inserter(answer));
    return answer;
}

std::string delStr(const std::string& str, int sIdx, int nChar) {
    std::string answer;
    int i{};
    while (i < str.size()) {
        if (i == sIdx)
            i += nChar;
        else
            answer.push_back(str[i++]);
    }
    return answer;
}

int main(int argc, const char *argv[]) {
    std::string s0 = ""abcdefghi"";
    cout << delete_substr(s0, 3, 3) << endl;
    cout << delStr(s0, 3, 3) << endl;
    return 0;
}

Output
abcghi
abcghi

",
76387893,How to fix dynamically generated runner files importing from 'cucumber.api' in Cucumber/Junit parallel testing?,"I was trying to achieve parallel testing using cucumber with Junit. When I am trying to run the feature file parallely the runner file generated is importing from cucumber.api instead of io.cucumber. I want to know why it is dynamically taking from cucumber.api and also how can i fix that.Dynamic generated runner file
Tried Adding latest version of cucumber and junit but didint worked
",76388837,"You're using a more modern version of Cucumber then the code generator you're using was written for.
You don't need the code generator anymore because Cucumber supports parallel execution now.
You can use the cucumber-java-skeleton to get started. The cucumber-junit-platform-engine docs should explain everything else.
",
76390027,"SQL ORACLE, How to subtract sysdate?","How can I subtract the system date from the created task date in SQL ORACLE to get the difference in days, hours, minutes?
I tried this code but i get wrong values:
TO_CHAR(TO_DATE('1970-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') + (sysdate - a.CREATE_DATE_PL ), 'dd:hh24:mi:ss')  AS TT_LIFETIME
My results is below (the report has been generated 02.06  14:05
enter image description here
",76390173,"Difference of two DATE datatype values is number of days between them, so you have to do some arithmetic to extract days/hours/minutes, or - another option - to try extract with numtodsinterval.
For example:
SQL> WITH
  2     test (create_date_pl)
  3     AS
  4        (SELECT TO_DATE ('01.06.2023 08:10', 'dd.mm.yyyy hh24:mi') FROM DUAL)
  5  SELECT SYSDATE,
  6         create_date_pl,
  7         --
  8         SYSDATE - create_date_pl diff,
  9         NUMTODSINTERVAL (SYSDATE - create_date_pl, 'day') c1,
 10         EXTRACT (DAY FROM NUMTODSINTERVAL (SYSDATE - create_date_pl, 'day')) cd,
 11         EXTRACT (HOUR FROM NUMTODSINTERVAL (SYSDATE - create_date_pl, 'day')) ch,
 12         EXTRACT (MINUTE FROM NUMTODSINTERVAL (SYSDATE - create_date_pl, 'day')) cm
 13    FROM test;

SYSDATE          CREATE_DATE_PL         DIFF C1                                               CD         CH         CM
---------------- ---------------- ---------- ---------------------------------------- ---------- ---------- ----------
02.06.2023 14:29 01.06.2023 08:10 1,26333333 +000000001 06:19:12.000000000                     1          6         19

SQL>

When I ran that code, it was 2th of June 2023 at 14:29. Date to be subtracted was 1st of June 2023 (yesterday) at 08:10.

diff represents number of days between those two values
c1 is what numtodsinterval returns
cd, ch and cm contain extracted number of days/hours/minutes, i.e. difference is 1 day, 6 hours and 19 minutes

",
76391760,dependency injection in Angular when extending a class,"I am a Java guy learning Angular.
I have a service and I would like to have injected that service into my object. This doc explains well how to do it in a normal case. As I understand the concept, in the example we use constructor dependency injection.
But in my case, this is not possible because I am extending a class that has its own constructor with 3 parameters and I am not allowed to modify the parent constructor. So I can not add a 4th parameter to my constructor to have injected the service.
This code does not work because the last constructor param is invalid.
my code:
export class ChildComponent extends ParentComponent {

  constructor(readonly service1: Service1,
              readonly service2: Service2,
              el?: ElementRef | null,
              readonly service3: Service3) { <-- invalid param
    super(service1, service2, el);
  }

What is the alternative dependency injection if the constructor injection is not an option?
",76391774,"Instead of injecting the service through the constructor, you can declare a property in your child component and annotate it with the @Inject decorator to specify the service you want to inject.
",
76388653,Spring Security with Spring Boot 3 - Get JWT token from Security Context Holder,"After migrating to spring boot 3 ,
extends ResourceServerConfigurerAdapter is deprecrated.

Hence , unable to use the overrriden method
   @Override
public void configure(ResourceServerSecurityConfigurer config) {
    config.tokenServices(tokenServices());
}

I am able to replace this with
@Bean
public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
    http.csrf().disable();
    http.authorizeHttpRequests(auth->auth.
            requestMatchers(whitelistedEndPoints()).permitAll()
            .anyRequest()
            .anonymous())
            .httpBasic().disable();
    return http.build();
}

I have an existing code to get jwt token from OAuth as
 public String getJwtTokenFromAuth() {
    OAuth2Authentication auth =(OAuth2Authentication) SecurityContextHolder.getContext().getAuthentication();
  
    OAuth2AuthenticationDetails oauth2AuthenticationDetails = (OAuth2AuthenticationDetails) auth.getDetails();
    return oauth2AuthenticationDetails.getTokenValue();
}

However,

OAuth2Authentication and OAuth2AuthenticationDetails are not available

How can i replace this code/functionality with new spring security module of spring boot 3.
Below is the pom dependencies , please suggest if i need to add/remove any ?
  <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-security</artifactId>
    </dependency>

    <dependency>
        <groupId>org.springframework.security</groupId>
        <artifactId>spring-security-oauth2-resource-server</artifactId>
    </dependency>

",76388845,"Spring Security OAuth2 is deprecated and removed for a while. The replacement is built in Spring Security itself and the dependencies to use are exactly what you already know:
the required dependencies are:
<dependency>
 <groupId>org.springframework.boot</groupId>
 <artifactId>spring-boot-starter-security</artifactId>
</dependency>


<dependency>
    <groupId>org.springframework.security</groupId>
    <artifactId>spring-security-oauth2-resource-server</artifactId>
</dependency>

In order to gain the token value in a resource server you can do something like this
class PrincipalIntrospector {
    
    public String token() {
        Authentication authentication = SecurityContextHolder.getContext().getAuthentication();
        JwtAuthenticationToken oauthToken = (JwtAuthenticationToken) authentication;
        return oauthToken.getToken().getTokenValue();
    }
    
}

in order to make sure to have the role properly configured in your principal you can have something like below:
@Bean
public JwtAuthenticationConverter jwtAuthenticationConverter() {
    JwtAuthenticationConverter jwtAuthenticationConverter = new JwtAuthenticationConverter();
    jwtAuthenticationConverter.setJwtGrantedAuthoritiesConverter(jwt -> {
        List<String> authorities = jwt.getClaim(""authorities"");
        return authorities.stream().map(SimpleGrantedAuthority::new).collect(toList());
    });

    return jwtAuthenticationConverter;
}

in order to decode and make to spring to validate your jwt you can configure something like below:
// used in case of public key
@Bean
public JwtDecoder jwtDecoder() {
    return NimbusJwtDecoder.withPublicKey(this.key).build();
}
// used in case of the private key
@Bean
public JwtDecoder jwtDecoder() {
    return NimbusJwtDecoder.withSecretKey(this.key).build();
}

The spring security pipeline can look like below:
@Bean
public SecurityFilterChain filterChain(
        JwtDecoder  decoder,
        JwtAuthenticationConverter converter,
        HttpSecurity http) throws Exception {
    http.csrf().disable();
    http.authorizeHttpRequests(auth->auth.
                    requestMatchers(whitelistedEndPoints()).permitAll()
                    .anyRequest()
                    .authenticated())
            .oauth2ResourceServer().jwt().decoder(decoder).jwtAuthenticationConverter(converter);
    return http.build();
}

",
76389421,Can't connect to outside from the docker container managed by ECS on EC2,"I have ECS which uses EC2 and EC2 has a contianer.
From EC2 node I can access the outside with this command.
[ec2-user@ip-172-31-23-50 ~]$ curl google.com
<HTML><HEAD><meta http-equiv=""content-type"" content=""text/html;charset=utf-8"">
<TITLE>301 Moved</TITLE></HEAD><BODY>
<H1>301 Moved</H1>
The document has moved
<A HREF=""http://www.google.com/"">here</A>.
</BODY></HTML>

Then I login container running on this node, docker exec -it container-name /bin/bash
 root@ip-172-31-24-234:/# curl google.com

I can't access(nothing appears).
So, I check the security group of ECS Service.
However all outbound connection is allowed

Test
on EC2 node I try to run another container (Becasue I wanted to test the container which is not managed by ECS service).
`docker run -it ubuntu`

then install curl and try curl in the this ubuntu containaer,I can connect to the outside from the container.
So the problem happens only in ECS managed container,
My task difinition is made by CDK, maybe this Network mode is related....??
const adminTaskDefinition = new ecs.Ec2TaskDefinition(this, 'AdminTaskDefinition', {
  networkMode: ecs.NetworkMode.AWS_VPC,
})

",76390179,"Perhaps set your Network Mode = host in your task definition.
",
76391141,Creating a new column in SQL Oracle with a subquery - any suggestions?,"I have the columns Yearmonth, code, type and clntamount in my table. I want to derive column clntamount_der.
Table view

I want the 110 value in clntamount with the type = Total to be in the column clntamount_der where the type = base and the rest two rows(adj and total) as zero.
Please help with the logic.
I tried case statemnt but it didnt work maybe a subquery will work
",76391779,"You may use the conditional max window function as the following:
select t.*, 
  case 
    when TYPE = 'BASE' 
       then max(case when TYPE = 'TOTAL' then CLNTAMOUNT end) over
               (partition by YearMonth, Code)
       else 0
    end as clntamount_der
from tbl t
order by 
  YearMonth, Code,
  case TYPE
    when 'BASE' then 1
    when 'ADJUST' then 2
    when 'TOTAL' then 3
  end

demo
Also, it seems that the TOTAL is the sum of other types (BASE, ADJUST), in this case, it would be better to calculate the TOTAL on the fly (and not store it as a separate row) to avoid data inconsistency. i.e. when you update the CLNTAMOUNT value where type = 'BASE' how would you reflect this to the value where type = 'TOTAL'?
consider the following without having the TOTAL type stored in the table.
select d.*
  from
(
  select t.YearMonth, t.Code, t.Type, t.CLNTAMOUNT,
    case 
      when TYPE = 'BASE' 
         then sum(t.CLNTAMOUNT) over (partition by t.YearMonth, t.Code)
         else 0
      end as clntamount_der
  from tbl t
  
  union all
  
  select t.YearMonth, t.Code, 'TOTAL',
          sum(t.CLNTAMOUNT), 0 
  from tbl t
  group by t.YearMonth, t.Code
) d
order by 
  d.YearMonth, d.Code,
  case d.TYPE
    when 'BASE' then 1
    when 'ADJUST' then 2
    when 'TOTAL' then 3
  end

demo
",
76391738,Assign new values later to the underlying object of the interface in go,"I am trying to assign new values to the underlying structure of an interface in code below. But it keeps the older values. Below is the example code.
package main

import (
    ""fmt""
    ""math""
)

type Shape interface {
    Area() float64
}

type Circle struct {
    Radius float64
    Name   string
}

func (c Circle) Area() float64 {
    return math.Pi * c.Radius * c.Radius
}

type Rectangle struct {
    Length float64
    Width  float64
    Name   string
}

func (r Rectangle) Area() float64 {
    return r.Length * r.Width
}

func assignRadius(s Shape, radius float64, name string) {
    switch s := s.(type) {
    case Circle:
        s.Radius = radius
        s.Name = name
    case Rectangle:
        s.Length = radius
        s.Name = name
    }
}

func main() {
    var s Shape
    c := Circle{Radius: 0, Name: ""My Circle""}
    s = c
    fmt.Println(s.Area())
    fmt.Println(c.Radius)
    fmt.Println(c.Name)
    assignRadius(s, 10, ""My New Circle"")
    fmt.Println(c.Radius)
    fmt.Println(c.Name)
}

The type of Shape isn't known beforehand in the assignRadius. I know it has something to do with the pointers. But can't figure it out.
",76391785,"The interface variable s contains a copy of the shape value. To modify it like you attempted to do, it has to contain a pointer to that shape:
var s Shape
c := Circle{Radius: 0, Name: ""My Circle""}
s = &c

and in the function modifying them, you have to type-assert the pointer value:
func assignRadius(s Shape, radius float64, name string) {
    switch s := s.(type) {
    case *Circle:
        s.Radius = radius
        s.Name = name
    case *Rectangle:
        s.Length = radius
        s.Name = name
    }



",
76389675,Active UITextView moving in UIScrollView,"I have a large scrolling canvas that is 2000 pixels wide, with an unlimited length, inside of which is a single large UIView.
You can interact with the UIView to add objects (lines, shapes, text boxes, images, pdfs etc).
When you add a text box, I create a subclass of UITextView that is configured to auto-grow with text - limited to 600px wide and unlimited length (you can force it wider). Key point - UITextViews never scroll internally.
The problem I am having is moving the UIscrollview (i.e. the big canvas) to keep the cursor of the active text box visible. When you are on a limited width display (such as an iPhone), you should be able to click on a text box and start typing. The scroll view should move around to keep your current cursor (text entry point) on the screen, even when the UITextView is very wide.
This is not too hard - I have a UITextViewDelegate with a ""DidChangeSelection:"" function. When the selection moves and the delegate method gets called - I find the current cursor (SelectedTextRange), convert that into a rect (GetFirstRectForRange), then convert from UITextView coordinates to the UIView coordinates (ConvertRectToView), then request the UIScrollView being that rect into view (ScrollRectToVisible).
What is weird is that even though my rect is correct - the UITextView is always aligning so the far right hand edge is visible. So if the UITextView is 1000px wide, and the visible part of the UIScrollView is only 500px wide, then only ½ of the UITextView can be seen. No matter where the cursor is, the UIScrollView scrolls so that the right hand side of the UITextView is always visible.
The UITextView seems to be trying to make itself visible when it is the active control, but it seems to always attempt to ""ScrollToRect"" to its full width, and if the  UITextView is wider than the visible area, the UIScrollView always seems to show the right hand side. I know this because I can turn off my delegate, and the UITextView does scroll into view, but only the right hand side.
Is this standard behaviour? Ideally I want to turn off whatever mechanism is automatically scrolling the active UITextView into view so that my correct ScrollToRect call will be used instead.
Thanks for any advice you can give.
Cheers.
",76390184,"Couple options to try...
First, subclass UIScrollView:
class MyScrollView: UIScrollView {
    override func scrollRectToVisible(_ rect: CGRect, animated: Bool) {
        // don't do anything
    }
}

This will prevent the built-in ""auto-scroll"" when the text view is active.
Second option:

Embed the text view in a ""container scroll view""
disable scrolling of that scroll view
constrain the text view to that scroll view's .frameLayoutGuide, so the text view controls the size of the container scroll view
add that container scroll view as a subview of your ""content"" view

Now, when the text view is ""active,"" its ""container"" scroll view will receive the auto-scroll messages instead of the ""main"" scroll view.
In either case, of course, your code will be responsible for managing any horizontal / vertical scrolling needed to keep the insertion point visible.

Edit
Quick example for Second Option:
class ViewController: UIViewController {
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        let tv = UITextView()
        let tvsv = UIScrollView()
        let cv = UIView()
        let sv = UIScrollView()
        
        [tv, tvsv, cv, sv].forEach { v in
            v.translatesAutoresizingMaskIntoConstraints = false
        }
        tvsv.addSubview(tv)
        cv.addSubview(tvsv)
        sv.addSubview(cv)
        view.addSubview(sv)
        
        let g = view.safeAreaLayoutGuide
        let cg = sv.contentLayoutGuide
        let fg = sv.frameLayoutGuide
        
        let tsfg = tvsv.frameLayoutGuide
        
        NSLayoutConstraint.activate([
            
            tv.topAnchor.constraint(equalTo: tsfg.topAnchor, constant: 0.0),
            tv.leadingAnchor.constraint(equalTo: tsfg.leadingAnchor, constant: 0.0),
            tv.trailingAnchor.constraint(equalTo: tsfg.trailingAnchor, constant: 0.0),
            tv.bottomAnchor.constraint(lessThanOrEqualTo: tsfg.bottomAnchor, constant: 0.0),
            
            tv.widthAnchor.constraint(equalToConstant: 600.0),

            tvsv.topAnchor.constraint(equalTo: cv.topAnchor, constant: 20.0),
            tvsv.leadingAnchor.constraint(equalTo: cv.leadingAnchor, constant: 20.0),
            tvsv.bottomAnchor.constraint(lessThanOrEqualTo: cv.bottomAnchor, constant: -20.0),
            
            tv.widthAnchor.constraint(equalToConstant: 600.0),
            

            cv.topAnchor.constraint(equalTo: cg.topAnchor, constant: 20.0),
            cv.leadingAnchor.constraint(equalTo: cg.leadingAnchor, constant: 20.0),
            cv.trailingAnchor.constraint(equalTo: cg.trailingAnchor, constant: -20.0),
            cv.bottomAnchor.constraint(equalTo: cg.bottomAnchor, constant: -20.0),
            
            cv.widthAnchor.constraint(equalToConstant: 1000.0),
            
            sv.topAnchor.constraint(equalTo: g.topAnchor, constant: 20.0),
            sv.leadingAnchor.constraint(equalTo: g.leadingAnchor, constant: 20.0),
            sv.trailingAnchor.constraint(equalTo: g.trailingAnchor, constant: -20.0),
            sv.bottomAnchor.constraint(equalTo: g.bottomAnchor, constant: -20.0),
            
        ])
        
        tv.isScrollEnabled = false
        tvsv.isScrollEnabled = false

        // so we can see the framing
        tv.backgroundColor = .yellow
        cv.backgroundColor = .systemBlue
        sv.backgroundColor = .systemRed
        view.backgroundColor = .systemYellow
        
    }
    
}

Note: I have not implemented any code for manually handling the scroll view's content offset, which would be needed to keep the insertion point visible. However, this will demonstrate that the ""auto-scroll"" is no longer in effect.
",
76388754,How to get a List of valid currencies from Stripe,"We provide to a bunch of customers a website so that their customers can make a booking and purchase services online using Stripe. I am in the process of Internationalizing but I have hit a barrier where some currencies are not valid for Stripe and we don't find out until we attempt to make the payment. I have tried to find a way of retrieving a list of valid currencies from Stripe so that our software can check against it before offering the option to purchase services and I can't find anything like that.
Does anybody know of a solution for automating the process?
I am curious to hear how other people have tackled this problem.
",76388863,"You can use country_specs endpoint - https://stripe.com/docs/api/country_specs
CountrySpecService is available in Stripe.NET and returns this kind of objects (see model below). I think you are looking for SupportedBankAccountCurrencies or SupportedPaymentCurrencies
// File generated from our OpenAPI spec
namespace Stripe
{
    using System.Collections.Generic;
    using Newtonsoft.Json;

    /// <summary>
    /// Stripe needs to collect certain pieces of information about each account created. These
    /// requirements can differ depending on the account's country. The Country Specs API makes
    /// these rules available to your integration.
    ///
    /// You can also view the information from this API call as <a
    /// href=""https://stripe.com/docs/connect/required-verification-information"">an online
    /// guide</a>.
    /// </summary>
    public class CountrySpec : StripeEntity<CountrySpec>, IHasId, IHasObject
    {
        /// <summary>
        /// Unique identifier for the object. Represented as the ISO country code for this country.
        /// </summary>
        [JsonProperty(""id"")]
        public string Id { get; set; }

        /// <summary>
        /// String representing the object's type. Objects of the same type share the same value.
        /// </summary>
        [JsonProperty(""object"")]
        public string Object { get; set; }

        /// <summary>
        /// The default currency for this country. This applies to both payment methods and bank
        /// accounts.
        /// </summary>
        [JsonProperty(""default_currency"")]
        public string DefaultCurrency { get; set; }

        /// <summary>
        /// Currencies that can be accepted in the specific country (for transfers).
        /// </summary>
        [JsonProperty(""supported_bank_account_currencies"")]
        public Dictionary<string, List<string>> SupportedBankAccountCurrencies { get; set; }

        /// <summary>
        /// Currencies that can be accepted in the specified country (for payments).
        /// </summary>
        [JsonProperty(""supported_payment_currencies"")]
        public List<string> SupportedPaymentCurrencies { get; set; }

        /// <summary>
        /// Payment methods available in the specified country. You may need to enable some payment
        /// methods (e.g., <a href=""https://stripe.com/docs/ach"">ACH</a>) on your account before
        /// they appear in this list. The <c>stripe</c> payment method refers to <a
        /// href=""https://stripe.com/docs/connect/destination-charges"">charging through your
        /// platform</a>.
        /// </summary>
        [JsonProperty(""supported_payment_methods"")]
        public List<string> SupportedPaymentMethods { get; set; }

        /// <summary>
        /// Countries that can accept transfers from the specified country.
        /// </summary>
        [JsonProperty(""supported_transfer_countries"")]
        public List<string> SupportedTransferCountries { get; set; }

        [JsonProperty(""verification_fields"")]
        public Dictionary<string, Dictionary<string, List<string>>> VerificationFields { get; set; }
    }
}

Source: https://github.com/stripe/stripe-dotnet/blob/3d8d1150c9136f3d5b78e8b9b803e858cda5fe53/src/Stripe.net/Entities/CountrySpecs/CountrySpec.cs
",
76391714,symbols in paths in React Router v6,"I'm migrating from v5 to v6. It v6 it seems you can't use regexp anymore. Is there a quick workaround for a path like this? The problem is the dash, of course.
<Route path=""/doc/:id-:version"" />

",76391787,"You can't use partial dynamic segments, it's all or nothing. Use a single param and then apply string splitting logic on it in the component.
Example:
<Route path=""/doc/:idVersion"" element={....} />

const { idVersion } = useParams();

const [id, version] = idVersion.split(""-"");

See Dynamic Segments for more information.
",
76390107,How do you declare that a type belongs to a marker interface?,"If I have these types:
type Orange struct {
  Size float32
}

type Lemon struct {
   Color string
}

type Wood struct {
   Variety string
}

And that interface:
type Fruit interface {
}

How do I declare that Orange and Lemon are Fruit,
so that, elsewhere, a function may return only things who are of kind Fruit?
(Fruit being a marker interface).
",76390190,"To declare that a type belongs to a marker interface in Go, you need to explicitly state that the type implements the interface. In your case, to declare that Orange and Lemon types are of kind Fruit, you can do the following:
type Fruit interface {

}
type Orange struct {
Size float32
}
func (o Orange) MethodOfFruit() {
// Implement any methods of Fruit interface if required

}
type Lemon struct {
Color string

}
func (l Lemon) MethodOfFruit()
{
// Implement any methods of Fruit interface if required

}
",
76391509,API Mocking - How do I implement request forward while using a Javascript mock?,"I am aware that in a mock server feature file, it is possible to forward requests to a specific endpoint using karate.proceed(): https://karatelabs.github.io/karate/karate-netty/#karateproceed.
We recently decided to opt for Javascript mocks, as they enable us to write more complex business logic for simulating the API we consume. Is there out-of-the-box functionality equivalent to karate.proceed() that I can use in a JS mock? Is there a way I can invoke karate.proceed() within this mock? Or is one expected to write Javascript code to enable this functionality?
",76391791,"Great question, and yes there is no context.proceed(). You need to use context.http() and manually create a new request and handle the response. All the data you need will be on the request object. For example request.method will give you the HTTP method as a string.
This is indeed an un-documented part of Karate, yet to become ""mainstream"". Feel free to add a feature request for a context.proceed() that will auto-use the request in scope, that does sound useful. If you could contribute a PR or two that would be awesome :)
",
76380583,vue styling in view does not apply to html,"I have set up a new project with vue3. I started working with some package implementations but then I realized my CSS written in component is not applied in HTML. Here is a very simple view example
<template>
  <div>
    <p class=""asd"">Hello</p>
  </div>
</template>

<style>
.asd {
  color: purple;
}
</style>

In the result, the text is not purple.
I have checked if it is overridden by some other css but no, it does not event show up in the styles screen of dev tools:

What I have tried and checked:

I have tried writing style as ""p.asd {..."" and adding an !important in the end. No change.
I have checked if I am working on the correct page by changing the text from ""Hello"" to ""Hello World"" and it changes, so I am working on the correct page.
I got a production build if it is a dev env issue, but no, got the same result from the built html.
I have tried moving CSS to App.vue instead, and surprisingly styling written there works but not in the views.
I have tried on a new browser for caching issues, no change.
I have tried adding scoped, no change.
I have tried adding script tags, no change. Script tags work fine, console.log script works, but still no styling update.
I have tried inline CSS and it works.

Here is my package.json:
  ""name"": ""new-project"",
  ""version"": ""0.0.0"",
  ""private"": true,
  ""scripts"": {
    ""dev"": ""vite"",
    ""build"": ""vite build"",
    ""preview"": ""vite preview"",
    ""lint"": ""eslint . --ext .vue,.js,.jsx,.cjs,.mjs --fix --ignore-path .gitignore"",
    ""format"": ""prettier --write src/""
  },
  ""dependencies"": {
    ""express"": ""^4.18.2"",
    ""firebase"": ""^9.22.1"",
    ""pinia"": ""^2.0.36"",
    ""vue"": ""^3.3.2"",
    ""vue-router"": ""^4.2.0"",
    ""vue-tel-input"": ""^8.1.1""
  },
  ""devDependencies"": {
    ""@rushstack/eslint-patch"": ""^1.2.0"",
    ""@vitejs/plugin-vue"": ""^4.2.3"",
    ""@vue/eslint-config-prettier"": ""^7.1.0"",
    ""eslint"": ""^8.39.0"",
    ""eslint-plugin-vue"": ""^9.11.0"",
    ""prettier"": ""^2.8.8"",
    ""vite"": ""^4.3.5"",
    ""vite-plugin-webfont-dl"": ""^3.7.4""
  }
}

codesandbox example: https://codesandbox.io/p/sandbox/github/ihsanser/vue-vite-style-bug/tree/master
I don't know if I am missing something very simple, but I could not figure out the reason for hours now.
",76388872,"The space character used in .asd { is not an actual space. It is a NBSP 1.
Replace it with an actual space and everything will work as expected. This is not related to Vue, the behavior would be the same in React, Angular, Svelte or vanilla.
See it working.
As a side-note, this shows how important a runnable mcve is. In your question, you posted the code with a space. Not sure if this is due to the NBSP > Space conversion made by the browser when rendering the current page's HTML or if you simply didn't copy/paste the code in question.
I suspect it's the first.

1 - encoding NBSP outputs %C2%A0, whereas encoding a normal space outputs %20
",
76390181,How to control what classes can extend my class in Java,"So I was asked this question in interview that if I have a class A how do I restrict which classes will be able to extend A. For example if I want only class C and E to be able to extend A and classes B and D should not be able to do so. I mentioned about bounded wildcards in generics but I think that didn't answer the question. I am not aware about any new feature added in newer versions of Java. Please let me know what is the correct answer to that.
",76390195,"Sealed classes seems to be one method to restrict which classes can extend another. This feature is available since Java 17.
Contraints:

All permitted subclasses must belong to the same module as the sealed class.
Every permitted subclass must explicitly extend the sealed class.
Every permitted subclass must define a modifier: final, sealed, or non-sealed.

Example:
public abstract sealed class Vehicle permits Car, Truck {...}

This restricts that only classes Car or Truck can extend Vehicle.
",
76391528,PostgreSQL - Aggregate attributes from different columns recursively,"



id_start
id_end




1
2


2
3


3
4




I want to collect elements who are ""connected"" like [1, 2, 3, 4]. (in an array for example)
I tried a recursive query like:
WITH RECURSIVE q_rec AS (
    SELECT id_start,
           id_end
    FROM my_table
    UNION
    SELECT t.id_start
           t.id_end
    FROM       my_table t
    INNER JOIN q_rec r ON r.id_start = t.id_end
) 
SELECT *
FROM q_rec;

But how can I aggregate them in an array despite they are not in the same column ?
",76391798,"Another approach gathers all ids in the recursive query, as a table. Then applies aggregation.
WITH RECURSIVE cte AS (
  SELECT id_start AS first_id, 
         id_start AS id
  FROM tab t1
  WHERE NOT EXISTS(SELECT 1 FROM tab t2 WHERE t1.id_start = t2.id_end)

  UNION ALL

  SELECT cte.first_id,
         tab.id_end
  FROM       cte
  INNER JOIN tab
          ON cte.id = tab.id_start
)
SELECT first_id, 
       ARRAY_AGG(id) AS ids
FROM cte
GROUP BY first_id

Output:




first_id
ids




1
[1,2,3,4]




Check the demo here.
",
76388802,Does overloading the new operator in C++ redefine the operator?,"When overloading the new operator in a global scope in C++, are we just redefining the original functionality? From what I understand operator and function overloading works when the overloads have different signatures, however when overloading new operator using
void* operator new(size_t n){
    return malloc(n);
}

we change the underlying functionality itself and whenever we call new this new overload is called? Does this not violate the idea of overloads having different and unique signatures?
I tried overloading the new array operator with extra parameters and how that works is consistent with my current understanding of operator/function overloads. However overloading the new operator and new array operator with just one parameter is where I'm confused.
",76388888,"operator new is replacable (from cppreference, same link):

The versions (1-4) are implicitly declared in each translation unit even if the  header is not included. Versions (1-8) are replaceable: a user-provided non-member function with the same signature defined anywhere in the program, in any source file, replaces the default version. Its declaration does not need to be visible.

This is not the usual function overloading. You cannot overload a function with same signature. Overloads must be distinguishable by their arguments. Nevertheless, colloquially one often talks about ""overloading the new operator"" which is ok in the wider sense of ""overloading"", but in strict c++ terminology not right.
",
76389588,Oracle partition / archive strategy for type 2 dimension table,"I have a really wide table in Oracle that's at type 2 dimension.
Records have from_dates and to_dates with the latest 'current' records having a high end date of 31st Dec 9999.  There are currently two partitions on the table, one for the 'current' records and one for 'history' records.
There's a new requirement to only keep the last 12 months of records in the 'history' partition. I interpret this as keeping records that were valid in the last 12 months i.e. where the record's to_date < (this month- 11 months).
Normally if I wanted to get rid of records I'd just drop a partition, but in this case that wouldn't work as I need to retain some of the records in the existing 'history' partition.
Is there any partitioning strategy that could support this or am I barking up the wrong tree?
",76390201,"You aren't accomplishing much with merely two partitions, ""current"" and ""history"". You need to repartition this by month. Then you can implement a rolling partition drop of partitions older than 12 months, which will require a bit of scripting.
Normally we use interval partitioning INTERVAL(NUMTOYMINTERVAL(1,'MONTH')) so we don't have to maintain partition adds manually or through scripting. However, unfortunately in your case you won't be able to because of your use of the special date 12/31/9999. This is the maximum date allowable in Oracle. Interval partitioning will internally add the interval to date values when determining whether a new partition is needed or not, and that will overflow the maximum date value allowed and raise an error. The use of this special date essentially disables the use of interval partitioning.
You have no choice but to either change your special ""eternity"" date to something less than one interval away from 12/31/9999 (anything less than 12/01/9999 would permit monthly interval partitioning, or anything less than 12/31/9998 would permit yearly interval partitioning). Or, as usually happens because code would have to be changed to accommodate these solutions, you have to manually build out partitions ahead of time or create a scheduled script that does it for you.
",
76391632,How to find the matching element in a list from different list? in C#,"How to find the matching element in array from different array? in  C#
I have different varities of products and dynamically created attributes of every variety
public class SingleVariety
    {
        [JsonProperty(""varietyId"")]
        public int VarietyId { get; set; }

        [JsonProperty(""varietyName"")]
        public string VarietyName { get; set; }

        [JsonProperty(""sku"")]
        public string Sku { get; set; }

        public List<SingleAttribute> Attributes { get; set; }= new List<SingleAttribute>();

    }

   public class SingleAttribute
    {

        [JsonProperty(""attributeName"")]
        public string AttributeName { get; set; }

        [JsonProperty(""attributeValue"")]
        public string AttributeValue { get; set; }

        [JsonProperty(""varietyId"")]
        public int VarietyId { get; set; }

    }


and I have a filter array and trying to determine the selected variety based on the selection of attributes
sample variety model

 var varities = new List<SingleVariety>() { 
                new SingleVariety { 
                    Sku=""testsku"", VarietyId=1, VarietyName=""test 1"", 
                Attributes = new List<SingleAttribute> 
                { new SingleAttribute { AttributeName = ""Size"", AttributeValue=""Large"" },
                    new SingleAttribute{ AttributeName = ""Color"", AttributeValue = ""Red""} 
                }},
                new SingleVariety {
                    Sku=""testsku2"", VarietyId=2, VarietyName=""test 2"",
                Attributes = new List<SingleAttribute>
                { new SingleAttribute { AttributeName = ""Size"", AttributeValue=""Small"" },
                    new SingleAttribute{ AttributeName = ""Color"", AttributeValue = ""Red""}
                }},
                new SingleVariety {
                    Sku=""testsku3"", VarietyId=3, VarietyName=""test 3"",
                Attributes = new List<SingleAttribute>
                { new SingleAttribute { AttributeName = ""Size"", AttributeValue=""Very Large"" },
                    new SingleAttribute{ AttributeName = ""Color"", AttributeValue = ""Black""}
                }}

            };


sample filter array
 var filterObject = new List<SingleAttribute> { 
                new SingleAttribute { AttributeName = ""Size"", AttributeValue=""Large"" },
            new SingleAttribute{ AttributeName = ""Color"", AttributeValue = ""Red""} 
            };

variety with variety id 1 should be found because the filter object contains size=large, and color=red anyone can help me this?
I have used predicates, linq but I was not successfull
",76391801,"One (flexible) way of doing this would be to chain your queries:
//Start with the whole set
var results = (IEnumerable<SingleVariety>)varities;

for (int i = 0; i < filterObject.Count; i++)
{
    var filter = filterObject[i];
    //Refine with each attribute match
    results = results.Where(i => i.Attributes.FirstOrDefault(a => a.AttributeName == filter.AttributeName)?.AttributeValue == filter.AttributeValue);
}

If we inspect the output:
Console.WriteLine($""Count: {results.Count()}"");
foreach (var result in results)
{
    Console.WriteLine($""Name: {result.VarietyName}, Id: {result.VarietyId}"");
}

We get the following:
Count: 1
Name: test 1, Id: 1

We cast it to the IEnumerable<SingleVariety> interface so that the chaining can work since the Linq methods will return that type.
A couple of things worth noting is that this will support any number of filters but it will be an and operation.
Technically, you don't need the filter variable. You could just do it inline but I like to write code this way for clarity.
",
76389895,The UI theme state is not updating until I resave the code while using flutter_riverpod stateprovider,"I need to change the Theme using stateprovider of flutter riverpod ,I dont understand what i did wrong here
void main() {
  WidgetsFlutterBinding.ensureInitialized();
  runApp(const ProviderScope(child: MyApp()));
}

class MyApp extends ConsumerWidget {
  const MyApp({super.key});
  @override
  Widget build(BuildContext context, WidgetRef ref) {
    final isDarkTheme = ref.watch(isDarkThemeProvider.notifier).state;

    return GestureDetector(
      onTap: () {
        FocusScope.of(context).unfocus();
      },
      child: ScreenUtilInit(
        designSize: const Size(360, 690),
        minTextAdapt: true,
        splitScreenMode: true,
        builder: (context, child) {
          return MaterialApp(
              builder: FToastBuilder(),
              debugShowCheckedModeBanner: false,
              title: 'code',
              theme: isDarkTheme ? Themes().darkTheme : Themes().lightTheme,
              onGenerateRoute: onAppGenerateRoute(),
              routes: appRoutes(),
              initialRoute: SplashPage.route);
        },
      ),
    );
  }
}

this is my theme  class and theme stateprovider
import 'package:flutter/material.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:tr_qr_code/utils/colors.dart';

class Themes {
  final ThemeData lightTheme = ThemeData(
    scaffoldBackgroundColor: colorWhite,
    splashColor: Colors.transparent,
    highlightColor: Colors.transparent,
    appBarTheme: const AppBarTheme(color: colorWhite),
    fontFamily: 'OpenSans',
    useMaterial3: true,
  );

  final ThemeData darkTheme = ThemeData(
    scaffoldBackgroundColor: colorBlack,
    splashColor: Colors.transparent,
    highlightColor: Colors.transparent,
    appBarTheme: const AppBarTheme(color: colorBlack),
    fontFamily: 'OpenSans',
    useMaterial3: true,
  );
}

final isDarkThemeProvider = StateProvider<bool>((ref) => false);

on the toggle switch inside the onTap the below code is used to update the state
onTap: () {
    
              ref.read(isDarkThemeProvider.notifier).update(
                  (state) => !ref.read(isDarkThemeProvider.notifier).state);
            },

i tried flutter clean, flutter pub upgrade etc ..
",76390260,"Inside of MyApp class it should be
final isDarkTheme = ref.watch(isDarkThemeProvider);
And alternatively, inside your onTap you can toggle value like this:
onTap: () {
                ref
                    .read(isDarkThemeProvider.notifier)
                    .update((state) => !state);
          },

",
76388828,Why does initializing a negatively-sized array cause an overflow exception?,"I was unit testing in C#, and I found the following code gives an overflow exception:
using System;
                    
public class Program
{
    public static void Main()
    {
        int i = 0;
        Console.WriteLine(new float[i - 1]);
        // System.OverflowException: Arithmetic operation resulted in an overflow.
    }
}

https://dotnetfiddle.net/clbgZ3
However, if you explicitly attempt to initialize a negative array, you get the following error:
Console.WriteLine(new float[-1]);
// Compilation error: Cannot create an array with a negative size

Why does initializing a negatively-sized array cause an overflow exception, and not a different type of error?
",76388921,"This behaviour is explicitly specified in C# language specification, section 12.8.16.5

The result of evaluating an array creation expression is classified as
a value, namely a reference to the newly allocated array instance. The
run-time processing of an array creation expression consists of the
following steps:
(...)

The computed values for the dimension lengths are validated, as follows: If one or more of the values are less than zero, a
System.OverflowException is thrown and no further steps are executed.


(emphasis mine)
",
76390204,"how to use min-height, aspect-ratio, ... with container queries?","I am currently struggling with container queries. As long as I just use min-width and max-width everything is fine and works well. As soon as I try to use logic operators like and/or it doesn´t work anymore.


.wrapper {
  width: 300px;
  container-name: wrapper;
  container-type: inline-size;
}

.box {
  background-color: #0000ff;
  color: #ffffff;
  width: 100px;
  display: flex;
  align-items: center;
  justify-content: center;
}

@container wrapper (min-width: 300px) {
  .box {
    background-color: #00ff00;
  }
}

@container wrapper (min-width: 300px) and (min-height: 0px) {
  .box {
    background-color: #ff0000;
  }
}
<div class=""wrapper"">
  <div class=""box"">
    <span>test</span>
  </div>
</div>



See the codepen here: https://codepen.io/Resolver1412/pen/dygxWKY
I would expect that the box would become red instead of green. Since the last container query would overwrite the previous one.
Anyone knows whats wrong or what might have happened? I currently use this chrome version: Version 112.0.5615.121
",76390266,"According to the docs:

The inline-size CSS property defines the horizontal or vertical size
of an element's block, depending on its writing mode. It corresponds
to either the width or the height property, depending on the value of
writing-mode.
– inline-size | MDN Web Docs

In your example, since writing-mode has the default value of horizontal-tb, only width is usable in @container queries.
You can switch to container-type: size to use both inline and block size in queries:
.wrapper {
  width: 300px;
  height: 60px;
  container-name: wrapper;
  container-type: size;
}

@container wrapper (width >= 300px) and (height >= 0px) {
  .box {
    background-color: #ff0000;
  }
}

Try it:


.wrapper {
  width: 300px;
  height: 60px;
  container-name: wrapper;
  container-type: size;
}

.box {
  background-color: #0000ff;
  color: #ffffff;
  width: 100px;
  display: flex;
  align-items: center;
  justify-content: center;
}

@container wrapper (width >= 300px) {
  .box {
    background-color: #00ff00;
  }
}

@container wrapper (width >= 300px) and (height >= 0px) {
  .box {
    background-color: #ff0000;
  }
}
<div class=""wrapper"">
  <div class=""box"">
    <span>test</span>
  </div>
</div>



Or, you can use aspect-ratio instead of width and height:
@container wrapper (aspect-ratio > 1 / 2) {
  .box {
    background-color: #ff0000;
  }
}

Try it:


.wrapper {
  container-name: wrapper;
  container-type: size;
}

.box {
  background-color: #0000ff;
  color: #ffffff;
  width: 100px;
  display: flex;
  align-items: center;
  justify-content: center;
}

@container wrapper (width >= 300px) {
  .box {
    background-color: #00ff00;
  }
}

@container wrapper (aspect-ratio > 1 / 2) {
  .box {
    background-color: #ff0000;
  }
}
<div class=""wrapper"">
  <div class=""box"">
    <span>test</span>
  </div>
</div>



",
76391160,Does it make a difference to pass a pointer to a pointer as an argument or simply pass the first pointer?,"I'm sorry if this sounds vague or idiotic, but consider the following:
if I have a main function like so:
int main(void)
{
   int red_wins = 0;
  game_loop(&red_wins); 
  // code 
  printf(""red has %d wins\n"", red_wins); 
} 

then have a a game_loop function which calls another function that uses red_wins:
void game_loop(int *red_wins)
{
  // code 
  particle_move(red_wins);
}

and then another function called particle_move which uses this red_wins variable:
void particle_move(int *red_wins) 
{
  // code 
  if (such and such)
  {
    *red_wins+= 10; 
  } 
}

where red_wins is only modified in the particle_move function and particle_move is only called from the game_loop function, would it be better to do it as shown above or by passing a pointer to a pointer in the particle_move function? Or, but I've been told it's bad practice, by using a global variable red_wins? Or is there another, better, way that I've overlooked?
",76391823,"You only need to pass a pointer to pointer in the following situations:

Where the called function is updating the pointer value, not the thing being pointed to:void update( T **p )
{
  *p = new_TStar_value();
}
int main( void )
{
  T *var;
  update( &var ); // updates var
}

Where the pointer is pointing to the first in a sequence of pointers:void foo( int **p )
{
  // do something interesting
}
int main( void )
{
  int **p = malloc( sizeof *p * ROWS );
  if ( p )
  {
    for ( size_t i = 0; i < ROWS; i++ )
      p[i] = malloc( sizeof *p[i] * COLS )
  }
  foo( p );
  return 0;
}

In your case, particle_move is updating the same thing that game_loop is updating, so you don't need to add another layer of indirection.  You're doing it right, here.
",
76388620,Connect 2 Docker images for label-studio,"I'm on Windows, I try to run label-studio on a docker-img and enable automatic annotations with a tesseract machine learning model provided from label-studio-ml-backend on another docker-img. (I'm discovering docker these days...)
Set up:
So far I was able to launch a docker with label-studio, and a docker with tesseract:
# Run the label-studio
docker run --name lbl-studio -it -p 8080:8080 -v label-studio-data:/label-studio/data/ heartexlabs/label-studio:latest label-studio

# DL and run tesseract image
git clone https://github.com/heartexlabs/label-studio-ml-backend
cd label-studio-ml-backend/label_studio_ml/examples/tesseract/
docker-compose up

At this point I have 2 images running on docker (or 3/4, I don't really know how to interpret the 'tesseract' image)

Network:
Here are some network info I could gather, I don't know how bas is the fact that lbl-studio is on 172.17 and the two others on 172.18...

# Get ips of images 
docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' server # 172.18.0.3
docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' redis  # 172.18.0.2
docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' lbl-studio  # 172.17.0.2
# server: 172.18.0.3
# redis: 172.18.0.2
# lbl-studio: 172.17.0.2

so far redis can ping server but can't ping lbl-studio
Problem:
But, when I go to http://127.0.0.1:8080/, create a project, and try to link the machine learning wizard (http://127.0.0.1:8080/projects/1/settings/ml > add model), I'm not able to connect the tesseract server to the lbl-studio.
The urls I tried to connect are:

http://127.0.0.1:9090/
http://172.18.0.3:9090/

Going Further:
I tried to dig deepper and ping the server from lbl-studio, but nothing happened
docker exec -it --user root lbl-studio /bin/bash
apt update
apt install iputils-ping
ping 127.18.0.3  # Nothing happening: 100% packet loss ;)

Question:
How can I connect lbl-studio to the server ?
Thank you for your help :)
",76388953,"Add lbl-studio to tesseract's docker-compose file as third service. For connect from computer to services use http://127.0.0.1:9090 and http://127.0.0.1:9090. To connect between tesseract and lbl-studio use services name:  http://lbl-studio:8080 and http://server:9090. Example:
version: ""3.8""

services:
  redis:
    image: redis:alpine
    container_name: redis
    hostname: redis
    volumes:
      - ""./data/redis:/data""
    expose:
      - 6379
  server:
    container_name: server
    build: .
    environment:
      - MODEL_DIR=/data/models
      - RQ_QUEUE_NAME=default
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    ports:
      - 9090:9090
    depends_on:
      - redis
    links:
      - redis
    volumes:
      - ""./data/server:/data""
      - ""./logs:/tmp""

  lbl-studio:
   image: heartexlabs/label-studio:latest
   ports:
    - 8080:8080
   volumes: 
    - label-studio-data:/label-studio/data/

volumes:
  label-studio-data:

",
76391465,How to start from example diverging REST interface?,"In the uvicorn exmaple, one writes uvicorn filename:attributename and by that start the server. However, the interface I have generated has no such method attributename in filename. Therefore, I am unsure what to pass as attributename.
Generated code in main.py
""""""
    Somename

    Specification for REST-API of somename.

    The version of the OpenAPI document: 1.0.0
    Generated by: https://openapi-generator.tech
""""""
from fastapi import FastAPI
from openapi_server.apis.some_api import router as SomeApiRouter

app = FastAPI(
    title=""SomeName"",
    description=""Specification for REST-API of somename"",
    version=""1.0.0"",
)

app.include_router(SomeApiRouter)

",76391838,"The attribute name that you should specify is the name of the variable that holds your FastAPI instance. As they say in the docs:

The ASGI application should be specified in the form path.to.module:instance.path.

In this case for you, it would be uvicorn main:app where main.py is the file your code is in and app is the name of the variable in that file that holds your FastAPI instance.
",
76390244,Trying to clear kivymd MDList,"Trying to clear an MDList
New and trying to learn :)
I have a simple gui using kivy and kivymd.
one button adds a list of TwoLineListItem's
and i would like the other button to clear the previously generated list.
the idea is for one button to add the list, the other button to clear it
so i can populate the list and clear it over and over but by clicking the relevant buttons.
.py file code -
from kivy.app import App
from kivymd.app import MDApp
from kivy.uix.boxlayout import BoxLayout
from kivy.uix.widget import Widget
from kivy.properties import ObjectProperty
from kivy.lang import Builder
from kivy.uix.gridlayout import GridLayout
from kivymd.uix.list import TwoLineListItem

#designate our kv design file
Builder.load_file('shd_proto_kv_cfg.kv')


class Search(TwoLineListItem):
    pass


class MyLayout(BoxLayout):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def add_entries(self):
        for x in range(0, 10):
            item = Search()
            self.ids.List.add_widget(item)

    def rem_entries(self):
        self.ids.List.remove_widget()
        pass


class ShdApp(MDApp):
    def build(self):
        return MyLayout()


if __name__ == '__main__':
    ShdApp().run()

.kv file code -
<Search>:
    text: ""Title""
    secondary_text: ""Description""

<MyLayout>:
    orientation: ""horizontal""
    padding: 25
    spacing: 10

    BoxLayout:
        orientation: ""vertical""



        Label:
            text: ""Marker01""
            font_size: 25
            background_color: (196/255, 140/255, 96/255, 1)
            canvas.before:
                Color:
                    rgba: self.background_color
                Rectangle:
                    size: self.size
                    pos: self.pos

        Button:
            text: ""add em""
            on_press: root.add_entries()

        Button:
            text: ""clear em""
            on_press: root.rem_entries()

    BoxLayout:
        orientation: ""vertical""
        cols: 1

        ScrollView:
            MDList:
                id: List

the error i get from running the above code -
TypeError: Layout.remove_widget() missing 1 required positional argument: 'widget'
as mentioned i would like to populate with one button and clear with the other
and be able to populate, clear, populate, clear.
im stuck on this and i do feel its my lack of understanding as a learner thats holding me back
can someone please help me with this code so i can get it working and play around to understand it better?
eventually i will repurpose it into my first project but i need some help understanding where im going wrong here lol
Thanks!
",76390337,"Use:
self.ids.List.clear_widgets()

instead of:
self.ids.List.remove_widget()

The clear_widgets() method removes all the children of the object. The remove_widget() method removes just one child (and that child must be specified).
",
76390299,R tapply : how to use INDEX names as a FUN additional argument?,"I would like to use the names of the INDEX factor in my FUN function in tapply.
My data and function are more complex but here is a simple reproducible example :
data <- data.frame(x <- c(4,5,6,2,3,5,8,1), 
                   name = c(""A"",""B"",""A"",""B"",""A"",""A"",""B"",""B""))
myfun <- function(x){paste(""The mean of NAME is "", mean(x))}
tapply(data$x, data$name, myfun)

Result :
                         A                          B 
""The mean of NAME is  4.5""   ""The mean of NAME is  4"" 

Where I would like NAME to be A or B.
",76390358,"One option would be to pass both the value and and the index column to your function:
data <- data.frame(
  x = c(4, 5, 6, 2, 3, 5, 8, 1),
  name = c(""A"", ""B"", ""A"", ""B"", ""A"", ""A"", ""B"", ""B"")
)

myfun <- function(x) {
  sprintf(""The mean of %s is %f"", unique(x[[2]]), mean(x[[1]]))
}

tapply(data[c(""x"", ""name"")], data$name, myfun)
#>                           A                           B 
#> ""The mean of A is 4.500000"" ""The mean of B is 4.000000""

",
76388795,Incrementing/Decrementing a number using checkbox,"In my code, I need to add the value of the specific checkbox that has been checked to the span element. The problem is it's first clicks of the checkbox, the calculation goes wrong.
As you can see, if you click a checkbox for the first time, it substract the value instead of adding it. Is there something I forgot to add? My code is listed below. Thank you in advance


const s = document.querySelectorAll('#enroll-subject');
        const cue = document.getElementById('cu');
        let cu = parseInt(cue.textContent.replace('Current Units: ', '').trim());
        s.forEach(cb => {
          cb.addEventListener('change', updateTotalUnits);
        });
        function updateTotalUnits() {
          let totalUnits = cu;
          s.forEach(cb => {
            if (cb.checked) {
              console.log(""checked"");
              totalUnits += parseInt(cb.value);
            } else {
              console.log(""not checked"");
              totalUnits -= parseInt(cb.value);
            }
          });
          cue.innerHTML = `Current Units: ${totalUnits}`;
        }
    <div class=""irreg-container"" style=""display:flex; flex-direction:column; text-align: center;"">
      <div class=""header"" style=""display:flex; flex-direction:column;"">
        <span style=""padding: 1em;"" id=""cu"">Current Units: 15</span>
        <span style=""padding: .7em;font-size:1.3em;"">Checkboxes</span>
      </div>
      <div class=""subjects"" style=""display:flex; flex-direction: column;"">

      <table>
        <tbody>
          <tr>
            <td style=""width: 100%;"">Checkbox 1</td>
            <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
            </td>
          </tr>
          <tr>
            <td style=""width: 100%;"">Checkbox 2</td>
            <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
            </td>
         </tr>
         <tr>
           <td style=""width: 100%;"">Checkbox 3</td>
           <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
           </td>
         </tr>
         <tr>
           <td style=""width: 100%;"">Checkbox 4</td>
           <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
         </td>
         </tr>
       </tbody>
     </table>
     <div class=""button-container"" style=""text-align: center;"">
       <button class=""submit""> Submit </button>
     </div>
   </div>
 </div>



",76388962,"It makes no sense that you are looping over all checkboxes each time, and then subtract the value of those that are not checked - because you never added the values of those in the first place.
Just keep working with the current cu value, and then either add or subtract the value of the currently changed checkbox only.


const s = document.querySelectorAll('#enroll-subject');
const cue = document.getElementById('cu');
let cu = parseInt(cue.textContent.replace('Current Units: ', '').trim());
s.forEach(cb => {
  cb.addEventListener('change', updateTotalUnits);
});

function updateTotalUnits() {
  if (this.checked) {
    console.log(""checked"");
    cu += parseInt(this.value);
  } else {
    console.log(""not checked"");
    cu -= parseInt(this.value);
  }
  cue.innerHTML = `Current Units: ${cu}`;
}
<div class=""irreg-container"" style=""display:flex; flex-direction:column; text-align: center;"">
  <div class=""header"" style=""display:flex; flex-direction:column;"">
    <span style=""padding: 1em;"" id=""cu"">Current Units: 15</span>
    <span style=""padding: .7em;font-size:1.3em;"">Checkboxes</span>
  </div>
  <div class=""subjects"" style=""display:flex; flex-direction: column;"">

    <table>
      <tbody>
        <tr>
          <td style=""width: 100%;"">Checkbox 1</td>
          <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
          </td>
        </tr>
        <tr>
          <td style=""width: 100%;"">Checkbox 2</td>
          <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
          </td>
        </tr>
        <tr>
          <td style=""width: 100%;"">Checkbox 3</td>
          <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
          </td>
        </tr>
        <tr>
          <td style=""width: 100%;"">Checkbox 4</td>
          <td style=""width: 100%;""><input class=""sbj-checkbox"" type=""checkbox"" name=""enroll-subject"" value=""4"" id=""enroll-subject"">
          </td>
        </tr>
      </tbody>
    </table>
    <div class=""button-container"" style=""text-align: center;"">
      <button class=""submit""> Submit </button>
    </div>
  </div>
</div>



",
76391373,SQL window function and (date+interval) as a border of range,"Is there any way to use hard borders for RANGE?
The correct code is:
SELECT user_id,
       created_at,
       COUNT(*) OVER (ORDER BY created_at 
                      RANGE BETWEEN '30 days' PRECEDING 
                                AND '30 days' FOLLOWING) AS qty_in_period

But this sample is wrong:
SELECT user_id,
       created_at,
       COUNT(*) OVER (ORDER BY created_at 
                      RANGE BETWEEN UNBOUNDED PRECEDING 
                                AND (reg_date+interval) FOLLOWING) AS qty_in_period

reg_date is timestamp type, interval is '1 month'.
I know, it can be done with WHERE construction but I need to check possibility to do it via window function.
Please help.
",76391893,"
Is there any way to use hard borders for RANGE?

No. The documentation is explicit about it:

In the offset PRECEDING and offset FOLLOWING frame options, the offset must be an expression not containing any variables, aggregate functions, or window functions.

Attempting to use such syntax raises the following error:

ERROR:  argument of RANGE must not contain variables

One alternative uses a correlated subquery, or lateral join. The last query in your example could be written as:
select user_id, created_at, x.*
from mytable t
cross join lateral (
    select count(*) qty_in_period
    from mytable t1
    where t1.created_at <= t.created_at + t.intval
) x

",
76390309,Algorithm question - Stack and Queue - easy,"Here's an easy algorithm question about stack and queue, could anyone please help me to look at what's wrong with my code?
Implement a queue with two stacks. The declaration of the queue is as follows. Implement its two functions appendTail and deleteHead, which perform the functions of inserting an integer at the end of the queue and deleting an integer at the head of the queue, respectively. (If there are no elements in the queue, the deleteHead operation returns -1 )
text
class CQueue(object):

    def __init__(self) -> None:
        self.__stackA = [] 
        self.__stackB = [] 

    def appendTail(self, val: int) -> None:
        self.__stackA.append(val)
    
    def deleteHead(self) -> int:
        if self.__stackB == 0:
            if self.__stackA == 0:
                return -1
            else:
                self.__stackB.append(self.__stackA.pop())
                return self.__stackB.pop()
        else:
            return self.__stackB.pop()

My code is above. I tried to separate the situation into: 1) B = 0, A = 0 (returns -1); 2) B = 0, A != 0 (transferring the elements from A to B), and 3) B != 0 (directly popping the front element)
The correct input and output should be:
input:
[""CQueue"",""appendTail"",""deleteHead"",""deleteHead"",""deleteHead""] [[],[3],[],[],[]]
output:
[null,null,3,-1,-1]
Thank you for paying attention to the question and would be appreciated if you could help.
",76390403,"There are two issues:

Comparing a list with 0 is not really useful, as that will never be true. To test whether a list is empty, you can use the not self.__stackA or len(self.__stackA) == 0, or a combination of the two.

When stack B is empty, but stack A has values, you should not transfer one element from the top of stack A to stack B, but should transfer all its elements, so that the element that was at the bottom of stack A gets to be the top element of stack B.


With those two remarks taken care of, your code would look like this:
class CQueue(object):

    def __init__(self) -> None:
        self.__stackA = [] 
        self.__stackB = [] 

    def appendTail(self, val: int) -> None:
        self.__stackA.append(val)
    
    def deleteHead(self) -> int:
        if not self.__stackB:  # don't compare with 0
            if not self.__stackA:
                return -1
            else:
                while self.__stackA:  # Transfer ALL elements
                    self.__stackB.append(self.__stackA.pop())
                return self.__stackB.pop()
        else:
            return self.__stackB.pop()

You can also avoid some duplication of code:
    def deleteHead(self) -> int:
        if not self.__stackB:
            while self.__stackA:
                self.__stackB.append(self.__stackA.pop())
            if not self.__stackB:  # Still nothing there...
                return -1
        return self.__stackB.pop()  # Common action when there is data

Remark: I would have expected the stacks to be created with a specific class, because when you create them as standard lists, there is no reason why you could do non-stacky things with them, like reversing them, etc, which obviously is not to be allowed in this challenge.
",
76389693,Apply effect of button as per the attached screemshot on hover,"Here i attached the screenshot of the button when on hover take a effect like.
https://prnt.sc/xJSqNRU-IqdQ
I am try with skew effect but it doesnt work. I am also trying with skew effect but this doesn't work with button here. Let me provide the solution here.


.skew-button {
  display: inline-block;
  padding: 10px 20px;
  background-color: #333;
  color: #fff;
  border: none;
  font-size: 16px;
  transition: all 0.3s;
  position: relative;
  overflow: hidden;
}

.skew-button:before {
  content: """";
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: red;
  transform-origin: top left;
  transform: skewX(-20deg);
  transition: all 0.3s;
  z-index: -1;
  opacity: 0;
}

.skew-button:hover {
  background-color: red;
}

.skew-button:hover:before {
  left: -100%;
  opacity: 1;
  transform-origin: top right;
  transform: skewX(0deg);
}
<button class=""skew-button"">Hover Me</button>



",76390406,"The easiest way to do this, that I can think of, is the following. There are explanatory comments in the code:


/* simple reset to remove default margins and padding, and to
   force all browsers to use the same algorithm for sizing
   elements: */
*,
::before,
::after {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

body {
  background-image: radial-gradient(circle at 0 0, currentColor, slategray);
  block-size: 100vh;
}

main {
  /* to take all available space on the block-axis: */
  block-size: 100%;
  /* just an easy means of centering the content
     visually, in both the block and inline axes: */
  display: grid;
  place-content: center;
}

button {
  /* overriding the default background-color of the
     <button> element: */
  background-color: transparent;
  /* removing the default border: */
  border: 0 none transparent;
  font-size: 2rem;
  /* creates a stacking context so that the pseudo-
     elements are positioned ""within"" this element
     and can't be positioned ""behind,"" or lower-than,
     the <button>: */
  isolation: isolate;
  padding-block: 1em;
  padding-inline: 2em;
  /* in order to position the pseudo-elements in
     relation to this element: */
  position: relative;
}

button::before,
button::after {
  /* custom CSS property for consistency across
     the demo: */
  --offset: 2em;
  /* setting the background-color of both
     pseudo-elements to the --background CSS
     custom property (this is declared later)
     or to the default color of white (#fff): */
  background-color: var(--background, #fff);
  /* required in order to render the pseudo-elements
     to the page: */
  content: '';
  /* using clip-path to control the shape, rather
     than using transforms; this is the initial state,
     the simple rectangle: */
  clip-path: polygon(0 0, 100% 0, 100% 100%, 0 100%);
  position: absolute;
  /* using inset to position the pseudo-elements with
     an offset of 0 on the top, right, bottom and left;
     this means the element takes all available space: */
  inset: 0;
  /* transitioning the clip-path: */
  transition: clip-path 0.3s linear;
}

button::before {
  /* declaring the custom --background property: */
  --background: red;
  /* positioning the element lower down the 'stack'
     than the parent element (the use of isolation: isolate
     was to keep the pseudo-elements in front of any
     background-color property that might be set on
     the <button>, despite a lower z-index): */
  z-index: -1;
}

button::after {
  --background: yellow;
  z-index: -2;
}

/* here we update the clip-path, using the --offset variable,
   var() and calc(), to set the clipping to create the
   parallelogram shape */
button:hover::before {
  clip-path: polygon( var(--offset) 0, 100% 0, calc(100% - var(--offset)) 100%, 0 100%);
}

button:hover::after {
  clip-path: polygon( 0 0, calc(100% - var(--offset)) 0, 100% 100%, var(--offset) 100%);
}
<main>
  <button>Some generic text</button>
</main>



JS Fiddle demo.
References:

background-color.
border.
box-sizing.
block-size.
calc().
content.
clip-path.
display.
font-size.
inline-size.
inset.
isolation.
margin.
padding.
padding-block.
padding-inline.
place-content.
position.
transition.
var().
z-index.

",
76391827,How to keep WAR files running in Tomcat while I'm using IntelliJ?,"There are two WAR files that rarely change and it should be running in my machine.
The tomcat path is /Users/myuser/Downloads/apache-tomcat-9.0.53 and the tomcat server in IntelliJ configuration use it also.
If I deploy the WARs in webapps directory, run another Java project that uses Tomcat in IntelliJ then I can't access these WARs. It seems like these wars are missing but there are in webapp path.
After stop tomcat server in IntelliJ I got these WARs available again.
In order to solve the problem, my current project configuration builds and deploys these WARs every time but it takes more time. How can I solve this?
Tomcat Server
Running
My project [local]
sec-web-services:war exploded [Republish] (first war file that rarely changes)
documents-web-services:war exploded [Republish] (second war file that rarely changes)
api-web-services:war exploded [Republish]
project-webapp:war exploded [Republish]

Tomcat Server Config in IntelliJ
Tomcat Home /Users/myuser/Downloads/apache-tomcat-9.0.53
Tomcat base directory /Users/myuser/Downloads/apache-tomcat-9.0.53

",76391909,"IntelliJ IDEA Tomcat run configuration has an option to deploy applications already present in webapps directory:

",
76391039,Excel Macro for changing references with numbers separated by a dash into the full set of references,"I have been dealing with customer bill of materials that contains references with numbers that are separated by a dash, rather than the full sequence of references spelled out, e.g. C1-4 instead of C1, C2, C3, C4 or C1 C2 C3 C4
Some customers will use a comma to separate references, some only space, sometimes there is a mix of the two, which also complicates things. Here is an example:
   R161-169
 
   R2 R5, R7 R11

   R103-7
   
   R26 R28-30 R42, R45-46, R62-65, R70-71, R92-102, R113-114
   
   R31-35 R40-41 R56-61 R72-79 R86-91
   
   R36, R38-39

I'm trying to make a macro that will generate the full set of references automatically for only the selected portion of the references column, and generate that full set of references in the column next to it.
Sometimes customers leave a blank line between sections of references. Empty cells should remain empty in the output.
I found one place online that had asked a very similar question - https://www.mrexcel.com/board/threads/splitting-out-numbers-separated-by-dash.679290/ but I did not understand the code there and it did not work for what I've been trying to do.
I am not great with VBS, but I got the below code running without throwing any errors so far, but it doesn't generate the full set of references. It just copies them as they are, and I do not know where I went wrong.
Sub SplitReferences()
'June 1, 2023
Dim inputRange As Range
Dim outputCell As Range
Dim inputArea As Range
Dim inputCell As Range
Dim startNum As Long
Dim endNum As Long
Dim i As Long
Dim outputString As String

' Set the input range where your values are
Set inputRange = Selection ' Use the selected range as input

' Set the output range where you want the split references
Set outputCell = inputRange.Offset(0, 1).Cells(1) ' Output in the column next to the input

' Loop through each area in the input range
For Each inputArea In inputRange.Areas
    ' Loop through each cell in the area
    For Each inputCell In inputArea
        ' Split the value by dash
        Dim parts() As String
        parts = Split(inputCell.Value, ""-"")

        ' Check if there is a dash in the value
        If UBound(parts) > 0 Then
            ' Extract the start and end numbers
            startNum = Val(parts(0))
            endNum = Val(parts(1))
        Else
            ' If there is no dash, treat it as a single value
            startNum = Val(parts(0))
            endNum = Val(parts(0))
        End If

        ' Loop through the numbers and add them to the output range
        For i = startNum To endNum
            outputCell.Value = inputCell.Offset(i - startNum).Value
            Set outputCell = outputCell.Offset(1) ' Move to the next row
        Next i
    Next inputCell
Next inputArea
End Sub

",76391910,"Another approach:
Sub Tester()
    Dim c As Range, arr, el, txt As String, rv As String, sep As String
    
    For Each c In Selection.Cells 'loop selected range
        txt = Trim(c.Value)
        If Len(txt) > 0 Then      'cell has a value?
            arr = Split(Normalize(txt), "" "")
            rv = """"
            sep = """"
            For Each el In arr
                'convert to sequence if value has a dash
                If InStr(el, ""-"") > 0 Then el = Sequence(CStr(el), "" "")
                If Len(el) > 0 Then rv = rv & sep & el
                sep = "" ""
            Next el
            With c.Offset(0, 1)
                .WrapText = True
                .Value = rv
                .EntireRow.AutoFit
            End With
        End If 'has content
    Next c
End Sub

'Normalize the input to replace unwanted characters with spaces
'  Remove runs of >1 space, and spaces around ""-""
Function Normalize(ByVal txt As String)
    Dim arr, el
    arr = Array(vbLf, vbCr, Chr(160), "","", "";"", "":"") 'replace these with a space
    For Each el In arr
        txt = Replace(txt, el, "" "")
    Next el
    Do While InStr(1, txt, ""  "") > 0  'remove any multi-space runs
        txt = Replace(txt, ""  "", "" "")
    Loop
    txt = Replace(txt, "" -"", ""-"") 'remove any spaces next to dashes
    txt = Replace(txt, ""- "", ""-"")
    Normalize = txt
End Function

'Return a sequence from a pattern like [letter][number1]-[number2],
'  separated by `sep`
Function Sequence(txt As String, sep As String)
    Dim prefix, rv As String, sp, arr, v1, v2, i As Long
    prefix = GetPrefix(txt) 'extract leading non-numeric character(s)
    arr = Split(txt, ""-"")
    v1 = NumberOnly(arr(0))
    v2 = NumberOnly(arr(1))
    If Len(v1) > 0 And Len(v2) > 0 Then
        'handle case like R102-4, R102-24
        If Len(v2) < Len(v1) Then v2 = Left(v1, Len(v1) - Len(v2)) & v2
        v1 = CLng(v1)
        v2 = CLng(v2)
        For i = v1 To v2 'assumes V2 > v1...
            rv = rv & sp & prefix & i
            sp = sep
        Next i
    End If
    Sequence = rv
End Function

'return the first [whole] number found in `txt`
Function NumberOnly(txt)
    Dim i As Long, c, rv
    For i = 1 To Len(txt)
        c = Mid(txt, i, 1)
        If c Like ""#"" Then
            NumberOnly = NumberOnly & c
        Else
            If Len(NumberOnly) > 0 Then Exit Function
        End If
    Next i
End Function

'Return leading non-numeric character(s)
Function GetPrefix(txt As String)
    Dim i As Long, c As String, rv
    For i = 1 To Len(txt)
        c = Mid(txt, i, 1)
        If c Like ""#"" Then Exit For
        rv = rv & c
    Next i
    GetPrefix = rv
End Function

",
76388622,Understanding NuGet Package dependecies from Nuget.org,"I am a bit confused as how to interpret the dependencies list available on nuget.org and in the NuGet Package Manager in Visual Studio...
Sometimes, the list contains frameworks and a sub-list of dependencies per framework. Sometimes it does not contain the framework I have targeted for a particular project at all, how do I interpret this?
For example, this package's latest stable version is 7.0.5. I have chosen to only update my version to 6.0.16 because that is the latest version which mentions net6 as a dependency.

Versions 7 and above only mention net7.0 with dependencies, hence why I only dare to update to latest 6.X.X. Is my interpretation correct? Should I only update to the latest version mentioning net6 in the dependecy list (when my project targets net6), or can I update to the 7.X.X versions anyway?
",76388983,"A package author can choose what target framework monikers (TFMs) to support; this can be diverse or ultra-specific. In this case (Microsoft.AspNetCore.Diagnostics.EntityFrameworkCore), they have gone ""specific"", with the v6 versions of the library only targeting net6, v7 versions of the library only targeting net7, etc; so yes, your interpretation is correct and 6.0.16 looks to be the latest you can use with .net 6; if you attempt to install a v7 version of the lib, it should fail to install the package and/or build, because something targeting net7 could be using APIs that simply do not exist in net6, giving runtime failures - the package system attempts to protect you from that.
Now, it might be that the package could work on net6, but targeting multiple frameworks is effort that requires testing, and may involve code changes (in particular #if sections or similar, to use better approaches when available, or fallback approaches when not). It is not unreasonable for authors to say, more simply:

The vOlder version is what it is - we may supply out-of-band updates for security fixes or bugs that cross a certain threshold, and you can keep using vOlder with your netOlder applications, but if you want newer features you'll need to use vNewer on netNewer.

This is on a per-package basis, and many packages are far more diverse in what they target, either by having a wide range of TFMs, or by having wide-reaching targets such as netstandard2.0 (which in theory works on a wide range of platforms, by virtue of only consuming a common intersection of APIs).
",
76388773,Xpath: evaluate condition at parent node along with filtering duplicate entries,"Need an Input on the below XPath requirement:
XML:
<component>
    <Bundle>
        <entry>
            <resource>
                <Condition>
                    <id value=""123456""/>
                </Condition>
            </resource>
            <search>
                <mode value=""match""/>
            </search>
        </entry>
        <entry>
            <resource>
                <Condition>
                    <id value=""123456""/>
                </Condition>
            </resource>
            <search>
                <mode value=""match""/>
            </search>
        </entry>
        <entry>
            <resource>
                <Condition>
                    <id value=""654321""/>
                </Condition>
            </resource>
            <search>
                <mode value=""include""/>
            </search>
        </entry>        
    </Bundle>   
</component>

XSLT:
<xsl:with-param name=""entries"" select=""//Bundle/entry/resource/Condition[not(id/@value=following::Condition/id/@value)]""/>

Currently it is eliminating duplicate condition entries. Now need to enhance it to
consider <entry> with <mode value=""match""/> and its respective conditions only.
tried with different approaches likes //Bundle/entry[<<expression>>]/resource/Condition[not(id/@value=following::Condition/id/@value)] and //Bundle/entry/resource/Condition[not(id/@value=following::Condition/id/@value) and ../../<<expression>>]
Not giving expected result. Any input will be helpful.
expected output:
<Condition>
    <id value=""123456""/>
</Condition>

want only conditions which are not duplicates as well as matching <mode value=""match""/> only.
",76389015,"Consider the following example of Muenchian grouping:
XSLT 1.0
<xsl:stylesheet version=""1.0"" 
xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">
<xsl:output method=""xml"" version=""1.0"" encoding=""UTF-8"" indent=""yes""/>
<xsl:strip-space elements=""*""/>

<xsl:key name=""k1"" match=""entry[search/mode/@value='match']"" use=""resource/Condition/id/@value"" />

<xsl:template match=""/component"">
    <output>
        <xsl:copy-of select=""Bundle/entry[search/mode/@value='match'][count(. | key('k1', resource/Condition/id/@value)[1]) = 1]/resource/Condition""/>
    </output>
</xsl:template>

</xsl:stylesheet>

Applied to your input example, this will return:
Result
<?xml version=""1.0"" encoding=""UTF-8""?>
<output>
  <Condition>
    <id value=""123456""/>
  </Condition>
</output>


In XSLT 2.0 or higher, you could reduce this to:
<xsl:stylesheet version=""2.0"" 
xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">
<xsl:output method=""xml"" version=""1.0"" encoding=""UTF-8"" indent=""yes""/>

<xsl:template match=""/component"">
    <output>
        <xsl:for-each select=""distinct-values(Bundle/entry[search/mode/@value='match']/resource/Condition/id/@value)"">
            <Condition>
                <id value=""{.}""/>
            </Condition>
        </xsl:for-each> 
    </output>
</xsl:template>

</xsl:stylesheet>

",
76390149,Android room: Don't override properties with null,"class ArticleResponse(
    val id: Int,
    val previewContent: String
)

fun ArticleResponse.mapToEntity() = Article(
    id = id,
    previewContent = previewContent,
    content = null
)

class SingleArticleResponse(
    val id: Int,
    val content: String
)

fun SingleArticleResponse.mapToEntity() = Article(
    id = id,
    previewContent = null,
    content = content
)

@Entity(tableName = ""articles"")
class Article(
    @PrimaryKey
    @ColumnInfo(name = ""id"")
    val id: Int,

    @ColumnInfo(name = ""preview_content"")
    val previewContent: String?,

    @ColumnInfo(name = ""content"")
    val content: String?
)

Explanation: 
When you look at the code, you have to know that a call to API-Endpoint X gives me a list of ArticleResponse. This response doesn't give me the content property.
When I then want to view the article, I call API-Endpoint Y, which gives my SingleArticleResponse as a Response. This response doesn't give me the previewContent property.
Problem: 
No matter what I do, one of the properties content or previewContent will always be null in my local database.
Question: 
How can I tell Room, that it should not override the property with null, if it previously has not been null? Is this possible?
",76390411,"I think the easiest and most robust way to do this is to have 6 separate update queries and run them all inside a transaction.
@Dao
interface ArticleDAO {
    @Query(""UPDATE Article SET content = :content WHERE id = :id"")
    suspend fun updateContent(id: Int, content: String)

    @Query(""UPDATE Article SET previewContent = :previewContent WHERE id = :id"")
    suspend fun updatePreviewContent(id: Int, previewContent: String)

    //Other update queries here
}

fun updateArticle(article: Article) {
    database.withTransaction {
        if(article.content != null) {
            articleDao.updateContent(article.id, article.content)
        }
        if(article.previewContent != null) {
            articleDao.updatePreviewContent(article.id, article.previewContent)
        }

        //Other updates here ...
    }
}

Let me know if this answers your question.
",
76390317,"systemd-run memory limit is not shown in /proc/meminfo, is there another way?","I'm trying to write a program that waits when it sees it's memory is becoming full. It finds out what the current available memory is using /proc/meminfo. Now I'm trying to test it by running systemd-run --scope -p MemoryMax=100M -p MemorySwapMax=0 but /proc/meminfo is still returning the old values (which I kind of get why it does that).
Is there another place or way I can retrieve the available memory that does look at the limits set by systemd-run?.
Thanks in advance!
",76390432,"Systemd uses cgroups.
$ systemd-run -P --user -p MemoryMax=10240000 -p MemorySwapMax=0 bash -c 'd=/sys/fs/cgroup/$(cut -d: -f3 /proc/self/cgroup); tail $d/memory{.swap,}.max'
Running as unit: run-u923.service
==> /sys/fs/cgroup//user.slice/user-1000.slice/user@1000.service/app.slice/run-u923.service/memory.swap.max <==
0

==> /sys/fs/cgroup//user.slice/user-1000.slice/user@1000.service/app.slice/run-u923.service/memory.max <==
10240000

",
76388859,How to render a subroute and trigger react router parents loader?,"I have this router:
const router = createBrowserRouter(
  [
    {
      path: '/',
      element: <Navigate to={'/dashboards'}/>
    },
    {
      path: '/dashboards',
      element: <Dashboards/>,
      loader: () => store.dispatch(retrieveWarehouses()),
      children: [
        {
          path: ':warehouse',
          element: <Dashboard/>,
          loader: ({ params }) => store.dispatch(monitorWarehouse(params.warehouse))
        }
      ]
    }
  ]
)

Defined as is, the <Dashboard/> component is not rendered, only its parent dashboard list (Dashboards, notice the plural). The loader of the child Dashboard is still triggered though.
If I don't use a nested route:
const router = createBrowserRouter(
  [
    {
      path: '/',
      element: <Navigate to={'/dashboards'}/>
    },
    {
      path: '/dashboards',
      element: <Dashboards/>,
      loader: () => store.dispatch(retrieveWarehouses()),
    },
    {
      path: '/dashboards/:warehouse',
      element: <Dashboard/>,
      loader: ({ params }) => store.dispatch(monitorWarehouse(params.warehouse))
    }
  ]
)

The child component Dashboard is rendered properly, but the loader of the parent is not triggered.
Here are the components:
Dashboards
const Dashboards: React.FC<any> = () => {
  const {
    warehouses,
    loading
  } = useAppSelector(selectWarehouseListState)

  if (loading) {
    return (
      <div className={'warehouse-list'}>
        <h1>Select warehouse</h1>
        <Spinner/>
      </div>
    )
  }

  return (
    <div className={'warehouse-list'}>
      <h1>Select warehouse</h1>
      {
        warehouses.map((wh: Warehouse) => (
          <NavLink to={`/dashboards/${wh.name}`} key={wh.name}>
            <div className={'selectable-warehouse container'}>
              {wh.name}
            </div>
          </NavLink>
        ))
      }
    </div>
  )
}

Dashboard
const Dashboard: React.FC<any> = () => {
  const { loading } = useAppSelector(selectWarehouseState)
  const { warehouse } = useParams()
  const dispatch = useAppDispatch()

  useEffect(() => {
    return () => {
      dispatch(stopMonitorWarehouse(warehouse))
    }
  }, [dispatch])

  if (loading) {
    return (
      <div className={'dashboard loading shrinkable'}>
        <div className={'header'}>
          <NavLink to={'/dashboards'} className={'nav-back'}>
            <ArrowBack/>
          </NavLink>
          <div className={'selected-warehouse-name'}>{warehouse}</div>
        </div>
        <div className={'status connecting'}>status: connecting</div>
        <Spinner/>
      </div>
    )
  }

  return (
    <div className={'dashboard active shrinkable'}>
      <div className={'header'}>
        <NavLink to={'/dashboards'} className={'nav-back'}>
          <ArrowBack/>
        </NavLink>
        <div className={'selected-warehouse-name'}>{warehouse}</div>
      </div>
      <div className={'status connected'}>status: connected</div>
      <div className={'logs-metrics'}>
        <Logs/>
      </div>
    </div>
  )
}

How can I access /dashboards/foo and trigger both loaders ?
",76389025,"If the Dashboards component is rendered as a layout route then it necessarily should render an Outlet for its nested routes to render their content into.
Example:
import { Outlet } from 'react-router-dom';

const Dashboards: React.FC<any> = () => {
  const {
    warehouses,
    loading
  } = useAppSelector(selectWarehouseListState)

  return (
    <div className={'warehouse-list'}>
      <h1>Select warehouse</h1>
      {loading ? (
        <Spinner />
      ) : (
        <>
          {warehouses.map((wh: Warehouse) => (
            <NavLink to={`/dashboards/${wh.name}`} key={wh.name}>
              <div className={'selectable-warehouse container'}>
                {wh.name}
              </div>
            </NavLink>
          ))}
          <Outlet />
        </>
      )}
    </div>
  );
};


However in my use case I don't want both components to be rendered, the
latter should take precedence of the other.

If I understand this part you want the Dashboards and Dashboard components rendered independently, but the Dashboards's loader function to still be called even while on a nested route. For this you'll render Dashboards as a nested index route where the loader function is on the parent layout route.
Example:
const router = createBrowserRouter(
  [
    {
      path: '/',
      element: <Navigate to=""/dashboards"" />
    },
    {
      path: '/dashboards',
      loader: () => store.dispatch(retrieveWarehouses()),
      children: [
        {
          index: true,
          element: <Dashboards />,
        },
        {
          path: ':warehouse',
          element: <Dashboard />,
          loader: ({ params }) => {
            store.dispatch(monitorWarehouse(params.warehouse));
          },
        }
      ]
    }
  ]
);

The ""/dashboards"" route will render an Outlet by default when no element is specified, and the Dashboards component will be rendered when the parent route is matched. No changes to the Dashboards component would be required.
",
76390643,Is Mapstruct capable of passing a source object to an @AfterMapping method?,"Consider this code
@Mapper
@RequiredArgsConstructor
public abstract class QuestionCommentMapper {
    protected final QuestionService questionService;

    public abstract QuestionComment dtoAndAuthenticationToQuestionComment(QuestionCommentRequestDto dto,
                                                                          @Context Authentication auth);

    @AfterMapping
    protected void enrichWithOwner(@MappingTarget QuestionComment questionComment, @Context Authentication auth) {
        Account owner = AuthenticationProcessor.extractAccount(auth);
        questionComment.setOwner(owner);
    }

    @AfterMapping
    protected void enrichWithQuestion(@MappingTarget QuestionComment questionComment,
                                      @Context QuestionCommentRequestDto dto) {
        Long questionId = dto.questionId();
        Question question = questionService.getById(questionId);
        questionComment.setQuestion(question);
    }
}

Would Mapstruct pass a QuestionCommentRequestDto object to the enrichWithQuestion() method which is a source in the original mapping method? If not, how can I perform that second ""enriching"" without forgoing Mapstruct's code generation? If I write anything in the mapping method (e.g. use the QuestionCommentRequestDto instance to set QuestionComment's Question field, as in my enrichWithQuestion() method), Mapstruct won't generate anything, and I'll have to basically write everything manually
",76391915,"Mapstruct is capable of that with no extra code needed. Here's the generated method
@Override
    public QuestionComment dtoAndAuthenticationToQuestionComment(QuestionCommentRequestDto dto, Authentication auth) {
        if ( dto == null ) {
            return null;
        }

        QuestionComment questionComment = new QuestionComment();

        questionComment.setText( dto.text() );

        enrichWithOwner( questionComment, auth );
        enrichWithQuestion( questionComment, dto ); // ← look at it

        return questionComment;
    }

What made me post this question was largely the fact that a chat bot said Mapstruct wouldn't pass the source object because

Any source parameters used in the regular mapping method are not passed to the @AfterMapping method, as they are not needed for any further mapping operations at that point.

It appears the chat bot was mistaken on that
",
76388819,Firebase firestore db orderBy document id,"I need to retrieve a big amount of data.
I'm trying to order by 'id'. But the query returns an empty collection. if remove orderBy('id') it works properly.
How to sort by document id?
mQuery = docRef.orderBy('id')
              .limit(bulk_size)
                .get();

",76389035,"When you call .orderBy('id') it means that you trying to order the documents that you get from Firestore according to a field called id. If you want to order the documents according to the document ID, then please use:
mQuery = docRef.orderBy(firebase.firestore.FieldPath.documentId())
               .limit(bulk_size)
               .get();

",
76390160,"DJANGO Get First, Second and Third most found Value in a Model","I got a Dashboard model that is filled by a schduled Job.
dashboard model
class dashboard(models.Model):
    topvul1=models.IntegerField(default=0)
    topvul2=models.IntegerField(default=0)
    topvul3=models.IntegerField(default=0)


I want to show the most found, second most found and third most found VID from the clientvul model. And fill it once per Day to my dashboard model.
clientvul model
class clientvul(models.Model):
    
    client= models.ForeignKey(client, on_delete=models.CASCADE)
    vid=models.ForeignKey(vul, on_delete=models.CASCADE)
    path=models.CharField(max_length=1000)
    product=models.CharField(max_length=1000)
    isactive=models.BooleanField(default=True)
    class Meta:
        constraints = [
            models.UniqueConstraint(
               fields=['client', 'VID'], name='unique_migration_host_combination' # legt client und VID als Primarykey fest
            )
        ]

",76390445,"You can count the number of clientvuls for each vul and then order and return the first three:
from django.db.models import Count

vul.objects.alias(
    num_client=Count('clientvul_set')
).order_by('-num_client')[:3]

there is no need to make a model for this. Unless we are talking about billions of records, such queries run in milliseconds, and are more robust since it will also change the order in case a clientvul is removed for example.
",
76390300,Give the CLANG compiler a loop length assertion,"I have a loop that loads two float* arrays into __m256 vectors and processes them. Following this loop, I have code that loads the balance of values into the vectors and then processes them. So there is no alignment requirement on the function.
Here is the code that loads the balance of the data into the vectors:
size_t constexpr            FLOATS_IN_M128              = sizeof(__m128) / sizeof(float);
size_t constexpr            FLOATS_IN_M256              = FLOATS_IN_M128 * 2;

...
assert(bal < FLOATS_IN_M256);

float ary[FLOATS_IN_M256 * 2];    
auto v256f_q = _mm256_setzero_ps();
_mm256_storeu_ps(ary, v256f_q);
_mm256_storeu_ps(&ary[FLOATS_IN_M256], v256f_q);   
float *dest = ary;
size_t offset{};

while (bal--)
{
    dest[offset] = p_q_n[pos];
    dest[offset + FLOATS_IN_M256] = p_val_n[pos];
    offset++;
    pos++;
}

// the two vectors that will be processed
v256f_q = _mm256_loadu_ps(ary);
v256f_val = _mm256_loadu_ps(&ary[FLOATS_IN_M256]);    

When I use Compiler Explorer, set to ""x86-64 clang 16.0.0 -march=x86-64-v3 -O3"" the compiler unrolls the loop when the assert(bal < FLOATS_IN_M256); line is present. However, assert() is ignored in RELEASE mode, meaning the loop won't be vectorized and unrolled.
To test, I defined NDEBUG and the loop is vectorized and unrolled.
I have tried adding the following in the appropriate places, but they don't work:
#pragma clang loop vectorize(enable)
#pragma unroll
#undef NDEBUG

The compiler should be able to see from the code before the snippet above that bal < 8 but it doesn't. How can I tell it this assertion is true when not in DEBUG mode?
",76390453,"You can use __builtin_assume to give the compiler constraint information that is not explicitly in the code. This should work for gcc and clang.
In the posted code, just replace the assert with __builtin_assume(bal < FLOATS_IN_M256).
",
76391255,Date Validation in Pyspark,"I have a data file having multiple date fields coming in string data type. I am trying to validate the date field and discard the records having wrong date format. Data looks like below.
schema = StructType([StructField(""id"",StringType(),True), \
StructField(""dt1"",StringType(),True), \
StructField(""dt2"",StringType(),True)])
df = spark.createDataFrame([(1, ""01/22/2010"",""03/25/2012""), (2, ""01/12/2014"",None),(3,""04/09/2011"",""12/23""),(5,None,""01/22/2010""),(6,""2005/12/04"",""2000/12/04""),(7,""01/01/2020"",""30/12/2019""),(8,""12/1999/21"",""05/01/2021""),(9,""12/2013/21"",None),(9,None,None)], schema=schema)

Only the two date formats ""M/d/y"", ""M/yy"" are allowed to be passed in the validation. Records that do not pass the validation will be loaded in the error table.

This is a sample data. In actual file there are n numbers of date fields present. I am trying to get a function which can do this validation on all the date fields.
",76391923,"I think you can try to parse the date and then filter rows with None values, you can have a list of date columns to be more generic
from pyspark.sql.functions import coalesce, to_date, col
from pyspark.sql.types import StructField, StructType, StringType

schema = StructType([StructField(""id"",StringType(),True), \
StructField(""dt1"",StringType(),True), \
StructField(""dt2"",StringType(),True)])
df = spark.createDataFrame([(1, ""01/22/2010"",""03/25/2012""), (2, ""01/12/2014"",None),(3,""04/09/2011"",""12/23""),(5,None,""01/22/2010""),(6,""2005/12/04"",""2000/12/04""),(7,""01/01/2020"",""30/12/2019""),(8,""12/1999/21"",""05/01/2021""),(9,""12/2013/21"",None),(9,None,None)], schema=schema)


def custom_to_date(col):
    formats = (""M/d/y"", ""M/yy"")
    return coalesce(*[to_date(col, f) for f in formats])

# Set CORRECTED mode to deal with invalid dates as None
spark.sql(""set spark.sql.legacy.timeParserPolicy=CORRECTED"")

df.cache()

date_columns = [""dt1"", ""dt2""]

valid_df = df
for c in date_columns:
    valid_df = valid_df.filter(custom_to_date(col(c)).isNotNull())

error_df = df.subtract(valid_df)

error_df.show()
+---+----------+----------+
| id|       dt1|       dt2|
+---+----------+----------+
|  2|01/12/2014|      null|
|  5|      null|01/22/2010|
|  6|2005/12/04|2000/12/04|
|  7|01/01/2020|30/12/2019|
|  8|12/1999/21|05/01/2021|
|  9|12/2013/21|      null|
|  9|      null|      null|
+---+----------+----------+

valid_df.show()
+---+----------+----------+                                                     
| id|       dt1|       dt2|
+---+----------+----------+
|  1|01/22/2010|03/25/2012|
|  3|04/09/2011|     12/23|
+---+----------+----------+

Update: If we need to replace the String columns as well with the parsed dates then we can do something like this:
from pyspark.sql.functions import coalesce, to_date, col
from pyspark.sql.types import StructField, StructType, StringType

schema = StructType([StructField(""id"",StringType(),True), \
StructField(""dt1"",StringType(),True), \
StructField(""dt2"",StringType(),True)])
df = spark.createDataFrame([(1, ""01/22/2010"",""03/25/2012""), (2, ""01/12/2014"",None),(3,""04/09/2011"",""12/23""),(5,None,""01/22/2010""),(6,""2005/12/04"",""2000/12/04""),(7,""01/01/2020"",""30/12/2019""),(8,""12/1999/21"",""05/01/2021""),(9,""12/2013/21"",None),(9,None,None)], schema=schema)


def custom_to_date(col):
    formats = (""M/d/y"", ""M/yy"")
    return coalesce(*[to_date(col, f) for f in formats])

# Set CORRECTED mode to deal with invalid dates as None
spark.sql(""set spark.sql.legacy.timeParserPolicy=CORRECTED"")

df.cache()

date_columns = [""dt1"", ""dt2""]


for c in date_columns:
    df = df.withColumn(c, custom_to_date(col(c)))

valid_df = df
for c in date_columns:
    valid_df = valid_df.filter(col(c).isNotNull())

error_df = df.subtract(valid_df)

>>> valid_df.show()
+---+----------+----------+
| id|       dt1|       dt2|
+---+----------+----------+
|  1|2010-01-22|2012-03-25|
|  3|2011-04-09|2023-12-01|
+---+----------+----------+

>>> error_df.show()
+---+----------+----------+
| id|       dt1|       dt2|
+---+----------+----------+
|  2|2014-01-12|      null|
|  5|      null|2010-01-22|
|  6|      null|      null|
|  7|2020-01-01|      null|
|  8|      null|2021-05-01|
|  9|      null|      null|
+---+----------+----------+

",
76388794,ListView Item - Generate column value based on criteria not part of Item's class,"Suppose I've a simple model class:
public class Car
{
    public string Make { get; init; }
    public string Model { get; init; }
    public string Year { get; init; }
}

In my ViewModel, I've two lists:
public class ViewModel
{
    public ObservableCollection<Car> Cars { get; }
    public List<Car> CanBeSold { get; }

    public ViewModel()
    {
        Car car1 = new() { Make = ""Toyota"", Model = ""Corolla"", Year = ""2020"" };
        Car car2 = new() { Make = ""Honda"", Model = ""Civic"", Year = ""2021"" };
        Car car3 = new() { Make = ""Mitsubishi"", Model = ""Lancer"", Year = ""2017"" };

        Cars = new();
        CanBeSold = new();

        Cars.Add(car1);
        Cars.Add(car2);
        Cars.Add(car3);

        CanBeSold.Add(car2);
    }
}

In my view, I'm bidining a ListView to the Cars collection:
<ListView ItemsSource=""{Binding Cars}"">
    <ListView.View>
        <GridView>
            <GridViewColumn Header=""Make"" DisplayMemberBinding=""{Binding Path=Make}""/>
            <GridViewColumn Header=""Model"" DisplayMemberBinding=""{Binding Path=Model}""/>
            <GridViewColumn Header=""Year"" DisplayMemberBinding=""{Binding Path=Year}""/>
            <GridViewColumn Header=""Can Be Sold""/>
        </GridView>
    </ListView.View>
</ListView>

How can I also show a Yes/No based on if the Car is in the list CanBeSold?

Thanks for any help.
",76389049,"You may use a MultiBinding that contains a Binding to the CanBeSold property of the parent view model and a Binding to the current Car element.
<GridViewColumn Header=""Can Be Sold"">
    <GridViewColumn.DisplayMemberBinding>
        <MultiBinding>
            <MultiBinding.Converter>
                <local:ListElementConverter/>
            </MultiBinding.Converter>
            <Binding Path=""DataContext.CanBeSold""
                     RelativeSource=""{RelativeSource AncestorType=ListView}""/>
            <Binding />
        </MultiBinding>
    </GridViewColumn.DisplayMemberBinding>
</GridViewColumn>

The Binding Converter checks if the element is contained in the list:
public class ListElementConverter : IMultiValueConverter
{
    public object Convert(
        object[] values, Type targetType, object parameter, CultureInfo culture)
    {
        return values.Length == 2 &&
            values[0] is IList list &&
            list.Contains(values[1])
            ? ""Yes""
            : ""No"";
    }

    public object[] ConvertBack(
        object value, Type[] targetTypes, object parameter, CultureInfo culture)
    {
        throw new NotSupportedException();
    }
}

",
76389832,Polars - How to add two series that contain lists as elements,"Trying to add, subtract, two Series that contains datatype List[i64]. The operation seems to be not supported.
a = pl.Series(""a"",[[1,2],[2,3]])
b = pl.Series(""b"",[[4,5],[6,7]])
c = a+b

this gives the error:
PanicException: `add` operation not supported for dtype `list[i64]`

I would expect a element-wise sum, like would happen with numpy array for example:
c = [[5,7],[8,10]]

What's the correct syntax to add two series of lists?
",76390470,"you can do the following:

c = (a.explode() + b.explode()).reshape((2,-1)).alias('c')

shape: (2,)
Series: 'a' [list[i64]]
[
    [5, 7]
    [8, 10]
]

Final thoughts: if your list has a fixed size, then you might consider using the new Polars Array datatype.
",
76383169,Is there a secure way to provide users with Shared Access Signature Tokens for Azure Storage containers?,"My API allows users to upload and download files to my Azure Storage account. To do this, they need a SAS token with permissions based on if they want to download or upload a file. I was wondering if there was a secure method to provide users with these tokens, other than sending it through more unsecure methods such as email.
Edit for Clarification:
I plan on having hundreds of users accessing my Azure Storage account. I was planning on generating my token through Azure itself but I have been considering generating the SAS token inside of the API or in a separate Azure Function. My API uses an Azure Function with NodeJS.
",76391967,"Proposal 1: You can create a new Azure function as a proxy on your storage account for uploading/downloading. Thanks to managed identity, you won't have to provide a SAS token. User authorization on the Azure Function will ensure that the permission is removed when the user is no longer authorized.
Proposal 2: You can create a SAS token with an Azure Function and send it to the user inside your application (can be transparent to the user). This will enable you to create a SAS token with a short lifetime. If communication between clients and server uses TLS, it will guarantee secure transmission of your token.
",
76387974,Migration from RestEasy to RestEasyReactive with ResteasyContext and ContainerRequestFilter,"I'm migrating old Quarkus project from RestEasy to ResteasyReactive and I have some difficulties migrating ResteasyContext.pushContext since there is no real 1:1 alternative in rest easy.
I'm using the ResteasyContext.pushContext in my ContainerRequestFilter to push some custom object to Context and later retrieve it using @Context.
Something like in this minimal example i provided.
Filter:
package org.acme.filter;

import org.acme.pojo.CustomHttpRequest;
import org.jboss.resteasy.core.ResteasyContext;

import javax.enterprise.context.ApplicationScoped;
import javax.ws.rs.container.ContainerRequestContext;
import javax.ws.rs.container.ContainerRequestFilter;
import javax.ws.rs.ext.Provider;
import java.time.LocalDateTime;
import java.util.Random;

@Provider
@ApplicationScoped
public class HttpRequestFilter implements ContainerRequestFilter {

    @Override
    public void filter(ContainerRequestContext requestContext) {
        CustomHttpRequest request = CustomHttpRequest.builder()
                .headers(requestContext.getHeaders())
                .dateTime(LocalDateTime.now())
                .text(""Some random text for example "" + new Random().nextInt(100))
                .build();
        ResteasyContext.pushContext(CustomHttpRequest.class, request);
    }
}

Custom object I want to push to context:
package org.acme.pojo;

import lombok.Builder;
import lombok.Getter;
import lombok.ToString;

import javax.ws.rs.core.MultivaluedMap;
import java.time.LocalDateTime;

@Getter
@Builder
@ToString
public class CustomHttpRequest {

    private String text;
    private LocalDateTime dateTime;
    private MultivaluedMap<String, String> headers;
    private boolean secured;
}

And the later read it from context in my rest endpoint:
package org.acme;

import org.acme.pojo.CustomHttpRequest;
import org.acme.pojo.ResponseData;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;

@Path(""/hello"")
public class GreetingResource {

    @GET
    @Path(""{pathText}"")
    @Produces(MediaType.APPLICATION_JSON)
    public ResponseData testContext(@Context CustomHttpRequest httpRequest,
                                    @PathParam(""pathText"") String queryText) {

       return ResponseData.builder()
                .queryText(queryText)
                .httpRequestText(httpRequest.getText())
                .secured(httpRequest.isSecured())
                .build();
    }
}

Here is the full example on GitHub: https://github.com/pkristja/resteasy_context/tree/main
I have found some alternatives that work with RestEasyReactive like using ContainerRequestContext and setting the data using setProperty.
Build file changes:
Changed from implementation(""io.quarkus:quarkus-resteasy-jackson"") to implementation(""io.quarkus:quarkus-resteasy-reactive-jackson"")
Filter for setting oblect to context:
package org.acme.filter;

import org.acme.pojo.CustomHttpRequest;

import javax.enterprise.context.ApplicationScoped;
import javax.ws.rs.container.ContainerRequestContext;
import javax.ws.rs.container.ContainerRequestFilter;
import javax.ws.rs.core.Context;
import javax.ws.rs.ext.Provider;
import java.time.LocalDateTime;
import java.util.Random;

@Provider
@ApplicationScoped
public class HttpRequestFilter implements ContainerRequestFilter {

    @Context
    ContainerRequestContext crContext;

    @Override
    public void filter(ContainerRequestContext requestContext) {
        CustomHttpRequest request = CustomHttpRequest.builder()
                .headers(requestContext.getHeaders())
                .dateTime(LocalDateTime.now())
                .text(""Some random text for example "" + new Random().nextInt(100))
                .build();

        crContext.setProperty(""customHttpRequest"", request);
    }
}

Retrieving object from context:
package org.acme;

import org.acme.pojo.CustomHttpRequest;
import org.acme.pojo.ResponseData;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.container.ContainerRequestContext;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;

@Path(""/hello"")
public class GreetingResource {

    @GET
    @Path(""{pathText}"")
    @Produces(MediaType.APPLICATION_JSON)
    public ResponseData testContext(@Context ContainerRequestContext crContext,
                                    @PathParam(""pathText"") String queryText) {

        CustomHttpRequest httpRequest = (CustomHttpRequest) crContext.getProperty(""customHttpRequest"");

       return ResponseData.builder()
                .queryText(queryText)
                .httpRequestText(httpRequest.getText())
                .secured(httpRequest.isSecured())
                .build();
    }
}

Is there any way to get same functionality in RestEasyReactive like you had in RestEasy using ResteasyContext.pushContext because it's really verbose and inefficient to retrieve each object from context and cast it because in my real example I have multiple custom objects pushed to context with ResteasyContext.pushContext.
Thank you!
",76389074,"When using RESTEasy Reactive, there is a far easier way of doing things like this: just use a CDI request scoped bean.
Something like the following should be just fine:
@Singleton
public class CustomHttpRequestProducer {

    @RequestScoped
    @Unremovable
    public CustomHttpRequest produce(HttpHeaders headers) {
        return new CustomHttpRequest(headers.getRequestHeaders(), LocalDateTime.now(), ""dummy"");
    }
}

Then you would use it in your JAX-RS Resource as easily as:
@GET
@Produces(MediaType.TEXT_PLAIN)
public String hello(@Context CustomHttpRequest customHttpRequest) {
    return customHttpRequest.getText();
}

Note that @Unremovable is only needed if you use CustomHttpRequest  as method parameter.
If you however inject it as a field, @Unremovable is unnecessary.
UPDATE
After https://github.com/quarkusio/quarkus/pull/33793 becomes part of Quarkus (likely in 3.2) then @Unremovable will no longer be necessary even for the method parameter
",
76391970,How to find an index of the ArrayList from the starting index in Java?,"I want to search for an index of the element in the ArrayList but I want to start searching from starting index different than 0.
I tried like this:
import java.util.*;

public class Test{
    public static void main(String[] args) {
        ArrayList<String> bricks = new ArrayList<String>(List.of(""BBBB"",""CCCC"",""DDDD""));
        System.out.println(bricks.subList(1, bricks.size()).indexOf(""CCCC""));
    }
}

Output:
0

Expected output:
1

I want to start searching for ""CCCC"" in ""bricks"" from the starting index ""1"" not from ""0""
",76392001,"Your code finds the index within the sublist.
To find the index within the original list, add the index used to create the sublist to the result:
System.out.println(bricks.subList(1, bricks.size()).indexOf(""CCCC"") + 1);

Some refactoring makes this clearer:
public static <T> int indexOfAfter(List<T> list, T item, int from) {
    int result = list.subList(from, list.size()).indexOf(item);
    return result == -1 ? -1 : (from + result);
}

",
76388533,How do I create a single slicer from a group of columns,"I have a data loaded into Power Bi that look like this:




ID
TYPE
Product 1
Product 2
Product 3




1
A
1
1
0


1
B
0
0
1


2
A
0
1
1


2
B
1
0
1


3
A
1
0
0




So every column besides the ""ID"" and ""TYPE"" columns, is a binary 0/1 column, that indicates whether certain person acquire given product.
What I want to do, is to create a single dropdown slicer with Product 1, Product 2 and Product 3 values, that will filter only certain persons which acquire selected Product.
",76389099,"If you cant get a table with one product column before you connect to Power-BI and everything else fails you can to the following:
In the query editor create a new query and refer it to your table above
= YourTableName
So you get the table 3 times. Now for table one delete the columns 4 & 5, for table two delete columns 3 & 5 and for table three delete column 3 & 4. Then rename all columns identical ID, Type, Product.
In the last step you under Home > Append Queries you can append all three tables. Now you got your final table with just one Product column.
",
76391852,Recreate hist() binning in ggplot2 with geom_histogram(),"(if you are only interested in the problem, then go to ""What if in short?"")
What kind of stupid question?
I'm doing work and before that I built all graphics with x and I don't want to change the style.
And now I need a histogram, but it does not suit me with ggplot2.
What do I mean?
I took the width of the column from hist(), so there will be the same number of them
(which can be seen from the graphs),
but in hist() and as I want,
the bars do NOT cross the important/magic number 0.0012,
and in `geom_histogramm' intersects.
And if it's short?
How to ""shift"" histogram bars with ggplot2 so that they do not cross a certain number (0.0012)?
Or, how to make a histogram shorter with ""data"" from hist() and design with ggplot2?
Here is my code:
# check bin width
  standart_hist <- hist(my_vector, plot = F)
  bw <- standart_hist$breaks[2] - standart_hist$breaks[1]
  
  # create hist with ggplot and bw from standart hist
  gghist <- ggplot(mapping = aes(my_vector)) +
    geom_histogram(
      binwidth = bw,
      color = ""black"",
      fill = ""white""
    ) 

and result:

my hist



standard hist


FIX:
from joran --- instead of geom_histogram() use stat_bin() as here:
stat_bin(geom = 'bar',breaks = <breaks vector from hist() output>)

My data:
my_vector <- (0.001201367, 0.001199250, 0.001198337, 0.001199200, 0.001199353, 0.001198439, 0.001202447, 0.001205639, 0.001207056, 0.001209714, 0.001204478, 0.001200064, 0.001199386, 0.001199976, 0.001200569, 0.001204738, 0.001208508, 0.001201491, 0.001200995, 0.001199861, 0.001200242, 0.001196367, 0.001200365, 0.001201807, 0.001194364, 0.001197196, 0.001192705, 0.001196178, 0.001192991, 0.001189777, 0.001194227, 0.001197158, 0.001204336, 0.001201081, 0.001201100, 0.001204755, 0.001198810, 0.001202090, 0.001194370, 0.001188529, 0.001191450, 0.001193616, 0.001195733, 0.001198886, 0.001201353, 0.001206878, 0.001201262, 0.001194806, 0.001196192, 0.001193215, 0.001195030, 0.001198202, 0.001184351, 0.001191890, 0.001192882, 0.001194621, 0.001203256, 0.001204150, 0.001197425, 0.001198002, 0.001196185, 0.001194915, 0.001198281, 0.001201858, 0.001195349, 0.001196401, 0.001205476, 0.001201740, 0.001197276, 0.001189442, 0.001192760, 0.001196846, 0.001201342, 0.001204854, 0.001202979, 0.001203136, 0.001199926, 0.001197398, 0.001199905, 0.001199252, 0.001198486, 0.001197114, 0.001196829, 0.001200228, 0.001199666, 0.001194918, 0.001204005, 0.001201363, 0.001204183, 0.001205889, 0.001204553, 0.001202369, 0.001203922, 0.001197001, 0.001200020, 0.001202672, 0.001201746, 0.001203532, 0.001198699, 0.001200975, 0.001202635, 0.001203121, 0.001190614, 0.001199029, 0.001200372, 0.001193731, 0.001193428, 0.001200259, 0.001195203, 0.001194854, 0.001193173, 0.001198266, 0.001195362, 0.001195252, 0.001201008, 0.001199291, 0.001196653, 0.001200357, 0.001201623, 0.001207463, 0.001199381, 0.001198047, 0.001196305, 0.001200419, 0.001208689, 0.001197434, 0.001193885, 0.001198708, 0.001204741, 0.001204281, 0.001193663, 0.001200234, 0.001203809, 0.001199003, 0.001195127, 0.001192189, 0.001187610, 0.001191390, 0.001200602, 0.001197817, 0.001202045, 0.001203998, 0.001205508, 0.001201051, 0.001202057, 0.001208911, 0.001203928, 0.001202267, 0.001201434, 0.001202647, 0.001210024, 0.001210509, 0.001207881, 0.001206928, 0.001206128, 0.001203866, 0.001202204, 0.001204511, 0.001202310, 0.001197504, 0.001199019, 0.001200713, 0.001204197, 0.001204649, 0.001207965, 0.001201847, 0.001200585, 0.001203446, 0.001195972, 0.001202405, 0.001197182, 0.001191603, 0.001197663, 0.001202259, 0.001201008, 0.001200354, 0.001198090, 0.001193479, 0.001202457, 0.001201156, 0.001196038, 0.001201092, 0.001205488, 0.001212173, 0.001203497, 0.001208846, 0.001198349, 0.001200047, 0.001200799, 0.001206939, 0.001207142, 0.001201970, 0.001202742, 0.001204795, 0.001198463, 0.001201559, 0.001201344, 0.001206085, 0.001205526, 0.001197508)

",76392026,"Using your data, I believe this does what you want:
h <- hist(my_vector)

ggplot(data = data.frame(x = my_vector),aes(x = x)) + 
  stat_bin(geom = 'bar',breaks = h$breaks)

",
76388593,Jackson ignore unrecognized property and collect the errors,"There are a lot of topics around how can we configure the Jackson to ignore additional properties during the un-marshaller process. But I didn't find any answer how can we ignore it but also collect the unrecognized properties.
Our flow is: We would like to ignore them but collect all of the unrecognized properties in order to be aware of these properties and fix them.
Somebody knows how can I achieve this?
",76389100,"Using override DeserializationProblemHandler.handleUnknownProperty get unknown property name?
public class IgnoreUnknownPropertiesHandler extends DeserializationProblemHandler {
            @Override
            public boolean handleUnknownProperty(DeserializationContext ctxt, JsonParser p, JsonDeserializer<?> deserializer, Object beanOrClass, String propertyName) throws IOException {
                // print ignored property
                System.out.println(""Ignored property: "" + propertyName);
                return true;
            }
        }
        
ObjectMapper objectMapper = new ObjectMapper();
objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
objectMapper.addHandler(new IgnoreUnknownPropertiesHandler());

",
76390394,Why nvim (vim) adds \< and \> to search pattern and what they means?,"I use the * command to fill the search register (/) with the current word (under cursor) so I don't have to paste it into the substitute command.
To do a find and replace I can do it quickly like so:
:%s//MyNewValue/g

instead of
:%s/MyOldValue/MyNewValue/g

But sometimes I just want to change one character in the word (like a typo). So after I used * on the word, I do the following:
:%s//<c-r>//g

But I get this:
:%s//\<MyOldValue\>/g

because the / register contains \<MyOldValue\>.
So, here's my question:
How can I get rid of these \< and \>? Or is there a better way to edit all occurrences of a word in vim?
The only way I found, it's to yank the word and paste it twice in the substitute pattern.
yiw
:%s/<c-r>""/<c-r>""/g

Also, what do \< and \> mean and what are they use for?
",76390482,"The \< and \> are word boundaries, see :help \< or :help \>.
This is similar to the ""match whole word"" checkbox in the search dialog of graphical editors.
For example, it will do the following:
Put the cursor on ""foo"" and press star.
It will match foo in this line.
But not foobar in this one.

If you don't want this, use g* instead of *. It will not add the word boundaries.
If you're using the :substitute command, you may as well use Ctrl+RCtrl+W (Ctrl+RCtrl+A for words-with-non-word-characters) to add the word under your cursor to the command line. In Vim, one would write it like this:
:%s/<C-R><C-W>/MyNewValue/

",
76388334,Firestore save data in strange formation using Flutter,"I try to save data on Firebase but the data is stored in strange way although the value is string like this:
Description
""TextEditingController#d9024(TextEditingValue(text: ┤├, selection: 
TextSelection.invalid, composing: TextRange(start: -1, end: -1)))""
Price
""TextEditingController#c11e0(TextEditingValue(text: ┤├, selection: 
TextSelection.invalid, composing: TextRange(start: -1, end: -1)))""
Title
""TextEditingController#dcebb(TextEditingValue(text: ┤├, selection: 
TextSelection.invalid, composing: TextRange(start: -1, end: -1)))""
UID
""gWYqXqsvTdVI1Sfa8MP4SrFTEmB2""

here is my code:
final adTitleController = TextEditingController();
final priceController = TextEditingController();
final adDescription = TextEditingController();

Future<void> createAds() async {
CollectionReference ads = FirebaseFirestore.instance.collection('Ads');

String? adid = ads.id.toString();

String? title = adTitleController.toString();
String? desc = adDescription.toString();
String? price = priceController.toString();
String? userId;
String? adId;
ads.add({
  ""UID"": FirebaseAuth.instance.currentUser?.uid,
  ""AdId"": adId,
  ""Title"": title,
  ""Description"": desc,
  ""Price"": price,
});
}

",76389142,"In order to get a text from the controller you need to use text property of TextEditingController
String? title = adTitleController.text.toString();
String? desc = adDescription.text.toString();
String? price = priceController.text.toString();

",
76391687,Struggling to type a TypeScript function,"I've got this TypeScript error and I don't fully understand what's going on:
src/helpers.ts:11:14 - error TS2322: Type '<T extends ""horizontal"" | ""vertical"" | undefined, U extends AriaRole | undefined>(ariaOrientation: T, role: U) => ""horizontal"" | ""vertical"" | NonNullable<T> | ""both""' is not assignable to type 'ResolveOrientationFunction'.
  Type '""horizontal"" | ""vertical"" | NonNullable<T> | ""both""' is not assignable to type 'NonNullable<T> | ""both""'.
    Type '""horizontal""' is not assignable to type 'NonNullable<T> | ""both""'.

Here is my function:
import { type HTMLAttributes } from ""react"";

type ResolveOrientationFunction = <
  T extends HTMLAttributes<HTMLElement>[""aria-orientation""],
  U extends HTMLAttributes<HTMLElement>[""role""]
>(
  ariaOrientation: T,
  role: U
) => ""both"" | NonNullable<T>;

export const resolveOrientation: ResolveOrientationFunction = (ariaOrientation, role) => {
  if (ariaOrientation === undefined) {
    switch (role) {
      case ""menubar"":
      case ""slider"":
      case ""tablist"":
      case ""toolbar"": {
        return ""horizontal"";
      }

      case ""listbox"":
      case ""menu"":
      case ""scrollbar"":
      case ""tree"": {
        return ""vertical"";
      }
    }
  }

  return ariaOrientation ?? ""both"";
};

The function is supposed to return ""both"" | ""horizontal"" | ""vertical"".
HTMLAttributes<HTMLElement>[""aria-orientation""] is actually ""horizontal"" | ""vertical"" | undefined and HTMLAttributes<HTMLElement>[""role""] is React.AriaRole | undefined.
I'm actually trying to make this function match the type ""both"" | NonNullable<HTMLAttributes<HTMLElement>[""aria-orientation""]>.
",76392032,"Your ResolveOrientationFunction type definition,
type ResolveOrientationFunction = <
  T extends HTMLAttributes<HTMLElement>[""aria-orientation""],
  U extends HTMLAttributes<HTMLElement>[""role""]
>(
  ariaOrientation: T,
  role: U
) => ""both"" | NonNullable<T>;

is generic in both T, the type of ariaOrientation, and U, the type of role.  It returns a value of type ""both"" | NonNullable<T>.  So if T is undefined because ariaOrientation is undefined, then the function must return ""both"" | NonNullable<undefined> which is ""both"".  But your implementation doesn't do that.  It can instead return ""horizontal"" or ""vertical"" depending on role.
So your resolveOrientation function is not a valid ResolveOrientation.

It's not clear that you need the function to be generic at all.  Certainly the U type parameter isn't useful as written, since it has no effect on the return type.  And you don't really want the T type parameter to be reflected directly in the output type either.  It seems like your return type should just be ""both"" | ""vertical"" | ""horizontal"" without reference to T or U.  And if you have a generic function where there's no obvious dependency on the type parameters, then you might not want a generic function in the first place.
If you change the generics to specific types like this:
type AriaOrientation = HTMLAttributes<HTMLElement>[""aria-orientation""];
type AriaRole = HTMLAttributes<HTMLElement>[""role""]
type ResolveOrientationFunction =
    (ariaOrientation: AriaOrientation, role: AriaRole) => 
    ""both"" | NonNullable<AriaOrientation>;

Then your function compiles cleanly:
export const resolveOrientation: ResolveOrientationFunction = (ariaOrientation, role) => {
    if (ariaOrientation === undefined) {
        switch (role) {
            case ""menubar"":
            case ""slider"":
            case ""tablist"":
            case ""toolbar"": {
                return ""horizontal"";
            }

            case ""listbox"":
            case ""menu"":
            case ""scrollbar"":
            case ""tree"": {
                return ""vertical"";
            }
        }
    }

    return ariaOrientation ?? ""both"";
};

Playground link to code
",
76390348,Is there a way to make Google wait until all data is loaded for my React website hosted on Firebase with a DB from Firebase before indexing?,"I created a Website for my Wife's Blog in React and am hosting it on Firebase. I also use DB from firebase.
After the site release I found out that Google is not waiting until all data is finally loaded from the site to index the final site, but is trying to index a loading site.
For now I solved the issue with prerender with the following package (as per simple implementation in our site):
https://github.com/egoist/presite
The Issue of it is it prerenders only the English (default) site and not any other languages so indexing is mostly done on English sites (other languages are ignored) and as the site is prerendered Google sometimes loses the index of the site because of it.
I also checked multiple other prerender options but currently no other has the simplicity of implementation and support of dynamic site (some sites --> each recipe has a template site that is loaded from DB and rendered on the client).
Any Ideas how to solve these issue? Possible to use without prerender and get Google wait on loading data for the site?
For Information/check the site itself: https://fromapot.com/
",76390491,"SEO is known problem of SPA websites built with front-end frameworks/libraries. The only options are to: prerender or use SSR (Server Side Rendering).
For your case, I would suggest using the second option since you want to have dynamic indexing depending on language.
When using SSR, you typically need a NodeJS server that listens to requests and creates the initial layout based on the requested route on the server. This layout is then sent to the front-end along with some additional information, which allows the rest of the website to function properly.
Most easy and powerful way to do that is NextJS framework, but depending on your build system, you may have other options. For example, for Vite, there is a simple plugin which enables SSR with almost zero effort.
There is no way to configure that Google waits for dynamic front-end render because it will require Google to support JS runtime which is too heavy for indexing engine which visits probably millions of websites each day.
",
76388160,search method optimization for searching field area in odoo15,"I have a function
    def test(self):
        tech_line = self.env['tech_line']
        allocated_technician = self.env['allocated_technician']
        users = self.env['res.users']
        tech_line = tech_line.search(
            [('service_type_id', '=', self.service_type_id.id)])
        al_a6 = self.env['tech_line'].filtered(lambda rec: rec.service_type_id.id == self.service_type_id.id)

        area = []
        area_new = []
        for tec in tech_line:
            territory = self.env['territory']
            territories = territory.search(
                [('technicians', 'in', tec.technician_allociated_id.user_id.id)])

            territories_lam = self.env['territory'].filtered(
                lambda t_lam: t_lam.technicians.id in tec.technician_allociated_id.user_id.id)

            for territory in territories:
                area.append(territory.id)
            for tet in territories_lam:
                area_new.append(tet.id)

        print('##################33', len(area))
        print('%%%%%%%%%%%%%%%%%%%%', len(area_new))
        
        print('$$$$$$$$$$$$$$$$$$$', tech_line)
        print('***************8***', al_a6)

this method when executed screen gets loading and I need to optimize this method, please do share your thoughts on how to optimize this code
I cannot limit the value which is generated from the search method as we need all of its value so instead of that I thought to use filtered instead of search method, but when I use filtered it gives an empty recordset. need help with that
",76389158,"You can avoid searching in the for loop by using a search on all users:
def test(self):
    tech_line = self.env['tech_line']
    tech_lines = tech_line.search(
        [('service_type_id', '=', self.service_type_id.id)])
    # get all users to avoid search in a for loop
    users = tech_lines.mapped(""technician_allociated_id.user_id"")
    # search territories
    territories = territory.search([('technicians', 'in', users.ids)])
    
    area = territories.ids

",
76391751,"if match between column ID in two different datasets, then create a new dataset with the difference of other columns r","I have two datasets using r:
df_100= data.frame(siteid=c(seq(1,5,1),conflu=c(3,2,4,5,6),diflu=c(9,2,30,2,5))
df_full= data.frame(siteid=c(seq(1,10,2),conflu=c(6,3,5,2,3),diflu=c(5,9,2,30,7))

If the siteid is the same between df_100 and df_full, I want to take the difference between the conflu columns of each data frame and the same with the diflu columns. I also want that output to be put into a new dataframe, where the siteid is retained and the difference between the columns creates a new column. For example:
df_difference=data.frame(siteid=c(1,3,5), diff_con=c(3,1,-3), diff_dif=c(-4,-18,2))

",76392039,"I don't follow the calculations to get what you have as the sample output, but based on your description:
library(dplyr)

df_100 <- data.frame(siteid= seq(1,5,1),conflu=c(3,2,4,5,6),diflu=c(9,2,30,2,5))

df_full <- data.frame(siteid = seq(1,10,2),conflu=c(6,3,5,2,3),diflu=c(5,9,2,30,7))

df_difference <- df_100 |> 
  inner_join(df_full, by = ""siteid"", suffix = c(""_100"", ""_full"")) |> 
  mutate(
    diff_con = conflu_full - conflu_100,
    diff_dif = diflu_full - diflu_100
  ) |> 
  select(siteid, diff_con, diff_dif)

inner_join will match and keep only the rows with same ""siteid"". Then use mutate to do the calculations and select the columns you want.
",
76391960,R Studio add_row with dynamic field name,"I have a function here, where my intent is to add a record to the table.  The column name is dynamically defined based on the firstCharVar variable.
The dataframe tblname is a blank table.  The first character field in that table is called myvar.  There are other columns in that table, and they should remain blank.
#update tables if no records
NoData = function(tblname) {
  if (nrow(tblname) == 0) {

  #get column name of first character field
  allColumns = data.frame(
    colName = colnames(tblname),
    colIndex = 1:ncol(tblname),
    colClass = sapply(tblname, class)
  )
  charVars     = allColumns[allColumns$colClass == 'character', ]
  firstCharVar = unfactor(charVars$colName[1])

  #run insert statement
  #this doesn't work
  #Error: unexpected '=' in ""tblname = tblname %>% add_row(!!firstCharVar =""
  #tblname = add_row(tblname, !!firstCharVar = 'No Data Found')

  #but this does
  tblname = add_row(tblname, myvar = 'No Data Found')

  
  #clean up stuff used in function
  #rm(allColumns, charVars, firstCharVar)
}}

temp2 = NoData(temp2)

",76392049,"As in other dpylr verbs you could assign values to dynamically created LHS names by using the walrus operator := and !!sym(col) or glue syntax ""{col}"".
Using a minimal reproducible example based on mtcars:
library(dplyr, warn=FALSE)

col <- ""cyl""
mtcars |> 
  head() |> 
  add_row(cyl = 1) |> 
  add_row(""{col}"" := 2) |> 
  add_row(!!sym(col) := 3)
#>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
#> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
#> ...7                NA   1   NA  NA   NA    NA    NA NA NA   NA   NA
#> ...8                NA   2   NA  NA   NA    NA    NA NA NA   NA   NA
#> ...9                NA   3   NA  NA   NA    NA    NA NA NA   NA   NA

",
76390325,Change rgb color of text on overlap,"I'm using mix-blend-mode to change the background color of an image. The problem I'm facing is that I've a grey-ish text which overlaps the background image intentionally when shown on a mobile device. This makes however the text unreadable. I've tried all variations of mix-blend-modes and believe my only option is to entirely change the color of the text when it's overlapping.
How can I change the color of a text when it's overlapping another element?
Here is the fiddle:
https://jsfiddle.net/zr8men95/
Relevant portion:
.promo__text {
  grid-area: text;
  z-index: 1;
  margin-left: 1rem;
  margin-right: 1rem;
  color: #6B7F92;
  text-shadow: 0 0 0 black;
}

",76390505,"I believe your best shot would be to change the text color when you are on an mobile device. I've read your fiddle and you already have some @media zone defined. We can take advantage of that to add some CSS to change the text color whilst in mobile mode.
This block of code:
@media (min-width: 640px) and (max-width: 1024px) {
  .promos {
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-gap: 1rem;
  }
}

Will change for this:
@media (min-width: 640px) and (max-width: 1024px) {
  .promos {
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-gap: 1rem;
  }
  
  .promo__text {
    color: white; // You can change the color to whatever fits your need
  }
}

Hope that help!
",
76391983,How to only get _source fields of elasticsearch using Spring Data Elastic?,"I am using Spring Boot and Spring Data Elastic. I have an accounts index data like below

public interface AccountsRepository extends ElasticsearchRepository<Accounts, Long> {
    
    List<Accounts> findByLastname(String lastname);
    
    List<Accounts> findByAge(Integer age);
}

I am getting below error when perform findAll(). How to fix the below issue?
@Service
public class AccountsService {
    @Autowired
    private AccountsRepository repository;

    public List<Accounts> findAllAccounts(){
        return (List<Accounts>) repository.findAll();
    }
}

Model
@Data
@AllArgsConstructor
@NoArgsConstructor
@Document(indexName = ""accounts"", createIndex = false)
public class Accounts {
    @Id
    private Long account_number;
    private Long balance;
    private String firstname;
    private String lastname;
    private Integer age;
    private String gender;
    private String address;
    private String employer;
    private String email;
    private String city;
    private String state;
}

Error:
2023-06-02 21:23:23.784  WARN 44796 --- [nio-8080-exec-1] org.elasticsearch.client.RestClient      : request [POST http://localhost:9200/accounts/_search?typed_keys=true&max_concurrent_shard_requests=5&search_type=query_then_fetch&batched_reduce_size=512] returned 1 warnings: [299 Elasticsearch-7.15.0-79d65f6e357953a5b3cbcc5e2c7c21073d89aa29 ""Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.""]
2023-06-02 21:23:23.878  WARN 44796 --- [nio-8080-exec-1] org.elasticsearch.client.RestClient      : request [POST http://localhost:9200/accounts/_search?typed_keys=true&max_concurrent_shard_requests=5&search_type=query_then_fetch&batched_reduce_size=512] returned 1 warnings: [299 Elasticsearch-7.15.0-79d65f6e357953a5b3cbcc5e2c7c21073d89aa29 ""Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.""]
2023-06-02 21:23:24.170 ERROR 44796 --- [nio-8080-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type [java.lang.String] to type [java.lang.Long] for value 'KH-9fIgBhfTLJt8QzMP_'; nested exception is java.lang.NumberFormatException: For input string: ""KH-9fIgBhfTLJt8QzMP_""] with root cause

java.lang.NumberFormatException: For input string: ""KH-9fIgBhfTLJt8QzMP_""
    at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:na]
    at java.base/java.lang.Long.parseLong(Long.java:692) ~[na:na]
    at java.base/java.lang.Long.valueOf(Long.java:1144) ~[na:na]
    at org.springframework.util.NumberUtils.parseNumber(NumberUtils.java:214) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.core.convert.support.StringToNumberConverterFactory$StringToNumber.convert(StringToNumberConverterFactory.java:64) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.core.convert.support.StringToNumberConverterFactory$StringToNumber.convert(StringToNumberConverterFactory.java:50) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.core.convert.support.GenericConversionService$ConverterFactoryAdapter.convert(GenericConversionService.java:437) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.core.convert.support.ConversionUtils.invokeConverter(ConversionUtils.java:41) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.core.convert.support.GenericConversionService.convert(GenericConversionService.java:192) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.core.convert.support.GenericConversionService.convert(GenericConversionService.java:175) ~[spring-core-5.3.21.jar:5.3.21]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.getPotentiallyConvertedSimpleRead(MappingElasticsearchConverter.java:562) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.readValue(MappingElasticsearchConverter.java:460) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.readValue(MappingElasticsearchConverter.java:442) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader$ElasticsearchPropertyValueProvider.getPropertyValue(MappingElasticsearchConverter.java:621) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.readProperties(MappingElasticsearchConverter.java:404) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.readEntity(MappingElasticsearchConverter.java:327) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.read(MappingElasticsearchConverter.java:258) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter$Reader.read(MappingElasticsearchConverter.java:217) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter.read(MappingElasticsearchConverter.java:161) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.convert.MappingElasticsearchConverter.read(MappingElasticsearchConverter.java:83) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.AbstractElasticsearchTemplate$ReadDocumentCallback.doWith(AbstractElasticsearchTemplate.java:745) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) ~[na:na]
    at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) ~[na:na]
    at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ~[na:na]
    at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[na:na]
    at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[na:na]
    at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:na]
    at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[na:na]
    at org.springframework.data.elasticsearch.core.AbstractElasticsearchTemplate$ReadSearchDocumentResponseCallback.doWith(AbstractElasticsearchTemplate.java:778) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.AbstractElasticsearchTemplate$ReadSearchDocumentResponseCallback.doWith(AbstractElasticsearchTemplate.java:763) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.core.ElasticsearchRestTemplate.search(ElasticsearchRestTemplate.java:404) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.repository.support.SimpleElasticsearchRepository.lambda$findAll$1(SimpleElasticsearchRepository.java:123) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.repository.support.SimpleElasticsearchRepository.execute(SimpleElasticsearchRepository.java:355) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.repository.support.SimpleElasticsearchRepository.findAll(SimpleElasticsearchRepository.java:123) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at org.springframework.data.elasticsearch.repository.support.SimpleElasticsearchRepository.findAll(SimpleElasticsearchRepository.java:112) ~[spring-data-elasticsearch-4.4.1.jar:4.4.1]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
    at org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:289) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:530) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:286) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:640) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.21.jar:5.3.21]
    at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:164) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:139) ~[spring-data-commons-2.7.1.jar:2.7.1]
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.21.jar:5.3.21]
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97) ~[spring-aop-5.3.21.jar:5.3.21]
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.21.jar:5.3.21]
    at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215) ~[spring-aop-5.3.21.jar:5.3.21]
    at com.sun.proxy.$Proxy59.findAll(Unknown Source) ~[na:na]
    at com.example.service.AccountsService.findAllAccounts(AccountsService.java:16) ~[classes/:na]
    at com.example.controller.AccountsController.findAllAccounts(AccountsController.java:19) ~[classes/:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-5.3.21.jar:5.3.21]
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150) ~[spring-web-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1067) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:963) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:655) ~[tomcat-embed-core-9.0.64.jar:4.0.FR]
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883) ~[spring-webmvc-5.3.21.jar:5.3.21]
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:764) ~[tomcat-embed-core-9.0.64.jar:4.0.FR]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:227) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-embed-websocket-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100) ~[spring-web-5.3.21.jar:5.3.21]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117) ~[spring-web-5.3.21.jar:5.3.21]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93) ~[spring-web-5.3.21.jar:5.3.21]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117) ~[spring-web-5.3.21.jar:5.3.21]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) ~[spring-web-5.3.21.jar:5.3.21]
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117) ~[spring-web-5.3.21.jar:5.3.21]
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:197) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:541) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:135) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:360) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:399) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:890) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1787) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-9.0.64.jar:9.0.64]
    at java.base/java.lang.Thread.run(Thread.java:834) ~[na:na]

",76392056,"You marked the property account_number of your entity, which is a Long, as the @Id. But the id in your index is a String (here with the value ""KH-9fIgBhfTLJt8QzMP_"").
Add proper id property:
@Id
private String id;

Edit: 03.06.2023:
As for the error

java.lang.ClassCastException: class org.springframework.data.domain.PageImpl cannot be cast to class java.util.List (org.springframework.data.domain.PageImpl

findAll() is defined to return an Iterable, which is an interface. The actual returned objects is of type PageImpl which implements Page which extends Iterable.
You cannot cast a Page implementation to a List<Accounts>.
",
76388624,How can I filter differences of datasets in a table,"I am relativly new to SQL and I have a short question. I already searched for similar questions in stackOverflow but I couldn't find anything.I have created some Views. These Views change from one Version to antother. To make migration easier for the customer, I want to filter differences in data_types of columns between two versions. Currently I'm working PostgreSQL Version 11.16
I have table that looks like this:




versionsnummer
install_timestamp
column_name
data_type




D1
2023-06-02 06:42:14.531588
t0801_01
integer


D1
2023-06-02 06:42:14.531588
t0801_04
character varying


D2
2023-07-02 06:42:14.531588
t0801_01
integer


D2
2023-07-02 06:42:14.531588
t0801_04
integer




Now I want to find all rows where the value of the column data_type has changed between two versions.
So I'm expecting the following result:




versionsnummer
install_timestamp
column_name
data_type




D1
2023-06-02 06:42:14.531588
t0801_04
character varying


D2
2023-06-02 06:42:14.531588
t0801_04
integer




What I've tried is this:
SELECT DISTINCT ON (column_name, data_type) column_name, data_type FROM mytable WHERE versionsnummer = 'D1' OR versionsnummer = 'D2';
Unfortunately I didn't get the expectes Result with this query. Could you please tell me waht i'm doing wrong here?
Thank you very much :)
",76389229,"I think you can achieve this via ""SELF JOIN"". Join the tables with itself on ""column_name"" column.
Here is the code:
SELCT t1.versionsnummer, t1.install_timestamp, t1.column_name, t1.data_type
FROM [your_table_name] t1
JOIN [your_table_name] t2 ON t1.column_name = t2.column_name
WHERE t1.data_type <> t2.data_type;

Example: If you select all columns from the table, you will get:




versionsnummer
install_timestamp
column_name
data_type




D1
2023-06-02 06:42:14.531
t0801_04
character_varing


D2
2023-07-02 06:42:14.531
t0801_04
integer




Tested here:

*Don't forget to change table name in this query!
",
76390367,Defining an emacs minor mode,"Have coded an emacs minor mode with the definition shown below.
Would it be possible to simplify this, to perhaps call a function instead?  Or is it not such a big deal having a minor-mode defined in this way ?
What would be the general way to set a minor-mode in terms of functionality (e.g. enable, disable, ...) ?
(defcustom komis-luna-signal t
  ""todo""
  :group 'komis-luna)

;;;###autoload
(define-minor-mode komis-luna-minor-mode
  ""Uses large geometric shapes for displaying heading levels.""
  nil nil nil

  (let*
      (($keyword
        `((""^\\*+ ""
           (0 (let* ( ($kondor
                         (- (match-end 0) (match-beginning 0) 1))
                      ($inline-task
                         (and (boundp 'org-inlinetask-min-level)
                              (>= $kondor org-inlinetask-min-level))) )
                ;;--------------------------------------
                (compose-region (- (match-end 0) 2)
                                (- (match-end 0) 1)
                                (komis-luna-shape-select $kondor))
               ;;---------------------------------------
               (when $inline-task
                  (compose-region (- (match-end 0) 3)
                                  (- (match-end 0) 2)
                                  (komis-luna-shape-select $kondor)))
               ;;---------------------------------------
               (when (facep komis-luna-typeface)
                  (put-text-property
                     (- (match-end 0) (if $inline-task 3 2))
                     (- (match-end 0) 1)
                     'face komis-luna-typeface))
               ;;---------------------------------------
               (put-text-property
                  (match-beginning 0)
                  (- (match-end 0) 2)
                  'face (list :foreground
                        (face-attribute 'default :background)))
               ;;---------------------------------------
               (put-text-property (match-beginning 0)
                                  (match-end 0)
                                  'keymap komis-luna-mouse-sweep)
               ;;---------------------------------------
               nil )) ))))

    (if komis-luna-signal

        (progn
          (font-lock-add-keywords nil $keyword)
          (font-lock-fontify-buffer))

      (save-excursion
        (goto-char (point-min))
        (font-lock-remove-keywords nil $keyword)
        (while (re-search-forward ""^\\*+ "" nil t)
          (decompose-region (match-beginning 0) (match-end 0)) )
        (font-lock-fontify-buffer)) ) ))

",76390507,"I think you did it the proper way.
Next would be to toggle your minor mode in your emacs configuration file .emacs.d/init.el with a hook probably like this :
(add-hook 'org-mode-hook 'komis-luna-minor-mode)

If you want your minor mode to be enable when entering org-mode.
For further and more specifics questions and problems, you should probably take this post to Emacs Stack Exchange.
",
76391607,How to pass array to Ansible URI module,"I'm trying to pass an array using extravars to the Ansible URI module.
% ansible-playbook someplaybook.yml -e '{ ""letters"": [ ""aa"", ""ab"", ""ac"", ""ad"", ""ae"" ] }'

When I run it through the URI module, I'm getting [ ""['aa', 'ab', 'ac', 'ad', 'ae']"" ] in the response. How do I send the entire array without the brackets and quotes. I've tried ""{{ letters | replace(""["", """") | replace(""]"", """") }}"" but that doesn't get rid of the exterior quotes.
  - name: Create a letter job
    uri:
      url: 'http://localhost:8000/api/app'
      method: POST
      headers:
        Content-Type: application/json
      body: '{ ""letters"": [ ""{{ letters }}"" ] }'
      body_format: json
      return_content: yes
    register: response

Current Output:
""response"": {
    ""id"": 2796831356421,
    ""letters"": [
      ""['aa', 'ab', 'ac', 'ad', 'ae']""
    ],
    ""lastError"": null
}

Desired Output:
""response"": {
    ""id"": 2796831356421,
    ""letters"": [
      'aa', 'ab', 'ac', 'ad', 'ae'
    ],
    ""lastError"": null
}

",76392057,"Your letters variable is a list, but when you write:
""{{ letters }}""

You are asking -- explicitly -- to render it as a simple string. You don't want that; you want to maintain the structure of the data. You can do that like this:
- name: Create a letter job
  uri:
    url: 'http://localhost:8000/api/app'
    method: POST
    headers:
      Content-Type: application/json
    body: '{{ { ""letters"": letters } }}'
    body_format: json
    return_content: yes
  register: response

Here, we're using a Jinja template ({{ ... }}) to create dictionary with a single key, letters, whose value is the content of your letters variable.
Using this sample application:
from pydantic import BaseModel
from fastapi import FastAPI


class Letters(BaseModel):
    letters: list[str]


app = FastAPI()


@app.post(""/api/app"")
def echo(letters: Letters) -> Letters:
    return letters

And the following playbook:
- hosts: localhost
  gather_facts: false
  tasks:
    - name: Create a letter job
      uri:
        url: 'http://localhost:8000/api/app'
        method: POST
        headers:
          Content-Type: application/json
        body: '{{ { ""letters"": letters } }}'
        body_format: json
        return_content: yes
      register: response

    - debug:
        var: response

We see as output:
TASK [debug] ********************************************************************************************
ok: [localhost] => {
    ""response"": {
        ""changed"": false,
        ""connection"": ""close"",
        ""content"": ""{\""letters\"":[\""aa\"",\""ab\"",\""ac\"",\""ad\"",\""ae\""]}"",
        ""content_length"": ""38"",
        ""content_type"": ""application/json"",
        ""cookies"": {},
        ""cookies_string"": """",
        ""date"": ""Fri, 02 Jun 2023 16:42:50 GMT"",
        ""elapsed"": 0,
        ""failed"": false,
        ""json"": {
            ""letters"": [
                ""aa"",
                ""ab"",
                ""ac"",
                ""ad"",
                ""ae""
            ]
        },
        ""msg"": ""OK (38 bytes)"",
        ""redirected"": false,
        ""server"": ""uvicorn"",
        ""status"": 200,
        ""url"": ""http://localhost:8000/api/app""
    }
}

...which I think is exactly what you were after.
",
76388538,Cannot write a paragraph to a pdf file using iText pdf,"I am using Java 17 and the iText pdf library (5.5.4), I'm currently attempting to write a paragraph on an existing pdf file inside a rectangular area, however I seem to have a NullPointerExeption when invoking the go() method, I'm not sure exactly why. I have included my code, any help would be appreciated.
public class Main {
    public static void main(String[] args) {
        try {

            PdfReader reader = new PdfReader(""src/main/resources/test_file.pdf"");


            PdfStamper stamper = new PdfStamper(reader, new FileOutputStream(""src/main/resources/output.pdf""));

            PdfContentByte cb = stamper.getOverContent(1);
            ColumnText ct = new ColumnText(cb);
            ct.setSimpleColumn(new Rectangle(36, 600, 200, 800));
            ct.addElement(new Paragraph(""I want to add this text in a rectangle defined by the coordinates llx = 36, lly = 600, urx = 200, ury = 800""));
            int status = ct.go();



        } catch (DocumentException | IOException e) {
            throw new RuntimeException(e);
        }
    }
}

Exception in thread ""main"" java.lang.NullPointerException: Cannot invoke ""com.itextpdf.text.pdf.PdfStructureElement.getAttribute(com.itextpdf.text.pdf.PdfName)"" because ""this.parent"" is null
",76389242,"I could reproduce your issue using your example file with iText 5.5.4. Then I tried again with the current 5.5.13.3. There was no issue. Thus, please update.
Comparing the 5.5.4 code (where the exception occurs) with the corrsponding 5.5.13.3 code, one sees that there indeed was an unconditional call of a method of a parent object but that now there is a call to a helper method that first checks the parent and only calls its method if it isn't null.
This fix has been applied in early 2015, the commit comment was ""Fixed NPE when modifying content of TaggedPDF document."".
",
76380856,How can I replace matches in a Python regex with a modified version of the match?,"I wrote this code to search a specific folder's text files for word matches and to specify them:
import re, os, sys
from pathlib import Path

#Usage: regs directory
try:
    if len(sys.argv) == 2:
        folder = sys.argv[1]
        fList = os.listdir(folder)
        uInput = input('input a regex: ')
        regObj = re.compile(f'''{uInput}''')
        wordReg = re.compile(r'''([A-Za-z0-9]+|\s+|[^\w\s]+)''')
        matches = []
        print(fList)

        for file in fList:
            if not os.path.isdir(Path(folder)/Path(file)):
                currentFileObj = open(f'{folder}/{file}')
                content = currentFileObj.readlines()
                currentFileObj.seek(0)
                text = currentFileObj.read()
                words = wordReg.findall(text)
                matches = list(filter(regObj.match, words))
                instances = 0
                print(f""matches in ({file}):\n'"", end='')
                for word in words:
                    if word in matches:
                        print(""\u0333"".join(f""{word} ""), end='')
                    else:
                        print(word, end='')
                print(""'"")
                for line in content:
                    matches = regObj.findall(line)
                    for match in matches:
                        print(""\u0333"".join(f""{match} ""), end=' ')
                        print(f""in line number {content.index(line)+1}"")
                        if match != '':
                            instances = instances + 1
                print(f'number of instances found: {instances}\n')
            else:
                continue
    else:
        print('Usage: regs directory')
except FileNotFoundError:
    print(""that file doesn't exist."")
except PermissionError:
    print(""you don't have permission to search that folder."")

it works for the most part except for a few regular expressions, if the regular expression has punctuation or a white space character next to other characters it wouldn't underline it, it may work if i find out a way to substitute matches with a modified version of the match (replacing the match with an underlined version)
Anyone knows a fix ?
here's what it looks like for any other regex.
you can see in the first text file it doesn't underline the match (out.)
i tried looking for functions that would substitute matches with a modification of said match, doesn't appear like there's any ?
also there's the minor problems of it not being able to underline whitespaces and punctuation properly, and the underline character doesn't appear in the windows7 command prompt, maybe a different character other than the underline can work ?
",76389538,"I've figured out the answer:
using a lambda function as a repl= variable with re.sub i was capable of modifying the matches and then using them to substitute.
    import re, os, sys
    from pathlib import Path

    #Usage:regs directory
    try:
        if len(sys.argv) == 2:
            folder = sys.argv[1]
            fList = os.listdir(folder)
            print(""folder contents: "", end=' ')
            for f in fList:
                if not f == fList[-1]:
                    print(f, end=', ')
                else:
                    print(f, end='.\n\n')
            uInput = input('input a regex: ')
            print()
            regObj = re.compile(f'''{uInput}''')
            wordReg = re.compile(r'''([A-Za-z0-9]+|\s+|[^\w\s]+)''')
            matches = []
            
            for file in fList:
                if os.path.isfile(Path(folder)/Path(file)):
                    currentFileObj = open(f'{folder}/{file}')
                    lines = currentFileObj.readlines()
                    currentFileObj.seek(0)
                    text = currentFileObj.read()
                    words = wordReg.findall(text)
                    matches = list(filter(regObj.match, words))
                    instances = 0
                    print(f""matches in ({file}):\n'"", end='')
                    print(regObj.sub(lambda match: ""("" + match.group() + "")"", text)+""'"")
                    for line in lines:
                        matches = regObj.findall(line)
                        for match in matches:
                            print((f""({match})""), end=' ')
                            print(f""in line number {lines.index(line)+1}"")
                            if match != '':
                                instances = instances + 1
                    print(f'number of instances found: {instances}\n')
                else:
                    continue
        else:
            print('Usage:regs directory')
    except FileNotFoundError:
        print(""that file doesn't exist."")
    except PermissionError:
        print(""you don't have permission to search that folder."")

instead of having a loop that goes over the list of the string's words, it just prints the match group between parenthesis like so:
            print(regObj.sub(lambda match: ""("" + match.group() + "")"", text)+""'"")

The output now looks like this.
it also prints the folder contents now.
",
76389533,Kafka consumer unique cluster ID?,"Does a Kafka cluster have some unique ID that I can get programmatically out of a Kafka consumer? I checked for message headers, but it looks like there's no metadata in them by default and I don't see any methods in a KafkaConsumer or ConsumerRecord for retrieving such a value either.
",76390559,"You can use AdminClient to get the cluster id, but generally, there's no reason for clients to know internal server information
",
76390462,How to return 0 for all time intervals instead of nothing when counting,"I have a query for deployment table. There is no data for hotfix column now. I want to show all change count without hotfix and with hotfix for time intervals.
Table data:




deployTime
changeno
hotfix




2022-08
aaa



2022-08
bbb



2022-11
ccc



2023-01
ddd





First attempted query:
SELECT deployTime               AS times ,   
       COUNT(DISTINCT changeno) AS ""Change Count"" 
FROM deployments 
WHERE hotfix = ''
GROUP BY deployTime 

which returns all dates with Change count:




times
ChangeCount




2022-08
2


2022-11
1


2023-01
1




Second attempted query:
SELECT deployTime               AS times ,   
       COUNT(DISTINCT changeno) AS ""Change Count"" 
FROM deployments 
WHERE hotfix != ''
GROUP BY deployTime 

which returns no records if there's no record with hotfix != ''.
How we can get 0 count for every date instead of nothing?




times
HotfixCount




2022-08
0


2022-11
0


2023-01
0




Thanks
",76390585,"The problem with your query is that you're using a WHERE clause, that removes records. What you should do instead, is apply conditional aggregation on presence/absence of hotfix values:
SELECT deployTime AS times ,   
       COUNT(DISTINCT changeno) FILTER(WHERE hotfix = '') AS ""Change Count"" 
FROM deployments 
GROUP BY deployTime 

And change it to WHERE NOT hotfix = '' conversely to obtain zeroes.
Check the demo here.
Note: It's better to have NULL values instead of empty strings, when you need to indicate missing data.
",
76392054,How do I break down data that I am grouping by?,"I have a table where each instance in the table is a sold ticket and tickets can have different ticket types. Looking something like this:




Event
Ticket Type




Event 1
a


Event 2
a


Event 1
b


Event 2
a


Event 1
a




I want it to be grouped by the event but displaying both the total tickets for that event as well as breaking down the number of each ticket type.




Event
Total Tickets
Ticket Type a
Ticket Type b




Event 1
3
2
1


Event 2
2
2
0




I have tried a few different queries but nothing that is showing me the results I'm looking for. Is this possible in one query?
",76392081,"Yes, it is possible by using GROUP BY to group the event data and then calculating the totals. The following query works by counting the number of tickets, then counting the number of tickets for each type ""a"" and ""b"".
SELECT 
    Event,
    COUNT(*) AS 'Total Tickets',
    COUNT(CASE WHEN TicketType = 'a' THEN 1 END) AS 'Ticket Type a',
    COUNT(CASE WHEN TicketType = 'b' THEN 1 END) AS 'Ticket Type b'
FROM
    Tickets
GROUP BY
    Event;

",
76389631,Polymorphic serialization of property,"I'm trying to create a Client library that will allow users to push serialized data to a service.
so I created a class
public class Data
{
   public string Prop1 {get; set;}
   public SubData Prop2 {get; set;}
}

public abstract class SubData
{
   public string Prop3 {get; set;}
}

I would like to allow users to extend that SubData to add custom properties. but I'm having issues in my Serialization, it doesn't serialize the properties of the extended inner object
JsonSerializer.Serialize(data) 

I know that I could decorate my SubData class with JsonDerivedType attribute, but my problem is that SubData is in a package, and it doesn't know about who will extend it and with what (and it doesn't care)
I don't know if I'm clear what my problem is so here is a full test to replicate:
using System.Text.Json;

public class Data
{
    public string Prop1 { get; set; }
    public SubData SubData { get; set; }
}

public abstract class SubData
{
    public string Prop2 { get; set; }
}

public class ExtendedSubData : SubData
{
    public string Prop3 { get; set; }

}

var data = new Data
{
    Prop1 = ""1"",
    SubData = new ExtendedSubData
    {
        Prop2 = ""2"",
        Prop3 = ""3""
    }
};

SerializData(data);

void SerializData(Data data)
{
    var serialized = JsonSerializer.Serialize<object>(data);
    // serialized at this point doesn't contain Prop3 of 
    // ExtendedSubData
}

",76390588,"As an alternative to the custom JsonConverter, you can replace the SubData property with generic
public class Data<TSubData>
    where TSubData : SubData
{
    public string Prop1 { get; set; }
    public TSubData SubData { get; set; }
}

string SerializData<TSubData>(Data<TSubData> data)
    where TSubData : SubData
{
    return JsonSerializer.Serialize<object>(data);
}

Test it on dotnetfiddle:
https://dotnetfiddle.net/lkOM0Z
",
76388586,How to sign dylib file which can be replaced?,"I have poor understanding in this question. The major step to distribute any application is the code signing, it signs application with dependant dynamic library. As I understand OS will check signed application during installation and subsequent calls. If application or dynamic library was changed then os rejects the launch. Many dylib files are supplied to conform LGPL license and therefore dylib files can be potentially substituted by user later. But it will break the launch of application (because signed dylib was replaced). Are my assumptions correct? Maybe there is comprehensive book/guide which covers this topic? I found apple documentation pretty bad
",76389618,"That's not how this works.
The signature of the library isn't going to matter if the entire library will be replaced. What matters is the signature of the main executable of the process, specifically whether it enforces library validation. But even if you disable library validation, that is likely only going to work for dlopen() scenarios.
The problem is that app bundles are signed as a whole. Even non-executable resource files within the bundle are hashed, and then this list of hashes is hashed again and stored in the code signature of the main binary. While it looks syntactically possible to exclude files from this, I don't know whether Gatekeeper would accept this, and I don't know whether it would work for dylibs in particular.
But even if you found a combination that works, it will likely only be a matter of time before Apple breaks it, because the whole point of codesigning is that only pre-approved binaries are allowed to be executed.
The simple solution is: if you replace the library, you re-sign the entire bundle.
",
76392070,"Cannot run updated file from terminal of VSC, for Python","So to make it very simple.
If I write print(""Hello World"") and run it on the RUN icon on top right corner it works perfectly fine.
And only then if I type in terminal   python hello.py   executes again just fine.
But if I change program to print(""Hello to everybody"") and I type in terminal   python hello.py   it executes the previous program and giving me Hello World on screen. But then again if I click on run icon and after correct execution repeat in terminal python hello.py, now it runs correctly.
Well I tried CTRL,SHift P, to change terminal and like select interpreter but couldn't figure out from which file is it trying to start program. And why is it correct after I use run icon.
",76392085,"Are you saving the file after changing it? The Run button will run the code that is currently in the editor, while running it via python in the command line will run it from the saved file.
",
76391253,Vue 3: Mount component onto existing app instance,"I have a plugin written in Vue 2 that programmatically mounts a component onto existing app instance.
export default {
  install (Vue) {
    const component = new (Vue.extend(Component))()
    const vm = component.$mount()
    document.body.appendChild(vm.$el)
  }
}

Trying to migrate this to Vue 3 and this is the closest I've got.
export default {
  install (app) {
    const component = defineComponent(Component)
    const container = document.createElement('div')
    document.body.appendChild(container)
    const vm = createApp(component).mount(container)
  }
}

The problem is that createApp creates a new app instance, while Vue.extend created only a subclass. I need to have all globally available components in the main app, available in plugin's component as well. Creating a new app instance prevents this.
Component needs to be programatically mounted. Manually inserting it into template is not an option.
Please help.
",76392089,"Interesting problem! There is a discussion on the Vue GitHub and they created a module that you can use: mount-vue-component. I found it very helpful to look at the module's code, it does exactly what you want to do.
To instantiate the component, you have to create a VNode, which you can do with createVNode() (which is the same as h()). To get access to the app's context, including global components, you have to set the appContext property. Finally, the render() function mounts the component instance to an HTML Element.
So for you example, that gives you:
export default {
  install (app) {
    const container = document.createElement('div')
    document.body.appendChild(container)

    const vNode = createVNode(component)
    vNode.appContext = app._context
    render(vNode, container)
  }
}


Here it is in a snippet:


const { createApp, createVNode, ref, render  } = Vue;

const App = {
  data() {
    return {
      msg: ref('Hello from app!')
    }  
  }
}
const app = createApp(App)

app.component('inner-component', {
  template:  '<div style=""color: green;"">This is a global component in app</div>'
})
app.mount('#app')


const component = {
  props: ['color'],
  template: '<span :style=""{color: color}"">Hello from mounted component!</span><inner-component></inner-component>'
}
const el = document.getElementById('leComponent')
const vNode = createVNode(component, {color: 'blue'}, [])
vNode.appContext = app._context
render(vNode, el)
div{
  margin: 4px;
  padding: 4px;
  border: 1px solid #333;
}
<div id=""app"">
  {{msg}}
  <inner-component></inner-component>
</div>
<div id=""leComponent""></div>
<script src=""https://unpkg.com/vue@3/dist/vue.global.js""></script>



",
76390122,Matrix Circuit Analysis using Algorithms,"spsolve is then - sometimes - unable to find a solution.
Our teacher gave us test cases that we have to satisfy however I passed all of them but seems to fail the hidden test cases.
My code checks for the following: If they share a node and only those two resistors are connected then print SERIES else NEITHER. If their previous resistor is in series, then its is in SERIES (see test case no 2). If the resistor's ends are connected to the same nodes then print Parallel.
Can you suggest some inputs or any possible scenarios that a code wouldn't be able to answer correctly? or maybe a suggestion on what type of algorithm I should use for this problem
As I am performing nodal analysis, a singular matrix is expected since the position of the ground potential is generally not well-defined. However, before the update, a solution was found in 99% of the cases, maybe more. Now, I'm at 10% for large systems at best. I have not changed the algorithm and for a few tests, I have used identical code as before. Here is how I set up my calculation:
I generate a random three-dimensional network of resistors (I realize that I could accidentally create unsolvable networks but the percentages above should not change that drastically). The only SciPy/NumPy functions used here is np.random
I create a sparse lil-matrix which I fill with conductance values extracted from my resistor network. I also create a solution vector which is not sparse.
I convert the conductance matrix to csr-format and use the spsolve method. This is where my code lately fails.
",76390602,"Electron travel between Vdd and ground.  The Dijkstra algorithm ( google it ) finds these routes.  If two resisters are on the same route, they are in series.
Setup

LOOP over every pair of resisters

IF the ends of the two resistor are connected to the same nodes
- mark as parallel


Create adjacency list for resisters, combining parallels together into one edge.

To check if R1 and R2 are in series:

IF R1 and R2 are marked parallel return PARALLEL
Use Dijkstra to find all routes from Vdd to R1
IF no route return NO
Use Dijkstra to find all route from R1 to GND
IF no route return NO
IF R2 itself, or as part of a parallel, on any routes found return SERIES

Note that this will return YES for R3 and R4 in sample input 1.  I do not understand why R3 and R4 are not in series - electricity will flow through R3 and then R4 to reach ground.
Obviously, I do not understand your definition of ""in series"".  Please provide this definition.  In particular:
R1 Vdd a
R2 a b
R3 a b
R4 b GND

Are R1 and R4 in series or not?
",
76390958,"Does a StatefulSet pod, when deleted or failed, get redeployed on the same worker node, or is it deployed on another available worker node?","Does a StatefulSet pod, when deleted or failed, get redeployed on the same worker node, or is it deployed on another available worker node?
Seeking clarification on the default behavior of StatefulSet pods in Kubernetes when they are deleted or failed whether they are rescheduled on the same worker node or on different worker nodes.
",76392099,"By default, k8s will schedule a new pod on any 1 of the nodes which has sufficient CPU and memory resources available. If you specified any special conditions like Pod affinity, Pod affinity or Node affinity, k8s will follow them accordingly. Please check out this official document
",
76387953,Image duplicated when using fig.canvas.tostring_rgb(),"I am plotting 3D data using matplotlib==3.3.4:
fig = plt.figure(figsize=(15, 10))
ax = fig.gca(projection=""3d"")
ax.view_init(30, 0)

# facecolors is a 3D volume with some processing
ax.voxels(
    x, y, z, facecolors[:, :, :, -1] != 0, facecolors=facecolors, shade=False
)
fig.canvas.draw()
image_flat = np.frombuffer(fig.canvas.tostring_rgb(), dtype=""uint8"")
image_shape = (*fig.canvas.get_width_height(), 3)  # (1500, 1000, 3)
ax.imshow(image_flat.reshape(*image_shape))
plt.show()

(I am making some improvements on BraTS20_3dUnet_3dAutoEncoder with inspiration from Figure to image as a numpy array).
However, when I actually plot the image, there are two copies:

What am I doing wrong?  I can't figure out where the second image is coming from.
",76389700,"The NumPy array ordering is (rows, cols, ch). The code image_shape = (*fig.canvas.get_width_height(), 3) switches rows and cols, which leads to the output image being incorrectly shaped, which looks like two copies.

Replace image_shape = (*fig.canvas.get_width_height(), 3) with:
image_shape = (*fig.canvas.get_width_height()[::-1], 3)


For avoiding confusion, we better use two lines of code:
cols, rows = fig.canvas.get_width_height()
image_shape = (rows, cols, 3)


Reproducible example (using data from here):
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(15, 10))
ax = fig.gca(projection=""3d"")
ax.view_init(30, 0)

# https://stackoverflow.com/questions/76387953/image-duplicated-when-using-matplotlib-fig-canvas-tostring-rgb
# prepare some coordinates
x, y, z = np.indices((8, 8, 8))

# draw cuboids in the top left and bottom right corners, and a link between
# them
cube1 = (x < 3) & (y < 3) & (z < 3)
cube2 = (x >= 5) & (y >= 5) & (z >= 5)
link = abs(x - y) + abs(y - z) + abs(z - x) <= 2

# combine the objects into a single boolean array
voxelarray = cube1 | cube2 | link

# set the colors of each object
colors = np.empty(voxelarray.shape, dtype=object)
colors[link] = 'red'
colors[cube1] = 'blue'
colors[cube2] = 'green'

# and plot everything
#ax = plt.figure().add_subplot(projection='3d')
ax.voxels(voxelarray, facecolors=colors, edgecolor='k')

fig.canvas.draw()
image_flat = np.frombuffer(fig.canvas.tostring_rgb(), dtype=""uint8"")
#image_shape = (*fig.canvas.get_width_height(), 3)  # (1500, 1000, 3)
#image_shape = (*fig.canvas.get_width_height()[::-1], 3)  # It should be (1000, 1500, 3) instead of (1500, 1000, 3)
cols, rows = fig.canvas.get_width_height()
image_shape = (rows, cols, 3)
img = image_flat.reshape(*image_shape)

plt.figure()
plt.imshow(img)
plt.show()


Output image before fixing the code:

Output image after fixing the code:

",
76389973,Are there any default value tricks / Elvis Operator in Ocaml?,"Are there any elvis like operator in Ocaml ?
Any sort of optional chaining that return the right value when the left one is empty, a default value operator, like the |> operator with opposite effect.
If not what are the good practices ?
As an example of a use case :
let get_val val_1 val_2 = 
  if (val_1) then (val_1) else (val_2);;

Are there any syntactic sugar ?
",76390614,"First, in OCaml if ... then ... else ...  is an expression and thus there is no needs to have a distinct ternary operator.
Second, OCaml has optional arguments.
If I guess correctly the supposed semantics of your get_val function, it can be written:
let default = []
let get_val ?(val_1=default) () = val_1

which gives you [] as the default value when the named argument val_1 is absent
let () =
  assert ([] = get_val ())

or the val_1 argument otherwise:
let () = 
  assert ([2;3;4] = get_val ~val_1:[2;3;4] ())

",
76388435,send a get http request by axios on react to laravel 10 backend on localhost,"i am using axios on react to send http request to laravel backend. but cors prevent laravel to answer my request. i catch below error:
p://localhost/todo/laravel/public/api/test' from origin 'http://localhost:3000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
my react code:
    axios.get('http://localhost/todo/laravel/public/api/test', { withCredentials: true })
        .then(function (response) {
            // handle success
            console.log(response);
        })
        .catch(function (error) {
            // handle error
            console.log(error);
        })
        .then(function () {
            // always executed
        });

my laravel code:
public function all(): bool|string
{
    $todos = Todo::all();
    return json_encode($todos);
}

my cors config:
<?php

return [

/*
|--------------------------------------------------------------------------
| Cross-Origin Resource Sharing (CORS) Configuration
|--------------------------------------------------------------------------
|
| Here you may configure your settings for cross-origin resource sharing
| or ""CORS"". This determines what cross-origin operations may execute
| in web browsers. You are free to adjust these settings as needed.
|
| To learn more: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS
|
*/
'paths' => ['*'],

'allowed_methods' => ['*'],

'allowed_origins' => ['*'],

'allowed_origins_patterns' => [],

'allowed_headers' => ['*'],

'exposed_headers' => false,

'max_age' => false,

'supports_credentials' => false,
];

what is problem?
",76389815,"try
'paths' => ['todo/*'],
I' not sure, that ['*'] is allowed
",
76387968,"Sometimes, in a Data JPA environment, when three queries occur in one method, one query does not work normally in DB","Sometimes, in a Data JPA environment, when three queries occur in one method, one query does not work normally in DB.
@Transactional
public void method1(Log log){
        Long userId = log.getUserId();
        User user = userRepository.findById(userId).orElseThrow(RuntimeException::new)

        BigDecimal point = BigDecimal.valueOf(100L);
        user.minusPoint(point);
        log.minusPoint(point);
        logRepository.save(new Log(point));
}

It works normally for one case.
However, I ran it on several occasions, and one did not work properly.
ex) logList.forEach(log-> a.method1(log));
did not work ""log.minusPoint(point);"" in all cases.
The data in the DB has not changed about log.
But the new log data was inserted and the user data has been changed.
It was confirmed that the update, insert query was created successfully in the test environment.
",76389842,"You can enable SQL logging and check what data is actually passed to the database.
One of the reason can be: @Transactional annotation doesn't work. You need to get a service from the Spring Context to allow Spring to wrap method1() with a proxy method to do dirty checking and save data.
@Transactional annotation is a bit messy. It is primary not about transactions, but about dirty checking and Hibernate session control.
When you do
logRepository.save(new Log(point));

data is saved because logRepository.save() has it is own @Transactional annotation and does explicit call to save the data.
log.minusPoint(point) could work, if @Transactional works correctly. A Spring proxy could open Hibernate session (Persistent Context) before method1() call and close it in the end. During closing session Hibernate does dirty checking — checks, if any of objects in session were changed and saves such objects.
Another reason can be that you load log in a method without @Transactional or in a method with own @Transactional. Different @Transactional means different Persistent Context.
One exception — when @Transactional of method1() is wrapped with another @Transactional that is used for a method that calls method1().
It is bad practice to solve all issues with putting @Transactional everywhere. You can just do
logRepository.save(log)

",
76392047,How can I count over multiple fields in mongo?,"I am trying to write a mongo query to help produce a report from the data.
The documents look something like this:
[{
    ""DeviceType"" : ""A"",
    ""DeviceStatus"" : ""On""
},
{
    ""Device Type"" : ""B"",
    ""Device Status"" : ""On""
},
{
    ""DeviceType"" : ""A"",
    ""DeviceStatus"" : ""Off""
},
{
    ""DeviceType"" : ""A"",
    ""DeviceStatus"" : ""On""
}]

The DeviceType field can take any string value and the DeviceStatus field can be any one of On, Off or InRepair. I want to write a mongo query that displays DeviceStatus count for each one of the devices. The result should be something like this:
[{
    ""DeviceType"" : ""A"",
    ""DeviceStatusOnCount"" : ""15"",
    ""DeviceStatusOffCount"" : ""13"",
    ""DeviceStatusInRepairCount"" : ""12"",
},
{
    ""DeviceType"" : ""B"",
    ""DeviceStatusOnCount"" : ""6"",
    ""DeviceStatusOffCount"" : ""14"",
    ""DeviceStatusInRepairCount"" : ""2"",
}]

How can I achieve this in mongo?
My current query can only group based on the DeviceType:
db.Collection_Name.aggregate([
    { $group: { _id: ""$DeviceType"", count: { $sum: 1 } } }
])

",76392103,"You can use $cond + $eq to return either 1 or 0 to $sum operator:
db.collection.aggregate([
  {
    $group: {
      _id: ""$DeviceType"",
      DeviceStatusOnCount: { $sum: { $cond: [ { $eq: [ ""$DeviceStatus"", ""On"" ] }, 1, 0 ] } },
      DeviceStatusOffCount: { $sum: { $cond: [ { $eq: [ ""$DeviceStatus"", ""Off"" ] }, 1, 0 ] } },
      DeviceStatusInRepairCount: { $sum: { $cond: [ { $eq: [ ""$DeviceStatus"", ""InRepair"" ] }, 1, 0 ] } }
    }
  },
  {$addFields: {DeviceType: '$_id'}}, {$project: {_id:0}}
])

Test: mongoplayground
",
76390575,"How to remove the ""locked"" flag (programmatically)","(MacOS Monterey):
I have external hard disks with files written under Windows, using Cygwin rsync.
The files are perfectly readable under MacOS, but when I want to delete/overwrite them, quite some appear ""locked"" (operation not permitted when doing a rm) on MacOS. Using the Finder, I can remove the lock by getting the context menu on the file, choose ""GetInfo"", and unchecked the Locked property.
But there are too many of them to do it by hand. I would like to run recursively over the file tree and uncheck the ""locked"" attribute programmatically. Of course the problem is not the traversal of the directory tree (can done by find for instance), but the change of the attribute from the command line.
chmod does not help here (as the files are 0777). Any other command I can use?
",76390615,"I just found the command to do so:
chflags nouchg FILENAME

The command is described here
",
76388274,Updating web redirect uri of Azure AD app registration,"I have a scenario where my pipeline should update the app registration with an additional redirectUrl.
I have managed to extract the current web.redirectUris with the following:
 existing_urls=$(az ad app show --id '<client-id>' --query ""[web.redirectUris]"" --output tsv)
I would like to achieve something like this
existing_urls=$(az ad app show --id '<client-id>' --query ""[web.redirectUris]"" --output tsv)
az ad app update --id '<client-id>' --web-redirect-uris ""$existing_urls https://hostname.com/newCallback""

I have tried updating the web.redirectUris in two ways and both of them have failed when I pass multiple redirect URIs.
Attempt 1
az ad app update --id '<client-id>' --web-redirect-uris ""https://hostname.com/callbackx https://hostname.com/callbacky""

One or more properties contains invalid values.

However when having only one uri this worked fine
az ad app update --id '<client-id>' --web-redirect-uris ""https://hostname.com/callbackx""

Attempt 2
This one fails regardless of number of redirectUris that are passed
az ad app update --id '<client-id>' --set ""web.redirectUris=['https://hostname.com/callbackx', 'https://hostname.com/callbacky']""

Couldn't find 'web' in ''. Available options: []

",76389975,"Tried as shown :But got the same error:
az ad app show --id 1e7bxxx7830

existing_urls=$(az ad app show --id 1e7b8fxxxx830 --query ""[web.redirectUris]"")

az ad app update --id 1e7xxx0a7830 --web-redirect-uris ""$existing_urls https://hostname.com/newCallback""

$updated_urls=""$existing_urls https://hostname.com/newCallback""

az ad app update --id 1e7b8xxx0a7830  --set ""web.redirectUris='$updated_urls'""

az ad app update --id 1e7b8fxxxd0a7830  --set ""web.redirectUris='$updated_urls'""

Error:
Couldn't find 'web' in ''. Available options: []


Following command worked foe me in azure cli in updating multiple Redirect Urls:
az ad app update --id '1e7bxxxa7830' --web-redirect-uris ""https://hostname.com/callback""  ""https://jwt.ms"" ""https://myexampleapp.com""
here --id is clientId .

So  give the command with required urls as
az ad app update --id '1e7bxxxa7830' --web-redirect-uris ""<url1>""  ""<url2>"" ""<url3>""
upon az ad app show --id 1e7b8xxxx830

",
76390603,I'm trying to find the closest value in a reference data frame but it is not outputting the correct row. Where am I going wrong?,"I'm trying to find the closest value in a reference data frame but it is not outputting the correct row.
I am using the below data frame which is then used to find the relevant row corresponding to the closest value in column 'P' to a defined variable. For example if p = 0.22222 then the code should output row 2.
DF:
      P   n1      n2      n3      n4      n5      n6      n7      n8      n9   
0   NaN  0.0  0.2000  0.4000  0.6000  0.8000  1.0000  1.2000  1.4000  1.6000
1   0.0  1.0  0.8039  0.6286  0.4855  0.3753  0.2929  0.2318  0.1863  0.1520   
2   0.2  1.0  0.7983  0.6201  0.4771  0.3683  0.2876  0.2279  0.1835  0.1500   
3   0.4  1.0  0.7789  0.5924  0.4508  0.3473  0.2720  0.2167  0.1754  0.1442   
4   0.6  1.0  0.7349  0.5377  0.4043  0.3124  0.2470  0.1989  0.1628  0.1351   
5   0.8  1.0  0.6301  0.4433  0.3368  0.2658  0.2147  0.1762  0.1465  0.1234   
6   1.0  0.5  0.3828  0.3105  0.2559  0.2130  0.1787  0.1510  0.1286  0.1102   
7   1.2  0.0  0.1544  0.1871  0.1795  0.1621  0.1433  0.1257  0.1103  0.0965   
8   1.4  0.0  0.0717  0.1101  0.1216  0.1197  0.1120  0.1024  0.0925  0.0831   
9   1.6  0.0  0.0400  0.0682  0.0829  0.0876  0.0865  0.0824  0.0765  0.0707   
10  1.8  0.0  0.0249  0.0449  0.0580  0.0647  0.0668  0.0659  0.0633  0.0597   
11  2.0  0.0  0.0168  0.0312  0.0418  0.0485  0.0519  0.0528  0.0520  0.0502   
12  3.0  0.0  0.0042  0.0082  0.0118  0.0149  0.0174  0.0193  0.0207  0.0216

The function I am using however outputs the incorrect value:
p = 0.2020202
closest_p = df.iloc[(df['P']-p).abs().argsort()[:1]]

Expected output:
      P   n1      n2      n3      n4      n5      n6      n7      n8      n9   
2   0.2  1.0  0.7983  0.6201  0.4771  0.3683  0.2876  0.2279  0.1835  0.1500

However it is only outputting the last row -
      P   n1      n2      n3      n4      n5      n6      n7      n8      n9   
12  3.0  0.0  0.0042  0.0082  0.0118  0.0149  0.0174  0.0193  0.0207  0.0216

Where am i going wrong????
",76390619,"You need to use idxmin:
closest_p = (df['P']-p).abs().idxmin()

Output: 2
For the row: df.loc[(df['P']-p).abs().idxmin()]
A fix of your approach would have been to use sort_values (but it's less efficient):
closest_p = df.loc[(df['P']-p).abs().sort_values().index[0]]

",
76391811,Unused parameter warning in templated function with if constexpr,"In this code
#include <type_traits>
#include <iostream>

template <class T>
void func(const T& a)
{
    if constexpr(std::is_same_v<T,int>)
    {
        static_cast<void>(a);
    }
    else if constexpr(std::is_same_v<T,double>)
    {
        // oops forgot to use it here   
    }
    else
    {
        
    }
}

int main() {
    func(4);
    func(""this"");
}

why doesn't the compiler warn about unused variable in the else-s()? (with -Wall)
My understanding is that logically the instantiations of the method are completely different. If the argument is not being used in one of the instantiations of the template, isn't it an unused variable?
Or does the language/compiler not interpret it like that.
",76392128,"We can say that the parameter a is not fully unused, it's conditionally unused.
The C++ standard requires to generate diagnostic messages for ill-formed programs. Your program is not ill-formed. The C++ standard does not require compilers to generate other warnings at all. So it's up to the compiler vendor to decide how to implement that.
While I personally would like to be warned about conditionally unused variables (your example is a good demo why), no compiler vendor has implemented that. Why? They test new version of compilers on large code bases to see whether new C++ features will cause breaking changes. Likely, during those tests, a lot of warnings were generated for code like yours, but the code was considered totally fine by C++ experts. So the warnings were adapted to not occur in such situations. That's to ensure that code with 0 warnings is possible.
",
76391083,Trouble with undefined symbols in Rust's ffi when using a C/Fortran library,"I have a library, namely pfapack (1), that I want to use in rust. So the initial code is written in Fortran, but a C interface exists and works well. I want to make a Rust crate (2) that ships this code so I can use it in any other Rust project (3). Doing so, (3) gives an undefined symbol error.
I have written a build script in (2) that calls (1)'s build method. I then use cc to combine the object files and link the needed libraries. I then used bindgen to generate bindings for the functions I need. I would expect that (3) would be able to see the object that were compiled at (2) build time, but it can't.
The exact step taken are:

New crate with (1)'s source code.
(2) build.rs

use std::process::Command;

fn main() {
// This makefile call a custom root makefile that only calls the two
// makefiles in c_interface/ and fortran/
    Command::new(""make"").output() 
        .expect(""Failed to make"");

    println!(""cargo:rustc-link-search=c_interface"");
    println!(""cargo:rustc-link-search=fortran"");
    println!(""cargo:rustc-link-lib=static=pfapack"");
    println!(""cargo:rustc-link-lib=static=cpfapack"");
    println!(""cargo:rustc-link-lib=gfortran"");

}


(3) build.rs

fn main() {
    println!(""cargo:rustc-link-lib=lapack"");
    println!(""cargo:rustc-link-lib=blas"");
}

Original example compilation to use pfapack
gcc -O3 -I c_interface/ foo.c -o foo.out c_interface/libcpfapack.a fortran/libpfapack.a -lm -lblas -llapack -lgfortran

The command used to generate bindings came from https://github.com/blas-lapack-rs/lapack-sys/blob/master/bin/generate.sh as it uses the same naming convention:
generate.sh
#!/bin/bash
set -eux

bindgen --allowlist-function='^.*_$' --use-core pfapack.h \
  | sed -e 's/::std::os::raw:://g' \
  | sed -e '/__darwin_size_t/d' \
  > pfapack.rs

rustfmt pfapack.rs

Compiling (3) gives this error https://pastebin.com/4FubsYx9
Ignoring a big blob of flags, the error:
  = note: /usr/bin/ld: /home/dumbo/Documents/test_pfapack/target/debug/deps/test_pfapack-d08bc25fe63b6ef8.ka29pyd46xgunxk.rcgu.o: in function `pfapack_sys::dskpfa':
          /home/dumbo/Documents/pfapack-sys/src/pfapack-bind.rs:249: undefined reference to `dskpfa'
          collect2: error: ld returned 1 exit status
 
  = note: some `extern` functions couldn't be found; some native libraries may need to be installed or have their path specified
  = note: use the `-l` flag to specify native libraries to link
  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo (see https://doc.rust-lang.org/cargo/reference/build-scripts.html#cargorustc-link-libkindname)
 
error: could not compile `test_pfapack` due to previous error

(2) Cargo.toml
[package]
name = ""pfapack-sys""
version = ""0.1.0""
edition = ""2021""
links = ""pfapack""

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
libc = ""0.2""

[dependencies.num-complex]
version = ""0.4""
default-features = false

[lib]
name = ""pfapack_sys""

[build-dependencies]
cc = ""1.0.79""

",76392133,"The solution was quite simple and right under my nose. Note that the symbol not found does not have an ending underscore, as pfapack functions do after mangling. This was because I tried to do the bindings to the C interface and the pretty Rust functions in the same crate. Looking at the Rust book, https://doc.rust-lang.org/cargo/reference/build-scripts.html#-sys-packages, the convention is to have a *-sys crate that just does the binding, and have another crate named * that does the pretty functions. Thus, I needed to update the build script.
Here is the updated (2) build.rs
// build script
use std::process::Command;

fn main() {
    Command::new(""make"").output()
        .expect(""Failed to make"");

    println!(""cargo:rustc-link-search=c_interface"");
    println!(""cargo:rustc-link-search=fortran"");
    println!(""cargo:rustc-link-lib=static=pfapack"");
    println!(""cargo:rustc-link-lib=static=cpfapack"");
    println!(""cargo:rustc-link-lib=gfortran"");
    println!(""cargo:rustc-link-lib=lapack"");
    println!(""cargo:rustc-link-lib=blas"");
}

I renamed this package pfapack-sys. Created a new crate named pfapack(4) that depends on pfapack-sys. Now, update Cargo.toml(3) to depend on (4). Now works out of the box.
",
76390438,Why metadata is written at the end of the file in Apache Parquet?,"I wonder why Apache Parquet writes metadata at the end of the file instead of the beginning?
In the official documentation of Apache Parquet, I found that Metadata is written after the data to allow for single pass writing.. Is the metadata written at the end to ensure the integrity of the file? I don't understand what this sentence really means, if someone could explain it to me, I'd appreciate it.
",76390623,"I think the main reason is so you can write bigger than memory data to the same file.
The meta data contains information about the schema of the data (type of the columns) and its shape (number of row groups, size of each row groups).
So in order to generate the metadata you need to know what the data is made of. This can be a problem if your data doesn't fit in memory.
In this case, you should still be able to split your data in manageable row groups (that fit in memory) and append them to the file one by one, keeping track of the meta data, and appending the meta data at the end.
import pyarrow as pa
import pyarrow.parquet as pq


schema = pa.schema([pa.field(""col1"", pa.int32())])

with pq.ParquetWriter(""table.parquet"", schema=schema) as file:
    for i in range(0, 10):
        file.write(pa.table({""col1"": [i] * 10}, schema=schema))

If you're looking for an alternative where the data can be streamed, with the meta data being written at the beginning, you should look at the arrow IPC format.
",
76387965,NewRelic Not working with multiple workers Fast API Uvicorn,"We are trying to integrate NewRelic with our Fast API Service. It works fine when we are not providing numbers of worker in uvicorn config
if __name__ == ""__main__"":
    # newrelic.agent.register_application()
    import newrelic.agent

    newrelic.agent.initialize()
    print(""api key "", os.environ.get(""NEW_RELIC_LICENSE_KEY"", 1))
    print(""app name "", os.environ.get(""NEW_RELIC_APP_NAME"", 1))
    # printing to make sure licence key and app name are defined in env variables.
    uvicorn.run(app, host='0.0.0.0', port=5600)


But when we are defining numbers of workers in the uvicorn config, NewRelic does not show any data in dashboard.
if __name__ == ""__main__"":
    # newrelic.agent.register_application()
    import newrelic.agent

    newrelic.agent.initialize()
    print(""api key "", os.environ.get(""NEW_RELIC_LICENSE_KEY"", 1))
    print(""app name "", os.environ.get(""NEW_RELIC_APP_NAME"", 1))
    # printing to make sure licence key and app name are defined in env variables.
    uvicorn.run(""new_relic_test:app"", host='0.0.0.0', port=5600, workers=15)


Is that due to multiple server process being created by uvicorn workers?
I tried removing workers and it worked fine. But with numbers of workers it does not work
",76390012,"The reason is the following: when you run uvicorn.run with only one process, it will start the server as a normal Python function. But, when you run with workers=n, uvicorn will start n new processes, and the original Python process will remain as an orchestrator between these. In these new processes, it will not start your code with a different entrypoint, meaning the if __name__ == ""__main__"" will not run (this is also why you must specify your app as a string instead of the Python instance when running more than one worker, since uvicorn needs to know where to import your app from). So in your case, newrelic.agent.initialize() is not run.
I would suggest moving everything except uvicorn.run out of the if __name__ == ""__main__"" block and put it in the same file as where you define your app.
",
76392029,How can I open a C-file in Python using 'subprocess' without triggering an Exception?,"I am trying to open a C-file in Python by using the ""subprocess""-module.
I cannot execute the program without triggering a 'FileNotFoundError', and if I put in the full path of the file, I get an 'Exec format error'.
I don't necessarily need to use the 'subprocess'-module, I just don't know of any other method.
The Python-Skript:
#!/usr/bin/env python
import subprocess

shellcode_file = ""shellcode.bin""

try:
    with open(shellcode_file, ""rb"") as f:
        shellcode = f.read();
    
    subprocess.call([""script.c"", shellcode])
except FileNotFoundError as e:
    print(e, ""not found."")

I am pretty sure that I am opening the File wrong but I couldn't find a way to fix it.
",76392136,"You are passing the C source file itself, not the compiled executable (*.exe).
For further info if needed, C is not an interpreted/scripted language, so it cannot be run through its code file like other scripting languages (e.g. Python, Batch). It's a compiled language, meaning that it has to be run through a compiler to generate a separate file (almost always a .exe file) that you can then run your program with.
Compile your C program first and pass the executable file.
    subprocess.call([""script.exe"", shellcode])

",
76390211,WebScraping - BeautifulSoup Python,"I am trying to scrape the medium website. Here is my code.
import requests
from bs4 import BeautifulSoup as bs

class Publication:
    def __init__(self, publication):
        self.publication = publication
        self.headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}# mimics a browser's request
    def get_articles(self):
        ""Get the articles of the user/publication which was given as input""
        publication = self.publication
        r = requests.get(f""https://{publication}.com/"", headers=self.headers)
        soup = bs(r.text, 'lxml')
        elements = soup.find_all('h2')
        for x in elements:
            print(x.text)

publication = Publication('towardsdatascience')
publication.get_articles()

It is working somewhat good but it is not scraping all the titles. It is only getting the some of the articles from the top of the page. I want it to get all the article names from the page. It also getting the side bar stuff like who to follow and all. I dont want that. How do I do that?
Here is the output of my code:
How to Rewrite and Optimize Your SQL Queries to Pandas in 5 Simple Examples
Storytelling with Charts
Simplify Your Data Preparation with These Four Lesser-Known Scikit-Learn Classes
Non-Parametric Tests for Beginners (Part 1: Rank and Sign Tests)
BigQuery Best Practices: Unleash the Full Potential of Your Data Warehouse
How to Test Your Python Code with Pytest
7 Signs You’ve Become an Advanced Sklearn User Without Even Realizing It
How Data Scientists Save Time
MLOps: What is Operational Tempo?
Finding Your Dream Master’s Program in AI
Editors
TDS Editors
Ben Huberman
Caitlin Kindig
Sign up for The Variable

",76390625,"As Barry the Platipus mentions in a comment, the content you want is loaded via Javascript. A complicating factor is that this content is only loaded when you scroll the page, so even a naive Selenium-based solution like this will still return only the same set of results as your existing code:
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options


options = Options()
options.add_argument(""--headless"")
driver = webdriver.Chrome(options=options)


class Publication:
    def __init__(self, publication):
        self.publication = publication

    def get_articles(self):
        ""Get the articles of the user/publication which was given as input""
        publication = self.publication
        driver.get(f""https://{publication}.com/"")
        elements = driver.find_elements(By.CSS_SELECTOR, ""h2"")
        for x in elements:
            print(x.text)


publication = Publication(""towardsdatascience"")
publication.get_articles()

To get more than the initial set of articles, we need to scroll the page. For example, if we add a simple loop to scroll the page a few times before querying for h2 elements, like this:
    def get_articles(self):
        ""Get the articles of the user/publication which was given as input""
        publication = self.publication
        driver.get(f""https://{publication}.com/"")
        for x in range(3):
            driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")

            # The sleep here is to give the page time to respond.
            time.sleep(0.2)
        elements = driver.find_elements(By.CSS_SELECTOR, ""h2"")
        for x in elements:
            print(x.text)

Then the output of the code is:
Large Language Models in Molecular Biology
How to Rewrite and Optimize Your SQL Queries to Pandas in 5 Simple Examples
Storytelling with Charts
Simplify Your Data Preparation with These Four Lesser-Known Scikit-Learn Classes
Non-Parametric Tests for Beginners (Part 1: Rank and Sign Tests)
BigQuery Best Practices: Unleash the Full Potential of Your Data Warehouse
How to Test Your Python Code with Pytest
7 Signs You’ve Become an Advanced Sklearn User Without Even Realizing It
How Data Scientists Save Time
MLOps: What is Operational Tempo?
Finding Your Dream Master’s Program in AI
Temporary Variables in Python: Readability versus Performance
Naive Bayes Classification
Predicting the Functionality of Water Pumps with XGBoost
Detection of Credit Card Fraud with an Autoencoder
4 Reasons Why I Won’t Sign the “Existential Risk” New Statement
The Data-centric AI Concepts in Segment Anything
3D Deep Learning Python Tutorial: PointNet Data Preparation
Why Trust and Safety in Enterprise AI Is (Relatively) Easy
The Principles of a Modern Computer Scientist

That page appears to be an ""infinite scroll"" style of page, so you probably want to set a limit on how many times you scroll to find new content.
",
76390552,Python show output of remote SSH command in web page in Django,"I have a simple Django app to show the output of an SSH remote command.
views.py:
from django.http import HttpResponse
from subprocess import *

def index(request):
    with open('/home/python/django/cron/sites.txt', 'r') as file:
        for site in file:
            # out = getoutput(f""ssh -o StrictHostKeyChecking=accept-new -p1994 root@{site} crontab -l"")
            out = run([""ssh"", ""-o StrictHostKeyChecking=accept-new"", ""-p1994"", f""root@{site}"".strip(), ""crontab"", ""-l""])
    return HttpResponse(out)

urls.py:
from django.contrib import admin
from django.urls import path
# imported views
from cron import views

urlpatterns = [
    path('admin/', admin.site.urls),
    # configured the url
    path('',views.index, name=""homepage"")
]

sites.txt:
1.1.1.1
2.2.2.2
3.3.3.3

The issue is when I run localhost:5000, I see this:
CompletedProcess(args=['ssh', '-o StrictHostKeyChecking=accept-new', '-p1994', 'root@3.3.3.3', 'crontab', '-l'], returncode=0)

While I should see something like this:
* * * * * ls
* * * * * date
* * * * * pwd

I tried with both run and getoutput, but they either don't connect or the output is shown in terminal only.
How can I run this and show the output in the webpage?
",76390629,"You're returning the CompletedProcess object instead of its output.
Try updating the run command to capture output by adding the optional parameter
out = run([...], capture_output=True)

AND changing the output to show the stdout instead of the object
return HttpResponse(out.stdout)

",
76392117,"Can I pass a variable string into the ""Value"" attribute for a HTML input tag?","sorry if this is a simplistic question or demonstrates some misunderstanding of HTML, this is my first time using it. If there's any missing info or stuff I should have included in the question, I'm happy to provide it.
I have a python program, app.py, and it calls and renders an HTML page. That HTML page contains a few inputs and I understand how to place a given string as the default value for the input with the Value attribute.
This is the html:
<form action=""/new"" method=""post"">
    <label for=""qty_wheels"">Number of wheels:</label>
    
    <input type=""text"" name=""qty_wheels"" value = ""Qty_Wheels""     />
    <label for=""flag_color"">Colour of flag:</label>
    <input type=""text"" name=""flag_color"" />
    <input type=""submit"" class=""button"">
</form>

Where ""Qty_Wheels"" is written though, I want to place a string I've passed in from the python end.
This is the python call for this html page:
return render_template(""buggy-form.html"", Qty_Wheels = given_qty)

I've passed in the Qty_Wheels string here, and I want to print the value of that in the input. Is this possible and how can I do it? I've tried a lot of different syntax variants, but I can't find how I get it to interpret it as a string and print the content.
",76392146,"Flask's templates use the {{ val }} syntax:
Put the double curly brackets around Qty_Wheels:
<form action=""/new"" method=""post"">
    <label for=""qty_wheels"">Number of wheels:</label>
    
    <input type=""text"" name=""qty_wheels"" value = ""{{ Qty_Wheels }}""     />
    <label for=""flag_color"">Colour of flag:</label>
    <input type=""text"" name=""flag_color"" />
    <input type=""submit"" class=""button"">
</form>

For more on Flask templates see: https://flask.palletsprojects.com/en/2.3.x/tutorial/templates/
",
76388849,Why does Tailwind declare CSS variables multiple times?,"I am working on a react project with the tailwind. I checked the inspection of Chrome and saw the same tailwinds variables multiple times. I thought maybe something is not working properly in our project and checked Shopify and it was the same, I wonder why it is working in this way?
Screenshots are taken from first page of Shopify


",76390020,"There are Tailwind CSS default variables defined for ::backdrop in a separate rule from the *, ::before, ::after rule for two reasons:

The *, ::before, ::after selector does not cover ::backdrop. As per tailwindlabs/tailwindcss#8526:

This PR adds the ::backdrop pseudo-element to our universal default rules, which fixes an issue where utilities like backdrop:backdrop-blur would not work because the variables that rule depended on were not defined.


As for why it is in a separate rule, it could be due to browser support. According to MDN, the last major web browser to support ::backdrop was Safari 15.4 which was released on March 14th 2022. The aforementioned pull request tailwindlabs/tailwindcss#8526 was merged on June 6th 2022 and released with v3.1.0 on June 8th 2022.
This means at that time, only very recent Safari browsers would have support for the ::backdrop element. If ::backdrop would be with the *, ::before, ::after rule selector as *, ::before, ::after, ::backdrop, this would break sites on older Safari browsers, since the *, ::before, ::after, ::backdrop rule would not apply since one of the components were not supported. This could be a major regression so they separated out the ::backdrop selector into its own rule in pull request #8567 that was released in v3.1.1.


",
76389558,"In Strapi V4, How should I register a middleware to alter the responses returned by /user or /user/:id?","I've started to play with middlewares, it's great !
Here's an example of how I can inject a middleware when calling endpoints /api/playlists or /api/playlists/:id (I edited the file src/api/playlist/routes/playlist.js).
module.exports = createCoreRouter('api::playlist.playlist', {
 config: {
   find: {
     middlewares: ['api::playlist.playlist.find']
   },
   findOne: {
     middlewares: ['api::playlist.playlist.find-one']
   },
 },
})

Of course, I also created my middlewares in src/api/playlist/middlewares/find.js and src/api/playlist/middlewares/find-one.js)
But know, I want to add another middleware to update the responses returned by the API when calling /api/users or /api/users/:id.
Since there is no directory src/api/user in the filetree, how should I register a middleware for this ?
Thanks
",76390634,"I finally found out :

create the file src/extensions/user-permissions/strapi-server.js

This is mine. It registers a middleware for each of those endpoints:

GET /users/me : plugin::spiff-api.user-me

GET /users : plugin::spiff-api.user-find

GET /users/:id : plugin::spiff-api.user-find-one
(""use strict"");

module.exports = (plugin) => {
  //if you see this, the configuration do loads:
  console.log(""Custom strapi-server.js for user-permissions"");

  //get api routes for 'user-permissions'
  const apiRoutes = plugin.routes['content-api'].routes;

  //add middleware for GET /users/me
  apiRoutes
    .filter(route => route.handler === 'user.me')
    .map(route => {
      route.config.middlewares = [
        ...(route.config.middlewares || []),
        'plugin::spiff-api.user-me'//middleware name
      ];
      return route;
    });

    //add middleware for GET /users/:id
    apiRoutes
      .filter(route => route.handler === 'user.findOne')
      .map(route => {
        route.config.middlewares = [
          ...(route.config.middlewares || []),
          'plugin::spiff-api.user-find-one'//middleware name
        ];
        return route;
      });

  //add middleware for GET /users
  apiRoutes
    .filter(route => route.handler === 'user.find')
    .map(route => {
      console.log(route)
      route.config.middlewares = [
        ...(route.config.middlewares || []),
        'plugin::spiff-api.user-find'//middleware name
      ];
      return route;
    });

return plugin;
};



Then, create your middlewares middleware, eg.
    export default (config, { strapi })=> {
      return async (ctx, next) => {
        console.info(""running middleware 'user-find-one.js'"");
        console.log();

        //update your query here if needed
        //eg. populate 'favoritePosts'
        ctx.query.populate = {
          ...ctx.query.populate ?? {},
          favoritePosts: {}
        }

        const controller = strapi.plugin('users-permissions').controller('user');
        await controller.findOne(ctx);//this populates ctx.body
        const response = ctx.body;

        //update your response here if needed

        ctx.body = response;
        
        await next();//not sure why this stands for

      }
    }

",
76392112,Input validation with regular expression not working,"I'm trying to create a game in which, Starting of the game I have to ask the user for 'How many times he wants to play' and the user input must be an integer.
So I write the code I shared below. But it doesn't work as I expected.
If anyone can help me to figure out what I did wrong it will be helpful to me.
Thanks in advance.


let playCount = 0;
let regxNum = /^[0-9]+$/g; //Regular Expression to select all numbers between 0-9.
let checkPlayCount = 0;
let askPlayCount = () => { //Function to get game play count.
  checkPlayCount = 0;
  playCount = Number(prompt(""How many times you want to play: "")); //Gets play count and convert into number;
  checkPlayCount = Array.from(String(playCount), Number); //Converts number into array
}

askPlayCount();

//Code for validating input : must be number.
for(let i = 0; i < checkPlayCount.length; i++){
  if(checkPlayCount[i] != regxNum){
    console.log(""Please enter valid input"");
    askPlayCount();
  }
}



",76392173,"The problem is you convert the number into the array, while you simply can do this by directly checking the input against regex. Here is my version of the problem


let playCount = 0;
let regxNum = /^[0-9]+$/; // Regular Expression to match all numbers between 0-9

let askPlayCount = () => {
  playCount = Number(prompt(""How many times do you want to play: ""));
}

askPlayCount();

// Code for validating input: must be a number.
while (!regxNum.test(playCount)) {
  console.log(""Please enter a valid input"");
  askPlayCount();
}



",
76392174,Java and Python return different values when converting the hexadecimal to long,"I noticed this difference when comparing xxhash implementations in both Python and Java languages. Calculated hashes by xxhash library is the same as hexadecimal string, but they are different when I try to get calculated hash as an integer(or long) value.
I am sure that this is some kind of ""endian"" problem but I couldn't find how to get the same integer values for both languages.
Any ideas how and why this is happening?
Java Code:
String hexString = ""d24ec4f1a98c6e5b"";
System.out.println(new BigInteger(hexString,16).longValue());
// printed value -> -3292477735350538661

Python Code:
hexString = ""d24ec4f1a98c6e5b""
print(int(hexString, 16))
# printed value -> 15154266338359012955

",76392197,"You converted the BigInteger to long which is the reason for the difference. Because long is a signed 64-bits integer it overflows to a negative. If you just print the BigInteger as it is it gives the same result.
      System.out.println(new BigInteger(hexString,16));
# 15154266338359012955

",
76388858,change the size and orientation of legend title while plotting raster,"library(terra)
library(RColorBrewer)

# sample polygon
p <- system.file(""ex/lux.shp"", package=""terra"")
p <- terra::vect(p)

# sample raster
r <- system.file(""ex/elev.tif"", package=""terra"")
r <- terra::rast(r)
r <- terra::crop(r, p , snap = ""out"", mask = T)

terra::plot(r,
           col = brewer.pal(9, ""pastel1""),
           cex.main = 2,
           smooth = T,
           legend = T,
           plg = list(title = ""Score""),
           axes = TRUE,
           mar=c(3,3,3,6))
plot(p, add = T)

How do I change the size and orientation of the legend title 'Score'.
I want to orient the title so that it is vertical and follows along the
legend and also change the size of the legend title?
",76390104,"You can add text whereever you want. For example
library(terra)
p <- terra::vect(system.file(""ex/lux.shp"", package=""terra""))
r <- terra::rast(system.file(""ex/elev.tif"", package=""terra""))

plot(r, mar=c(3,3,3,6))
text(x=6.77, y=49.95, ""Score"", srt=-90, cex=2, xpd=NA, pos=4)
lines(p)


",
76392129,Why is babel home page different from the editor in the online tool,"When I learned React to build virtual DOM，I use the editor on babel's website，Why are the two editors turning different code?


Why are the two editors turning different code?
",76392209,"In the editor tool screenshot we can see that the selected option for the ""React Runtime"" configuration is ""Automatic"", while Babel's website editor looks like it's using the ""Classic"" option.
The difference between both is documented in https://babeljs.io/docs/babel-plugin-transform-react-jsx
There is also this blog post on React's blog, which explains the reasons behind.
",
76389865,How to register custom Spring validator and have automatic dependency injection?,"When working with ConstraintValidators, we just need to define the class and constraint and Spring is able to detect the validators and instantiate them automatically, injecting any beans we ask for in the constructor.
How do I add my custom Spring Validators (https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/validation/Validator.html) in the same manner?
",76390685,"You have to let the controller to know which validator will validate your request. I use following code to find supported validators:
@ControllerAdvice
@AllArgsConstructor
public class ControllerAdvisor {

    private final List<Validator> validators;

    @InitBinder
    public void initBinder(@NonNull final WebDataBinder binder) {
        final Object request= binder.getTarget();
        if (request== null) {
            return;
        }
        this.validators.stream()
            .filter(validator -> validator.supports(request.getClass()))
            .forEach(binder::addValidators);
    }

}


Note that you have to create those validators as beans. Therefore, you should annotate those validators with @Component. Hope this help you.
",
76388494,"Python3 - Kivy RecycleView's data Dictionary succefully updated, but not phisically the Widget","In a Kivy form i set 2 Recycleviews(A and B). I would add the item clicked in Recycleview A, to the RecycleView B, using a Python3 script:
from functools import partial
from kivy.app import App
from kivy.lang.builder import Builder
from kivy.uix.recycleview import RecycleView
class B(RecycleView):
    def __init__(self,**kwargs):
        super(B, self).__init__(**kwargs)
    def doet(self,x):
        self.data.append({'text':x})
        print(self.data) #it prints correctly, so why doesn't update?
class A(RecycleView):
    def __init__(self,**kwargs):
        super(A, self).__init__(**kwargs)
        self.data=[{'text':'FROM HERE','on_press':partial(app.b.doet,'TO HERE')}]
class app(App):
    b=B()
    def build(self):
        return Builder.load_file('lab.kv')
app().run()

'lab.kv':
BoxLayout:
    A:
        viewclass:'Button'
        RecycleBoxLayout:
            default_size: None, dp(56)
            default_size_hint: 1, None
            size_hint_y: None
            height: self.minimum_height
            orientation: 'vertical'
    B:
        viewclass:'Button'
        RecycleBoxLayout:
            default_size: None, dp(56)
            default_size_hint: 1, None
            size_hint_y: None
            height: self.minimum_height
            orientation: 'vertical'

My script correctly updates the Data's Dictionary, as I can see by printing it, but in RecycleView 'B' no items are phisically added.
",76390287,"The main problem with your code is that the line:
b=B()

is creating a new instance of B that is unrelated to the instance of B that is created in your kv. So anything done to that new instance of B will have no effect on what you see on the screen.
There are many possible approaches to do what you want. Here is one:
First, add an id to the B in your kv:
B:
    id: b

Then, add a modify the App class:
class app(App):
    def build(self):
        root = Builder.load_file('lab.kv')
        self.b = root.ids.b
        return root

    def doit(self, x):
        self.b.doet(x)

The above build() method uses the new id to get a reference to the correct B instance, and saves that reference. The doit() method is just an intermediary to direct the call to the correct instance of B.
Then modify the A class to use this:
class A(RecycleView):
    def __init__(self,**kwargs):
        super(A, self).__init__(**kwargs)
        self.data=[{'text':'FROM HERE','on_press':partial(App.get_running_app().doit,'TO HERE')}]

",
76390793,How to export results from Query Summary from Kusto Explorer,"I am trying to compare different where statements in my Kusto query using Kusto Explorer app, so I would like to export the result from the Query Summary tab.
In case has a query or a way to export this manually, it would be awesome.
Is it possible?

",76392258,"In Kusto.Explorer, if you switch to the QueryCompletionInformation tab in the result set, you can see & copy the QueryResourceConsumption payload

",
76390320,How do I include GitHub secrets in a python script?,"I am trying to move my bot written using discord.py/pycord to github for easier access, and I accidentally pushed my bot tokn to the hub, thankfully discord reset it for me and nothing hapened.
Now i want to use GitHub repository secrets to prevent this from happening, but i am having some trouble when trying to import the token into my code.
Here I've made a simple repo to experiment with this:
test.py:
import os
SECRET = os.environ['SECRET']
if SECRET == ""TrustNo1"":
    print(""No one can be trusted"")
print(SECRET)

workflow.yml:
name: build bot.py
on: [push]

jobs:
    build:
        runs-on: ubuntu-latest
        steps:
            - name: load content
              uses: actions/checkout@v2
              
            - name: load python
              uses: actions/setup-python@v4
              with:
                python-version: '3.10' # install the python version needed

            - name: start bot
              env:
                TOKEN: ${{ secrets.SECRET }}
              run: python test.py

The following error occurs at the ""start bot"" step in the workflow:
Traceback (most recent call last):
  File ""/home/runner/work/test-repo/test-repo/test.py"", line 2, in <module>
    SECRET = os.environ['SECRET']
  File ""/opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/os.py"", line 680, in __getitem__
    raise KeyError(key) from None
KeyError: 'SECRET'
Error: Process completed with exit code 1.

If i try to echo the SECRET in the workflow.yml i get ***, so it has has acces to the token, but when it imports to python it all breaks down.
I'm still quite new to git and GitHub, so please don't use advanced terms.
",76390714,"            - name: start bot
              env:
                TOKEN: ${{ secrets.SECRET }}

In the GitHub Action you named the variable secrets.SECRET but in environment variables you named it TOKEN. Either change the name of the environment variable to SECRET:
            - name: start bot
              env:
                SECRET: ${{ secrets.SECRET }}

or change your code:
    SECRET = os.environ['TOKEN']

",
76381802,Pyscript color input - triggers to soon,"I'm using this html to have the user choose a color:
<input type=""color"" id=""main-color"" name=""head"" value=""#15347d"""">
Then I'm using a button to call my python function
button py-click=""generate_color_schemes(False)"" id=""button-submit"">Update!</button>
Works well. (generate_color_schemes() examines the Element(""main-color"").value to get the hex color.)
But I'd like to combine the two, so clicking on the input - opens a picker and fires the python function as the user clicks inside the color picker as well as when they leave.
But (as expected) adding py-click fires my function when the input is first clicked and not when the color chooser popup closes or as the user clicks inside the color picker.
I think I want pyclick to be triggering on the oninput event as well as the onchange event.
Is there a way to combine the input type='color' with py-click to get this behaviour ?
",76390352,"As you say, you've got the right syntax but the wrong event(s). Rather than listening for the click event, you can listen to the input event (which is dispatched every time the value of an input changes) or the change event (which is dispatched, in this case, when the color picker closes). To do this in PyScript (or to listen to any event), the HTML attribute is py-[event] where [event] is the type of the event. In this case, you'll use the py-input and py-change attributes.
Here's a working example in the current release of PyScript (2023.03.1), which I mention in case the events API changes in the future. The MDN docs have a longer example in JavaScript.
<!-- Written for PyScript 2023.03.1 -->
<input type=""color"" id=""main-color"" py-input=""do_something_with_color()"" py-change=""do_something_else()"">

<py-script>
    import js
    def do_something_with_color():
        elem = js.document.getElementById(""main-color"")
        value = elem.value
        print(f'Chosen color: {value}') # do something here

    def do_something_else():
        elem = js.document.getElementById(""main-color"")
        value = elem.value
        print(f'The final color chosen was: {value}') # do something here
</py-script>

",
76389003,How to synchronize a draw call with a dispatch call as late as possible?,"I have a compute shader which updates a storage image which is later sampled by a fragment shader within a render pass.
From khronos vulkan synchronization examples I know I can insert a pipeline barrier before the render pass to make sure the fragment shader samples the image without hazards. Note the example is modified slightly to include more draw calls.
vkCmdDispatch(...); // update the image

VkImageMemoryBarrier2KHR imageMemoryBarrier = {
  ...
  .srcStageMask = VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT_KHR,
  .srcAccessMask = VK_ACCESS_2_SHADER_WRITE_BIT_KHR,
  .dstStageMask = VK_PIPELINE_STAGE_2_FRAGMENT_SHADER_BIT_KHR,
  .dstAccessMask = VK_ACCESS_2_SHADER_READ_BIT_KHR,
  .oldLayout = VK_IMAGE_LAYOUT_GENERAL,
  .newLayout = VK_IMAGE_LAYOUT_READ_ONLY_OPTIMAL
  /* .image and .subresourceRange should identify image subresource accessed */};

VkDependencyInfoKHR dependencyInfo = {
    ...
    1,                      // imageMemoryBarrierCount
    &imageMemoryBarrier,    // pImageMemoryBarriers
    ...
}

vkCmdPipelineBarrier2KHR(commandBuffer, &dependencyInfo);

... // Render pass setup etc.

vkCmdDraw(...); // does not sample the image
vkCmdDraw(...); // does not sample the image
vkCmdDraw(...); // does not sample the image

...

vkCmdDraw(...); // sample the image written by the compute shader, synchronized.

In the example, I have a bunch of draw calls within the same render pass that do not need the synchronization with the compute shader. They merely render a static geometry / textures which do not update dynamically. Yet in this configuration they must wait for the compute shader.
Ideally, I would like the independent draw calls between the vkCmdDispatch call and last vkCmdDraw to be able to run concurrently.
If I understand the spec correctly, I can't put the same pipeline barrier within the render pass.
Another alternative I considered is to use external subpass dependencies and record the draw call which samples the texture in a second subpass. But I don't know if this is a valid approach, and in any case it will be hard to maintain as this configuration is hard coded into the renderpass object.
So Is there a different synchronization approach that can achieve better concurrency?
",76390723,"You should put that in a subpass external dependency for the subpass you need to use it within. However, unless the rendering commands you want to overlap with the compute shader are in a prior subpass in the dependency graph, this probably won't give you any greater performance.
Note that not even dynamic rendering helps you here, as vkCmdBeginRendering starts a render pass instance. This means that you still can't have pipeline barriers or events within them.
Essentially, collective rendering operations (either the render pass as a whole or subpasses within it) defines an inflexible synchronization boundary between themselves as a group and the outside world. You can put synchronization around them, but not within them.
That being said, since rendering and compute operations are both eating up the same resources (shader stages), you probably weren't going to get too much overlap anyway.
",
76392154,Find BFS best path array,"Description:
I have a problem very similar to https://leetcode.com/problems/jump-game-ii/description/.
Apart from discovering the minimum steps to reach the end, I need to retrieve the winning path array.
The winning path that is needed for my problem must satisfy the rule that given path A like [0,2,4,7] there are not any other paths with the same price (steps to the end, 3 in this case) whose nodes starting from the path's end has lower indexes.
The single step from i to i+1 has not price equal to 1 but is 0 <= x <= 2^32-1.
An example:

[0,2,4,5,7] is the winner
[0,1,4,6,7] is not the winner because 6 > 5 at index 3

This has to work also in reverse:

[7,6,3,1,0] is the winner
[7,5,4,2,0] is not the winner because 2 > 1 at index 3

Note that this rule is applied from right to left.
Clearly, I can produce all the paths with BFS and hence compare them but is there a more efficient way?
",76392264,"
The winning path that is needed for my problem must satisfy the rule that given path A like [0,2,4,7] there are not any other paths with the same price (steps to the end, 3 in this case) whose nodes starting from the path's end has lower indexes.


I can produce all the paths with BFS and hence compare them but is there a more efficient way?

Yes.
Search backwards, from end to start, in BFS fashion.  When you enqueue each node's unvisited successors, do it in increasing order by index. The first path you discover will then be the reverse of the path you're looking for.
",
76390601,spring boot application not working bean error,"I am new to Spring Boot and I'm getting the following error when writing a login validationAPI:
Field userservice in com.Productwebsite.controller.UserController required a bean of type 'com.Productwebsite.service.UserService' that could not be found.
The injection point has the following annotations:
- @org.springframework.beans.factory.annotation.Autowired(required=true)
Controller class:
package com.Productwebsite.controller;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RestController;

import com.Productwebsite.service.UserService;

@RestController
public class UserController {
    
    @Autowired
    private UserService userservice;
    
    @GetMapping(""user/{username}/{password}"")
    public int UserLogin(@PathVariable(""username"")String username1, @PathVariable(""password"")String password1) {
        int flag = userservice.loginValidation(username1, password1);
        if (flag == 0) {
            return 0;
        }
        return flag;
    }
}

Modelclass:
package com.Productwebsite.model;

public class users {
    String username;
    String password;

    public String getUsername() {
        return username;
    }

    public void setUsername(String username) {
        this.username = username;
    }

    public String getPassword() {
        return password;
    }

    public void setPassword(String password) {
        this.password = password;
    }
    
    @Override
    public String toString() {
        return ""users [username="" + username + "", password="" + password + ""]"";
    }

    public users(String username, String password) {
        super();
        this.username = username;
        this.password = password;
    }

    public users() {
        super();
        // TODO Auto-generated constructor stub
    }
}

Service interfface:
package com.Productwebsite.service;

import org.springframework.stereotype.Repository;

@Repository
public interface UserService {
    
    public int loginValidation(String username,String password);
}

Serviceimplementation class:
package com.Productwebsite.serviceImp;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;

import org.springframework.stereotype.Service;

import com.Productwebsite.Dbutil.dbutil;
import com.Productwebsite.service.UserService;

@Service
public class ServiceImp implements UserService {
    Connection connection;
    int flag = 0;

    public ServiceImp() throws SQLException {
        connection = dbutil.getConnection();
    }

    @Override
    public int loginValidation(String username, String password) {
        try {
            PreparedStatement statement 
 = connection.prepareStatement(""SELECT * FROM users WHERE username='""+username+""'""); 
            ResultSet rs=statement.executeQuery();
            
            while(rs.next()) {
                if (rs.getString(1).equals(username)&&rs.getString(2).equals(password)) {
                    flag=1;
                }
                else {
                    System.out.println(""Invalid username/password"");
                }
            }
        } catch (SQLException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        return flag;
    }
}

",76390737,"Add @ComponentScan to com.Productwebsite and restart the app.
@ComponentScan(basePackages = ""com.Productwebsite"")
@SpringBootApplication
public class YourMainClass{
  ...
}


Try the following, if the above changes do not work:
Change annotation by adding the bean name:
@Repository(""userservice"")

",
76388789,How to hide/show specific Cell in Grid,"Im using renderer function as below and want to hide/show specific checkcolumn-cells depending on variable record.data.hidden in my gridview.
{
   xtype: 'checkcolumn',
   renderer: function(value, metaData, record, rowIndex, colIndex, store, view) {
       /*
       if (record.data.hidden === true) {
           // checkbox hidden
       } else {
           // checkbox shown
       }
       */
    },
    itemId: 'mycheck',
    bind: {
        text: '{warranty}'
    }
}

How do i do this?
",76390508,"You can use the metaData passed to the renderer function to apply styling to the cell element, see the documentation here.
One easy way is to set the display CSS property depending on your criteria. This will be applied to the HTML <td> element created by Ext JS for the cell.
Try this:
{
   xtype: 'checkcolumn',
   renderer: function(value, metaData, record, rowIndex, colIndex, store, view) {
       if (record.data.hidden === true) {
           metaData.tdStyle = 'display: none;';
       } else {
           metaData.tdStyle = 'display: table-cell;';
       }
       return value;
    },
    itemId: 'mycheck',
    bind: {
        text: '{warranty}'
    }
}

(I am not sure about the right syntax, maybe you don't need the semicolon after none and table-cell, and maybe you have to use quotations somewhere. Try it.)
But you create a CSS style and use tdCls as well, and perhaps also tdAttr but I am not sure about the latest.
",
76392232,Generic Component in React - Type is not assignable to type 'ReactNode'. But no error with JSON.stringify(),"I'm trying to create a generic component in React TypeScript. Here is the code
interface Props<T> {
    name: T;
}

function CheckChild<T>(props: Props<T>) {
    return <div>My name is {props.name}</div>;
}

export default function Check() {
    const name = ""Alex"";

    return (
        <div>
            <CheckChild name={name} />
        </div>
    );
}

VS code gives me a type error about props.name:
Type 'T' is not assignable to type 'ReactNode'.
  Type 'T' is not assignable to type 'ReactPortal'.ts(2322)
Check2.tsx(7, 21): This type parameter might need an `extends React.ReactPortal` constraint.
Check2.tsx(7, 21): This type parameter might need an `extends React.ReactNode` constraint.

But if I wrap props.name in JSON.stringify() - type error disappears.
return <div>My name is {JSON.stringify(props.name)}</div>;

Why does this happen?
This topic does not answer my question. I am interested in ""Why JSON.stringify removes error""?
",76392265,"The reason why the type error disappears when you wrap props.name in JSON.stringify() is because JSON.stringify() converts the value to a string representation. In TypeScript, the type ReactNode represents the type of a valid React node, which can be a string, a number, a React component, an array, etc.
When you directly use props.name without JSON.stringify(), the TypeScript compiler assumes that the type of props.name should be compatible with ReactNode. However, in your code, you have a generic type T for the name property in the Props interface. The compiler doesn't have enough information to determine the actual type of T, so it gives a type error.
By using JSON.stringify(props.name), you explicitly convert the value to a string, which satisfies the ReactNode type requirement. This makes the type error disappear because JSON.stringify() always returns a string, and a string is a valid type for ReactNode.
If you want to avoid using JSON.stringify(), you can provide a more specific type constraint for the T generic in the Props interface. For example, if you know that props.name will always be a string, you can update the Props interface like this:
interface Props<T extends string> {
   name: T;
}

",
76388077,Fetch data Firestore Flutter,"I want to get the data where I marked. But since I made the side collection a custom name (as you can see in the picture TbarRow and latpuldown), I cannot fetch the last data I want to reach. Actually, when I write the name of the collection by hand, I reach it, but I don't know how to fetch that name without writing it myself.

I created an array with the name of gunharaketisimleri of the documentation (4 documents Karın bacak itiş çekiş) and tried to fetch the data from there.

and try it like this:
Future<void> fetchData() async {
  try {
    var snapshot = await FirebaseFirestore.instance
        .collection(""günler"")
        .doc(currentUserID + "" GUN"")
        .collection(""gün"")
        .doc(""itiş"")
        .get();
    snapshot.data()!.forEach((key, value) {
      List<dynamic> favorites1 = [];
      favorites1.add(value);
      print(favorites1);
    });
  } catch (e) {
    // Handle any errors
    print(e.toString());
  }
}

and output:

I/flutter (24013): [[TbarRow, latpuldown]]


I/flutter (24013): [itiş]


I/flutter (24013): [null]


I/flutter (24013):[gdc0mIEbwmdVFqZoLDC0gnIFCAy2]

I tried to reach it this way and got stuck here. Can you suggest another method or how should the continuation be from the final version?
",76390533,"To get the array from the document, would be something like:
var array = snapshot.get('gunharaketisimleri') as List;

Then you can loop over the list to get the individual values.
",
76389459,Stubbing Models contained within an Object,"I am having real issues stubbing one particular thing using sinon.  I have a simple function I am testing
const floatAPIModels = require(""models/float/floatAPIModels"");

const insertFloatData = async (databaseConnection, endpoint, endpointData) => {
  try {
    const floatModel = floatAPIModels(databaseConnection);
    await databaseConnection.sync();

    if (endpoint === ""people"") {
      endpointData.forEach(async (record) => {
        await floatModel.Person.upsert(record);
      });
    }

    return true;
  } catch (error) {
    console.log(""Unable to insert data into the database:"", error);
    return error;
  }
};

The problem is with floatAPIModels being an Object that returns things.  My implementation is this
const { DataTypes } = require(""sequelize"");

const floatAPIModels = (sequelize) => {
  const Person = sequelize.define(
    ""Person"",
    {
      people_id: { type: DataTypes.INTEGER, primaryKey: true },
      job_title: { type: DataTypes.STRING(200), allowNull: true },
      employee_type: { type: DataTypes.BOOLEAN, allowNull: true },
      active: { type: DataTypes.BOOLEAN, allowNull: true },
      start_date: { type: DataTypes.DATE, allowNull: true },
      end_date: { type: DataTypes.DATE, allowNull: true },
      department_name: { type: DataTypes.STRING, allowNull: true },
      default_hourly_rate: { type: DataTypes.FLOAT, allowNull: true },
      created: { type: DataTypes.DATE, allowNull: true },
      modified: { type: DataTypes.DATE, allowNull: true },
    },
    {
      timestamps: true,
      tableName: ""Person"",
    }
  );

  return {
    Person,
  };
};

module.exports = floatAPIModels;

I have removed some things to cut down on code.  At the moment I am doing something like this
const { expect } = require(""chai"");
const sinon = require(""sinon"");
const floatAPIModels = require(""src/models/float/floatAPIModels"");
const floatService = require(""src/services/float/floatService"");

describe(""insertFloatData"", () => {
  let databaseConnection;
  let floatModelMock;

  beforeEach(() => {
    databaseConnection = {};

    floatModelMock = {
      Person: { upsert: sinon.stub().resolves() },
    };

    sinon.stub(floatAPIModels, ""Person"").returns(floatModelMock.Person);
  });

  afterEach(() => {
    sinon.restore();
  });

  it(""should insert endpointData into the 'people' endpoint"", async () => {
    const endpoint = ""people"";
    const endpointData = [{ record: ""data"" }];

    await floatService.insertFloatData(databaseConnection, endpoint, endpointData);

    expect(floatModelMock.Person.upsert.calledOnce).to.be.true;
    expect(floatModelMock.Person.upsert.firstCall.args[0]).to.deep.equal(endpointData[0]);
  });
});

With the above, I get
TypeError: Cannot stub non-existent property Person

But I have tried default, and a lot of other ways, but none of them seems to work.
How can I properly stub this and get the unit test working?
Thanks
",76390741,"floatAPIModels is a function that returns { Person } object. There is no Person property on this function. That's why you got the error.
In order to stub the floatAPIModels function, I will use the proxyquire module to do this.
E.g.
model.js:
const { DataTypes } = require(""sequelize"");

const floatAPIModels = (sequelize) => {
  const Person = sequelize.define(
    ""Person"",
    {
      people_id: { type: DataTypes.INTEGER, primaryKey: true },
      // rest fields, don't matter for this test
      // ...
    },
    { timestamps: true, tableName: ""Person"", }
  );

  return {
    Person,
  };
};

module.exports = floatAPIModels;

service.js:
const floatAPIModels = require(""./model"");

const insertFloatData = async (databaseConnection, endpoint, endpointData) => {
  try {
    const floatModel = floatAPIModels(databaseConnection);
    await databaseConnection.sync();
    if (endpoint === ""people"") {
      endpointData.forEach(async (record) => {
        await floatModel.Person.upsert(record);
      });
    }
    return true;
  } catch (error) {
    console.log(""Unable to insert data into the database:"", error);
    return error;
  }
};

module.exports = { insertFloatData }

service.test.js:
const sinon = require(""sinon"");
const proxyquire = require('proxyquire');

describe(""insertFloatData"", () => {
  let databaseConnection;

  beforeEach(() => {
    databaseConnection = {
      define: sinon.stub(),
      sync: sinon.stub()
    };
  });

  afterEach(() => {
    sinon.restore();
  });

  it(""should insert endpointData into the 'people' endpoint"", async () => {
    const endpoint = ""people"";
    const endpointData = [{ record: ""data"" }];
    const PersonStub = {
      upsert: sinon.stub()
    }
    const floatAPIModelsStub = sinon.stub().returns({ Person: PersonStub })
    const floatService = proxyquire('./service', {
      './model': floatAPIModelsStub
    })

    await floatService.insertFloatData(databaseConnection, endpoint, endpointData);

    sinon.assert.calledOnce(PersonStub.upsert)
    sinon.assert.match(PersonStub.upsert.firstCall.args[0], endpointData[0])
  });
});

Test result:
  insertFloatData
    ✓ should insert endpointData into the 'people' endpoint (4168ms)


  1 passing (4s)

------------|---------|----------|---------|---------|-------------------
File        | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s 
------------|---------|----------|---------|---------|-------------------
All files   |   76.47 |       50 |   66.67 |   76.47 |                   
 model.js   |      60 |      100 |       0 |      60 | 4-24              
 service.js |   83.33 |       50 |     100 |   83.33 | 14-15             
------------|---------|----------|---------|---------|-------------------

",
76391788,How do I resolve the time limit?,"Write a program that determines for two nodes of a tree whether the first one is a parent of another.
Input data
The first line contains the number of vertices n (1 ≤ n ≤ 10^5) in a tree. The second line contains n numbers, the i-th one gives the vertex number of direct ancestor for the vertex i. If this value is zero, then the vertex is a root of a tree.
The third line contains the number of queries m (1 ≤ m ≤ 10^5). Each of the next m lines contains two different numbers a and b (1 ≤ a, b ≤ n).
Output data
For each of the m queries print on a separate line number 1, if the vertex a is one of the parents of a vertex b, and 0 otherwise.

Input example #1
6
0 1 1 2 3 3
5
4 1
1 4
3 6
2 6
6 5
Output example #1
0
1
1
0
0
Problem
How can I fix the time limit?
I also tried with <stdio.h>(scanf();printf()).
But problem gives time limit.
It gives time limit for input(many input things)
My solution:
#include<iostream>
#include<vector>
#include<algorithm>
using namespace std;
vector<vector<int>> g;
vector<int> time_in,time_out;
int Time=0;
void dfs(int node,int parent){
    time_in[node]=++Time;
    for(int &to:g[node]){
        if(to!=parent){
                dfs(to,node);
        }
    }
    time_out[node]=++Time;// or  time_out[node]=Time;
}
bool isAnchestor(int anch,int node){
    return time_in[anch]<=time_in[node] and time_out[anch]>=time_out[node];
}
void readInt(int &n){
    char ch;
    int sign = 1;
    while(ch = getchar(), isspace(ch)){//getchar()-->getchar_unlocked()

    };
    n = 0;
    if(ch == '-')
        sign = -1;
    else n = ch - '0';
    while(ch = getchar(), isdigit(ch))
        n = (n << 3) + (n << 1) + ch - '0';
    n *= sign;
}
int main(){
    ios_base::sync_with_stdio(0);
    //cin.tie(0);
    cout.tie(0);
    int n,start;
    readInt(n);
    time_in.resize(n+1);
    time_out.resize(n+1);
    g.resize(n+1);
    vector<int> anchestors(n+2);
    for(int i=1; i<=n; ++i){// I am thinking: the anchestors is sorted.
            readInt(anchestors[i]);
            if(anchestors[i] != 0) {


        g[anchestors[i]].push_back(i);
        g[i].push_back(anchestors[i]);
        g[anchestors[i]].push_back(i);
    }else{
    start=i;
    }
    }
    dfs(start,1);
    int q,u,v;
    readInt(q);
    while(q--){
        readInt(u),readInt(v);
        cout<<isAnchestor(u,v)<<'\n';// The anchestor of v is u. //(u,v)-->(a,b)
    }
    return 0;
}


",76392266,"Your approach using a depth first search is inefficient. This in general will look at more than just the path from b to a though the ""parent pointers"" or to the root, if b is not a descendant of a.
Instead start the search at b and go to the parent node repeatedly until you find the root or a. This will only consider the path from b to the root node. You don't need to consider any alternative paths, but you always know the edge to traverse.
#include <iostream>
#include <vector>

int main()
{
    constexpr int RootParent = -1;

    int n;
    std::cin >> n;
    std::vector<int> parents;
    parents.reserve(n);
    for (int i = 0; i != n; ++i)
    {
        int v;
        std::cin >> v;
        parents.push_back(v - 1); // note: storing 0-based indices of parents here (root's parent becomes -1)
    }

    int queryCount;
    for(std::cin >> queryCount; queryCount > 0; --queryCount)
    {
        int searchedAncestor;
        int descendant;
        std::cin >> searchedAncestor >> descendant;
        
        // inputs are still 1-based, so we need to decrement here
        --searchedAncestor;
        --descendant;

        // repeatedly go to the parent node until we reach the root or the ancestor to attempt to find
        while ((searchedAncestor != descendant) && (descendant != RootParent))
        {
            descendant = parents[descendant];
        }
        std::cout << (searchedAncestor == descendant) << '\n';
    }
}

Demo on godbolt
",
76387967,How to print as PDF all pages in HTML?,"I have this html/JS code that generates N tables based on user input. I'm trying to separate the content in pages of ""A4"" size and with no more that 6 tables within each page in order to give the option to print as PDF.
I'm using the built-in JS function window.print(). The issue is that when the browser opens up the printing preview window, only detects 2 pages and only first page content appears correct, the rest appears empty.
You can try for example to generate 24 tables (that would be 4 pages) and when click on print, only 2 pages are detected.
Basically I'm using this to convert as PDF the content generated by the other 2 JS codes.
<button onclick=""printPDF()"">Print PDF</button>

    function printPDF() {
        window.print();
    }

How to fix this, in order to print all pages? Thanks
",76390557,"One problem is the hidden page overspill
so simply remove that such that all contents are eligible for printing. Note you may want to move hidden to just the button at print time.
And the 2nd is you may need a page-break-after: always;
when the table container blocks reach 6 to a page limit, so try add that in. Note even with all 6 it will not add a blank page unless the table is too close to the edge so it overspills, hence the 7th (13th ... etc.) triggers the page break.
Later Edit I seems that an auto page-break is added in the fiddle so may not be needed? However I also had issues with FireFox assessment of page height so for my MS printing had to reduce between 3-6mm to not trigger an extra page!

https://jsfiddle.net/61qshzvc/
",
76390332,Fixed Output sequence for selectOutput5,"Problem statement:
I want to experiment with different sequences for the routing through the selectOutput5 block.
Example:
Random (all 5 Outputs can be choosen by probability 0.2) (thats the easy part that i solved)
Fixed: (Output1: 6 agents, after those six agents, Output2: six agents and so on...
Do you have an idea on how i can do this based on expressions or conditions?
Thank you very much and have a great weekend
",76390748,"Set the ""Use:"" property in your SelectOutput5 block to ""Exit number"". Then you can define which exit to use.
For your example, I would create an int type variable named currExit and set the initial value to 1. Set the value of ""Exit number [1..5]:"" in your SelectOutput5 block to currExit and write this code to the ""On enter:"" property:
if (self.in.count()%6==0) {
    currExit++;
    if (currExit>5) currExit=1;
}

This switches to the next exit for every 6th agent (self.in.count() returns the number of agents that have entered the block so far).
",
76388627,Exception when executing JDBC query - Illegal instant due to time zone offset transition with Logstash,"I have the following JDBC input:
input {
  jdbc {
    id => ""mypipeline""
    jdbc_driver_class => ""com.mysql.jdbc.Driver""
    jdbc_driver_library => ""/usr/share/logstash/drivers/mysql-connector-j-8.0.32.jar""
    jdbc_connection_string => ""my-connection-string""
    jdbc_password => ""my-user""
    jdbc_user => ""my-pass""
    jdbc_fetch_size => 5000
    schedule => ""*/5 * * * *""
    statement => ""SELECT l.id, l.name, l.log_date FROM logs l WHERE l.created_at >= :sql_last_value""
  }
}

The log_date column is as follows:
log_date DATE NULL DEFAULT NULL,

Warning; created_at and log_date are not the same column. The query that's being executed is as follows:
SELECT l.id, l.name, l.log_date FROM logs l WHERE l.created_at >= '1970-01-01 02:00:00'

which is wrong, since Istanbul does not go into DST, should be '1970-01-01 03:00:00' For values such as 1949-04-10 for log_date, I am receiving this error:
[2023-06-02T11:50:30,470][WARN ][logstash.inputs.jdbc     ][main][mypipeline] Exception when executing JDBC query {:exception=>Sequel::DatabaseError, :message=>""Java::OrgJodaTime::IllegalInstantException: Illegal instant due to time zone offset transition (daylight savings time 'gap'): 1949-04-10T00:00:00.000 (Europe/Istanbul)"", :cause=>""#<Java::OrgJodaTime::IllegalInstantException: Illegal instant due to time zone offset transition (daylight savings time 'gap'): 1949-04-10T00:00:00.000 (Europe/Istanbul)>""}

How do I resolve this?
",76390717,"""Illegal instant due to time zone offset transition"" - Istanbul does not currently change between daylight saving time and standard time, but it has done in the past.
It is currently observing what is effectively DST year-round - and has done so since March 27th 2016, when the clocks moved forward by 1 hour from a UCT timezone offset of +2 hours to +3 hours.

More specifically regarding the ""Illegal instant"" error: In 1949 on 10th April at midnight, the clocks moved forward by 1 hour - so the local time of 1949-04-10  00:00 never actually happened. That is why it is a ""gap"" value, as noted in the error message.
This time zone data is captured in the IANA Time Zone Database (TZDB), which I assume is what your application uses, behind the scenes (e.g. if you are using Joda-Time).
You can use an online tool such as this one (and probaby others):

WARNING - I cannot vouch for the accuracy of the data on this web site, but it does match the TZDB rule for this specific example, which I extracted as follows using Java (which uses TZDB data):
 - on 1949-04-10 at 00:00
 - the clocks moved forward by 1 hr (daylight saving)
 - from TZ offset +02:00 to offset +03:00


Regarding your comment:

""should be '1970-01-01 03:00:00'""

For 1970, the TZDB indicates that there were no adjustments made. So, the effective offset from UTC was the one previously made in 1964:
 - on 1964-10-01 at 00:00
 - the clocks moved back by 1 hr
 - from TZ offset +03:00 to offset +02:00

And that +02:00 is what you are (correctly) seeing in your data for that 1970 datetime.

If you want to avoid using a datetime which falls into one of these gaps (caused by the clocks moving forward), then you can do that programmatically - for example:

Assuming Java (since you mention Joda-Time): java.time discovering I'm in the daylight savings ""gap""

Other mainstream languages should have similar capabilities.

Also, since you mentioned Joda, maybe you can consider using Java's java.time classes now (if you have a suitable version of Java), instead of using Joda-Time:

Note that from Java SE 8 onwards, users are asked to migrate to java.time (JSR-310) - a core part of the JDK which replaces this project.


Specific solution to the case with Logstash
Adding jdbc_default_timezone => ""GMT"" to Logstash configuration and altering the timezone of the host machine will make Logstash to query the database without getting this error.
",
76389709,Clean column with different time records in powerbi,"I have a Power BI table with a column called ""hours"". It can have different time records with  following different formats:
PT5H15M{YearMonthDayTime}, PT0S{YearMonthDayTime}, PT5H15M, PT10H, etc.
How can I clean them up so that the hours are represented as numbers, for example, PT5H15M{YearMonthDayTime} would be 5,25 and PT3H30M would be 3,5.
Can't find any easy way to filter the column since some rows have {YearMonthDayTime} ending and others doesn't. I don't want to transform every record manually.
Thanks already!
",76390777,"You can achieve this in power query
Steps followed in Power Query

Extract Text between delimiters PT and H to populate Hours column. Change type to decimal
Extract Text between delimiters H and M to populate Minutes column. Change type to decimal
Replace Errors with zero. Replace all null values with zero.
Devide Minutes column by 60 to convert into hours.
Add Hours column in step 1) to hours column in step 4)
Convert result column type to Text
In result column: Replace . with , using Transform -> Replace Values

M code:
let
    Source = Excel.Workbook(File.Contents(""C:\Ashok\Power BI\Stack Overflow\Data_02_jun2_2023.xlsx""), null, true),
    Data_Sheet = Source{[Item=""Data"",Kind=""Sheet""]}[Data],
    #""Changed Type"" = Table.TransformColumnTypes(Data_Sheet,{{""Column1"", type text}}),
    #""Promoted Headers"" = Table.PromoteHeaders(#""Changed Type"", [PromoteAllScalars=true]),
    #""Changed Type1"" = Table.TransformColumnTypes(#""Promoted Headers"",{{""Hours"", type text}}),
    #""Inserted Text Between Delimiters"" = Table.AddColumn(#""Changed Type1"", ""Text Between Delimiters"", each Text.BetweenDelimiters([Hours], ""PT"", ""H""), type text),
    #""Inserted Text Between Delimiters1"" = Table.AddColumn(#""Inserted Text Between Delimiters"", ""Text Between Delimiters.1"", each Text.BetweenDelimiters([Hours], ""H"", ""M""), type text),
    #""Changed Type2"" = Table.TransformColumnTypes(#""Inserted Text Between Delimiters1"",{{""Text Between Delimiters.1"", type number}}),
    #""Inserted Division"" = Table.AddColumn(#""Changed Type2"", ""Division"", each [Text Between Delimiters.1] / 60, type number),
    #""Changed Type3"" = Table.TransformColumnTypes(#""Inserted Division"",{{""Text Between Delimiters"", type number}}),
    #""Replaced Errors"" = Table.ReplaceErrorValues(#""Changed Type3"", {{""Text Between Delimiters"", 0}}),
    #""xyz"" = Table.TransformColumns(#""Replaced Errors"", {{""Text Between Delimiters"", each if _ is null then 0 else _}, 
                                                        {""Text Between Delimiters.1"", each if _ is null then 0 else _}, {""Division"", each if _ is null then 0 else _}}),
    #""Changed Type4"" = Table.TransformColumnTypes(xyz,{{""Text Between Delimiters"", type number}, {""Division"", type number}}),
    #""Inserted Addition"" = Table.AddColumn(#""Changed Type4"", ""Addition"", each [Text Between Delimiters] + [Division], type number),
    #""Changed Type5"" = Table.TransformColumnTypes(#""Inserted Addition"",{{""Addition"", type text}}),
    #""Replaced Value"" = Table.ReplaceValue(#""Changed Type5"",""."","","",Replacer.ReplaceText,{""Addition""})
in
    #""Replaced Value""


",
76391920,"Write a Python program to remove duplicates from a list of integers, preserving order","So this was the question and I implemented it using python language but there is error in code and I am not able to fix it so please help. I am attaching code also.
def delete23(a):
    for i in range(0, len(a) - 1):
        for j in range(0, len(a) - 1):
            if a[i] == a[j] and i != j:
                a.pop(j)
    print(a)


a = [1, 2, 3, 4, 4, 1, 7]
print(len(a))
delete23(a)


",76392279,"To filter duplicates while preserving the order, a set of seen values is used quite often. The logic is very simple: if a value is not present yet in seen, the value is encountered for the first time.
lst = [1, 2, 3, 4, 4, 1, 7]

seen = set()
lst2 = []
for x in lst:
    if x not in seen:
        seen.add(x)
        lst2.append(x)
    
print(lst2)

With a little trick, this can be shortened:
seen = set()
lst2 = [(seen.add(x) or x) for x in lst if x not in seen]

print(lst2)

To explain the expression inside (...): set.add returns None and (None or VALUE) is evaluated to VALUE

Warning: ALL solutions using a set or a dict require that all values are hashable.
",
76388344,Apply a weighting to a 4 parameter regression curvefit,"The below code generates a plot and 4PL curve fit, but the fit is poor at lower values. This error can usually be addressed by ading a 1/y^2 weighting, but I dont know how to do it in this instance. Adding sigma=1/Y_data**2 to the fit just makes it worse.
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

def fourPL(x, A, B, C, D):
    return ((A-D) / (1.0 + np.power(x / C, B))) + D

X_data = np.array([700,200,44,11,3,0.7,0.2,0])
Y_data = np.array([600000,140000,30000,8000,2100,800,500,60])


popt, pcov = curve_fit(fourPL, X_data, Y_data)

fig, ax = plt.subplots()    
ax.scatter(X_data, Y_data, label='Data')
X_curve = np.linspace(min(X_data[np.nonzero(X_data)]), max(X_data), 5000)
Y_curve = fourPL(X_curve, *popt)
ax.plot(X_curve, Y_curve)

ax.set_xscale('log')
ax.set_yscale('log')

plt.show()


",76390724,"Don't add inverse square weights; fit in the log domain. Always add bounds. And in this case, curve_fit doesn't do a very good job; consider instead minimize.
import numpy as np
from scipy.optimize import curve_fit, minimize
import matplotlib.pyplot as plt


def fourPL(x: np.ndarray, a: float, b: float, c: float, d: float) -> np.ndarray:
    return (a - d)/(1 + (x / c)**b) + d


def estimated(x: np.ndarray, a: float, b: float, c: float, d: float) -> np.ndarray:
    return np.log(fourPL(x, a, b, c, d))


def sqerror(abcd: np.ndarray) -> float:
    y = np.log(fourPL(x_data, *abcd)) - np.log(y_data)
    return y.dot(y)


x_data = np.array([700, 200, 44, 11, 3, 0.7, 0.2, 0])
y_data = np.array([600000, 140000, 30000, 8000, 2100, 800, 500, 60])
guess = (500, 1.05, 1e6, 1e9)
bounds = np.array((
    (1, 0.1, 1, 0),
    (np.inf, 10, np.inf, np.inf),
))

popt, _ = curve_fit(
    f=estimated, xdata=x_data, ydata=np.log(y_data), p0=guess,
    bounds=bounds,
)
print('popt:', popt)
result = minimize(
    fun=sqerror, x0=guess, bounds=bounds.T, tol=1e-9,
)
assert result.success
print('minimize x:', result.x)

x_curve = 10**np.linspace(-1, 3, 1000)

fig, ax = plt.subplots()
ax.scatter(x_data, y_data, label='Data')
ax.plot(x_curve, fourPL(x_curve, *popt), label='curve_fit')
ax.plot(x_curve, fourPL(x_curve, *result.x), label='minimize')
ax.plot(x_curve, fourPL(x_curve, *guess), label='guess')
ax.set_xscale('log')
ax.set_yscale('log')
ax.legend()
plt.show()


",
76392252,Multithreading with std::future in C++: Accessing shared data,"I am currently developing a multi-threaded application in C++ where different threads are expected to process data from a shared data structure. I'm aware that the standard library provides std::future and std::async to easily handle asynchronous operations, and I'm trying to use these in my application.
Here's a simplified sketch of my code:
#include <vector>
#include <future>

std::vector<int> shared_data;

// Some function to be executed asynchronously
void process_data(size_t start, size_t end) {
    for (size_t i = start; i < end; ++i) {
        // Do something with shared_data[i]
    }
}

int main() {
    std::future<void> fut1 = std::async(std::launch::async, process_data, 0, 10);
    std::future<void> fut2 = std::async(std::launch::async, process_data, 10, 20);

    // Other operations...

    return 0;
}

I have the following questions regarding this code:
Since shared_data is being accessed by multiple threads, do I need to protect it with a std::mutex or other synchronization primitives?
Is there a way to pass std::future objects to other functions or store them in a data structure, and what would be the potential implications of doing so?
How can I handle exceptions thrown by the process_data function and propagated through the std::future objects?
Any guidance or best practices related to the usage of std::future in multithreaded scenarios would be greatly appreciated.
In order to make the shared data access thread-safe, I attempted to introduce an std::mutex and lock it using std::lock_guard in the process_data function like so:
std::mutex mtx;

void process_data(size_t start, size_t end) {
    std::lock_guard<std::mutex> lock(mtx);
    for (size_t i = start; i < end; ++i) {
        // Do something with shared_data[i]
    }
}

I also attempted to store std::future objects in a std::vector for later use, and tried to handle exceptions using a try/catch block around the get() function of std::future.
I was expecting that locking the std::mutex would ensure that only one thread can access the shared data at a time, preventing race conditions. I also expected that I would be able to easily store the std::future objects in a vector and handle exceptions from the asynchronous tasks.
However, I'm unsure if these methods are the most efficient or even correct, given the lack of detailed examples or guidelines on these topics in the documentation and tutorials I've found. I'm particularly interested in understanding the correct way to use std::future and std::async in more complex scenarios, and how to handle exceptions properly in this context.
",76392286,"If the data is readonly (and its not too much, just copy it).
Otherwise make a shared_ptr to your data (and using a lambda expression) you can capture the shared_ptr by value (! not reference!!!) This will extend the lifetime of the data to the lifetime of the thread that uses it longest. So something like this :
std::shared_ptr<SharedData> data; auto future = std::async(std::launch::async( [data]{ process_data(data); };
If the data is read/write then add a mutex or some other synchronization mechanism to your data class and use getters/setters with lock to update/read the values in the data.
",
76390268,How can I correctly check the type of a Redis key in Lua scripting?,"Correct way to check redis key's type in lua script.
I'm trying to wrap my head around redis lua scripting and I can't find the correct way to check what type the key has.
Here is what I've tried:
127.0.0.1:6379> SET test_key test_value
OK
127.0.0.1:6379> GET test_key
""test_value""
127.0.0.1:6379> EVAL 'local type = redis.call(""TYPE"", KEYS[1]); return type' 1 test_key
string

So I see that the type = ""string"", but:
127.0.0.1:6379> EVAL 'local type = redis.call(""TYPE"", KEYS[1]); local res; if type == string then res = ""ok"" else res = ""not ok"" end return res' 1 test_key
""not ok""
127.0.0.1:6379> EVAL 'local type = redis.call(""TYPE"", KEYS[1]); local res; if type == string then res = ""ok"" else res = ""not ok"" end return res' 1 ""test_key""
""not ok""
127.0.0.1:6379> EVAL 'local type = redis.call(""TYPE"", KEYS[1]); local res; if type == ""string"" then res = ""ok"" else res = ""not ok"" end return res' 1 ""test_key""
""not ok""
127.0.0.1:6379> EVAL 'local type = redis.call(""TYPE"", KEYS[1]); local res; if type == ""string"" then res = ""ok"" else res = ""not ok"" end return res' 1 test_key
""not ok""

",76390820,"I've found the answer here: Using the TYPE command inside a Redis / Lua Script
Short answer is that in lua scripts redis.call(""TYPE"", key) returns not string but lua table with key ""ok"" which holds string value of the type.
So to check the type of the key you should compare like this:
if redis.call(""TYPE"", key)[""ok""] == ""string""

for example:
127.0.0.1:6379> EVAL 'local type = redis.call(""TYPE"", KEYS[1])[""ok""]; local res; if type == ""string"" then res = ""ok"" else res = ""not ok"" end return res' 1 test_key
""ok""

",
76392221,How can I prioritize dimensions in a Redis vector similarity search?,"I am currently using Redis as a vector database and was able to get a similarity search going with 3 dimensions (the dimensions being latitude, longitude, and timestamp). The similarity search is working but I would like to weigh certain dimensions differently when conducting the search. Namely, I would like the similarity search to prioritize the timestamp dimension when conducting the search.
How would I go about this? Redis does not seem to have any built-in feature that does this.
I turn each set of lat, long, and time coordinates into bytes that can be put into the vector database with the following code. Note that vector_dict stores all the sets of lat, long, and timestamp:
p = client.pipeline(transaction=False)
for index in data:
        # create hash key
        key = keys[index]

        # create hash values
        item_metadata = data[index] # copy all metadata
        item_key_vector = np.array(vector_dict[index]).astype(np.float32).tobytes() # convert vector to bytes
        p.hset(key, mapping=item_metadata) # add item to redis using hash key and metadata

I then conduct the similarity search using the HNSW index here:
def create_hnsw_index(redis_conn, vector_field_name, number_of_vectors, vector_dimensions=3, distance_metric='L2', M=100, EF=100):
    redis_conn.ft().create_index([
        VectorField(vector_field_name, ""HNSW"", {""TYPE"": ""FLOAT32"", ""DIM"": vector_dimensions, ""DISTANCE_METRIC"": distance_metric, ""INITIAL_CAP"": number_of_vectors, ""M"": M, ""EF_CONSTRUCTION"": EF})
    ])

I talked with others and they said it is a math problem that deals with vector normalization. I'm unsure how to get started with this though in code and would like some guidance.
",76392309,"You can re-weight the vector to make certain dimensions longer than others. You're using an L2 distance metric. That uses the standard Pythagorean theorem to calculate distance:
dist = sqrt((x1-x2)**2 + (y1-y2)**2 + (z1-z2)**2)

Imagine you multiplied every Y value, in both your query and your database, by 10. That would also multiply the difference between Y values by a factor of 10.
The new distance function would effectively be this:
dist = sqrt((x1-x2)**2 + (10*(y1-y2))**2 + (z1-z2)**2)

dist = sqrt((x1-x2)**2 + 100*(y1-y2)**2 + (z1-z2)**2)

...which makes the Y dimension matter 100 times more than the other dimensions.
So if you want the dimension in position 2 to matter more, you could do this:
item_key_vector = np.array(vector_dict[index])
item_key_vector[2] *= 10
item_key_vector_bytes = item_key_vector.astype(np.float32).tobytes()

The specific amount to multiply by depends on how much you want the timestamp to matter. Remember that you need to multiply your query vector by the same amount.
",
76388796,Why pybind11 can not recognize PyObject* as a Python object and how to fix it?,"I am trying to build a library using this C++ code:
#include <pybind11/pybind11.h>

namespace py = pybind11;

PyObject* func() {
    return Py_BuildValue(""iii"", 1, 2, 3);
}

PYBIND11_MODULE(example, m) {
    m.def(""func"", &func);
}

But when I tried to run this Python 3 code:
import example
print(example.func())

It gives me following error:
TypeError: Unregistered type : _object

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
TypeError: Unable to convert function return value to a Python type! The signature was
        () -> _object

Why it is happening and how to fix it?
",76390832,"So, first off, what are you trying to do with your function? Making a raw python API call (Py_BuildValue, https://docs.python.org/3/c-api/arg.html#c.Py_BuildValue) is strongly discouraged, that is why you are using PyBind11, to handle that for you.
What are you trying to do with this line?
Py_BuildValue(""iii"", 1, 2, 3)

It looks like you are trying to return a tuple of 3 ints. Perhaps something like this would work instead:
py::tuple func() {
    return py::make_tuple(1, 2, 3);
}

With that said, I think the error is from Python not understanding what a PyObject* is. So you will need to expose the PyObject to Python using a py::class. I'm not sure if that makes sense though, I would need to test this.
From the docs I linked, it looks like Py_BuildValue might return a tuple. In which case I can suggest wrapping the return value in a py::tuple.
",
76391978,How to get time from datetime into a string/number for a colormap,"I'm trying to shade a scatter plot based on time of day, but my data is in datetime format. Getting either time of day or hours past 00:00 would work. I tried to get just the time from datetime, but I got the following error.

TypeError: float() argument must be a string or a number, not 'datetime.time'

Ideally, I'll have a scatterplot shaded based on time of day.
I initially tried this (also added .values to the end of the dt.time to see if it would help. It didn't).
x = dfbelow[1:].wind_speed.values
y = above_aligned[""WS""].values
plt.scatter(x, y, s=20, c=above_aligned[""timestamp""].dt.time, cmap='YlOrBr')

plt.xlabel(""Below Canopy Wind Speed (m/s)"")
plt.ylabel(""Above Canopy Wind Speed (m/s)"")
plt.title(""Above vs. Below Canopy Wind Speeds (m/s)"")
plt.colorbar(label=""Below Canopy Wind Direction"")
plt.show

But it understandably can't shade based off of a datetime form.
",76392317,"I figured it out and it was actually very simple. Instead of above_aligned[""timestamp""].dt.time, I just used above_aligned[""timestamp""].dt.hour.
",
76390504,AWS ECS Task on private subnet connectivity,"Is it possible to access a public domain e.g. foo.bar.com from an AWS ECS task running on a private subnet? It is convenient for me that the task is running on a private subnet since it can be easily accessible from other ECS tasks running on the same private subnet (same AZ, VPC and region).
I am reading different contradicting opinions:

Some say that this is not possible and I must configure a NAT Gateway or NAT instance
Others say that this is possible as long as you specify the right outbound rules for your security groups. But outbound rules can only be configured using IP ranges and not specific domains.

What is actually the case here?
",76390835,"You have to create a NAT Gateway and add a route in the route table for your subnet. Without a NAT Gateway (or NAT instance, but those are considered obsolete), you cannot connect to the public internet from your private subnets.
",
76391183,compare two PowerShell arrays only if they both contain values,"In the following PowerShell code, functions hour00_numbers and hour01_numbers create two arrays, that contain numbers matching a specific regex.
function hour00_numbers {
    foreach ($event in $filter00) {
        $hour00Numbers = @()
        $hour00Numbers += [regex]::Matches($event.Source.Summary -replace ' ', '9 *[45679]( *[0-9]){6}').Value
        $hour00Numbers
    }
}
    
function hour01_numbers {
    foreach ($event in $filter01) {
        $hour01Numbers = @()
        $hour01Numbers += [regex]::Matches($event.Source.Summary -replace ' ', '9 *[45679]( *[0-9]){6}').Value
        $hour01Numbers
    }
}

I then check if both arrays are not empty, and if they are not, I check to see if they contain matching numbers.
function matching_numbers01 {
    if ($null -ne $(hour00_numbers) -and $null -ne $(hour01_numbers)) {
        $script:matchingNumbers01 = Compare-Object $(hour00_numbers) $(hour01_numbers) -IncludeEqual | Where-Object { $_.SideIndicator -eq '==' } | Select-Object -ExpandProperty InputObject
    }
    else {
        $matchingNumbers01 = $null
    }

    if ($null -ne $(hour00_numbers)) {
        Write-Host ""hour00 numbers: $(hour00_numbers)"" -ForegroundColor Green
    }
    else {
        Write-Host ""hour00 is empty"" -ForegroundColor Yellow
    }

    if ($null -ne $(hour01_numbers)) {
        Write-Host ""hour01 numbers: $(hour01_numbers)"" -ForegroundColor Green
    }
    else {
        Write-Host ""hour01 is empty"" -ForegroundColor Yellow
    }

    if ($null -ne $matchingNumbers01) {
        Write-Host ""matching numbers are: $matchingNumbers01"" -ForegroundColor Yellow
    }
    else {
        Write-Host ""no matching numbers found"" -ForegroundColor Green
    }
}

I'm doing something wrong here though, because I will frequently get a message saying
Cannot bind argument to parameter 'ReferenceObject' because it is null

If either of the two arrays is empty, it shouldn't compare anything. It should only compare the arrays if both are not empty.
What I'm I doing wrong here?
",76392333,"Aside from the problem you've reported, your code has a few other issues:

Use of global variables to pass values into (and out of) functions - e.g. $filter00, $filter01 and $matchingNumbers01. It's hard to reason about these in isolation because they depend on program state that exists outside their scope.

Duplication of code as a result - e.g. the functions hour00_numbers and hour01_numbers are identical except for which global variable they reference - this makes maintenance harder.

Performance issues / race condition by repeatedly calling the hour00_numbers and hour01_numbers functions inside matching_numbers01 instead of caching the results into internal variables.


So, here's a refactor that uses function parameters and output streams to address these problems. It also uses some of the feedback in the comments, and it might just accidentally fix your root problem as a side-effect as well...
# define a function to filter the events, give it a meaningful name, and pass
# the raw event list as a parameter so we only need one version of the function
function Get-HourEvents {
    param( $Events )
    foreach($event in $Events) {
        [regex]::Matches($event.Source.Summary -replace ' ', '9 *[45679]( *[0-9]){6}').Value
    }
}

# pass the filtered events in as parameters '$Left' and '$Right',
# and return the results in the output stream so we don't need to
# read global variables. this makes it easier to reason about, and
# a lot easier to test with frameworks like pester
function Get-MatchingEvents {
    param( $Left, $Right )

    if ($null -eq $Left) {
        Write-Host ""Left is empty"" -ForegroundColor Yellow
    }
    else {
        Write-Host ""Left numbers: $Left"" -ForegroundColor Green
    }

    if ($null -eq $Right) {
        Write-Host ""Right is empty"" -ForegroundColor Yellow
    }
    else {
        Write-Host ""hour01 numbers: $Right"" -ForegroundColor Green
    }

    if (($null -eq $Left) -or ($null -eq $Right) {
        $results = $null
    }
    else {
        $results = Compare-Object $Left $Right -IncludeEqual | Where-Object { $_.SideIndicator -eq '==' } | Select-Object -ExpandProperty InputObject
    }

    if ($null -eq $results) {
        Write-Host ""no matching numbers found"" -ForegroundColor Green
    }
    else {
        Write-Host ""matching numbers are: $results"" -ForegroundColor Yellow
        $results
    }

}

# use the functions like this:

# get the filtered event lists
$hour00Events = Get-HourEvents -Events $filter00
$hour01Events = Get-HourEvents -Events $filter01

# match the event lists and assign the result to a variable
$matchingNumbers01 = Get-MatchingEvents -Left $hour00Events -Right $hour01Events

I also took the liberty of inverting some of your if( ... -ne ... ) { ... } else { ... } as I find it easier to think about when the else will trigger if the if uses a ""positive"" expression otherwise the else condition becomes a double-negative, but that's just personal style - ymmv.
If you're still seeing the same issue with this version of your code feel free to post a comment below...
",
76388777,PHP function file_get_contents($url) returns special characters,"I am trying to retrieve the meta data for a given links (url). I have implemented the following steps:
$url = ""url is here"";
$html = file_get_contents($url);
$crawler = new Crawler($html); // Symfony library
$description = $crawler->filterXPath(""//meta[@name='description']"")->extract(['content']);

Doing so, I manage to retrieve the meta data for some urls but not for all.
Some urls, the file_get_contents($url) function returns special characters like (x1F‹\x08\x00\x00\x00\x00\x00\x04\x03ì½}{ãÆ‘/ú÷øSÀœ'\x1E)! ‘z§¬qlÇI..........) that is why I could not retrieve the meta data.
Notice that, I am using the same website for $url values but passing different slugs (different blog urls like https://www.example.com/blog-1).
Attempts:

I used these functions  mb_convert_encoding and mb_detect_encoding
I made sure all urls I have passed are accessible through the browser.

Any thought, why I am getting special characters when I am calling file_get_contents function, and some time getting correct html format?

",76390883,"I have solved the issue by adding the following parameters to file_get_contents functions:
private const EMBED_URL_APPEND = '?tab=3&object=%s&type=subgroup';
      
private const EMBED_URL_ENCODE= 'CM_949A11_1534_1603_DAG_DST_50_ÖVRIGT_1_1';
            
$urlEncoded= sprintf($url.self::EMBED_URL_APPEND, rawurlencode(self::EMBED_URL_ENCODE));
            
$html =  file_get_contents($urlEncoded);

",
76390329,Change x or y position of density plot,"I have data plotted as points and would like to add density plots to the graph. The marginal plot solutions from ggExtra or other packages are not giving the freedom that I'd like and so want to generate the density plot at the same time as the ggplot.
df = data.frame(x = rnorm(50, mean = 10), 
                y = runif(50, min = 10, max = 20), 
                id = rep(LETTERS[1:5], each = 10))
ggppp = ggplot(data = df, mapping = aes(x, y, color = id)) + 
  geom_point() + theme_bw()

ggppp + 
  geom_density(mapping = aes(y = y, 
                             col = id),
               inherit.aes = FALSE, bounds = c(-Inf, Inf)) +
  geom_density(mapping = aes(x = x,
                             col = id),
               inherit.aes = FALSE, ) 


Is there a way to move the density plots to other values of x or y position (like moving the density lines to the tip of the arrow in the image below)?
",76390859,"you can shift the position with position_nudge:
## using your example objects:
ggppp + 
  geom_density(mapping = aes(y = y , col = id),
               position = position_nudge(x = 12),
               inherit.aes = FALSE
               ) +
  geom_density(mapping = aes(x = x, col = id),
               position = position_nudge(y = 20),
               inherit.aes = FALSE
)

",
76391826,Lambda expression returns parent method,"I'm trying to create a Snackbar in my Android Java application. It has an action, displayed as Cancel, that should stop (or return) the parent method.
I tried this:
snackbar.setAction(""Cancel"", v -> { return; });

But Android Studio told me that

'return' is unnecessary as the last statement in a 'void' method

showing me that this was returning from the lambda expression, not it's parent method.
I also tried super.return;, but that caused a whole lot of errors.
",76392344,"NB: This answer applies generally to UI frameworks/java, not android in particular.
What you want to do here makes fundamentally no sense.
The setAction method is telling the snackbar object: Hey, whenever the ""Cancel"" event occurs, run this code. Don't run it now, run it later. Maybe a year later, when the user bothers to click that cancel button, when this method is long gone - this method where I am informing you what to do when the user clicks that Cancel button, NOT actually doing the act that must be done when the user clicks that, maybe they never click it, after all!
Hence, 'return from this method' is fundamentally nonsensical. How? That method's probably already done ages ago.
After telling the object referred to by the snackbar variable what to do when the user presses cancel (which is a near instant operation, of course, and requires no user interaction or anything else that would take more than nanoseconds), this method will keep going.
It sounds like you are a bit confused about how to set up these actions.
Taking a bit of a guess on how this works, there are in broad strokes two obvious things you might do:
Have a dialog with OK and Cancel buttons
Nothing should happen until a user clicks one of the two buttons. Once they do, it happens and they can no longer stop that short of hard-killing your app or force-shutting down the phone.
In this case, you should have one call to .setAction(""Cancel"", ...) and one call to .setAction(""OK"", ....) and that's that. The cancel button just needs to dismiss the dialog and do nothing else.
Have a dialog with perhaps a progress bar and a cancel button
As in, the act occurs right now but will take a while, and you want to offer the user a button to tell your application to abort what it is doing. The dialog is explaining (via a progress bar, spinner, or simply with text) that the act is occurring now and whatever that act may be (say, send an email), once it is done, this dialog dismisses itself (and it is at that point no longer possible to cancel it; possibly it can be undone, but that'd be a separate UI action).
In this case: You can't just 'kill' a thread mid-stride in java. After all, just hard-killing one process of an app can (and often will) leave stuff in undefined state. Create some concurrency-capable mechanism (in basis, reading the same field from 2 different threads is just broken, because CPUs are mostly self-contained units and forcing them to communicate every change across all cores means there is pretty much no point to multiple cores in the first place, hence, software does not guarantee that writes to a field are seen by other threads unless you explicitly spell it out). Then use that to communicate to the running code that it should abort. that code should then dismiss the dialog.
The general process for the 'cancel' action code is:

Disable the button, both actually (it should no longer invoke the cancel handler), and visually (so the user knows their click is now being handled).
Set the concurrency capable flag. Might take the form of interrupting a thread, or setting some AtomicBoolean.
that's it. Do nothing else. Leave the dialog up as normal.

The code that does the slow thing (say, sending mail) should:

Set up a system that listens to that flag to abort as soon as it can. How to do this is tricky and depends on what, precisely, you are doing.
Once it sees that flag being up / catches InterruptedException, aborts the act, undoes whatever half-work it has done if it can, and it dismisses the dialog entirely. This then lets the user know the act of aborting it has succeeded.

",
76380894,FullCalendar in GWT : How to refresh calendar while keeping events,"I'm working on a GWT application using Domino/UI, Nalukit and the Javascript plugin FullCalendar v6.
I made a custom popup to modify and delete an event but when I validate the form, my calendar refreshes and all the event in my week view disappear.
Demo of the app running
I used the native function gotoDate to change to view of the calendar to the event's modified date.
Here's a sample from my controller's render and refreshCalendar methods :
    @Override
    public void render() {

        // Styling related lines omitted

        FcOptionOverrides mainOptions = new FcOptionOverrides();
        mainOptions.locale = ""fr"";
        mainOptions.initialView = ""timeGridWeek"";
        mainOptions.views = new FcViewOptions();
        mainOptions.views.timeGridWeek = new FcView();
        mainOptions.views.timeGridWeek.weekends = true;
        mainOptions.views.timeGridWeek.slotMinTime = ""07:00:00"";
        mainOptions.views.timeGridWeek.slotMaxTime = ""22:00:00"";
        mainOptions.eventSources = new FcEventSources();
        mainOptions.eventSources.events = this.getController()::onEventNeedMain;
        mainOptions.datesSet = this::onDatesSet;
        
        mainOptions.eventDidMount = this::onEventDidMount;
        mainOptions.dateClick = this::onBigCalendarDateClick;

        mainCalendar = new FcCalendar(mainAgendaContainer, mainOptions);

        FcOptionOverrides smallOptions = new FcOptionOverrides();
        smallOptions.locale = ""fr"";
        smallOptions.initialView = ""dayGridMonth"";
        smallOptions.height = ""330px"";
        smallOptions.aspectRatio = 1.0f;
        smallOptions.eventSources = new FcEventSources();
        smallOptions.eventSources.events = this.getController()::onEventNeedSmall;

        smallOptions.dateClick = this::onSmallCalendarDateClick;

        smallCalendar = new FcCalendar(smallAgendaContainer, smallOptions);

        // radio button for displaying week-ends
        RayflexRadio displayWeekendRadio = new RayflexRadio(""weekends"", ""Week-ends"", ""Afficher"", ""Masquer"");
        displayWeekendRadio.style(""position:absolute; top:18px; right:230px;"");
        displayWeekendRadio.addChangeHandler(event -> {

            if (displayWeekendRadio.getValue()) {

                DominoElement.of(mainAgendaContainer).removeCss(RxResource.INSTANCE.gss().calendar_main_no_weekends());
                DominoElement.of(mainAgendaContainer).css(RxResource.INSTANCE.gss().calendar_main());

            } else {

                DominoElement.of(mainAgendaContainer).removeCss(RxResource.INSTANCE.gss().calendar_main());
                DominoElement.of(mainAgendaContainer).css(RxResource.INSTANCE.gss().calendar_main_no_weekends());

            }
            displayWeekEnds = displayWeekendRadio.getValue();
            refreshCalendar();

        });

        displayWeekendRadio.setValue(displayWeekEnds);

        card.getBody().appendChild(displayWeekendRadio.element());

        initElement(card.element());
    }

 @Override
    public void refreshCalendar() {

        if (lastModifiedEvent != null) {
            
            Date beginDate = lastModifiedEvent.getBeginDate();
            JsDate jsDate = new JsDate(1900 + beginDate.getYear(), beginDate.getMonth(), beginDate.getDate());

            mainCalendar.gotoDate(jsDate);
            smallCalendar.gotoDate(jsDate);
        }
        
        mainCalendar.refetchEvents();
    }

Here's my wrapper class for FullCalendar's JS functions :
package com.alara.rayflex.ui.client.calendar;

import elemental2.core.JsDate;
import elemental2.dom.Element;
import jsinterop.annotations.JsType;

@JsType(isNative = true, namespace = ""FullCalendar"", name=""Calendar"")
public class FcCalendar {

    public FcCalendar(Element root) {}
    
    public FcCalendar(Element root, FcOptionOverrides optionOverrides) {}
    
    public native void render();
    public native void updateSize();
    
    public native void gotoDate(JsDate start);
    public native JsDate getDate();
    
    public native void setOption(String name, String value);
    public native void setOption(String name, int value);
    
    public native void select(JsDate date);
    
    public native void refetchEvents();
    
    public native void addEventSource(FcOnEventNeed needEvent);
    
    public native void changeView(String viewName, JsDate dateOrRange);
}


I tried to force the native javascript functions refetchEvents with gotoDate but I got the same result. Then I tried using addEventSource to restore my events but still no success there.
I'm expecting to rerender my calendar with the events of the week which the event has been modified.
",76391234,"Solved my issue by loading the events in a callback and refetching them in my refreshCalendar method :
@Override
public void refreshCalendar() {

    if (lastModifiedEvent != null)
        
        moveToLastEventModifiedDate();

    else
    
        mainCalendar.refetchEvents();         
}

/**
 * Use the last modified event to move the calendar to its begin date
 * then fetch the events to load them again in the calendar component 
 */
public void moveToLastEventModifiedDate() {
    
    Date eventDate = lastModifiedEvent.getBeginDate();
    JsDate jsEventDate = new JsDate(1900 + eventDate.getYear(), eventDate.getMonth(), eventDate.getDate());
    
    Date previousMonday = DateUtils.getPreviousMonday(eventDate);
    Date nextMonday = DateUtils.getNextMonday(eventDate);

    JsDate jsDateBegin = new JsDate(1900 + previousMonday.getYear(), previousMonday.getMonth(), previousMonday.getDate());
    JsDate jsDateEnd = new JsDate(1900 + nextMonday.getYear(), nextMonday.getMonth(), nextMonday.getDate());

    mainCalendar.gotoDate(jsEventDate);
    smallCalendar.select(jsEventDate);

    FcEventFetchInfo info = new FcEventFetchInfo();
    info.start = jsDateBegin;
    info.end = jsDateEnd;
    
    this.getController().onEventNeedMain(info, success -> {
        mainCalendar.setOption(""events"", fcOnEventNeed);
        mainCalendar.refetchEvents();
        
    }, failure -> {
        
        ErrorManager.displayServerError(""event.list"", failure.message, getController().getEventBus());
    });

    lastModifiedEvent = null;
}

",
76390427,Can SymPy Lambdify translate core functions like Add and Mul?,"My goal is to evaluate a basic symbolic equation such as ad(b + c) with my own custom implementaions of multiply and addition.
I'm trying to use lambdify to translate the two core SymPy functions (Add and Mul) with my own functions, but I cant get them recognised.
At this stage I'm just trying to get Add working. The code I have is below.
from sympy import *
import numpy as np
x, y = symbols('x y')
A = [1,1]
B = [2,2]

def addVectors(inA, inB):
    print(""running addVectors"")
    return np.add(inA, inB)

# Test vector addition
print(addVectors(A,B))

# Now using lambdify
f = lambdify([x, y], x + y, {""add"":addVectors})
print(f(A, B)) # <------- expect [3,3] and addVectors to be run a second time

# but I get the same as this
print(A + B)

which yields
running addVectors
[3 3]
[1, 1, 2, 2]
[1, 1, 2, 2]

I was expecting the + operator in the expression to be evaluated using the custom addVectors function. Which would mean the results looks like this.
running addVectors
[3 3]
running addVectors
[3 3]
[1, 1, 2, 2]

I tried several different configurations of the lambdify line and these all give the same original result.
f = lambdify([x, y], x + y, {""add"":addVectors})
f = lambdify([x, y], x + y, {""Add"":addVectors})
f = lambdify([x, y], x + y, {""+"":addVectors})
f = lambdify([x, y], Add(x,y), {""Add"":addVectors})
f = lambdify([x, y], x + y)

To confirm I have the syntax correct I used an example closer to the documentation and replaced the symbolic cos function with a sin implementation.
from sympy import *
import numpy as np
x = symbols('x')
    
def mysin(x):
    print('taking the sin of', x)
    return np.sin(x)

print(mysin(1))

f = lambdify(x, cos(x), {'cos': mysin})
f(1)

which works as expected and yields
taking the sin of 1
0.8414709848078965
taking the sin of 1
0.8414709848078965

Is it even possible to implement my own Add and Mul functions using lambdify?
I suspect my trouble is Add (and Mul) are not SymPy 'functions'. The documentation refers to them as an 'expression' and that somehow means they dont get recognised for substitution in the lambdify process.
Some links that I've been reading:
SymPy cos
SymPy Add
SymPy Lambdify
Any pointers would be appreciated. Thanks for reading this far.
EDIT: Got a more general case working
This uses a combination of the lambdify and replace functions to replace Add and Mul. This example then evaluates an expression in the form ad(b + c), which was the goal.
from sympy import *
import numpy as np
 
w, x, y, z = symbols('w x y z')
A = [3,3]
B = [2,2]
C = [1,1]
D = [4,4]
 
def addVectors(*args):
    result = args[0]
    for arg in args[1:]:
        result = np.add(result, arg)
    return result
 
def mulVectors(*args):
    result = args[0]
    for arg in args[1:]:
        result = np.multiply(result, arg)
    return result
 
expr = w*z*(x + y)
print(expr)
expr = expr.replace(Add, lambda *args: lerchphi(*args))
expr = expr.replace(Mul, lambda *args: Max(*args))
print(expr)
 
f = lambdify([w, x, y, z], expr, {""lerchphi"":addVectors, ""Max"":mulVectors})
print(f(A, B, C, D))
 
print(mulVectors(A,D,addVectors(B,C)))

which yields
w*z*(x + y)
Max(w, z, lerchphi(x, y))
[36 36]
[36 36]

A few things to note with this solution:

Using the replace function you can replace a type with a function (type -> func). See the docs.
The function I replace the types with have to accept multiple inputs because each type in the expression may have more than two arguments (like multiply in the example above). I only found 3 functions that accept *args as an input. These were Min, Max and lerchphi.
SymPy simplifies Min and Max functions since Max(x, Min(x, y)) = x. That meant I couldn't use Min and Max together. So I used lerchphi and Max. These functions are arbitary as I'll be translating their implementation to a custom function in the next step. However, this means I can only replace two.
Final step was to translate lerchphi and Max to the custom functions.

",76390861,"With sympy, addition is an operation. Hence, I'm not sure if it's possible to achieve your goal by passing in custom modules...
However, at the heart of lambdify there is the printing module. Essentially, lambdify uses some printer to generate a string representation of the expression to be evaluated. If you look at lambdify's signature, you'll see that it's possible to pass a custom printer.
Given a printer class, the addition with + is performed by the _print_Add method. One way to achieve your goal is to modify this method of the NumPyPrinter.
from sympy.printing.lambdarepr import NumPyPrinter
import inspect

class MyNumPyPrinter(NumPyPrinter):
    def _print_Add(self, expr, **kwargs):
        str_args = [self.doprint(t) for t in expr.args]
        return ""add(*[%s])"" % "", "".join(str_args)

f = lambdify([x, y], x + y, printer=MyNumPyPrinter)
print(inspect.getsource(f))
# def _lambdifygenerated(x, y):
#    return add(*[x, y])

print(f(A, B))
# [3 3]

Note that I've no idea what implication this might creates. That's for you to find out...
",
76388602,Why am I unable to retrieve a Xero item by identifier with a valid access token from Postman?,"xero developer api not authorizing
i generated the access_token from the endpoint postman screen shot for access token when i try to get xero item i am getting screen shot for item end point this endpoint should give the item with identifier 96d14376-4b75-4b4a-8fd3-b1caab075ab3 in the response also when i try this one in xero api explorer after login its working fine
",76391241,"Looking at the logs relating to the instance id in the error screen shot, the
access token does not include the accounting.settings scope.
Please can you go through the OAuth 2.0 process from the very beginning, making sure the scope is in the authorisation call.
When you add a new scope to a call you need to go through the whole authorisation process from scratch to update the access token.
When you get a new access token you can decode it to check the scopes before you use it to make sure that you have the scopes you need. You can use jwt.io to check this if you wish
",
76388769,Call Different Method in reactive pipeline,"I have a use case where I have to call two different methods in a reactive pipeline Java 8 on a post-API call.
1st Method:
will insert data in a master table and will return the pk of that table insertion.
2nd Method:
this method will insert data in the mapping table which will use the pk received from 1st method.
I try to do that Mono.zip, but that did not work as Zip is calling both methods simultaneously and is not able to pass method 1 output to 2nd method input.
",76391245,"You can easily do it using the map or flatMap operator, depending on what type of repository you have - reactive or not reactive.
Here is examples for both cases:
public class YourService {
    private final YourRepository repository;
    private final YourReactiveRepository reactiveRepository;

    void doAction() {
        Mono.fromCallable(() -> repository.saveToMainTable(""main data""))
            .map(mainTableId -> repository.saveToSecondaryTable(mainTableId, ""secondary data""))
            .subscribeOn(Schedulers.boundedElastic())
            .subscribe();
    }

    void doActionWithReactiveRepository() {
        reactiveRepository.saveToMainTable(""main data"")
            .flatMap(mainTableId -> reactiveRepository.saveToSecondaryTable(mainTableId, ""secondary data""))
            .subscribe();
    }

    interface YourRepository {
        int saveToMainTable(String someData);
        boolean saveToSecondaryTable(int mainTableId, String someData);
    }

    interface YourReactiveRepository {
        Mono<Integer> saveToMainTable(String someData);
        Mono<Boolean> saveToSecondaryTable(int mainTableId, String someData);
    }
}

You can read more about map here and about flatMap here
",
76384653,Sort pandas dataframe columns on second row order,"I need to sort a dataframe based on the order of the second row.  For example:
import pandas as pd

data = {'1a': ['C', 3, 1], '2b': ['B', 2, 3], '3c': ['A', 5, 2]}
df = pd.DataFrame(data)
df

Output:
  1a 2b 3c
0  C  B  A
1  3  2  5
2  1  3  2

Desired output:
  3c 2b 1a
0  A  B  C
1  5  2  3
2  2  3  1

So the columns have been order based on the zero index row, on the A, B, C.
Have tried many sorting options without success.
Having a quick way to accomplish this would be beneficial, but having granular control to both order the elements and move a specific column to the first position would be even better.  For example move ""C"" to the first column.
Something like make a list, sort, move and reorder on list.
mylist = ['B', 'A', 'C']
mylist.sort()
mylist.insert(0, mylist.pop(mylist.index('C')))

Then sorting the dataframe on ['C', 'A', 'B'] outputting
  1a 3c 2b
0  C  A  B
1  3  5  2
2  1  2  3

",76390867,"With the help of pyjedy and Stingher, I was able to resolve this issue. One of the problems was due to my input. The input consisted of lists instead of dictionaries, so I needed to transform it. As a result, I had indexes for rows and across the top for columns. Consequently, selecting elements from the list required obtaining the index.
import pandas as pd

def search_list_for_pattern(lst, pattern):
    for idx, item in enumerate(lst):
        if pattern in item:
            break
    return idx

data = [['1a', 'B', 2, 3], ['2b', 'C', 3, 1], ['3c', 'A', 5, 2]]
df = pd.DataFrame(data).transpose()
print(df)

#     0   1   2
# 0  1a  2b  3c
# 1   B   C   A
# 2   2   3   5
# 3   3   1   2

# Get the second row and convert it to a list
second_row = df.iloc[1, :].tolist()
print(second_row)

# ['B', 'C', 'A']

# Find the index of the column you want to move to the first position
target_column = search_list_for_pattern(second_row, ""C"")
print(target_column)

# 1

# Sort the list
sorted_columns = sorted(range(len(second_row)), key=lambda k: second_row[k])
print(sorted_columns)

# [2, 0, 1]

# Move the target column to the first position
sorted_columns.remove(df.columns.get_loc(target_column))
sorted_columns.insert(0, df.columns.get_loc(target_column))

# Reorder the columns of the DataFrame based on the sorted list
df = df.iloc[:, sorted_columns]
print(df)

#     1   2   0
# 0  2b  3c  1a
# 1   C   A   B
# 2   3   5   2
# 3   1   2   3

df.to_excel('ordered.xlsx', sheet_name='Sheet1', index=False, header=False)


",
76392302,Combine list of lists of tuples with different lengths,"I'm having a problem to combine list of lists of tuples, and the main problem comes from different sizes of those tuples. I'm also trying to do it ""pythonic"" way, which isn't very easy.
What I actually have is a list of objects, having coordinates given in tuple.
Objects (let's say: lines) always have start and end as (x1,y1) and (x2,y2), they also usually have some ""path"". The problem is that ""path"" is sometimes empty and in general number of points on the path is different.
start=[ (3,5), (23,50), (5,12), (51,33), (43,1)]
end = [(23,19), (7,2), (34,4),  (8,30), (20,10)]
path=[[(10,7),(14,9),(18,15)],
      [],
      [(15,7)],
      [(42,32),(20,31)],
      [(30,7)]]

Expected result should look like this:
whole_path = [[(3,5),(10,7),(14,9),(18,15),(23,19)],
              [(23,50),(7,2)],
              [(5,12),(15,7),(34,4)],
              [(51,33),(42,32),(20,31),(8,30)],
              [(43,1),(30,7),(20,10)]]

I was trying to use zip - it works well for similar size items in start/end/paths lists but not with their differences.
Promising solutions might come with use path.insert(0,start) and path.extend([end]), but I couldn't make that working, there is also an option to put that into two loops, but it doesn't look well and... it's not very ""pythonic"".
So: any suggestions would be nice.
",76392348,"A solution with zip and *-unpacking of the variable-length path element is reasonably clean:
from pprint import pprint

start=[ (3,5), (23,50), (5,12), (51,33), (43,1)]
end = [(23,19), (7,2), (34,4),  (8,30), (20,10)]
path=[[(10,7),(14,9),(18,15)],
      [],
      [(15,7)],
      [(42,32),(20,31)],
      [(30,7)]]

whole_path = [[s, *p, e] for s, p, e in zip(start, path, end)]
pprint(whole_path)

giving the required:
[[(3, 5), (10, 7), (14, 9), (18, 15), (23, 19)],
 [(23, 50), (7, 2)],
 [(5, 12), (15, 7), (34, 4)],
 [(51, 33), (42, 32), (20, 31), (8, 30)],
 [(43, 1), (30, 7), (20, 10)]]

",
76382247,Blade: anonymous component doesn't show when passing data,"I'm new to Blade and tried to use anonymous components for sections that I will use frequently.
The problem is that when I'm trying to pass down data to the component, it won't show anything.
Here is my Code:
Controller:
public function edit(Workingtime $workingtime)
    {
      $user = User::find(Auth::id());
      $workingtime_array = $user->getWorkingtimesOfThisDay($workingtime->id);
            
      return view('workingtime-edit', compact('workingtime_array', 'workingtime'));     
    }

workingtime-edit.blade.php:
<p>Overview of Workingtimes</p>

@if(count($workingtime_array) > 0)
    <x-workingtime-timeline :workingtimeArray = ""$workingtime_array""/>
@endif


workingtime-timeline.blade.php (component):
@props(['workingtimeArray']) 

@php

    if(isset($workingtimeArray) AND count ($workingtimeArray) > 0){
        $start = strtotime($workingtimeArray[0]->time_begin);
        $end = strtotime($workingtimeArray[count($workingtimeArray)-1]->time_end);
    } 

@endphp

@foreach ($workingtimeArray as $worktime)

    <p>Start: {{ $worktime->time_begin }} </p>
    <p>End: {{ $worktime->time_end }} </p>

@endforeach;

When I don't use variables and @props, it shows me the content of the component.
I tried to change the <x-workingtime-timeline :workingtimeArray = ""$workingtime_array""/> line but nothing worked.
Things I tried:
<x-workingtime-timeline :workingtimeArray = ""$workingtime_array""> </x-workingtime-timeline>
<x-workingtime-timeline workingtimeArray = ""$workingtime_array""/>
",76392356,"In workingtime-edit.blade.php you need to remove the spaces around the = for the attribute you are setting:
<x-workingtime-timeline :workingtimeArray=""$workingtime_array""/>

I think this is a limitation of Laravel's parsing of component attributes, since this is not a general requirement of HTML attributes.
",
76388523,How can I prevent existing categories from disappearing when I add a new category to an email item using Exchange Web Services in C#?,"I am trying to add a new category to an email item using Exchange WEB Services but when I run the code, existing categories disappear and the category I added becomes the only category.
For example an email has categories X and Y and after I add my category Z to this mail item, X and Y disappears and only category for the mail becomes Z
Any help is much appreciated.
Thanks in advance
Please note that I am running this code inside a software called Blue Prism a low code no code scripting software and it is near impossible to implement third party libraries etc due to corporate chain of approvals and stuff
When I do it manually on the outlook 2019 it works the way I intended
Here is the method I am using
The DLL That uses this code is Microsoft.Exchange.WebServices.dll
//Here I initialize exchange object and authenticate
ExchangeService exchange = null;

void ConnectEWS(string ewsURL, string _exchangeVersion, string username, string password)
{
    ServicePointManager.ServerCertificateValidationCallback = delegate {return true;} ;
    try
    {
        exchange                = new ExchangeService();
        exchange.Url            = new Uri(ewsURL);
        exchange.Credentials    = new WebCredentials(username,password);
    }
    catch (Exception ex)
    {
        throw new Exception(""failed to initialize exchange object!!=""+ex.Message);
    }   
}
//here is my void method that adds a new category to an email item
void addCategoryToMail(string msgid, string category)
{
    if(exchange==null)
    {
        throw new Exception(""exchange object is null!!"");
    }   
    EmailMessage message = EmailMessage.Bind(exchange,msgid, BasePropertySet.IdOnly );
    if(message.Categories.Contains(category)==false)
    {
        message.Categories.Add(category);
        //message.Update(ConflictResolutionMode.AlwaysOverwrite);
        message.Update(ConflictResolutionMode.AutoResolve);
    }
}

",76391330,"The line
EmailMessage message = EmailMessage.Bind(exchange,msgid, BasePropertySet.IdOnly );

only requests the message id. You need to request categories. See if BasePropertySet.FirstClassProperties brings categories in. If not, explicitly request categories.
",
76389988,npm run does not work as expected on a docker container,"I'm building a react app with vite and I'm deploying it with docker.
When deploying the container, npm run build runs but does nothing, even doing it inside the container manually won't work.
I get this output but the actual build does not happen:
$ npm run build

> frontend@0.0.0 build
> tsc && vite build

$

Thing is if I run tsc && vite build it works just fine.
Same goes with npm run dev or any other script.
Here's more info:

Tried with both Node Alpine and Debian (I'm actively developing in Alpine).
Tried creating the Node user and setting permissions accordingly since I read that npm might behave weird if ran by the root user.

$ npm config get ignore-scripts
false



These are my docker files
FROM node:20-bullseye
WORKDIR /app/frontend/
COPY package.json package-lock.json* /app/frontend/
RUN npm install
RUN npm install -g vite
RUN npm install -g typescript

COPY . .
RUN chown -R node:node /app/frontend/

USER node

CMD [""npm"", ""run"", ""build""]

docker-compose.yaml:
version: ""3.4""
name: fisy-prod
services:
  backend:
    volumes:
      - static:/static
    env_file:
      - ./.env
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    ports:
      - ""8000:8000""
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    volumes:
      - frontend:/app/frontend/dist
  nginx:
    build:
      context: ./nginx
    volumes:
      - static:/static
      - frontend:/var/www/frontend
    ports:
      - ""5173:80""
    depends_on:
      - backend
      - frontend
volumes:
  static:
  frontend:

I scouted the web all morning without really finding anything that might point to the issue. Would appreciate it if someone that has experienced this could help.
",76390898,"Your Compose file has volumes: blocks that replace all of the image content with content from named volumes.  This apparently works the first time you run the container, since Docker copies content from the image into an empty volume; but if you rebuild your application, the volume will not get updated, and the old content in the volume will replace your image content.
One important part of this problem is that your frontend container doesn't really do anything.  Its CMD is to build the application, but that will immediately exit.  This doesn't need to be a long-running container and it doesn't need to be listed in your Compose file.
What I'd do here is to use a multi-stage build to compile the frontend, then COPY all of the content into an Nginx-based image.  For example:
FROM node:20-bullseye AS frontend
WORKDIR /app/frontend/
# Note, run with the project root as the build context
COPY frontend/package.json frontend/package-lock.json ./
...
RUN npm run build

FROM nginx:1.25
COPY nginx/default.conf.tmpl /etc/nginx/default.conf.tmpl
COPY backend/static/ /static/
COPY --from=frontend /app/frontend/dist/ /var/www/frontend/

Now the final image contains all of the pieces you need, so you don't need to try to share files between containers; that means you can remove all of the volumes:.  The final image build also encapsulates the front-end build, so you don't need a separate frontend build-only container.  This leaves you with a Compose file
version: ""3.8""
services:
  backend:
    env_file:
      - ./.env
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    ports:
      - ""8000:8000""
  nginx:
    build:
      context: .  # needed to COPY files from subdirectories
      dockerfile: nginx/Dockerfile
    ports:
      - ""5173:80""
    depends_on:
      - backend

",
76387981,How to read and translate a filename.z.json file,"My file has a .z.json-extension and can be found here.
The content of the file is
""7ZQ7a8MwFIX/i2Yn3IekK3nv3EIztCkdQslgSpySuFPwf6+USMZZbiGzFyODPs7RuY+LeTmeu6E79qb9uJhNd9ifh93hx7SGgHgFbkVhg7Z10kJcB+YAIFvTmKd+OHX7s2kvBvPnddgNv+nXPPeb0+7rO115M+1KPGFj3vMJbTptTeuAx8aQAnkrcIMi51OGMENWg4TjDQrRuwJRghBUqTBJoZ9T2qu8g6IVyVOhJFOaw5QF1SwYZlmg18OoWkw4dxgUipArVJSszalrWQgGW/yJDUUJM6VmIVCoCBhmxSK1xN75SoU7inWt0k1RarXoSqmdYf2UYOCi5TMlao1j1UKZ584P9bvVHbrq8NpZk0PnVIdUe5dcHROXKK9nyK7Oyd1wiZqG5ykNmlNBn5PaUSnDML1rHJt/dg3CGoTFW1p2zbJrll2z7JrHd83n+Ac=""

Apparently, the most common type of file that contains the .z-file extension is compressed Unix files.
How do I translate / uncompress this file to its human-understandable version? I have no additional information.
",76391438,"That is Base-64 encoded raw deflate data. You need to decode the Base-64 to binary, and then use zlib to inflate it. The result is 2278 bytes of json.
",
76391843,Cast pandas series containing list elements to a 2d numpy array,"Take the following series:
import pandas as pd

s = pd.Series([1, 3, 2, [1, 3, 7, 8], [6, 6, 10, 4], 5])

I want to convert this series into the following array:
np.array([
    [ 1.,  1.,  1.,  1.],
    [ 3.,  3.,  3.,  3.],
    [ 2.,  2.,  2.,  2.],
    [ 1.,  3.,  7.,  8.],
    [ 6.,  6., 10.,  4.],
    [ 5.,  5.,  5.,  5.]
])

Currently, I am using this logic:
import numpy as np
import pandas as pd
from itertools import zip_longest

# Convert series and each element in series into list
ls = list(map(lambda v: v if isinstance(v, list) else [v], s.to_list()))
# Cast list elements to 2d numpy array with longest list element as column number
a = np.array(list(zip_longest(*ls, fillvalue=np.nan))).T
# Convert to DataFrame, apply 'ffill' row-wise and re-convert to numpy array
a = pd.DataFrame(a).fillna(method=""ffill"", axis=1).values

My solution is not really satisfying me, especially the last line where I convert my array to a DataFrame and then back to an array again. Does anyone know a better alternative? You can assume that all list elements have the same length.
",76392387,"Assuming all list elements have the same length (as indicated), what about using masks and numpy.repeat?
s2 = pd.to_numeric(s, errors='coerce')
m = s2.isna()

out = np.repeat(s2.to_numpy()[:, None], 4, axis=1)
out[m] = np.array(s[m].tolist())

Output:
array([[ 1,  1,  1,  1],
       [ 3,  3,  3,  3],
       [ 2,  2,  2,  2],
       [ 1,  3,  7,  8],
       [ 6,  6, 10,  4],
       [ 5,  5,  5,  5]])

",
76385124,ONNX performance compared to sklearn,"I have converted a sklearn logistic regression model object to an ONNX model object and noticed that ONNX scoring takes significantly longer to score compared to the sklearn.predict() method. I feel like I must be doing something wrong b/c ONNX is billed as an optimized prediction solution. I notice that the difference is more noticeable with larger data sets so I created X_large_dataset as as proxy.
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import datetime
from sklearn.linear_model import LogisticRegression
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
import numpy as np
import onnxruntime as rt

# create training data
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y)

# fit model to logistic regression
clr = LogisticRegression()
clr.fit(X_train, y_train)

# convert to onnx format
initial_type = [('float_input', FloatTensorType([None, 4]))]
onx = convert_sklearn(clr, initial_types=initial_type)
with open(""logreg_iris.onnx"", ""wb"") as f:
    f.write(onx.SerializeToString())
    
# create inference session from onnx object
sess = rt.InferenceSession(
    ""logreg_iris.onnx"", providers=rt.get_available_providers())
input_name = sess.get_inputs()[0].name

# create a larger dataset as a proxy for large batch processing
X_large_dataset = np.array([[1, 2, 3, 4]]*10_000_000)
start = datetime.datetime.now()
pred_onx = sess.run(None, {input_name: X_large_dataset.astype(np.float32)})[0]
end = datetime.datetime.now()
print(""onnx scoring time:"", end - start)

# compare to scoring directly with model object
start = datetime.datetime.now()
pred_sk = clr.predict(X_large_dataset)
end = datetime.datetime.now()
print(""sklearn scoring time:"", end - start)

This code snippet on my machine shows that sklearn predict runs in less than a second and ONNX runs in 18 seconds.
",76390930,"Simply converting a model to ONNX does not mean that it will automatically have a better performance. During conversion, ONNX tries to optimize the computational graph for example by removing calculations which do not contribute to the output, or by fusing separate layers into a single operator. For a generic neural network consisting of convolution, normalization and nonlinearity layers, these optimizations often result in a higher throughput and better performance.
So considering you are exporting just LogisticRegression, most likely both sklearn and the corresponding onnx implementations are already very optimized and the conversion will not lead to any performance gain.
As to why the InferenceSession.run is 20x slower than sklearn.predict

X_large_dataset is a np.int64 array over 300 MB in size. Casting it with astype when creating the input dictionary inside of run creates a new 150 MB array to which everything is copied. This obviously shouldn't be counted towards the model execution time.
onnxruntime has quite a bit of memory management overhead when executing models with dynamic inputs for the first time. Subsequent calls to run with inputs of the same shape should finish a lot faster.

",
76392330,How can I make sure a SQL record's column doesn't refer to its own primary key?,"I have a Node table with a ParentID column, which refers to a NodeID that is its parent. I want to make sure no Node can refers to itself (i.e. a Node's ParentID cannot its own NodeID), so I tried adding a check constraint CHECK(NodeID != ParentID).
However, I got this error: Error Code: 3818. Check constraint 'node_chk_1' cannot refer to an auto-increment column.
I also couldn't add the ParentID as a foreign key of Node.
Using MySQL, How can I make sure that there are no new records where NodeID = ParentID?
",76392389,"Use a trigger:
mysql> create table node ( 
  id int auto_increment primary key, 
  parentid int, 
  foreign key (parentid) references node (id)
);

mysql> delimiter ;;

mysql> create trigger no_self_ref after insert on node for each row begin
if NEW.parentid = NEW.id then
  signal sqlstate '45000' message_text = 'no self-referencing hierarchies';
end if;
end;;

mysql> delimiter ;

Note that it must be an AFTER trigger, because the auto-increment id has not yet been generated in a BEFORE trigger.
Demo:
mysql> insert into node values (1, null);
Query OK, 1 row affected (0.00 sec)

mysql> insert into node values (2, 2);
ERROR 1644 (45000): no self-referencing hierarchies

mysql> insert into node values (2, 1);
Query OK, 1 row affected (0.01 sec)

You will also need a similar AFTER UPDATE trigger, if you want to prevent users from updating the row and setting the parentid to the same value as the id in the same row.

An alternative solution would be to make the primary key non-auto-increment. You will have to specify every id value in your INSERT statements, instead of letting them be auto-incremented. But this will allow you to use a CHECK constraint.
",
76380833,Android PhotoPicker returns NULL URI,"ActivityResultLauncher<PickVisualMediaRequest> pickImage = registerForActivityResult(new ActivityResultContracts.PickVisualMedia(), uri -> {

        if(uri==null) {
            //URI always NULL here
        } else {
            //Never reached
        }


    });

pickImage.launch(new PickVisualMediaRequest.Builder()
                        .setMediaType(ActivityResultContracts.PickVisualMedia.ImageOnly.INSTANCE)
                        .build());


I have tried the same code in a new project and it is returning a valid URI. But PhotoPicker returns a NULL URI in my project. Any idea what could be the issue here?
",76391584,"Apparently, PhotoPicker (or ActivityResultLauncher in general) fails if onActivityResult() is also Overridden in the Activity. I removed that and now PhotoPicker is returning a valid URI.
",
76384047,host not found in upstream with nginx docker compose,"I know this question has been asked many times however, none of the solutions have worked for me.
I am trying to dockerize my angular app and node js backend using nginx.
What I have done is that I have created a docker-compose file.
It has three services and nginx.
1: space-frontend
2: space-api
3: mongodb
I am calling frontend and backend by their service name in nginx conf file like http://space-frontend:80 and http://space-api:3000 but I am getting a error in logs

[emerg] 1#1: host not found in upstream ""space-api"" in /etc/nginx/nginx.conf:23

and frontend is working fine.
I am unable to understand where I am missing something.
For reference,
My frontend docker file
FROM node:16-alpine AS builder
WORKDIR /app
COPY . .
RUN npm i && npm run build --prod
FROM nginx:alpine
RUN mkdir /app
COPY --from=builder /app/dist/Space-Locator/browser /app
COPY nginx.conf /etc/nginx/nginx.conf

My frontend nginx conf
events {
worker_connections 1024;
}
http {
include /etc/nginx/mime.types;

server {
listen       80;
server_name  localhost;
root /app;

location / {
    index  index.html;
    try_files $uri $uri/ /index.html;
}

error_page   500 502 503 504  /50x.html;
location = /50x.html {
    root   /usr/share/nginx/html;
}
}
}

Backend api docker file
FROM node:14-alpine as build-step
RUN mkdir -p /usr/app
WORKDIR /usr/app
COPY package.*json /usr/app/
RUN npm install
COPY . /usr/app/
EXPOSE 3000
CMD [ ""npm"", ""start"" ]

my docker-compose file
    version: ""3.8""
services:
  reverse_proxy:
    image: nginx:1.17.10
    container_name: reverse_proxy
    depends_on:
      - space-frontend
      - space-api
      - database
    volumes:
      - ./reverse_proxy/nginx.conf:/etc/nginx/nginx.conf  
    ports:
      - 80:80
  space-frontend:
    container_name: space-frontend
    image: space-frontend
    build: 
      context: ./space-frontend
    ports:
      - 4000:80
  space-api:
    container_name: space-api
    hostname: space-api
    image: space-api
    build: 
      context: ./space-api
    ports:
      - 3000:3000
    links:
      - database  
    environment:
      MONGO_INITDB_DATABASE: spaceLocator 
      MONGODB_URI: mongodb://db:27017
    depends_on:
      - database
    volumes:
      - ./db-data/mongo/:/data/database
    networks:
      - node-network
  database:
    container_name: db
    image: mongo
    restart: on-failure
    ports:
      - 27017:27017
    volumes:
      - ./mongodb:/data/database
    networks:
      - node-network  
volumes:
  dbdata6:
networks:
  node-network:
    external: true
    driver: bridge  

and my nginx.conf file for reverse proxy
   events {
    worker_connections 1024;
}
http {

  server {
        listen 80;
        server_name  127.0.0.1;

        root   /usr/share/nginx/html;
        index  index.html index.htm;
        
        location ~* \.(eot|ttf|woff|woff2)$ {
        add_header Access-Control-Allow-Origin *;
        }

        location / {
          proxy_pass http://space-frontend:80;
          proxy_set_header X-Forwarded-For $remote_addr;
        }

        location /api {
            proxy_pass http://space-api:3000;
            proxy_set_header X-Forwarded-For $remote_addr;
        }
    }
}

Can somebody please point to the direction where I am doing wrong?
I have tried adding hostname but they are as same as container-name.
",76390948,"So guys problem was that we have to run every container on same network.
  networks:

  - node-network  

I had to define this in my every service. Then it ran without any problem.
Thank you for everyone who helped :)
",
76389362,How can I update the master Username and password of and existing AWS RDS cluster using the serverless-framework?,"Can't update username of existing RDS cluster
Updating the masterUsername and masterUserPassword in the serverless.yml file for my postgresql database results in the password being updated but not the username.
i.e. I can only access the db with the old username and new password.
I am using the serverless framework to manage aws assests.
It's an RDS cluster.
Everything builds successfully and I can access the data (using the old username).
Waiting for over 30 minutes doesn't have an effect.
",76390959,"I understand you want to update the MasterUsername property on an existing RDS instance that you created with Serverless Framework.
Serverless Framework uses AWS CloudFormation. According to the AWS CF docs, the MasterUsername property cannot be updated on an existing RDS instance. Any attempt will result in a resource replacement.
If you still want to update the MasterUsername, your only option is to destroy the RDS instance and create a new one.
",
76392138,How do I modify a font and save it with Rust 'write-fonts'?,"I would like to open a font file and modify eg the usWeightClass and save it in a different location – using Rust: https://github.com/googlefonts/fontations
I am already able to load a file, read data from the font, but I am not able to modify and save it.
I expect, that it should work somehow with WriteFont (but I don't know how):
https://docs.rs/write-fonts/0.5.0/write_fonts/trait.FontWrite.html#
Any help would be much appreciated.
Thanks a lot in advance,
Olli
Cargo.toml
[dependencies]
write-fonts = ""0.5.0""
read-fonts = ""0.3.0""
font-types = ""0.1.8""

main.rs
use read_fonts::{FontRef, TableProvider};

fn main() {
    pub static FONT_DATA: &[u8] = include_bytes!(""/path/to/a/font/somefont.ttf"");
    let font = FontRef::new(FONT_DATA).unwrap();

    let mut os2 = font.os2().expect(""missing OS/2 table"");
    println!(""os2.version: {}"", os2.version());
    println!(""os2.us_weight_class: {}"", os2.us_weight_class());

    let mut name = font.name().expect(""missing name table"");

    for item in name.name_record() {
        println!(""name_id: {:?}"", item.name_id);
        println!(""language_id: {:?}"", item.language_id);
        let data = item.string(name.string_data()).unwrap();
        println!(""String entry: {:?}"", data.chars().collect::<String>());
    };
}

",76392427,"I haven't worked with this library or with fonts in general yet, but after a little digging in the documentation, this seems to work:
use read_fonts::{FontRead, FontRef, TableProvider, TopLevelTable};
use write_fonts::{dump_table, tables::os2::Os2, FontBuilder};

fn main() {
    {
        let font_data = std::fs::read(""Roboto-Regular.ttf"").unwrap();
        let font = FontRef::new(&font_data).unwrap();

        let os2 = font.os2().expect(""missing OS/2 table"");
        println!(""os2.us_weight_class: {}"", os2.us_weight_class());

        // Create a new font builder
        let mut builder = FontBuilder::default();

        // Iterate over tables and add them to the builder
        for table in font.table_directory.table_records() {
            let tag = table.tag();

            println!(""    Adding table {tag} ..."");

            let font_data = font
                .table_data(tag)
                .expect(&format!(""Table {tag} not found!""));

            let mut raw_data = font_data.as_ref().to_owned();

            // Modify the OS2 tag
            if tag == Os2::TAG {
                let mut os2 = Os2::read(font_data).unwrap();
                os2.us_weight_class = 420;
                raw_data = dump_table(&os2).unwrap();
            }

            builder.add_table(tag, raw_data);
        }

        // Build the font
        let data = builder.build();
        std::fs::write(""Roboto-Regular-modified.ttf"", data).unwrap();
    }

    {
        // Load the font again and check if it got modified
        let font_data = std::fs::read(""Roboto-Regular-modified.ttf"").unwrap();
        let font = FontRef::new(&font_data).unwrap();

        let os2 = font.os2().expect(""missing OS/2 table"");
        println!(""os2.us_weight_class: {}"", os2.us_weight_class());
    }
}

os2.us_weight_class: 400
    Adding table GDEF ...
    Adding table GPOS ...
    Adding table GSUB ...
    Adding table OS/2 ...
    Adding table cmap ...
    Adding table cvt  ...
    Adding table fpgm ...
    Adding table gasp ...
    Adding table glyf ...
    Adding table hdmx ...
    Adding table head ...
    Adding table hhea ...
    Adding table hmtx ...
    Adding table loca ...
    Adding table maxp ...
    Adding table name ...
    Adding table post ...
    Adding table prep ...
os2.us_weight_class: 420

",
76392283,"List comprehension in Python for if, elif, pass?","I see syntax for if pass but not finding syntax for if elif pass: List comprehension with else pass
Basically
if condition:
    something
elif condition
    something
else
    pass

",76392432,"Use an if at the end of the list comprehension (a filter) in order to completely exclude an item from the list.  For example, if you wanted to do this as a list comprehension:
result = []
for i in range(10):
    if i % 2:
        result.append(""two"")
    elif i % 3:
        result.append(""three"")
    else:
        pass  # note that you do not actually need this ""else"" at all

you could do:
result = [""two"" if i % 2 else ""three"" for i in range(10) if i % 2 or i % 3]

",
76390270,how do temporarily turn of vs-code error highlighting,"VS Code error hi-lighting can offer quite a bit of clutter during code edits with its squiggles and colorization. It is a useful horn but once the problem is noticed it can become a nuisance that even makes it difficult to perceive the text and fix the problem. I don't think this feature was intended to get in the way.
Is there a convenient way to toggle VSCode error hi-lighting on and off without going through settings.json and playing with a bunch of flags?
Conclusion...

Feature does not exist.

Work around is to install and use the When_File VSCode extension to mask the error noise with less visible colors when the file has unsaved edits, as clarified below.


",76391014,"you can use the extension When File
Add this setting to your workspace settings.json
  ""whenFile.change"": {
    ""byLanguageId"": {
      ""rust"": {
        ""whenDirty"": {
          ""editorError.foreground"": ""#ff000020"",
          ""editorWarning.foreground"": ""#ff000020"",
          ""editorInfo.foreground"": ""#ff000020""
        }
      }
    }
  }

",
76388820,How can the ASP.NET built-in DI read dependencies from a file at runtime?,"As you can see in C# Unity container, we can configure a container (a list of dependency and object creation policy) from a file, and it is very good when, I want change a dependency (backend for interface) in the runtime environment without need to upload my project again, or in a dockerise paradigm, I do not need to build the Docker image again.
At now, I want to migrate to the ASP.NET 6 built-in DI, How can I provide the same functionality in the built-in DI?
For example, something like
<?xml version=""1.0"" encoding=""utf-8"" ?>
<configuration>
  <configSections>
    <section name=""unity"" type=""Microsoft.Practices.Unity.Configuration.UnityConfigurationSection, Unity.Configuration""/>
  </configSections>
  <unity xmlns=""http://schemas.microsoft.com/practices/2010/unity"">
    <alias alias=""IProductService"" type=""UnityExample.Service.IProductService, UnityExample.Service"" />
    <containers>
      <container name=""Service"">
        <register type=""IProductService"" mapTo=""ProductService""/>
      </container>
    </containers>
  </unity>
</configuration>

",76391781,"While you could have a hard time making the new service provider compatible with Unity, consider switching to Autofac. Autofac can replace the builtin .NET Core's container  and also it supports loading configuration from a file.
",
76392223,"How can I XOR wchar_t input, write it to a file, and read it back in C?","I have problem working with unicode strings, wchar_t type.
In my program I'm getting input as wchar_t and I'm supposed to XOR it and write it to file and read it back and print it to command line.
This is my code,
const unsigned int XORKey = 0xff;



size_t XORit(const wchar_t* value, wchar_t* xorred)
{
    size_t length = wcslen(value);
    for (int i = 0; i < length; i++)
        xorred[i] = ((char)(value[i] ^ XORKey));

    return length;
}


int main()
{
    setlocale(LC_ALL, ""en_US.UTF-8"");
    // XOR it
    wchar_t sample[] = { L""TEST1自己人"" };
    int samplelen = wcslen(sample);
    printf(""%ls"", sample);
    printf(""\n"");

    printf(""Plain:\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%02X "", sample[i]);
    }
    printf(""\n"");



    wchar_t* xorred = (wchar_t*)malloc(samplelen);
    if (xorred == NULL) return -1;
    memset(xorred, 0, samplelen);
    XORit(sample, xorred);
    printf(""XOR'ed\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%02X "", xorred[i]);
    }
    printf(""\n"");

    // Write to file
    FILE* fpW = _wfopen(L""logon.bin"", L""wb"");
    fwrite(xorred, sizeof(wchar_t), samplelen, fpW);
    fclose(fpW);


    // Read from file
    FILE* fpR = fopen(""logon.bin"", ""rb"");
    fseek(fpR, 0, SEEK_END);
    int filesize = ftell(fpR);
    wchar_t* unxorred = (wchar_t*)malloc(filesize + sizeof(wchar_t));
    if (unxorred == NULL) return -1;

    rewind(fpR);

    fread(unxorred, sizeof(wchar_t), filesize, fpW);
    fclose(fpW);

    printf(""Reading\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%02X "", unxorred[i]);
    }
    printf(""\n"");

    printf(""Un-XOR'ed\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%02X "", unxorred[i] ^ XORKey);
    }
    printf(""\n"");
    printf(""%ls"", unxorred);

    return 0;
}

The values I'm reading back from the file doesn't match to what I wrote! :(
I'm new to C programming, I did my best to get this right, please forgive any noob mistakes in my understanding of the issue and implementing it.
Thanks in advance
I fixed the code based on the comments,
const unsigned int XORKey = 0xff;



size_t XORit(const wchar_t* value, wchar_t* xorred)
{
    size_t length = wcslen(value);
    for (int i = 0; i < length; i++)
        xorred[i] = (value[i] ^ XORKey);

    return length;
}


int main()
{
    setlocale(LC_ALL, ""en_US.UTF-8"");
    // XOR it
    wchar_t sample[] = { L""Test1自己人自己人A"" };
    int samplelen = wcslen(sample);
    printf(""%ls"", sample);
    printf(""\n"");

    printf(""Plain:\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%0*X "", (int)sizeof(wchar_t), (unsigned int)sample[i]);
    }
    printf(""\n"");



    wchar_t* xorred = (wchar_t*)malloc(samplelen);
    if (xorred == NULL) return -1;
    memset(xorred, 0, samplelen);
    XORit(sample, xorred);
    printf(""XOR'ed\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%0*X "", (int)sizeof(wchar_t), (unsigned int)xorred[i]);
    }
    printf(""\n"");

    // Write to file
    FILE* fpW = _wfopen(L""logon.bin"", L""wb"");
    fwrite(xorred, sizeof(wchar_t), samplelen, fpW);
    fclose(fpW);


    // Read from file
    FILE* fpR = fopen(""logon.bin"", ""rb"");
    fseek(fpR, 0, SEEK_END);
    int filesize = ftell(fpR);
    int whattoread = (filesize / sizeof(wchar_t));
    wchar_t* ReadXOR = (wchar_t*)malloc(filesize + 1);
    if (ReadXOR == NULL) return -1;
    memset(ReadXOR, 0, filesize + 1);

    rewind(fpR);

    fread(ReadXOR, sizeof(wchar_t), whattoread, fpR);
    fclose(fpW);

    printf(""Reading\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%0*X "", (int)sizeof(wchar_t), (unsigned int)ReadXOR[i]);
    }
    printf(""\n"");


    wchar_t* unxorred = (wchar_t*)malloc(whattoread);
    if (unxorred == NULL) return -1;
    memset(unxorred, 0, whattoread);
    printf(""Un-XOR'ed\n\t"");
    for (int i = 0; i < whattoread; i++)
    {
        unxorred[i] = ReadXOR[i] ^ 0xff;
        printf(""%0*X "", (int)sizeof(wchar_t), (unsigned int)unxorred[i]);
    }
    printf(""\n"");
    printf(""%ls\n"", unxorred);
    printf(""%ls\n"", sample);
    

    return 0;

The output looks like below,
Test1自己人自己人A
Plain:
        54 65 73 74 31 81EA 5DF1 4EBA 81EA 5DF1 4EBA 41
XOR'ed
        AB 9A 8C 8B CE 8115 5D0E 4E45 8115 5D0E 4E45 BE
Reading
        AB 9A 8C 8B CE 8115 5D0E 4E45 8115 5D0E 4E45 BE
Un-XOR'ed
        54 65 73 74 31 81EA 5DF1 4EBA 81EA 5DF1 4EBA 41
Test1自己人自己人A粘蹊?言?萉?
Test1自己人自己人A

When I modified the unicode string I got wrong text in the output!
",76392509,"Here ...
    printf(""Un-XOR'ed\n\t"");
    for (int i = 0; i < samplelen; i++)
    {
        printf(""%02X "", unxorred[i] ^ XORKey);
    }
    printf(""\n"");

... you print out the decoded values without storing them.
When you then ...
    printf(""%ls"", unxorred);

... you are printing the data as read back from the file, not the decoded string corresponding to the previously-printed code sequence.
Additionally,

here ...
    int filesize = ftell(fpR);
    wchar_t* unxorred = (wchar_t*)malloc(filesize);
    if (unxorred == NULL) return -1;

    rewind(fpR);

    fread(unxorred, sizeof(wchar_t), filesize, fpW);

... you are attempting to read back sizeof(wchar_t) * filesize bytes from the file, which is more than it actually contains or that you have allocated for (unless sizeof(wchar_t) is 1, which is possible, but unlikely, and is anyway not your case).

You do not allocate space for a (wide) string terminator or add one to the read-back data, yet you pass it to printf() is if it were a wide string.  This is erroneous.

Your approach to printing out the bytes of the wide strings is flawed.  The conversion specifier X requires a corresponding unsigned int argument, and wchar_t might neither be the same as unsigned int nor promote to unsigned int via the default argument promotions.  Additionally, you get varying-length outputs because your wchar_t is at least 16 bits wide, and your 02 only guarantees 2 hex digits.  Better would be, for example:
for (int i = 0; i < samplelen; i++) {
    printf(""%0*X "", (int) sizeof(wchar_t), (unsigned int) xorred[i]);
}

The * for a width says that the minimum field width will be passed as an argument of type int.  The casts match the arguments to the types required by the format.


",
76390597,What's the idiomatic way to add a calculated column to multiple data frames?,"I have a few dataframes, let's call them rates, sensors with ""session_start"", ""value_timestamp"" (timestamps) and ""value"" (float) columns. I want to add an ""elapsed"" column, which I've done successfully using the following code:
def add_elapsed_min(df):
  df[""elapsed""] = (
    df[""value_timestamp""] - df[""session_start""].min()
  ).dt.total_seconds() / 60.0

for df in [rates, sensors]:
  add_elapsed_min(df)

Now, this code does work, and the elapsed column is correct. The minor problem is that I keep getting the SettingWithCopyWarning. I've tried changing the code as suggested by the warning, tried adding a contextlib.suppress, but can't seem to remove this warning. This makes me think I must be breaking some idiomatic way to do this. So I'm wondering: If you want to add a calculated column to many dataframes at once, how are you supposed to do this?
",76391091,"Although I cannot create your warning in Pandas 1.5.3 and considering using .loc does not suppress the warning for you, one other option is to use df.insert instead.
def add_elapsed_min(df):
  elapsed = (df[""value_timestamp""] - df[""session_start""].min()) / 60.0
  df.insert(df.shape[1], 'elapsed', elapsed)

for df in [rates, sensors]:
  add_elapsed_min(df)

",
76388425,Javascript plugin and environment variables,"I was reading the documentation here https://developer.shopware.com/docs/guides/plugins/plugins/storefront/add-custom-javascript but cannot find a mention on how to make usage of environment variables in a  javascript plugin.
Currently I tried to put a .env file at the root of my plugin in custom/apps/MyPlugin/.env and to capture them via process.env but it fallbacks to my default values...
Is there a way to handle a .env file when you run bash bin/build-storefront.sh?
Thanks.
",76392087,"Here's one way to do it...
First create a custom webpack.config.js at src/Resources/app/storefront/build. Also in that build directory run npm install dotenv, as you will need it to parse your .env file.
Your webpack.config.js could then look like this:
const fs = require('fs');
const dotenv = require(`${__dirname}/node_modules/dotenv/lib/main.js`);

module.exports = () => {
    // given you `.env` is located directly in your plugin root dir
    const contents = fs.readFileSync(`${__dirname}/../../../../../.env`);
    const config = dotenv.parse(contents);

    return {
        externals: {
            myPluginEnv: JSON.stringify(config),
        }
    };
};

Then inside your sources you can import myPluginEnv.
import * as myPluginEnv from 'myPluginEnv';

/*
 * Example .env content:
 * FOO=1
 * BAR=1
 */

console.log(myPluginEnv);
// {FOO: '1', BAR: '1'}

",
76388652,Different scales for PyQtGraph chart axis in PyQt5,"I have an interface on PyQt5, in which, by pressing the Start button, a graph is built, which I made using PyQtGraph. Three lines are drawn on the chart. Green and blue have a y-axis range of 0 to 200, while red has a range of 0 to 0.5. How can I make different scales for different lines, as well as designate two value scales on the Y-axis - from 0 to 200 and from 0 to 0.5?

from pyqtgraph import PlotWidget
import pyqtgraph
from PyQt5 import QtCore
from PyQt5.QtCore import Qt, QThread, QTimer, QObject, pyqtSignal, QTimer
from PyQt5.QtWidgets import QHBoxLayout, QMainWindow,  QPushButton, QVBoxLayout, QWidget, QApplication
import sys
import random
import numpy as np


def get_kl_test():
    choices = [50, 50, 50, 51, 51, 51, 52, 52, 52]
    list = [random.choice(choices) for i in range(11)]
    return list


def get_iopd_test():
    choices = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]
    return random.choice(choices)


class Graph(PlotWidget):
    def __init__(self):
        super().__init__()
        self.setBackground('white')
        self.addLegend()
        self.showGrid(x=True, y=True)
        self.setYRange(0, 255, padding=0)


class ReadingWorker(QObject):
    update_graph = pyqtSignal(list, list, list, list)

    def __init__(self):
        super().__init__()
        self.time_from_start = 0
        self.time_values = []
        self.green_values = []
        self.blue_values = []
        self.red_values = []

    def run(self):
        self.read()
        self.update_time()

    def read(self):
        ipd_values = get_kl_test()
        iopd_value = get_iopd_test()

        self.green_values.append(ipd_values[0])
        self.blue_values.append(ipd_values[1])
        self.red_values.append(iopd_value)
        self.time_values.append(self.time_from_start)

        self.update_graph.emit(
            self.green_values, self.blue_values, self.red_values, self.time_values)
        QTimer.singleShot(1000, self.read)

    def update_time(self):
        self.time_from_start += 1
        QTimer.singleShot(1000, self.update_time)


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.central_widget = QWidget(self)
        self.setGeometry(50, 50, 1300, 700)
        self.setCentralWidget(self.central_widget)
        self.layout_main_window = QVBoxLayout()
        self.central_widget.setLayout(self.layout_main_window)

        self.layout_toolbar = QHBoxLayout()
        self.layout_toolbar.addStretch(1)
        self.btn_start = QPushButton(""Старт"")
        self.btn_start.clicked.connect(self.start)
        self.layout_toolbar.addWidget(self.btn_start)
        self.layout_main_window.addLayout(self.layout_toolbar)

        self.graph = Graph()
        self.layout_main_window.addWidget(self.graph)

        self.setup_graphs()
        self.window_size = 50

    def start(self):
        self.reading_thread = QThread(parent=self)
        self.reading_widget = ReadingWorker()
        self.reading_widget.moveToThread(self.reading_thread)
        self.reading_widget.update_graph.connect(self.draw_graph)
        self.reading_thread.started.connect(self.reading_widget.run)
        self.reading_thread.start()

    def setup_graphs(self):
        pen_ipd_1 = pyqtgraph.mkPen(color='green', width=4)
        pen_ipd_2 = pyqtgraph.mkPen(color='blue', width=4, style=Qt.DashDotLine)
        pen_iopd = pyqtgraph.mkPen(color='red', width=4, style=Qt.DashLine)
        self.line_ipd_1 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_1, name='1')
        self.line_ipd_2 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_2, name='2')
        self.line_iopd = pyqtgraph.PlotCurveItem([], [], pen=pen_iopd, name='3')
        self.graph.plotItem.addItem(self.line_ipd_1)
        self.graph.plotItem.addItem(self.line_ipd_2)
        self.graph.plotItem.addItem(self.line_iopd)

    @QtCore.pyqtSlot(list, list, list, list)
    def draw_graph(self, ipd_1_values, ipd_2_values, iopd_values, time_values):
        x, y = self.line_ipd_1.getData()
        x = np.append(x, time_values[-1])
        self.line_ipd_1.setData(y=np.append(y, ipd_1_values[-1]), x=x)
        _, y = self.line_ipd_2.getData()
        self.line_ipd_2.setData(y=np.append(y, ipd_2_values[-1]), x=x)
        _, y = self.line_iopd.getData()
        self.line_iopd.setData(y=np.append(y, iopd_values[-1]), x=x)
        if (len(x) > 0 and x[-1] -x [0] > self.window_size):
            self.graph.plotItem.setXRange(x[-1]-self.window_size, x[-1])


if __name__ == '__main__':
    app = QApplication(sys.argv)
    app.setStyle('Fusion')
    main_window = MainWindow()
    main_window.show()
    sys.exit(app.exec_())


",76392101,"Check out the MultiplePlotAxes.py example.
To add another axis on the right change the setup_graphs function and add update_views:
    def setup_graphs(self):
        pen_ipd_1 = pyqtgraph.mkPen(color='green', width=4)
        pen_ipd_2 = pyqtgraph.mkPen(color='blue', width=4, style=Qt.DashDotLine)
        pen_iopd = pyqtgraph.mkPen(color='red', width=4, style=Qt.DashLine)
        self.line_ipd_1 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_1, name='1')
        self.line_ipd_2 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_2, name='2')
        self.line_iopd = pyqtgraph.PlotCurveItem([], [], pen=pen_iopd, name='3')
        self.graph.plotItem.addItem(self.line_ipd_1)
        self.graph.plotItem.addItem(self.line_ipd_2)

        self.vb = pyqtgraph.ViewBox()
        self.pi = self.graph.plotItem
        self.pi.showAxis('right')
        self.pi.scene().addItem(self.vb)
        self.pi.getAxis('right').linkToView(self.vb)
        self.vb.setXLink(self.pi)

        self.update_views()
        self.pi.vb.sigResized.connect(self.update_views)
        self.vb.addItem(self.line_iopd)

        self.pi.setYRange(0,200)
        self.vb.setYRange(0,0.5)
        self.graph.plotItem.legend.addItem(self.line_iopd, self.line_iopd.name())

    def update_views(self):
        self.vb.setGeometry(self.pi.vb.sceneBoundingRect())
        self.vb.linkedViewChanged(self.pi.vb, self.vb.XAxis)

Result:

Edit to simulatiously scale both y-axes (there might also be something build in, padding is really annoing here):
    def setup_graphs(self):
        pen_ipd_1 = pyqtgraph.mkPen(color='green', width=4)
        pen_ipd_2 = pyqtgraph.mkPen(color='blue', width=4, style=Qt.DashDotLine)
        pen_iopd = pyqtgraph.mkPen(color='red', width=4, style=Qt.DashLine)
        self.line_ipd_1 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_1, name='1')
        self.line_ipd_2 = pyqtgraph.PlotCurveItem([], [], pen=pen_ipd_2, name='2')
        self.line_iopd = pyqtgraph.PlotCurveItem([], [], pen=pen_iopd, name='3')
        self.graph.plotItem.addItem(self.line_ipd_1)
        self.graph.plotItem.addItem(self.line_ipd_2)

        self.vb = pyqtgraph.ViewBox()
        self.pi = self.graph.plotItem
        self.pi.showAxis('right')
        self.pi.scene().addItem(self.vb)
        self.pi.getAxis('right').linkToView(self.vb)
        self.vb.setXLink(self.pi)

        self.update_views()
        self.pi.vb.sigResized.connect(self.update_views)
        self.vb.addItem(self.line_iopd)

        self.pi.setYRange(0,255, padding=0)
        self.vb.setYRange(0,0.5, padding=0)

        self.align = None
        self.update_secondary()
        self.pi.vb.sigYRangeChanged.connect(self.update_secondary)

        self.graph.plotItem.legend.addItem(self.line_iopd, self.line_iopd.name())

    def update_views(self):
        self.vb.setGeometry(self.pi.vb.sceneBoundingRect())
        self.vb.linkedViewChanged(self.pi.vb, self.vb.XAxis)

    def update_secondary(self):
        if self.align is None:
            self.align = [self.pi.getAxis('left').range, self.pi.getAxis('right').range]
        factor = (self.align[1][1]-self.align[1][0])/(self.align[0][1]-self.align[0][0])
        newRangeLeft = self.pi.getAxis('left').range
        newRangeRightMin = self.align[1][0]-(self.align[0][0]-newRangeLeft[0])*factor
        newRangeRightMax = self.align[1][1]+(newRangeLeft[1]-self.align[0][1])*factor
        self.vb.setYRange(newRangeRightMin, newRangeRightMax, padding=0)

",
76389456,Finding keys with the minimum values in each group in a dictionary with jumps (Python),"I have a dictionary where jumps happen in its keys. How can I find the key in between each group where the value is minimum?
For example, I have
myDict = {
        0.98:0.001,
        1.0:0.002,
        1.02: 0.0001,
        3.52:0.01,
        3.57:0.004,
        3.98: 0.005,
        4.01: 0.02,
        6.87: 0.01,
        6.90:0.02,
        6.98:0.001,
        7.0: 0.02
}

My desired output would be 1.02, 3.57, 6.98.
The actual dictionary I'm working with has over 1000 items.
",76391095,"Here is a solution, supposing the dictionary is sorted in ascending order according to key (explanations in the comments of the code):
def main():
    d = {
        0.98: 0.001, 1.0: 0.002, 1.02: 0.0001,
        3.52: 0.01, 3.57: 0.004, 3.98: 0.005, 4.01: 0.02,
        6.87: 0.01, 6.90: 0.02, 6.98: 0.001, 7.0: 0.02
    }

    all_groups = []  # list to store the groups
    minimums = []  # list to store all mins
    # initializing holders for minimum key and value
    min_k = 1000
    min_v = 0

    for k, v in d.items():
        # an if-statement just to add the first group with key inside
        if len(all_groups) == 0:
            all_groups.append([k])
            min_k = k
            min_v = d.get(k)
        else:
            # check if the difference is less or equal to 1
            if k - all_groups[-1][-1] <= 1.0:
                all_groups[-1].append(k)

                # each time we add a key to a group, we check if it is the minimum
                if d.get(k) < min_v:
                    min_k = k
                    min_v = d.get(k)
            else:
                minimums.append((min_k, min_v))

                # we append a new list with the new key inside to `all_groups`
                # in which we will store the next elements
                all_groups.append([k])
                min_k = k
                min_v = d.get(k)

    minimums.append((min_k, min_v))  # adding last minimums because for loop ends without adding them

    for i in minimums:
        print(i[0])  # 1.02, 3.57, 6.98


if __name__ == ""__main__"":
    main()

",
76391230,Change the font size of the output of python code chunk,"Consider the following quarto document:
---
title: ""Untitled""
format: pdf
---

```{python}
#|echo: false
#|result: 'asis'

import pandas as pd

df = pd.DataFrame({'A': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'],
    'B': ['one', 'one', 'two', 'two', 'one', 'one'],
    'C': ['dull', 'dull', 'shiny', 'shiny', 'dull', 'dull'],
    'D': [1, 3, 2, 5, 4, 1]})

print(df)
```

How to scale down the output of the python chunk, say, to 50%. Is that possible?
",76392528,"I can suggest two approaches to do this, use whatever suits you!
Option 01
Code chunk outputs are wrapped inside the verbatim environment. So to change the font size for a single code chunk, one option could be redefining the verbatim environment to have a smaller font size just before that code chunk and then again redefining the verbatim environment to get the default font size for later code chunk outputs.
---
title: ""Untitled""
format: pdf
---

\let\oldvrbtm\verbatim
\let\endoldvrbtm\endverbatim

<!-- % redefine the verbatim environment -->
\renewenvironment{verbatim}{\tiny\oldvrbtm}{\endoldvrbtm}

```{python}
#|echo: false
#|result: 'asis'

import pandas as pd

df = pd.DataFrame({'A': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'],
    'B': ['one', 'one', 'two', 'two', 'one', 'one'],
    'C': ['dull', 'dull', 'shiny', 'shiny', 'dull', 'dull'],
    'D': [1, 3, 2, 5, 4, 1]})

print(df)
```

<!-- % redefine the environment back to normal -->
\renewenvironment{verbatim}{\oldvrbtm}{\endoldvrbtm}

```{python}
#|echo: false
#|result: 'asis'

print(df)
```

Option 02
This idea actually is actually taken from this answer on TeXStackExchange. Here A command is defined to control the verbatim font size. So change the font sizes as needed.
---
title: ""Untitled""
format: pdf
include-in-header: 
  text: |
    \makeatletter
    \newcommand{\verbatimfont}[1]{\renewcommand{\verbatim@font}{\ttfamily#1}}
    \makeatother
---

\verbatimfont{\tiny}

```{python}
#|echo: false
#|result: 'asis'

import pandas as pd

df = pd.DataFrame({'A': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'],
    'B': ['one', 'one', 'two', 'two', 'one', 'one'],
    'C': ['dull', 'dull', 'shiny', 'shiny', 'dull', 'dull'],
    'D': [1, 3, 2, 5, 4, 1]})

print(df)
```

\verbatimfont{\normalsize}

```{python}
#|echo: false
#|result: 'asis'

print(df)
```

Note: The predefined font sizes that you can use in both of the above options are, \Huge, \huge, \LARGE, \Large, \large, \normalsize, \small, \footnotesize, \scriptsize, \tiny


",
76387093,Why does Vue.js not update variables inside data,"Can't update variables inside data in Vue
I'm creating a project using Vue 3 as frontend.
In Dashboard.vue, a GET request will be sent to backend with token in header, in order to let the backend identify user's identity, then Vue will receive the response with a json including info like this: {'uid': 'xxx', 'username': 'xxx'}.
My Dashboard.vue:
<script>
import axios from 'axios';

export default {
    data() {
        return {
            uid: '',
            username: '',
            temp: {},
            loaded: false
        }
    },
    methods: {
        require_get(url) {
            var token = localStorage.getItem('token');
            var config = {
                headers: {
                    'token': token,
                }
            };
            var _url = 'user/dashboard/' + url;
            axios.get(_url, config)
            .then(response => {
                this.temp = response.data;
            })
        },
        get_user_info() {
            this.require_get('info');
            this.uid = this.temp['username']; 
            this.username = this.temp['username'];
        }
    },

    mounted() {
        this.get_user_info();
    }
}
</script>

In this way, uid and username cannot be updated correctly.
For debugging, when I add console.log(this.uid) at the end of get_user_info() like this:
//...
this.require_get('info');
this.uid = this.temp['username']; 
this.username = this.temp['username'];
console.log(this.temp['uid']);

I get a undefined. But when I add console.log(this.uid) at the end of require.get() like this:
//...
.then(response => {
                this.temp = response.data;
                console.log(this.temp['uid]);
            })

The output shows that variable uid has already been updated at this moment.
After testing, I found that I can correctly update uid and username as long as I put
this.uid = this.temp['username']; 
this.username = this.temp['username'];

inside require_get().
Why is that? And how can I manage to update these variables with these two codes staying in get_user_info()?
Update
I changed my codes into""
    methods: {
        async require_get(url) {
            var token = localStorage.getItem('token');
            var config = {
                headers: {
                    'token': token,
                }
            };
            var _url = 'user/dashboard/' + url;
            axios.get(_url, config)
            .then(response => {
                return response.data;
            })
        },
        async get_user_info() {
            let info = await this.require_get('info');
            console.log(info);
            this.uid = info['username']; 
            this.username = info['username'];
        }
    },

    mounted() {
        this.get_user_info('info');
    }

and the output of console.log(info) is still undefined, now I don't understand...
",76392720,"The issue is asynchronous execution.
In require_get, you update this.temp in the .then() callback, which is only called when the Promise has resolved. In get_user_info, you are calling require_get and then immediately (synchronously) trying to read the data. Because it is fetched and set asynchronously, it is not yet present and you get undefined.
To fix it, make require_get an async function and return the data instead of using this.temp. Then make get_user_info an async function as well, await the call of require_get, and assign this.uid and this.username to the returned data.
You could also use .then() if you prefer that to async/await. I have included examples for both.
I assume that this.uid = data['username'] was meant to be this.uid = data['uid'], so changed that too.
With async/await
import axios from 'axios';

export default {
    data() {
        return {
            uid: '',
            username: '',
            loaded: false
        }
    },
    methods: {
        async require_get(url) {
            var token = localStorage.getItem('token');
            var config = {
                headers: {
                    'token': token,
                }
            };
            var _url = 'user/dashboard/' + url;
            var response = await axios.get(_url, config);
            return response.data;
        },
        async get_user_info() {
            var data = await this.require_get('info');
            this.uid = data['uid']; 
            this.username = data['username'];
        }
    },

    mounted() {
        this.get_user_info();
    }
}

With .then()
import axios from 'axios';

export default {
    data() {
        return {
            uid: '',
            username: '',
            loaded: false
        }
    },
    methods: {
        require_get(url) {
            var token = localStorage.getItem('token');
            var config = {
                headers: {
                    'token': token,
                }
            };
            var _url = 'user/dashboard/' + url;
            return axios.get(_url, config).then((response) => response.data);
        },
        get_user_info() {
            this.require_get('info').then((data) => {
                this.uid = data['uid']; 
                this.username = data['username'];
            });
        }
    },

    mounted() {
        this.get_user_info();
    }
}

",
76387206,Resolving a library projects SASS files from an application in the same workspace?,"Is it possible to resolve SASS contained in a workspace library using an approach that is similar to resolving ts files from an application within the same workspace?  For context I'll setup a real workspace as follows:
ng new theme-workspace --create-application=false
cd theme-workspace
ng g library theme
mkdir projects/theme/src/lib/styles
touch projects/theme/src/lib/styles/index.scss
ng g application playground

Within the directory projects/theme/src/lib/styles we will add the following content to index.scss.
$color: red;

And in order to include the style assets we need to update ng-package.json with an asset block like this:
  ""assets"": [
    { ""input"": ""src/lib/styles"", ""glob"": ""**/*.scss"", ""output"": ""styles"" }
  ]

If we build this project library like this:
ng build theme

We see that dist/theme/styles contains index.scss.
We can access the generated ts component ThemeComponent like this from the playground.
import { ThemeComponent } from 'theme';


When using @use to import the SASS index.scss module is it possible to use a similar namespace?
For example if we try this from the playground styles.scss module it fails:
@use `theme/styles` as t;

This is the error.
SassError: Can't find stylesheet to import.
  ╷
2 │ @use 'theme/styles' as t;

Now we could resolve by using a relative file import, but I'm curious whether there's a ""Shorthand"" way of doing it that uses the library name space?
",76392909,"Currently this is not supported, but there is a feature request for it.
",
76389947,"How to increase the compression ratio of a JPEG2000 file with ""avenc_jpeg2000"" GStreamer encoder?","I'm using the plugin avenc_jpeg2000, from gst-libav module, combined with videotestsrc and  filesink plugins for encoding a raw picture to a JPEG2000 picture:
gst-launch-1.0 videotestsrc num-buffers=1 ! avenc_jpeg2000 ! filesink location=/tmp/picture-ref.jp2

This pipeline works and produce a 31.85 KiB (32,616) file.
enter image description here
Now, I want to divide the size of my output file by two by increasing the compression ratio of the encoder avenc_jpeg2000. To achieve this, I want to minimize the number of bits required to represent the image with an allowable level of distortion. I know JPEG2000 standard support lossless and lossy compression mode. For my use case, the lossy compression mode is acceptable.
How should I proceed to increase the compression of my output file ? What encoder's properties should I play with for doing that ?
My test configuration:

i.MX 8M Plus

GStreamer 1.18.0

libav 1.18.0 (Release date: 2020-09-08)


I tried to play with ""bitrate"" and ""bitrate-tolerance"" properties, but it seems to have no effect on the size of the output file:
gst-launch-1.0 videotestsrc num-buffers=1 ! avenc_jpeg2000 bitrate=100000 bitrate-tolerance=10000 ! filesink location=/tmp/picture-test-01.jp2

I compare files by doing a checksum with sha224sum command :
d0da9118a9c93a0420d6d62f104e0d99fe6e50cda5e87a46cef126f9  /tmp/picture-ref.jp2

d0da9118a9c93a0420d6d62f104e0d99fe6e50cda5e87a46cef126f9  /tmp/picture-test-01.jp2

",76391113,"For lossy compression, you can increase the value of the quantization.
First, set the encoding type of the encoder to ""Constant Quantizer"" and then, find an appropriated quantizer value.
In my case, to produce a 15 KiB file, I used the following pipeline:
gst-launch-1.0 videotestsrc num-buffers=1 ! avenc_jpeg2000 pass=2 quantizer=10  ! filesink location=/tmp/picture-test-02.jp2

",
76392182,How to execute code every time a Composable is shown (and execute only once),"I am creating a Composable which is responsible for showing notifications to users. Every time the user goes to that Composable, I want to execute a query which will clear the notification count. I only want to execute that query when the Composable has appeared, not every time the Composable is recomposed due to configuration change and anything.
Essentially I am looking for an equivalent of https://developer.apple.com/documentation/swiftui/view/onappear(perform:).
Is there any method I can use in Jetpack Compose?
",76392541,"Use LaunchedEffect (link)
LaunchedEffect(Unit) {
    // Actions to perform when LaunchedEffect enters the Composition 
}

It takes one or more key parameters that are used to cancel the running effect and start a new one. Since you need to execute your code only once use something immutable as a key like Unit or true.
",
76387490,What needs to be done to make the Bootstrap 5.3.0 Collapsible button show in this simple web page?,"I've tried to create a simple web page that uses the Bootstrap (version 5.3.0) Collapsible and I cannot get it to work no matter what I try.
All I want is a Collapsible that 'holds' a few links in it which are shown when you click it.
But when that didn't work I simplified it to just a list of strings (code shown below) but the problem is still there.
The problem is that the Collapsible button does not show no matter what I try.
The <span class=""navbar-toggler-icon""></span> is not visible. I've added 'X' to both sides
of it just so there is an indication where that invisible button is.
Anyone knows how to fix that, make the Collapsible button visible?
Thanks.
Here's the code:


<link rel=""stylesheet"" href=""https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"">
<script src=""https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js""></script>

<header class=""bg-primary text-white text-center py-3"">
  <div class=""container"">
    <div class=""row"">
      <div class=""col-md-4"">
        <div class=""left-header"">
          <h3>Left Header</h3>
        </div>
      </div>
      <div class=""col-md-4"">
        <div class=""central-header"">
          <h3>Central Header</h3>
        </div>
      </div>
      <div class=""col-md-4"">
        <div class=""right-header"">
          <button class=""navbar-toggler"" type=""button"" data-bs-toggle=""collapse"" data-bs-target=""#collapsibleNav"">
              X<span class=""navbar-toggler-icon""></span>X
            </button>
        </div>
        <div class=""collapse"" id=""collapsibleNav"">
          <div class=""card card-body"">
            <ul class=""nav"">
              <li class=""nav-item"">Item 1</li>
              <li class=""nav-item"">Item 2</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</header>

<main class=""container mt-4"">
  <h2>Main Content</h2>
  <p>This is the main content of the page.</p>
</main>



",76393307,"Try adding .navbar selector to the parent container: right-header navbar, because it's actually coming from that class (--bs-navbar-toggler-icon-bg), not the icon, the hamburger is actually a variable defined in .navbar, so by adding it to the parent it becomes accessible:
.navbar-toggler-icon {
    background-image: var(--bs-navbar-toggler-icon-bg);
}

.navbar {
--bs-navbar-toggler-icon-bg: url(""data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'%3e%3cpath stroke='rgba%2833, 37, 41, 0.75%29' stroke-linecap='round' stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e"");
}



<link rel=""stylesheet"" href=""https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"">
<script src=""https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js""></script>

<header class=""bg-primary text-white text-center py-3"">
  <div class=""container"">
    <div class=""row"">
      <div class=""col-md-4"">
        <div class=""left-header"">
          <h3>Left Header</h3>
        </div>
      </div>
      <div class=""col-md-4"">
        <div class=""central-header"">
          <h3>Central Header</h3>
        </div>
      </div>
      <div class=""col-md-4"">
        <div class=""right-header navbar"">
          <button class=""navbar-toggler mx-auto"" type=""button"" data-bs-toggle=""collapse"" data-bs-target=""#collapsibleNav"">
              X<span class=""navbar-toggler-icon""></span>X
            </button>
        </div>
        <div class=""collapse"" id=""collapsibleNav"">
          <div class=""card"">
            <ul class=""nav card-body"">
              <li class=""nav-item"">Item 1</li>
              <li class=""nav-item"">Item 2</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</header>

<main class=""container mt-4"">
  <h2>Main Content</h2>
  <p>This is the main content of the page.</p>
</main>



",
76389994,I have a Blazor server app with windows authentication. But the username and password dialog is just coming when page is opened from the server side,"I have a Blazor server app (.Net6) with windows authentication on a Win 2019 Server with IIS. But the username and password dialog is just coming when I open the web page from the google chrome browser directly on the server where the Blazor app is running. When I try to open the page from another PC in the same domain (Chrome, Firefox...), then the page is opening directly without asking username and password.
What could be the reason for that?
",76391123,"The reason you are not prompted for a username and password when accessing the Blazor server app from another PC in the same domain is likely due to the integrated Windows authentication and the Single Sign-On (SSO) capabilities of the browser and the server. Here's a brief explanation of how this works:

Integrated Windows Authentication: When using integrated Windows authentication, the browser automatically sends the user's Windows credentials (username and password) to the server without prompting the user for them. This is possible because the browser and the server negotiate the authentication process using various protocols like Kerberos or NTLM.
Single Sign-On (SSO): SSO allows users to authenticate once and then access multiple resources without being prompted for credentials again. In your case, since the client PC and the server are in the same domain, and integrated Windows authentication is enabled, the user's credentials from the client PC's Windows session are automatically passed to the server without requiring a separate prompt.

To ensure that the username and password prompt appears consistently, you can try the following:

Check Browser Settings: Make sure that the browser settings on the client PC are configured to send Windows credentials. In Chrome, go to Settings > Privacy and Security > Site Settings > Additional permissions > Manage permissions. Ensure that the server's URL is listed and set to ""Allow"" or ""Automatic.""
Verify IIS Settings: Ensure that the IIS configuration on the server is set up correctly for integrated Windows authentication. Open IIS Manager, select the Blazor server app, and go to the Authentication settings. Make sure that only Windows Authentication is enabled, and other authentication methods like Anonymous Authentication are disabled.
Cross-Domain Considerations: If the Blazor server app is hosted on a different domain, you may encounter additional challenges with SSO due to browser security restrictions. In such cases, you may need to configure your server and browser settings to enable cross-domain SSO.
Clear Browser Cache: Clear the cache and cookies in the browser on the client PC to ensure that any cached credentials or settings are not causing unexpected behavior.

By checking these settings and ensuring that integrated Windows authentication is correctly configured on the server and the client PC, you should be able to consistently prompt for a username and password when accessing the Blazor server app.
",
76392367,Write to Google sheet skipping 1st row,"When using the package googlesheets4 is there any method for writing data to a sheet skipping the first row so that the data being written to a sheet starts at row 2? I am hoping to leverage something similar to when you read a sheet and utilize ex. skip = 2 to read data starting at the 3rd row
I have tried the following which does not work
write_sheet(data = df, ss = ""google_sheet_url"", skip = 1, sheet = ""test"")

",76392557,"skip=n is only for read_excel()
in the googlesheets4 library to start at a specified range, you'd use range_write(). This is similar to the startRow=n in the xlsx library.
range_write(ss = ""google_sheet_url"", data = df, range = ""B1"", sheet = ""test"")

",
76382024,TypeScript is unable to infer type properly from a typed string variable,"I am trying to use a ""freely"" created string as a key for an object.
interface CatState {
    name: string,
    selected: boolean,
    color: string,
    height: number
}

interface DogState{
    name: string,
    selected: boolean,
    race: string,
    age: number,
    eyeColor: string
}

export interface Animals {
    animals: {
        cat: {
            cats: CatState[],
            allSelected: boolean,
        },
        dog: {
            dogs: DogState[],
            allSelected: boolean,
        },
    }
};

const selectAnimal = (allAnimals: Animals, animal: keyof Animals['animals'], index:number) => {
    const animalPlural = `${animal}s` as keyof Animals['animals'][typeof animal]
    allAnimals.animals[animal][animalPlural][index].selected= true
}

This highlights .selected with the message

Property 'selected' does not exist on type 'boolean'.

Here is a Playground. Is there a workaround for this, or is this simply not possible?
",76393367,"In order for this to work you need to make selectAnimal generic.
You might think it should be able to deal with an animal input of a union type, but the compiler isn't able to properly type check a single block of code that uses multiple expressions that depend on the same union type.  It loses track of the correlation between `${animal}s` and allAnimals.animals[animal].  The formers is of type ""cats"" | ""dogs"" and the latter is of a type like {cats: CatState[]} | {dogs: DogState[]}, and you can't generally index into the latter with the former, because ""what if you've got {cats: CatState[]} and you're indexing with ""dogs""?""  That can't happen, but the compiler is unable to see it.  TypeScript can't directly deal with correlated unions this way.  That's the subject of microsoft/TypeScript#30581.
If you want a single code block to work for multiple cases, the types need to be refactored to use generics instead, as described in microsoft/TypeScript#47109.  Here's how it might look for your example:
interface AnimalStateMap {
    cat: CatState,
    dog: DogState
}

type AnimalData<K extends keyof AnimalStateMap> =
    { [P in `${K}s`]: AnimalStateMap[K][] & { allSelected: boolean } }

export interface Animals {
    animals: { [K in keyof AnimalStateMap]: AnimalData<K> };
};
    

const selectAnimal = <K extends keyof AnimalStateMap>(
    allAnimals: Animals, animal: K, index: number) => {
    const animalPlural = `${animal}s` as const;
    // const animalPlural: `${K}s`
    const animalData: AnimalData<K> = allAnimals.animals[animal]
    animalData[animalPlural][index].selected = true;
}

The AnimalStateMap is a basic key-value type representing the underlying relationship in your data structure.  Then AnimalData<K> is a mapped type that encodes as a template literal type the concatenation of s onto the type of the keys (giving such plurals as gooses and fishs 🤷‍♂️) and that the value type is of the expected animal array.  And that there's an allSelected property.
Then your Animals type explicitly written as a mapped type over keyof AnimalStateMap, which will help the compiler see the correlation when we index into it.
Finally, selectAnimal is generic in K extends keyof AnimalStateMap and the body type checks because animalPlural is of just the right generic type `${K}s` which is known to be a key of animalData, which is AnimalData<K>.
Playground link to code
",
76390577,Kubernetes: Frequently unable to communicate with kublet api (connection refused),"I'm deploying a new kubernetes cluster on a single node (Ubuntu 22.04)
The problem is I frequently get this error when running any kubectl commands (hostnames changed)
The connection to the server k8cluster.example.com:6443 was refused - did you specify the right host or port?
After I installed kubernetes (via apt install -y kubelet kubeadm kubectl) everything was stable, but obviously the node was not in a ready state. The problems started as soon as i deployed the Flannel container network, which I did as follows:
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Pods in the kube-system name space are frequently restarting
root@k8cluster:~/.ssh# kubectl get all -A
NAMESPACE      NAME                                                 READY   STATUS             RESTARTS         AGE
kube-flannel   pod/kube-flannel-ds-6h6zq                            1/1     Running            25 (46s ago)     98m
kube-system    pod/coredns-5d78c9869d-gmdpv                         0/1     CrashLoopBackOff   18 (4m40s ago)   130m
kube-system    pod/coredns-5d78c9869d-zhvxk                         1/1     Running            19 (14m ago)     130m
kube-system    pod/etcd-k8cluster.example.com                       1/1     Running            31 (7m21s ago)   130m
kube-system    pod/kube-apiserver-k8cluster.example.com            1/1     Running            37 (5m40s ago)   131m
kube-system    pod/kube-controller-manager-k8cluster.example.com    0/1     Running            46 (5m10s ago)   130m
kube-system    pod/kube-proxy-nvnkf                                 0/1     CrashLoopBackOff   41 (100s ago)    130m
kube-system    pod/kube-scheduler-k8cluster.example.com             0/1     CrashLoopBackOff   44 (4m43s ago)   129m

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  132m
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   132m

NAMESPACE      NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-flannel   daemonset.apps/kube-flannel-ds   1         1         1       1            1           <none>                   98m
kube-system    daemonset.apps/kube-proxy        1         1         1       1            1           kubernetes.io/os=linux   132m

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/coredns   2/2     2            2           132m

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/coredns-5d78c9869d   2         2         2       130m

I'm seeing these errors when running journalctl -u kubelet
Jun 02 13:16:21 k8cluster.example.com kubelet[19340]: I0602 13:16:21.848785   19340 scope.go:115] ""RemoveContainer"" containerID=""4da5cc966a4dcf61001cbdbad36c47917fdfeb05bd7c4c985b2f362efa92f464""
Jun 02 13:16:21 k8cluster.example.com kubelet[19340]: I0602 13:16:21.849006   19340 status_manager.go:809] ""Failed to get status for pod"" podUID=aae126ec9b57a8789f7682f92e81bd7a pod=""kube-system/etcd-k8cluster.example.com"" err=""Get \""https://k8cluster.example.com:6443/api/v1/namespaces/kube-system/pods/etcd-k8cluster.example.com\"": dial tcp 172.31.37.108:6443: connect: connection refused""
Jun 02 13:16:21 k8cluster.example.com kubelet[19340]: E0602 13:16:21.849262   19340 pod_workers.go:1294] ""Error syncing pod, skipping"" err=""failed to \""StartContainer\"" for \""kube-apiserver\"" with CrashLoopBackOff: \""back-off 2m40s restarting failed container=kube-apiserver pod=kube-apiserver-spotcluster.infdev.org_kube-system(ccdffaba21456689fa71a8f7b182fb0c)\"""" pod=""kube-system/kube-apiserver-k8cluster.example.com"" podUID=ccdffaba21456689fa71a8f7b182fb0c
Jun 02 13:16:21 k8cluster.example.com kubelet[19340]: I0602 13:16:21.849317   19340 status_manager.go:809] ""Failed to get status for pod"" podUID=ccdffaba21456689fa71a8f7b182fb0c pod=""kube-system/kube-apiserver-k8cluster.example.com"" err=""Get \""https://k8cluster.example.com:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8cluster.example.com\"": dial tcp 172.31.37.108:6443: connect: connection refused""
Jun 02 13:16:21 k8cluster.example.com kubelet[19340]: I0602 13:16:21.866932   19340 scope.go:115] ""RemoveContainer"" containerID=""46f9e127efbd2506f390486c2590232e76b0617561c7c440d94c470a4164448f""
Jun 02 13:16:21 k8cluster.example.com kubelet[19340]: E0602 13:16:21.867259   19340 pod_workers.go:1294] ""Error syncing pod, skipping"" err=""failed to \""StartContainer\"" for \""coredns\"" with CrashLoopBackOff: \""back-off 5m0s restarting failed container=coredns pod=coredns-5d78c9869d-gmdpv_kube-system(ddf0658a-260b-41d1-a0a0-595de4991ec6)\"""" pod=""kube-system/coredns-5d78c9869d-gmdpv"" podUID=ddf0658a-260b-41d1-a0a0-595de4991ec6
Jun 02 13:16:22 k8cluster.example.com kubelet[19340]: I0602 13:16:22.850577   19340 scope.go:115] ""RemoveContainer"" containerID=""4da5cc966a4dcf61001cbdbad36c47917fdfeb05bd7c4c985b2f362efa92f464""

Also dmesg is showing these messages:

[Fri Jun  2 13:02:11 2023] IPv6: ADDRCONF(NETDEV_CHANGE): veth11eea1b5: link becomes ready
[Fri Jun  2 13:02:11 2023] cni0: port 1(veth11eea1b5) entered blocking state
[Fri Jun  2 13:02:11 2023] cni0: port 1(veth11eea1b5) entered forwarding state
[Fri Jun  2 13:11:54 2023] cni0: port 2(veth92694dfb) entered disabled state
[Fri Jun  2 13:11:54 2023] device veth92694dfb left promiscuous mode
[Fri Jun  2 13:11:54 2023] cni0: port 2(veth92694dfb) entered disabled state
[Fri Jun  2 13:11:55 2023] cni0: port 2(veth29e5e0d3) entered blocking state
[Fri Jun  2 13:11:55 2023] cni0: port 2(veth29e5e0d3) entered disabled state
[Fri Jun  2 13:11:55 2023] device veth29e5e0d3 entered promiscuous mode
[Fri Jun  2 13:11:55 2023] cni0: port 2(veth29e5e0d3) entered blocking state
[Fri Jun  2 13:11:55 2023] cni0: port 2(veth29e5e0d3) entered forwarding state
[Fri Jun  2 13:11:55 2023] IPv6: ADDRCONF(NETDEV_CHANGE): veth29e5e0d3: link becomes ready
[Fri Jun  2 13:13:19 2023] cni0: port 1(veth11eea1b5) entered disabled state
[Fri Jun  2 13:13:19 2023] device veth11eea1b5 left promiscuous mode
[Fri Jun  2 13:13:19 2023] cni0: port 1(veth11eea1b5) entered disabled state
[Fri Jun  2 13:13:20 2023] cni0: port 1(veth1f7fb9e0) entered blocking state
[Fri Jun  2 13:13:20 2023] cni0: port 1(veth1f7fb9e0) entered disabled state
[Fri Jun  2 13:13:20 2023] device veth1f7fb9e0 entered promiscuous mode
[Fri Jun  2 13:13:20 2023] cni0: port 1(veth1f7fb9e0) entered blocking state
[Fri Jun  2 13:13:20 2023] cni0: port 1(veth1f7fb9e0) entered forwarding state
[Fri Jun  2 13:13:20 2023] IPv6: ADDRCONF(NETDEV_CHANGE): veth1f7fb9e0: link becomes ready

If I look at the logs for the kube-apiserver pod I see this repeating itself.
 Err: connection error: desc = ""transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused""
W0602 13:21:03.884015       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  ""Addr"": ""127.0.0.1:2379"",
  ""ServerName"": ""127.0.0.1"",
  ""Attributes"": null,
  ""BalancerAttributes"": null,
  ""Type"": 0,
  ""Metadata"": null

Any ideas?
",76391128,"It seems I was having the same problem as mentioned in this question
Unable to bring up kubernetes API server
The solution here worked for me
containerd config default | tee /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml  
service containerd restart
service kubelet restart

",
76392163,How can I create a stamina meter that depletes as I sprint and recharges after waiting in Unity using C#?,"I'm trying to make a stamina meter for my game that depletes as you sprint and once it hits zero you have to wait for it to charge up again before sprinting, I don't know what I should do to fix the code and don't know where I went wrong, thanks.
float moveSpeed = 5f;
    float sprintSpeed = 8f;
    float maxStamina = 5f;
    float currentStamina = 0f;
    private void Movement()
    {
       float movementX = Input.GetAxis(""Horizontal"");
       float movementZ = Input.GetAxis(""Vertical"");`

       Vector3 moveDirection = new Vector3(movementX, 0f, movementZ);
       moveDirection = camera.forward * movementZ + camera.right * movementX;
       if (Input.GetKey(KeyCode.LeftShift) && currentStamina != maxStamina)
       {
          transform.position += (moveDirection).normalized * sprintSpeed * Time.deltaTime;
          currentStamina += Time.deltaTime;

          if (currentStamina >= maxStamina)
          {
             transform.position += (moveDirection).normalized * moveSpeed * Time.deltaTime;
             new WaitForSeconds(5f);
             currentStamina = 0f;
          }
       }
       transform.position += (moveDirection).normalized * moveSpeed * Time.deltaTime;
     }

whole script just in case:
private Transform transform;
    private Rigidbody rb;
    [SerializeField] private Transform camera;

    float moveSpeed = 5f;
    float sprintSpeed = 8f;
    float maxStamina = 5f;
    float currentStamina = 0f;
    float increaseStaminaPerSecond = 1f;
    void Start()
    {
        transform = GetComponent<Transform>();
        rb = GetComponent<Rigidbody>();
        rb.freezeRotation = true;
        currentStamina = maxStamina;
    }
    void Update()
    {
        Movement();
    }
    private void Movement()
    {
        float movementX = Input.GetAxis(""Horizontal"");
        float movementZ = Input.GetAxis(""Vertical"");

        Vector3 moveDirection = new Vector3(movementX, 0f, movementZ);
        moveDirection = camera.forward * movementZ + camera.right * movementX;
        if (Input.GetKey(KeyCode.LeftShift) && currentStamina != maxStamina)
        {
            transform.position += (moveDirection).normalized * sprintSpeed * Time.deltaTime;
            currentStamina += Time.deltaTime;

            if (currentStamina >= maxStamina)
            {
                transform.position += (moveDirection).normalized * moveSpeed * Time.deltaTime;
                new WaitForSeconds(5f);
                currentStamina = 0f;
            }
        }


        transform.position += (moveDirection).normalized * moveSpeed * Time.deltaTime;
    }

",76392619,"Encapsulate and simplify your logic so its easier to read and to maintain.
You need to introduce a few variables that will help you keep things clean and more readable.
Additionally, when unsure, write things as if you would talk in a conversation so that they make sense. For example, you wouldn't be able to sprint if you had no stamina, and not if your stamina wasn't ""full"", so write that in code if (currentStamina > 0) canSprint, etc.
private Transform transform;
private Rigidbody rb;
[SerializeField] private Transform camera;

float moveSpeed = 5f;
float normalSpeed = 5f; // add this
float sprintSpeed = 8f;
float maxStamina = 5f;
float currentStamina = 0f;
float increaseStaminaPerSecond = 1f;

bool isSprinting = false; // add this to check if player is sprinting
bool canSprint = true; // add this to check if player can sprint

void Start()
{
    transform = GetComponent<Transform>();
    rb = GetComponent<Rigidbody>();
    rb.freezeRotation = true;
    currentStamina = maxStamina;
}
void Update()
{
    UpdateStamina();
    Sprint();
    Movement();
}

// simplify logic to do one thing
private void Movement()
{
    Vector3 movementInput = GetMovementInput();
    Vector3 moveDirection = camera.forward * movementInput.z + camera.right * movementInput.y;
    transform.position += moveDirection.normalized * moveSpeed * Time.deltaTime; // apply movement only once
}

// separate sprinting logic
private void Sprint()
{
    if (Input.GetKey(KeyCode.LeftShift) && canSprint && currentStamina > 0)
    {
        moveSpeed = sprintSpeed;
        isSprinting = true;
    }
    else
    {
        moveSpeed = normalSpeed;
        isSprinting = false;
    }
}

private void UpdateStamina()
{
    float staminaToAdd = isSprinting ? -Time.deltaTime : Time.deltaTime; // if sprinting, decrease stamina by Time.deltaTime, otherwise increase
    currentStamina = Mathf.Clamp(currentStamina + staminaToAdd, 0, maxStamina); // prevent from going below 0 and over maxStamina

    if (currentStamina <= 0 && canSprint) // if no more stamina but could sprint up to this point
    {
        StartCoroutine(SprintCooldownRoutine()); // start cooldown
    }
}

private Vector3 GetMovementInput()
{
    float movementX = Input.GetAxis(""Horizontal"");
    float movementZ = Input.GetAxis(""Vertical"");

    return new Vector3(movementX, 0f, movementZ);
}

private IEnumerator SprintCooldownRoutine()
{
    canSprint = false; // disable sprinting

    while (currentStamina < maxStamina)
    {
        currentStamina += Time.deltaTime; // this will increase stamina twice as fast, because we're already increasing it inside UpdateStamina. Remove this line if you wish to ""just wait for stamina to refresh""
        yield return null;
    }
    
    canSprint = true;
}

",
76381814,How to share a variable in Cypress from one test (it) to another when the domains are different?,"How to share a variable from one test (it) to another when the domains are different?
I've tried in countless ways, with Alias, Closure, Environment Variable, Local Storage, even with Event Listener, but when the next test is executed, these variables are cleared from memory.
The point is that I need to obtain the ID of an open protocol in a Web application, go to the backoffice that is in another domain to validate if that protocol was really opened.
Here is the last version after giving up...
/// <reference types=""cypress"" />

describe(""Testar abertura de protocolo no fale conosco"", () => {
    it(""Deve acessar o FaleConosco, abrir um protocolo e depois validar no backoffice a abertura correta do mesmo"", () => {
        cy.visit(`${Cypress.env('FALE_CONOSCO_URL')}`)
        cy.get(""#BotaoCriarNovoChamado"").click()
        
        cy.get('#InputLabelCpfCnpj').type(""99999999999"")
        cy.get('#InputLabelEmail').type(""email@email.com"")
        cy.get('#InputLabelTelefone').type(""99999999999"")
        cy.get('#InputLabelAssunto').type(""Assunto de teste"")
        cy.get('#InputLabelDescricao').type(""Essa aqui e uma descrição bem detalhada, confia"")
        cy.get('#BotaoEnviar').click()

        cy.get('#spanNumeroDoChamado').should('contain', 'Número do chamado')
        cy.get('#divNumeroDoChamado').then($div => {
            const numero_do_chamado = $div.text().split(' ')[3].replace(/^#/, """");
            // cy.wrap(numero_do_chamado).as(""minhaVariavel"");

            // Enviar o valor do alias para o segundo domínio usando postMessage
            cy.window().then((win) => {
                win.postMessage({ type: ""aliasValue"", value: numero_do_chamado }, ""*"");
            });
            // Cypress.env('numero_do_chamado', numero_do_chamado);
            // cy.log(""numero_do_chamado  -  "" + Cypress.env('numero_do_chamado'));
            // cy.window().then(win => {
            // win.localStorage.setItem('numero_do_chamado', numero_do_chamado);
            // });
        });


        // cy.get('#divNumeroDoChamado').invoke(""text"").as(""minhaVariavel"")
        // // ($div => {
        // //     const numero_do_chamado = $div.text().split(' ')[3].replace(/^#/, """");
        // //     cy.wrap(numero_do_chamado).as(""minhaVariavel"");

        // // Enviar o valor do alias para o segundo domínio usando postMessage
        // cy.window().then((win) => {
        //     win.postMessage({ type: ""aliasValue"", value: cy.get(""@minhaVariavel"") }, ""*"");
        // });
        // //     // Cypress.env('numero_do_chamado', numero_do_chamado);
        // //     // cy.log(""numero_do_chamado  -  "" + Cypress.env('numero_do_chamado'));
        // //     // cy.window().then(win => {
        // //     // win.localStorage.setItem('numero_do_chamado', numero_do_chamado);
        // //     // });
        // // });

    });

    it(""Deve acessar o Conecta e validar a abertura correta protocolo"", () => {
        cy.visit(`${Cypress.env('URL')}`);

        // Receber a mensagem contendo o valor do alias enviado pelo primeiro domínio
        cy.window().then((win) => {
            win.addEventListener(""message"", (event) => {
                const message = event.data;

                // Verificar se a mensagem contém o valor do alias
                if (message.type === ""aliasValue"") {
                const aliasValue = message.value;
                cy.wrap(aliasValue).as(""meuAliasCompartilhado"");
                }
            });
        });

        // Fazer algo com o alias compartilhado no segundo domínio
        cy.get(""@meuAliasCompartilhado"").then((valor) => {
            // Faça algo com o valor do alias compartilhado
            cy.log(""Valor do alias compartilhado:"", valor);
            cy.login();
            cy.visit(`${Cypress.env('URL')}/ticket-container/${Cypress.env('valor')}`)

        });

    });

})

",76393564,"When the test runner changes domains the whole browser object is reset, so any variables written to browser memory are lost

closure variables
aliases
env var (Cypress.env).

That leaves you with fixture (disk storage) or task-related pseudo data store (see bahmutov/cypress-data-session).
For fixture, the code would be
it(""Deve acessar o FaleConosco..."", () => {
  cy.visit(`${Cypress.env('FALE_CONOSCO_URL')}`)
  ...
    const numero_do_chamado = $div.text().split(' ')[3].replace(/^#/, """")
    cy.writeFile('cypress/fixtures/numero_do_chamado.json', numero_do_chamado)
    ...
})

it(""Deve acessar o Conecta..."", () => {
  cy.visit(`${Cypress.env('URL')}`)
  ...
  const numero_do_chamado = cy.readFile('cypress/fixtures/numero_do_chamado.json')
  ...
})

Don't use cy.fixture() command as there is caching involved internally. Not a problem for your current scenario, but may cause unexpected errors when your test pattern changes.
",
76389708,spark DF multiple Iterations on Rows,"From the below data- col5 is holding the no of fruits to be distributed among plates from col1 to col4(4plates). Each time find the min from the plates(col1 to col4) add 1 fruit and reduce the fruit from col5 and repeat this process till col5(fruits becomes zero). Below is some sample code to find the min and add 1 fruit there. but how to do this recursively in spark - Scala.
Expected output:

  plate1 plate2 plate3  plate4  fruits  
  1 2   3   4   3   
  6 7   8   1   2   
  2 4   6   8   5   

Iteration 1:
  2 2   3   4   2   
  6 7   8   2   1   
  3 4   6   8   4   

Iteration 2:
  3 2   3   4   1   (If 2 plates has the same min value , left precedence)
  6 7   8   3   0   
  4 4   6   8   3   

Iteration 3:
  3 3   3   4   0   
  6 7   8   3   0   
  5 4   6   8   2   

Iteration 4:
  3 3   3   4   0   
  6 7   8   3   0   
  5 5   6   8   1   

Iteration 5:
  3 3   3   4   0   
  6 7   8   3   0   
  6 5   6   8   0   

code for first iteration:
      val data = Seq(
        (1.0, 2.0, 3.0, 4.0, 3.0),
        (6.0, 7.0, 8.0, 1.0, 2.0),
        (2.0, 4.0, 6.0, 8.0, 5.0)
      )
      val columns = List(""col1"", ""col2"", ""col3"", ""col4"", ""col5"")
      val df = spark.createDataFrame(data).toDF(columns: _*)

      val updatedColumns = columns.map { colName =>
        functions.when(col(colName) === functions.least(columns.map(col): _*), col(colName) + 1).otherwise(col(colName)).alias(colName)
      }

      val updatedDF = df.select(updatedColumns: _*)
      updatedDF.show()

",76391170,"Since each row is independent, then I think it is easier to iterate within the mapping function of each row until you get the final result, so that you don't have to iterate over the whole df multiple times.
import spark.implicits._

val data = Seq(
        (1, 2, 3, 4, 3),
        (6, 7, 8, 1, 2),
        (2, 4, 6, 8, 5)
      )

val columns = List(""col1"", ""col2"", ""col3"", ""col4"", ""col5"")
val arrayDf = spark.sparkContext.parallelize(data).map(row =>  {
    var plates: List[Int] = row.productIterator.toList.map {case i: Int => i}
    var store: Int = plates.last // Assuming last column is the store 
    plates = plates.dropRight(1) // remove the store column
    (0 until store).toList.foreach(i => {
        val min = plates.min
        val index = plates.indexOf(min)
        store -=1
        plates = plates.updated(index, min + 1)
    })
    plates :+ store // Add the store at the end (which is always 0) 
}).toDF(Seq(""result""): _*)

// Convert the list of result into columns again
val df = arrayDf.select(columns.zipWithIndex.map{case (name, idx) => arrayDf(""result"")(idx).alias(name)}: _*)

df.show()

+----+----+----+----+----+
|col1|col2|col3|col4|col5|
+----+----+----+----+----+
|   3|   3|   3|   4|   0|
|   6|   7|   8|   3|   0|
|   6|   5|   6|   8|   0|
+----+----+----+----+----+

",
76391470,Powershell: -ErrorAction SilentlyContinue not working with Invoke-Expression,"I am running this script
Invoke-Expression $expression -ErrorAction SilentlyContinue
The variable $expression may not  have a value sometimes. When It's empty, I get the error

Cannot bind argument to parameter 'Command' because it is null.

How can I avoid seeing the error? I want to execute Invoke-Expression regardless of $expression being empty, so an if statement checking that $expression has a value wouldn't work.

",76392625,"
First, the obligatory warning:

Invoke-Expression  (iex) should generally be avoided and used only as a last resort, due to its inherent security risks. Superior alternatives are usually available. If there truly is no alternative, only ever use it on input you either provided yourself or fully trust - see this answer.


To add to Mathias' helpful answer and zett42's simpler alternative mentioned in a comment on the question (Invoke-Expression ""$expression ""):
These solutions silence only the case where $null or an empty string is passed to Invoke-Expression - which may well be your intent.
To also cover the case where all error output should be silenced - whether due to invalid input or due to valid input causing errors during execution - the following variation is needed:
# Silences *all* errors.
try { Invoke-Expression $expression 2>$null } catch { }

# Alternative:
# Silences *all* errors and additionally *ignores* all *non-terminating* errors, 
# i.e. not only silences them, but also prevents their recording in $Error.
# By executing inside & { ... }, the effect of setting $ErrorActionPreference is 
# transitory due to executing in a *child scope*.
# Note that this also means that $expression is evaluated in the child scope.
& { $ErrorActionPreference = 'Ignore'; Invoke-Expression $expression }

Note:

First command:

The common -ErrorAction parameter fundamentally only acts on non-terminating errors, whereas terminating ones (both statement- and script-terminating ones) must be handled with a try / catch / finally statement.

Passing $null or the empty string to Invoke-Expression causes an error during parameter binding (that is the, cmdlet itself is never invoked, because invalid arguments were passed), which in effect is a statement-terminating error - hence the need for try / catch.

The try / catch with the empty catch block additionally prevents script-terminating errors that result from the Invoke-Expression call from terminating your script too (e.g, if $expression contained something like 'throw ""Fatal Error""'



Note that -ErrorAction SilentlyContinue was replaced with 2>$null in order to silence non-terminating errors (e.g., the error resulting from Get-ChildItem NoSuchDir), because - inexplicably - -ErrorAction is not effective with Invoke-Expression (even though the -ErrorVariable common parameter does work, for instance). See GitHub issue #19734.



Second command:

Setting the $ErrorActionPreference preference variable to Ignore causes all errors to be silenced, and additionally - for non-terminating errors only - prevents their recording in the automatic $Error variable.

If -ErrorAction worked in this case, -ErrorAction would have the same effect, but would act solely on non-terminating errors, as noted.  (This asymmetry between what should be equivalent mechanisms - preference variable vs. per-call common parameter, is one of the pitfalls of PowerShell's error handling - see GitHub issue #14819).


Unfortunately, (caught) terminating errors are invariably recorded in $Error as of PowerShell 7.3.4. From what I can tell, changing this in a future version has been green-lit a while ago, but is yet to be implemented:  see GitHub issue #3768

Using &, the call operator with a script block { ... } executes the enclosed statements in a child scope.

This causes $ErrorActionPreference = 'Ignore' to create a local copy of the preference variable, which automatically goes out of scope when the script block is exited, thereby implicitly restoring the previous value of $ErrorActionPreference.

However, this also means that the code executed by Invoke-Expression executes in that child scope.
If that is undesired, forgo the & { ... } enclosure and save and restore the previous $ErrorActionPreference value.




",
76390816,VBA Selenium Dynamic ID (Wildcard),"Is there a possibility to find an element by ID with an wildcard?
I have something like this:
stat = GC.FindElementByXPath(""//*[@id='C29_W88_V90_V94_admin_status']"").Value
stat = GC.FindElementByXPath(""//*[@id='C26_W88_V90_V94_admin_status']"").Value
stat = GC.FindElementByXPath(""//*[@id='C29_W88_V12_V94_admin_status']"").Value

The admin_status part won't change but the values before.
Sometimes I have the same problem with another element with values around it. So the best thing would be to find the element with some wildcard like this:
stat = GC.FindElementByXPath(""//*[@id='*admin_status*']"").Value

",76392656,"This code is tested in Excel VBA using Selenium.
Option Explicit
Sub sbXPathContains()
    Dim driver As ChromeDriver
    Set driver = New ChromeDriver
    Dim sURL As String
    sURL = ""https://davetallett26.github.io/table.html""
    Call driver.Start(""edge"")
    driver.get (sURL)
    driver.Window.Maximize
    sbDelay (100000)
    MsgBox driver.FindElementByXPath(""//*[contains(@id, 'ctl03_txtCash')]"").Attribute(""outerHTML"")
    '                                 //* = any element  contains  @id = within id  'ctl03_txtCash' = string to find   .Attribute(""outerHTML"") = return HTML of the element
    sbDelay (100000)
    driver.Quit
End Sub

Sub sbDelay(delay As Long): Dim i As Long: For i = 1 To delay:  DoEvents: Next i: End Sub

",
76390331,"Firebase Firestore Indexes issue: Oops, indexes failed to load","When I visit the Firebase Firestore Index page, it shows an issue as ""Oops, indexes failed to load!"". I inspect the error and it shows 429. When I view indexes through GCP, it shows as Failed to load indexes: Request throttled at the client by AdaptiveThrottler. We're in the Firebase Blaze plan and I do not see any quota limit reached. This particular account has 5 projects and all of the projects indicate this same issue. What would be the issue?

",76391176,"firebase here
That looks off indeed. I can't reproduce myself, but I asked around and will post an update here when I hear back.

7:57 AM PT: Engineering has acknowledged the problem, and is identifying potential causes.

8:12 AM PT: The problem may be isolated to databases in Europe.

9:23 AM PT: This only affects index creation and listing operations. It does not affect the ability read data from or write to the database. Sorry for not mentioning that earlier, as it was the first thing we determined when the problem was reported.

9:29 AM PT: We've identified the root cause, which was a resource exhaustion issue in region eur3, and have increased resources in the region to mitigate.

9:49 AM PT: This issue has been mitigated now. If you are still seeing this problem, please reach out to Firebase support for personalized help in troubleshooting.
The engineering team is planning work to prevent this problem from reoccurring in the future.


",
76388475,Ada2012: Assertion_Policy,"According Ada2012 RM Assertion_Policy:

10.2/3    A pragma Assertion_Policy applies to the named assertion aspects in a specific region, and applies to all assertion expressions
specified in that region. A pragma Assertion_Policy given in a
declarative_part or immediately within a package_specification applies
from the place of the pragma to the end of the innermost enclosing
declarative region. The region for a pragma Assertion_Policy given as
a configuration pragma is the declarative region for the entire
compilation unit (or units) to which it applies.

This means that if I have a package hierarchy as per the following example:
└───Root
    ├───Child1
    ├───Child2
    │   └───GrandSon
    └───Child3

And if I define the pragma Assertion_Policy at Root package specification, it will affect to the whole package hierarchy right?
",76393618,"
And if I define the pragma Assertion_Policy at Root package
specification, it will affect to the whole package hierarchy right?

No.
What your bolded text means is that (a) the pragma is placed immediately in a specification, like so:
Pragme Ada_2012;                -- Placed ""immediately"".
Pragma Assertion_Policy(Check); -- Also ""immediately"".

Package Some_Spec is --... and so on.

or (b) in a declarative part:
Procedure Outer_Scope is
   Pragma Assertion_Polucy( Ignore ); -- Declarative region.
   X : Some_Type:= Possibly_Assertion_Failing_Operation;
   Package Inner_Scope is
    -- the stuff in here would ALSO ignore assertions.
   End Inner_Scope;
   Package Body Inner_Scope is Separate;
Begin
  if X'Valid then
    Null; -- Do things on the valid value of X.
  end if; -- Because we're ignoring the invalid case.
End Outer_Scope;

So, they apply not to children, but to the spec/body/declarative-region itself.
",
76391482,"in ggplot2, print an expression in the facet_wrap","I want to plot some functions with their gradients using the ggplot2 package in r.
p = 3
n0 = 100
z0 = seq(0.01, 0.99, length = n0)
AB0 = matrix(rbeta(600,4,1), nrow = n0)

library(ggplot2)
ab.names=c(paste(""g"",1:p,sep=""""),paste(""g' "",1:p,sep=""""))
pl0=ggplot(data.frame(ab = c(AB0), ID = rep(ab.names, each = n0), Z = z0),
           aes(x = Z, y = ab)) +
  geom_point() +
  facet_wrap(~ID, scales = ""free"",nrow = 2, ncol = p) + theme_bw() +
  ggtitle(""Unpenalized VCM"",expression(nabla~""g  and  ""~""g"")) + ylab("""")

I want to switch the rows and add the nabla symbol in the title of the plots in the 1st row before switching them to the second row. To be clear, the 1st row is for the functions, and the 2nd row is for the gradients where the nabla symbol should appear in the title.
Here is a screenshot of the outcome

",76392685,"the labeller argument to facet_wrap might come in handy, if you set it to ""label_parsed"". Example:
d <- data.frame(x = rnorm(2),
                y = rnorm(2),
                ID = c(paste0('g~', 1:2),
                       paste0('g~nabla~', 1:2)
                       )
                )


d |>
    ggplot(aes(x, y)) +
    geom_point() +
    facet_wrap(. ~ ID, 
               labeller = label_parsed,
               nrow = 2
               )


",
76389930,Easy to read scientific notation on Rails console,"I have a Postgresql table column which's type is numeric(20,4). When I play with the data through rails console, ActiveRecord display the values in scientific notation.
All I can think of adding attribute :column_name, :float to the related modal, but not sure if there will be any side effects because of it.
So the question is; does anyone one if there is anything I can do to make it easier for reading the data on rails console and do not have any side effects on the app itself?
",76391235,"You can monkeypatch specifically BigDecimal class specifically for console
development.rb
  console do
    class BigDecimal
      def inspect
        to_f
      end
    end
  end

",
76389774,Laravel auth()->id() not working in prod server,"I have one seperate database for saving logs
I have middleware that saves every request with response
 public function handle($request, Closure $next, $logType)
    {
        $response = $next($request);
        CreateLog::dispatch($request, $response, $logType)->onQueue(config('queue.queues.logging'));
        return $response;
    }

Everything related to auth progress works after every work complated. beacuse u used         $response = $next($request);
'user_id' => auth()->id(),

Trying to save user_id like that. Everything is working perfect
But in prod server user_id is being null
In prod server project running using docker
",76391268,"you need to double check multiple things:

what session driver are you using?
what is the queue driver are you using?
is your middleware runs after session and auth middleware runs ( based on the configuration in kernal.php )

based on your question and the problem is happening only on production so probably your issue is communication, your queue handler can't access session data which is usually the case if you separate them into different modules and connections to improve performance so to achieve this you need to pass the information you need from one module to another so in your case
public function handle($request, Closure $next, $logType)
    {
        $response = $next($request);
        $userID = auth()->id();
        CreateLog::dispatch($request, $response, $logType, $userID)->onQueue(config('queue.queues.logging'));
        return $response;
    }

and in your CreateLog you store this userID and use it instead of depending on the auth
",
76394054,How can I get the maximum bid value in a blind auction?,"I'm trying to create a blind auction as part of my class, and I'm trying to get the max value.
Here is the code, I tried to use the max() function but I get this error:
File ""Day_9_Blind_Auction.py"", line 30, in <module>print(max(data))TypeError: '>' not supported between instances of 'dict' and 'dict'

import os

data = [
    
    
    
    ]
while True:
    name = input(""What is your name?: "")
    bid = input(""What is your bid: $"")

    other_user = input(""Are there any other bidders? Type Yes or No.\n"")
    
    if other_user in ['yes', 'Yes']:
        os.system('cls')
 
    def new_user(name, bid):
        brandnew_user = {
            name: bid
            
            },
        
        data.append(brandnew_user)
        
    new_user(name, bid)

    
    
    if other_user in ['no', 'No']:
        print(max(data))
        break

I tried to remove the max() and it prints the output totally fine.  But then I need to get the max value, which is I don't know how to do it.
this is the output if I removed the max() function:
[({'George': '87'},), ({'Josh': '74'},), ({'Eduardo': '89'},)]

",76394106,"You are trying to apply the max() function to a list of dictionaries.   The error message is telling you that you haven't specified how to tell when one dictionary is ""greater than"" another.
It is possible to add a definition for the '>' operator that would allow it to compare dictionaries, but it's probably better if you rethink the data structures you're using. A list of dictionaries, with only one key/value pair per dictionary, is kind of an awkward construction. Why not use one list for the names, another list for the bids, then use max() and indexof() to find the largest bid and get the corresponding name from the other list?
",
76389312,How can I securely manage my client's PayPal merchant account as a web developer freelancer?,"Currently, when i need to access my customer's PayPal merchant account (to manage IPNs or to update notifications preferences for examples), i login to the account using his credentials and his 2FA (asking him the SMS code).
Is there a feature similar to Stripe Team which allows team members to access a PayPal merchant dashboard with their own credentials? Right here i am talking about the account dashboard, not the developer dashboard, but my question applies to both!
All of this doesn't seem very efficient and secure, is there another way?
Am i supposed to use my credentials only to develop the website (on developer.paypal.com) and never connect to the actual merchant's PayPal account? And ask the customer to do every manipulation on his account himself telling him the way?
",76391303,"Typically you should only need to obtain API credentials to integrate with, which for current solutions are in the developer dashboard. Other account settings and operations in the account are not something web developers need access to, other than maybe some initial setup tweaks to the website preferences page. To the extent other settings matter, it is likely you are making wrong integration choices or using old legacy PayPal products (not good--for instance, you tagged the above with IPN -- what year is it? why would you be using IPN in 2023?)
Anyway, it is possible to create and manage user access within a PayPal account. This is usually only used for in-house staff, not a vendor/contractor since you should not need it, but in theory it can be used for what you're asking.
",
76391544,import.meta.env.PROD runs in development mode,"why below code run in the development mode? it should only run in production mode
Here is the running scripts in package.json
""build-watch-dev"": ""vite build --mode development --watch"",
import { useStore } from '@/store';
import { register } from 'register-service-worker';

export function registerSW() {
    if (import.meta.env.PROD) {
        register(`${import.meta.env.BASE_URL}service-worker.js`, {
            ready() {
                console.log(
                    'App is being served from cache by a service worker.\n' +
                    'For more details, visit 
                )
            },
            registered() {
                console.log('Service worker has been registered.')
            },
            cached() {
                console.log('Content has been cached for offline use.')
            },
            updatefound() {
                console.log('New content is downloading.')
            },
            updated() {
                console.log('New content is available; please refresh.');
                const $store = useStore();
                $store.serviceWorkerUpdate = true;
            },
            offline() {
                console.log('No internet connection found. App is running in offline mode.')
            },
            error(error) {
                const $store = useStore();
                if ($store.isOnline) {
                    console.error('Error during service worker registration:', error)
                }
            }
        })
    }
}

",76392695,"The --mode flag overrides the value of import.meta.env.MODE.
import.meta.env.PROD is effectively a boolean result of process.env.NODE_ENV === 'production'
You could try changing your conditional to something like:
if (import.meta.env.MODE !== 'development') {

Or you could look at setting NODE_ENV to development.
",
76394111,Typescript class made up of an array of other classes gives an error,"So im trying to make a typescript file for a network and there are three files involved.
a _Node.ts, Edge.ts, and a Graph.ts
These three files look like so:
_Node.ts
interface _Node {
  data: any;
  neighbours: number[];
}

class _Node {
  constructor(data) {
    // this data is an arbitrary thing with which I can create any object
    this.data = { ...data };
    // the neighbours bit is explicity set from the code outside
    this.neighbours = [];
  }
}

export { _Node };

Edge.ts
interface Edge {
  start: number;
  end: number;
  data: any;
}

class Edge {
  constructor(start, end, data) {
    this.start = start;
    this.end = end;
    this.data = { ...data };
  }
}

export { Edge };

Graph.ts
import { _Node } from ""./_Node"";
import { Edge } from ""./Edge"";

interface Graph {
  nodes: _Node[];
  edges: Edge[];
}

class Graph {
  constructor(nodes, edges) {
    this.nodes = nodes;
    this.edges = edges;
    // execute Internal methods
    // this.printData();
  }

  // test function
  printData() {
    const message =
      ""This is a graph with "" +
      this.nodes.size + // this gives an error ""Property size doesn't exist on nodes""
      "" nodes and "" +
      this.edges.size +
      "" edges"";
    console.log(message);
  }
}


The last line of node.size and edges .size give a VS code error of something like the property size doesn't exist on them despite me declaring them as arrays of the nodes and things - is there a way to fix it?
",76394119,"You're looking for
this.edges.length
as specified by
https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/length
size is not a valid property or function
",
76384590,Sqlalchemy count of records in the relation of one to many,"Help me figure out what I'm doing wrong?
I need to get the number of comments in 24 hours in the relation one-to-many table.
class Comment(Base):
    __tablename__ = 'comments'
    id = Column(Integer, primary_key=True)
    task_id = Column(Integer(), ForeignKey('tasks.id'), nullable=False)
    post_link = Column(String, nullable=False)
    date = Column(DateTime, default=datetime.datetime.utcnow())

    def __init__(self, task_id: int, post_link: str):
        super().__init__()
        self.task_id = task_id
        self.post_link = post_link

    def __repr__(self):
        return f'id - {self.id} | task_id - {self.task_id} | date - {self.date}'

class Task(Base):
    __tablename__ = 'tasks'
    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)

    comments = relationship('Comment', backref='tasks', lazy=True)

    def __init__(self, name: str):
        super().__init__()
        self.name = name

    def __repr__(self):
        return f'id - {self.id} | name - {self.name}'


I don't know if the query works correctly or if it even outputs the number of records.
Here's the request itself:
async def get_comments_for_day():
    start_day = datetime.utcnow() - timedelta(hours=23, minutes=50)
    async with get_async_session() as session:
        stmt = select(Comment.task_id, func.count(Comment.task_id).label('comments_found'))\
            .where(Comment.date >= start_day).subquery()
        main_stmt = select(Task, stmt.c.comments_found).outerjoin(stmt, Task.id == stmt.c.task_id)
        results = await session.execute(main_stmt)
        return results.scalars().all()

async def main():
    tasks = await get_comments_for_day()
    for task, comments_found in tasks:
        print(task.name, comments_found)

I get this error:
    for task, comments_found in tasks:
TypeError: cannot unpack non-iterable Task object

",76391320,"async def get_comments_for_day():
    start_day = datetime.utcnow() - timedelta(hours=24)
    async with get_async_session() as session:
        stmt = (
            select(func.count(Comment.id), Task)
            .select_from(Task)
            .join(Comment, Task.id == Comment.task_id, isouter=True)
            .where(Comment.date >= start_day)
            .group_by(Task.id, Task.name)
        )
        results = await session.execute(stmt)
        return results.all()

If you're interested, I found a solution to the problem, which was to write return results.all() instead of return results.execute().all().
",
76389324,How to change the tag column selection option,"I have a tag column in the active admin index page like
tag_column :result, interactive: true, sortable: false

in my model I have a enum for result as
 enum result: { winner: 0, first_runner_up: 1, second_runner_up: 2}

In the index page when I click on drop down the selection options are being dispalyed as
winner
first_runner_up
second_runner_up
How can we display them as
Winner
First Runner Up
Second Runner Up
Can someone please help...
Thank you in advance
",76391414,"I suggest you break the rules out to a hash.. then you will be free to validate against them later
WINNER_OPTIONS = {
  winner: {
    title: ""Winner""
  },
  second: {
    title: ""First Runner Up""
  },
  third: {
    title: ""Second Runner Up""
  }

RESULT_ENUMS = WINNER_OPTIONS
  .keys
  .with_index
  .to_h

enum result: RESULT_ENUMS

tag_column :result, WINNER_OPTIONS.values.map{|e| e[:title]}.with_index.to_h

",
76390202,How can I move specific values from one field in a JSON column to another in PostgreSQL?,"Let's assume I have a table users with a JSON column ""foo"". Values of that column look like this:
""{ field1: ['bar', 'bar2', 'bar3'], field2: ['baz', 'baz2', 'baz3'] }""

where field1 and field2 are optional, so column value may look like this: ""{}""
I want to move values 'bar2' and 'bar3' from field1 to field2 for all records in this table.
Sample Output:
""{ field1: ['bar'], field2: ['baz', 'baz2', 'baz3', 'bar2', 'bar3']}""

More examples:
""{ field1: ['bar3'], field2: ['baz', 'baz2', 'baz3'] }""

should be transformed into
""{ field1: [], field2: ['baz', 'baz2', 'baz3', 'bar3'] }""

etc.
Is there any way to do this?
Unfortunately I have no idea how to approach this problem.
",76391507,"Using a series of subqueries with jsonb_array_elements
select jsonb_build_object('field1', coalesce((select jsonb_agg(v.value) 
       from jsonb_array_elements(u.foo -> 'field1') v 
       where v.value#>>'{}' not in ('bar2', 'bar3')), '[]'::jsonb),
   'field2', (u.foo -> 'field2') || (select jsonb_agg(v.value) 
       from jsonb_array_elements(u.foo -> 'field1') v 
       where v.value#>>'{}' in ('bar2', 'bar3'))) 
from users u 
where u.foo ? 'field1' and u.foo ? 'field2'

See fiddle
",
76391464,How to add a list of specific object in a configuration file for a WorkerService,"As far as what I understand:

WorkerService is the new way to define a Windows Service (app that run as as service).
By default, using the contextual menu on the project, the type of configuration file associated to a WorkerService is stil a xaml file: ""App.config"". See: How to: Add an application configuration file to a C# project
According to Microsoft documentation, I understand that I should use the section <appsettings>.

appSettings is a dictionary of key value pair. But I can't find how to add a list of item as the value. Also, I would like to add my own object as the value if possible.
Is there a way to add a List of MyConfigObject (List<MyConfigObject>)into a config file? If yes, how? Should I use another section or should I use another type of config file (json, yaml) to have the simplest way to read/write settings?
",76392770,"Yes, you can, Like this:
appsettings.json
{
  ""Logging"": {
    ""LogLevel"": {
      ""Default"": ""Information"",
      ""Microsoft"": ""Warning"",
      ""Microsoft.Hosting.Lifetime"": ""Information""
    }
  },
  ""AllowedHosts"": ""*"",
  ""MyConfigObject"": [
    {
      ""Prop1"": ""1"",
      ""Prop2"": ""2""
    },
    {
      ""Prop1"": ""1"",
      ""Prop2"": ""2""
    }
  ]
}

MyConfigObject Class
public class MyConfigObject
{
    public string Prop1 { get; set; }
    public string Prop2 { get; set; }
}

In Startup, register the Configuration with the type MyConfigObject. Like this:
    public void ConfigureServices(IServiceCollection services)
    {
        services.AddControllersWithViews();
        services.Configure<List<MyConfigObject>>(Configuration.GetSection(""MyConfigObject""));
    }

Now you can use it in any service or controller, like this:
public class HomeController : Controller
    {
        private readonly ILogger<HomeController> _logger;
        private readonly List<MyConfigObject> myConfig;
        public HomeController(IOptions<List<MyConfigObject>> myConfig, ILogger<HomeController> logger)
        {
            _logger = logger;
            this.myConfig = myConfig.Value;
        }

        public IActionResult Index()
        {
            return View();
        }
}

",
76391280,"redux tookit when using a middleware builder function, an array of middleware must be returned","So I'm getting Uncaught Error: when using a middleware builder function, an array of middleware must be returned
This is my code
import {
  configureStore,
  compose,
  combineReducers,
  applyMiddleware
} from ""@reduxjs/toolkit"";
import thunk from ""redux-thunk"";

const rootReducer = combineReducers({});

const middleware = applyMiddleware(thunk);
const composeWithDevTools = 
  window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ || compose;
const composedMiddleWare = composeWithDevTools(middleware)

const store = configureStore({
  reducer: rootReducer,
  middleware: composedMiddleWare,
  devTools: 
    window.__REDUX_DEVTOOLS_EXTENSION__ && window.__REDUX_DEVTOOLS_EXTENSION__()
})

export default store;

I have no idea what's wrong and searching doesn't seem to be returning any useful result.
",76392775,"configureStore from redux-toolkit (RTK) works differently from the redux createStore function. The middleware property accepts an array of middleware to install and RTK handles applying them.
See configureStore

/**
 * An array of Redux middleware to install. If not supplied, defaults to
 * the set of middleware returned by `getDefaultMiddleware()`.
 */
middleware?: ((getDefaultMiddleware: CurriedGetDefaultMiddleware<S>) => M) | M


Note that while applyMiddleware is re-exported from redux that

You should not need to use this directly.

The following would be the code you'd use:
import { configureStore, combineReducers } from ""@reduxjs/toolkit"";
import thunk from ""redux-thunk"";

const rootReducer = combineReducers({});

const store = configureStore({
  reducer: rootReducer,
  middleware: [thunk], // <-- array of middlewares to install/apply
  devTools: window.__REDUX_DEVTOOLS_EXTENSION__ && window.__REDUX_DEVTOOLS_EXTENSION__()
});

export default store;

RTK also ships with opinionated optimizations:

The devTools are enabled by default, you'll only use this if you have custom needs.

Thunk middleware is also already included with the default middleware, e.g. if no middleware property is provided, then getDefaultMiddleware is provided and returns the following middleware value, which includes the thunk middleware:
const middleware = [
  actionCreatorInvariant,
  immutableStateInvariant,
  thunk, // <-- included thunk middleware!
  serializableStateInvariant,
]

Again, you'll really only need to specify the middleware property if you need to customize the immutability or serialization middlewares.


Your store configuration can be reduced to the following:
import { configureStore, combineReducers } from ""@reduxjs/toolkit"";

const rootReducer = combineReducers({});

const store = configureStore({
  reducer: rootReducer,
});

export default store;

Both the thunk middleware and dev tools will be active and working.
",
76390228,upsert_item into cosmosdb via python,"I have a this code:
import logging
import json
import os
from azure.cosmos import CosmosClient
import azure.functions as func

url = os.environ[""ACCOUNT_URI""]
key = os.environ[""ACCOUNT_KEY""]
client1 = CosmosClient(url, key)
client = CosmosClient.from_connection_string(os.environ[""CosmosDBConnStr""])

database_name = ""dmdb""
container_name = ""DataContract""

database = client.get_database_client(database_name)
container = database.get_container_client(container_name)

logging.info(f""container: {url}"")
def main(myblob: func.InputStream, doc: func.Out[func.Document]):
    logging.info(f""Python blob trigger function processed blob \n""
                 f""Name: {myblob.name}\n"")
    #reading file from blob
    contract_data=myblob.read()  
    
    try:
        logging.info(f""contract data: {contract_data}"")
        contract_json = json.loads(contract_data)       
        version = contract_json.get(""version"")
        name = contract_json.get(""name"")
        title = contract_json.get(""title"")
        logging.info(f""contract json: {contract_json}"")
        query = ""SELECT c.version,c.name,c.title,c.Theme,c.description,c['data owner'],c.confidentiality,c.table1 FROM c ""
      
       
        items = list(container.query_items(
            query=query,
            enable_cross_partition_query=True
        ))
        logging.info(f""item: {items[0]}"")
        for item in items:
            if item[""name""] == name and item[""version""] == version:
                if item[""title""] == title:
                    logging.info(f""Skipping, item already exists: {item}"")
                    return  # Skip saving the document
                
                container.upsert_item(body=contract_json,pre_trigger_include = 
                None,post_trigger_include= None)
                return

        doc.set(func.Document.from_json(contract_data))
                       
    except Exception as e:
            logging.info(f""Error: {e}"")

I added one document in my cosmosdb, I would like to replace same document has different title. but i could not do that i am getting  Code: BadRequest
Message: Message: {""Errors"":[""One of the specified inputs is invalid""]}
this is an example of item from my query:
{'version': 'V1', 'name': 'demo_contract2', 'title': 'title122', 'Theme': 'Theme12', 'description': 'test data contract management2', 'data owner': 'j.jansen@amsterdam.nl', 'confidentiality': 'open', 'table1': {'description:': 'testen', 'attribute_1': {'type': 'int', 'description:': 'testen', 'identifiability': 'identifiable'}}}

this is an example of contract_json from my file:
{'version': 'V1', 'name': 'demo_contrac1', 'title': 'title1', 'Theme': 'Theme1', 'description': 'test data contract management2', 'data owner': 'j.jansen@amsterdam.nl', 'confidentiality': 'open', 'table1': {'description:': 'testen', 'attribute_1': {'type': 'int', 'description:': 'testen', 'identifiability': 'identifiable'}}}

they are matching. How should I regulate my upsert_item function in my code?
",76391647,"You are missing the id in the Upsert content. A document's identity is defined by the combination of id and Partition Key value, if you don't specify the id then Upsert will always behave as a Create operation.
Because you are getting the items through a query, just add the id:
query = ""SELECT c.id, c.version,c.name,c.title,c.Theme,c.description,c['data owner'],c.confidentiality,c.table1 FROM c ""
     

You can now get the id from item[""id""] to use if needed. The body of the Upsert operation should contain id, and if that matches with an existing document, then the document will get updated.
",
76381180,Different intercept values for linear regression using statsmodels and numpy polyfit,"I get two different intercept values from using the statsmodels regression fit and the numpy polyfit. The model is a simple linear regression with a single variable.
From the statsmodels regression I use:
results1 = smf.ols('np.log(NON_UND) ~ (np.log(Food_consumption))', data=Data2).fit()

Where I recieve the following results:
                               coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------------------
Intercept                    5.4433      0.270     20.154      0.000       4.911       5.976
np.log(Food_consumption)     1.1128      0.026     42.922      0.000       1.062       1.164

When plotting the data and adding a trendline using numpy polyfit, I recieve a different intercept value:
x = np.array((np.log(Data2.Food_consumption)))
y = np.array((np.log(Data2.NON_UND)*100))

z = np.polyfit(x, y, 1)

array([ 1.11278898, 10.04846693])

How come I get two different values for the intercept?
Thanks in advance!
",76394128,"This is because you are using different linear models in the first and second regressions. In the first regression, you take logs of both the dependent and independent variables, while in the second regression, you are not, and additionally, you are multiplying y by 100.
In order to get the same results as the first regression in the second specification, you need to make sure the regression model is exactly the same as the first one. I suggest you do this:
x = np.log(np.array(((Data2.Food_consumption))))
y = np.log(np.array(((Data2.NON_UND))))

z = np.polyfit(x, y, 1)

And then the output you get with the second function should be the same as the one you get in the first one.
",
76390103,"I want get documents using ObjectId, but it's not getting that document","I am new to MongoDB and trying to get multiple documents using ObjectId
Below I have mentioned the demo data format.
db={
""store"": [
{
  ""_id"": ObjectId(""63da2f1f7662144569f78ddd""),
  ""name"": ""bat"",
  ""price"": 56,
  
},
{
  ""id"": ObjectId(""63da2f1f7662144569f78ddc""),
  ""name"": ""ball"",
  ""price"": 58,
  
},
{
  ""id"": ObjectId(""63da2f1f7662144569f78ddb""),
  ""name"": ""cap"",
  ""price"": 100,
  
},
{
  ""id"": ObjectId(""63da2f1f7662144569f78dda""),
  ""name"": ""red"",
  ""price"": 50,
  
},

]}

and my query is this
db.store.aggregate([
{
$match: {
  id: {
    $in: [
      ObjectId(""63da2f1f7662144569f78ddd""),
      ObjectId(""63da2f1f7662144569f78ddb""),
      ObjectId(""63da2f1f7662144569f78dda"")
    ]
  }
}
},
{
$group: {
  _id: null,
  totalPrice: {
    $sum: ""$price""
  }
}
},
{
$project: {
  _id: 0,
  
}
}
])

and output is
[
{
""totalPrice"": 150
}
]

but I'm getting objectid like this
{
  ""ids"": 
  [""63da2f1f7662144569f78ddd"",""63da2f1f7662144569f78ddb"",""63da2f1f7662144569f78dda""]
}

if I pass this array it's not getting any documents. How can I pass this array in place of objectids????
",76391730,"For your case, you can simply wrap the payload from client in a $map to convert them into ObjectIds with $toObjectId
db.store.aggregate([
  {
    $match: {
      $expr: {
        ""$in"": [
          ""$id"",
          {
            ""$map"": {
              // your payload from client here
              ""input"": [
                ""63da2f1f7662144569f78ddd"",
                ""63da2f1f7662144569f78ddb"",
                ""63da2f1f7662144569f78dda""
              ],
              ""as"": ""id"",
              ""in"": {
                ""$toObjectId"": ""$$id""
              }
            }
          }
        ]
      }
    }
  },
  {
    $group: {
      _id: null,
      totalPrice: {
        $sum: ""$price""
      }
    }
  },
  {
    $project: {
      _id: 0,
      
    }
  }
])

Mongo Playground
",
76392245,PHP Issue with array not printing the first element of the array,"I have an issue I cannot seem to fix. I have a function that takes a file and converts it to an array using the first row as the keys:
function parseCSVToArray($filePath)
{
    $csvData = [];

    if (($handle = fopen($filePath, ""r"")) !== false) {
        $keys = fgetcsv($handle); // Get the first row as keys

        while (($data = fgetcsv($handle)) !== false) {
            $rowData = array();
            foreach ($keys as $index => $key) {
                $rowData[$key] = $data[$index] ?? ''; // Assign each value to its corresponding key
            }
            $csvData[] = $rowData;
        }

        fclose($handle);
    }
    return $csvData;
}

Everything works as normal and creates the array as expected:
$getTheRecords = parseCSVToArray('Data/records.csv');

// File contents
record,subdomain,hub_knows,domain,type,value,action,rationale
mail.sub.domain.com.,sub.domain.com,Hub knows about this,domain.com,CNAME,dispatch.domain.com.,DELETE,Dispatch links can go

Array
(
    [record] => mail.sub.domain.com
    [subdomain] => sub.domain.com
    [hub_knows] => Hub knows about this
    [domain] => domain.com
    [type] => CNAME
    [value] => dispatch.domain.com.
    [action] => DELETE
    [rationale] => Dispatch links can go
)

Now the issue is when I go to use or print the data. When I loop through the array using:
foreach($getTheRecords as $element)
{
    echo ""<div style='margin-bottom: 20px'>"";
    echo($element['subdomain']); // This will print the subdomain as expected.
    echo ""</div>"";
}

If I change 'subdomain' to 'record' it prints nothing. However, every other 'key' prints the results just fine.
Thank you in advance for your help!
I have tried changing the name of the first key to 'mainrecord' or anything and it still will not print out.
Iside loop var_dmup():
array(8) {
  [""record""]=>
  string(31) ""mail.0lemonade.starchapter.com.""
  [""subdomain""]=>
  string(25) ""0lemonade.starchapter.com""
  [""hub_knows""]=>
  string(20) ""Hub knows about this""
  [""domain""]=>
  string(17) ""scdomaintest3.com""
  [""type""]=>
  string(5) ""CNAME""
  [""value""]=>
  string(22) ""dispatch.scnetops.com.""
  [""action""]=>
  string(6) ""DELETE""
  [""rationale""]=>
  string(21) ""Dispatch links can go""
}

",76392789,"Your file likely has a UTF8 Byte Order Mark [BOM] at the beginning which is throwing off the first key. While a BOM isn't necessary at all for UTF8, some programs still add it as a ""hint"" that the file is UTF8.
If you var_dump($keys[0], bin2hex($keys[0]) you'll likely see that the first key's is longer than what is visible, and the hex output will show it prefixed with EFBBBF which is the BOM.
Try replacing:
$keys = fgetcsv($handle);

With:
$keys = str_getcsv(preg_replace(""/^\xef\xbb\xbf/"", """", fgets($handle))); 

Which will trim off the BOM, if it exists.
Edit: A bit more broadly-applicable code.
function stream_skip_bom($stream_handle) {
    if( ! stream_get_meta_data($stream_handle)['seekable'] ) {
        throw new \Exception('Specified stream is not seekable, and cannot be rewound.');
    }
    
    $pos = ftell($stream_handle);
    $test = fread($stream_handle, 3);
    
    if( $test !== ""\xef\xbb\xbf"" ) {
        fseek($stream_handle, $pos);
    }
}

so after opening $handle you would call simply:
stream_skip_bom($handle);

",
76392254,Firestore subscribed document change type,"Whenever I subscribe to a collection of documents, I'm able to extract the changes to the documents, given that the listener returns DocumentChange. This way I can understand whether a given doc was created, modified or deleted.
How to get the DocumentChange when subscribing to a single document in the collection? The issue appears to be that this time the listener returns DocumentSnapshot instead of DocumentChange.
",76392834,"The DocumentSnapshot contains everything you need to know about that one document.  Either it exists() with data, or it does not.  You can check this state with each new snapshot for that one document, and react to that any way you like.  Since you are not performing a query with variable set of results, there is no need for more information.
",
76394114,Is it possible to deselect a ttk.Radiobutton?,"These are the ttk.radiobuttons I am using:
    def radioButtonGen(self):
        self.var = tk.IntVar()
        self.arithButton = ttk.Radiobutton(self, text = ""Arithmetic Series"", variable = self.var, value = 1, command = lambda: self.buttons.seqChoice(""arithmetic""))
        self.geomButton = ttk.Radiobutton(self, text = ""Geometric Series"", variable = self.var, value = 2, command = lambda: self.buttons.seqChoice(""geometric""))

I have attempted to deselect them with deselect() but it only works for tk.radiobuttons, which don't look as good and the method still leaves them with a dash.
I want to deselect them so they are not highlighted
",76394144,"Use the associated tkinter variable to deselect them:
self.var.set(0)

",
76392055,How to match multiple occurrences of strings given a start and end pattern in R?,"library(stringr)
string <- string <- c(""pat1 hello333\n pat2 ok i mean pat1 again pat2 some more text pat1"")

I want to match all strings that start with pat1 and end with pat2.
> str_extract_all(
  string,
  regex(
    ""pat1.+pat2"",
    dotall=TRUE
  )
)
[[1]]
[1] ""pat1 hello333\n pat2 ok i mean pat1 again pat2""

This gives me 1 string that starts with pat1 and ends with pat2. However, my desired output is something like:
> output
[1] ""pat1 hello333\n pat2""
[2] ""pat1 again pat2""

",76392839,"Change .+ to .+? to have non greedy match.
library(stringr)
str_extract_all(string, regex(""pat1.+?pat2"", dotall=TRUE))[[1]]
#[1] ""pat1 hello333\n pat2"" ""pat1 again pat2"" 

You can use gregexpr and regmatches with pat1.*?pat2 or in case they should be on a word boundary with \\bpat1\\b.*?\\bpat2\\b. Where .*? matches everything but minimal.
regmatches(string, gregexpr(""pat1.*?pat2"", string))[[1]]
#[1] ""pat1 hello333\n pat2"" ""pat1 again pat2""     

regmatches(string, gregexpr(""\\bpat1\\b.*?\\bpat2\\b"", string))[[1]]
#[1] ""pat1 hello333\n pat2"" ""pat1 again pat2""     

",
76390407,How can I programmatically add a tab to a GTKmm notebook in C++?,"I'm working on a GTKmm application(a simple text editor, as an exercise), in which I have a notebook to which I want to add a tab. The notebook shows, but the newly added tab doesn't.
I'm doing it in the following way:
void MainWindow::AddTabToNotebook()
{
    Gtk::Box box;

    notebook->append_page(box);

    notebook->show_all();
}

MainWindow is a class that inherits Gtk::Window and contains a pointer to Gtk::Notebook which is loaded from a Glade file using Gtk::Builder. Whenever I click the button that calls the function I get the following message in the terminal: Gtk-CRITICAL **: gtk_notebook_get_tab_label: assertion 'list != NULL' failed. Any help is appreciated.
MainWindow.h:
#ifndef MAIN_WINDOW_H
#define MAIN_WINDOW_H

#include <gtkmm.h>

class MainWindow : public Gtk::Window
{
protected:
    
    Gtk::Button* buttonOpenFile;
    Gtk::Button* buttonSave;
    Gtk::Button* buttonSaveAs;
    
    Gtk::Button *dialogButonOpen;
    Gtk::TextView *text;
    Gtk::MenuButton *buttonMenu;
    Gtk::Notebook *notebook;

    Gtk::FileChooserDialog *openFileDialog;

    Glib::RefPtr<Gtk::Builder> builder;
    void AddTabToNotebook();
    void OnButtonOpenFileClick();
    void OnButtonSaveClick();
    void OnButtonSaveAsClick();
    void OnFileChosen();
    void OnDialogButtonOpenClick();

public:
    MainWindow(BaseObjectType *cobject, const Glib::RefPtr<Gtk::Builder> &refGlade);
    ~MainWindow();
};
#endif

MainWindow.cpp:

#include""MainWindow.h""
#include<iostream>
#include<gtkmm.h>

MainWindow::MainWindow(BaseObjectType *cobject, const Glib::RefPtr<Gtk::Builder> &refGlade)
    :Gtk::Window(cobject),
    builder(refGlade)
{
    builder->get_widget(""buttonOpenFile"", buttonOpenFile);
    builder->get_widget(""buttonSave"", buttonSave);
    builder->get_widget(""buttonSaveAs"", buttonSaveAs);
    builder->get_widget(""buttonMenu"", buttonMenu);
    builder->get_widget(""openFileDialog"", openFileDialog);
    builder->get_widget(""dialogButtonOpen"", dialogButonOpen);
    builder->get_widget(""notebook"", notebook);

    buttonOpenFile->signal_clicked().connect(sigc::mem_fun(*this, &MainWindow::OnButtonOpenFileClick));
    dialogButonOpen->signal_clicked().connect(sigc::mem_fun(*this, &MainWindow::OnDialogButtonOpenClick));
    buttonSave->signal_clicked().connect(sigc::mem_fun(*this, &MainWindow::AddTabToNotebook));

    show_all_children();
}
MainWindow::~MainWindow()
{}
void MainWindow::OnButtonOpenFileClick()
{
    if(openFileDialog)
    {
        openFileDialog->show();
    }
}
void MainWindow::OnDialogButtonOpenClick()
{
    auto file=openFileDialog->get_file();
    std::cout<<file->get_path();

    if(openFileDialog)
    {
        openFileDialog->close();
    }
}
void MainWindow::AddTabToNotebook()
{
    Gtk::Box box;
    
    notebook->append_page(box);

    notebook->show_all();
    
}

The glade file:
<?xml version=""1.0"" encoding=""UTF-8""?>
<!-- Generated with glade 3.38.2 -->
<interface>
  <requires lib=""gtk+"" version=""3.24""/>
  <object class=""GtkWindow"" id=""MainWindow"">
    <property name=""name"">MainWindow</property>
    <property name=""width-request"">800</property>
    <property name=""height-request"">600</property>
    <property name=""can-focus"">False</property>
    <child>
      <object class=""GtkViewport"">
        <property name=""visible"">True</property>
        <property name=""can-focus"">False</property>
        <child>
          <object class=""GtkBox"">
            <property name=""visible"">True</property>
            <property name=""can-focus"">False</property>
            <property name=""orientation"">vertical</property>
            <child>
              <object class=""GtkBox"">
                <property name=""height-request"">30</property>
                <property name=""visible"">True</property>
                <property name=""can-focus"">False</property>
                <property name=""valign"">start</property>
                <property name=""hexpand"">True</property>
                <child>
                  <object class=""GtkButton"" id=""buttonOpenFile"">
                    <property name=""label"" translatable=""yes"">Open file</property>
                    <property name=""name"">buttonOpenFile</property>
                    <property name=""visible"">True</property>
                    <property name=""can-focus"">True</property>
                    <property name=""receives-default"">True</property>
                    <property name=""tooltip-text"" translatable=""yes"">Open a file</property>
                    <signal name=""clicked"" handler=""OnButtonOpenFileClicked"" swapped=""no""/>
                  </object>
                  <packing>
                    <property name=""expand"">False</property>
                    <property name=""fill"">True</property>
                    <property name=""position"">0</property>
                  </packing>
                </child>
                <child>
                  <object class=""GtkButton"" id=""buttonSave"">
                    <property name=""label"" translatable=""yes"">Save changes</property>
                    <property name=""name"">buttonSave</property>
                    <property name=""visible"">True</property>
                    <property name=""can-focus"">True</property>
                    <property name=""receives-default"">True</property>
                    <property name=""tooltip-text"" translatable=""yes"">Save changes to the current document</property>
                    <signal name=""clicked"" handler=""OnButtonSaveChangesClicked"" swapped=""no""/>
                  </object>
                  <packing>
                    <property name=""expand"">False</property>
                    <property name=""fill"">True</property>
                    <property name=""position"">1</property>
                  </packing>
                </child>
                <child>
                  <object class=""GtkButton"" id=""buttonSaveAs"">
                    <property name=""label"" translatable=""yes"">Save As...</property>
                    <property name=""name"">buttonSaveAs</property>
                    <property name=""visible"">True</property>
                    <property name=""can-focus"">True</property>
                    <property name=""receives-default"">True</property>
                    <signal name=""clicked"" handler=""OnButtonSaveAsClicked"" swapped=""no""/>
                  </object>
                  <packing>
                    <property name=""expand"">False</property>
                    <property name=""fill"">True</property>
                    <property name=""position"">2</property>
                  </packing>
                </child>
                <child>
                  <object class=""GtkMenuButton"" id=""buttonMenu"">
                    <property name=""name"">buttonMenu</property>
                    <property name=""visible"">True</property>
                    <property name=""can-focus"">True</property>
                    <property name=""focus-on-click"">False</property>
                    <property name=""receives-default"">True</property>
                    <property name=""use-popover"">False</property>
                    <signal name=""toggled"" handler=""OnButtonMenuToggled"" swapped=""no""/>
                    <child>
                      <placeholder/>
                    </child>
                  </object>
                  <packing>
                    <property name=""expand"">False</property>
                    <property name=""fill"">True</property>
                    <property name=""pack-type"">end</property>
                    <property name=""position"">3</property>
                  </packing>
                </child>
                <child>
                  <placeholder/>
                </child>
                <child>
                  <placeholder/>
                </child>
              </object>
              <packing>
                <property name=""expand"">False</property>
                <property name=""fill"">True</property>
                <property name=""position"">0</property>
              </packing>
            </child>
            <child>
              <object class=""GtkViewport"">
                <property name=""height-request"">780</property>
                <property name=""visible"">True</property>
                <property name=""can-focus"">False</property>
                <child>
                  <object class=""GtkScrolledWindow"">
                    <property name=""visible"">True</property>
                    <property name=""can-focus"">True</property>
                    <property name=""shadow-type"">in</property>
                    <child>
                      <object class=""GtkViewport"">
                        <property name=""visible"">True</property>
                        <property name=""can-focus"">False</property>
                        <child>
                          <object class=""GtkNotebook"" id=""notebook"">
                            <property name=""name"">notebook</property>
                            <property name=""width-request"">200</property>
                            <property name=""height-request"">200</property>
                            <property name=""visible"">True</property>
                            <property name=""can-focus"">True</property>
                            <child>
                              <object class=""GtkTextView"">
                                <property name=""visible"">True</property>
                                <property name=""can-focus"">True</property>
                              </object>
                            </child>
                            <child type=""tab"">
                              <object class=""GtkLabel"">
                                <property name=""visible"">True</property>
                                <property name=""can-focus"">False</property>
                                <property name=""label"" translatable=""yes"">page 1</property>
                              </object>
                              <packing>
                                <property name=""tab-fill"">False</property>
                              </packing>
                            </child>
                            <child>
                              <placeholder/>
                            </child>
                            <child type=""tab"">
                              <placeholder/>
                            </child>
                          </object>
                        </child>
                      </object>
                    </child>
                  </object>
                </child>
              </object>
              <packing>
                <property name=""expand"">False</property>
                <property name=""fill"">True</property>
                <property name=""position"">1</property>
              </packing>
            </child>
            <child>
              <placeholder/>
            </child>
          </object>
        </child>
      </object>
    </child>
  </object>
  <object class=""GtkFileChooserDialog"" id=""openFileDialog"">
    <property name=""name"">openFileDialog</property>
    <property name=""width-request"">800</property>
    <property name=""height-request"">600</property>
    <property name=""can-focus"">False</property>
    <property name=""type-hint"">dialog</property>
    <child internal-child=""vbox"">
      <object class=""GtkBox"">
        <property name=""width-request"">800</property>
        <property name=""height-request"">600</property>
        <property name=""can-focus"">False</property>
        <property name=""orientation"">vertical</property>
        <property name=""spacing"">2</property>
        <child internal-child=""action_area"">
          <object class=""GtkButtonBox"">
            <property name=""can-focus"">False</property>
            <property name=""layout-style"">end</property>
            <child>
              <object class=""GtkButton"" id=""dialogButtonOpen"">
                <property name=""label"" translatable=""yes"">Open</property>
                <property name=""name"">dialogButtonOpen</property>
                <property name=""visible"">True</property>
                <property name=""can-focus"">True</property>
                <property name=""receives-default"">True</property>
              </object>
              <packing>
                <property name=""expand"">True</property>
                <property name=""fill"">True</property>
                <property name=""position"">0</property>
              </packing>
            </child>
          </object>
          <packing>
            <property name=""expand"">False</property>
            <property name=""fill"">False</property>
            <property name=""position"">0</property>
          </packing>
        </child>
      </object>
    </child>
  </object>
</interface>

Whatever I try to append to the notebook, be it a Gtk::Box or Gtk:TextView, the new tab doesn't show.
Note: This is a work in progress, so the save button's clicked signal is responsible for adding a new tab(blank document)to the notebook.
",76391800,"The problem is that:

The Gtk::Notebook does not take ownership of the widget you are adding.
The widget you are adding is local to your callback (and hence destroyed when leaving it).

I was able to make your example work by adding a widget (here a Gtk::Label) attribute to your window (so it outlives the callback):
class MainWindow : public Gtk::Window
{
protected:
    
    Gtk::Button* buttonOpenFile;
    Gtk::Button* buttonSave;
    Gtk::Button* buttonSaveAs;
    Gtk::Label   label;        // <-- See here
    
    Gtk::Button *dialogButonOpen;

    ...

and then show()ing it explicitly in the callback:
void MainWindow::AddTabToNotebook()
{
    notebook->append_page(label);

    label.set_text(""test"");
    label.show();
}

on Ubuntu and Gtkmm 3.24.20. You can achieve the same thing by using the Glade file, I leave this part to you.
",
76385362,Dragging fields in CrafterCMS Studio,"I'm currently teaching myself how to use a headless CMS (CrafterCMS) with Next.js.
I have the following simple content type in CrafterCMS studio (just a title and a text):

And the respective code:
export default async function TestPage() {
    const model = await getModel('/site/website/test/index.xml');
    return (
        <TestComponent model={model} />
    );
}

export default function TestComponent(props) {
    const { isAuthoring } = useCrafterAppContext();
    const { model } = props;

    return (
        <ExperienceBuilder path={model.craftercms.path} isAuthoring={isAuthoring}>
            <Model model={model} >
                <div className=""space-y-4"">
                    <RenderField
                        model={model}
                        fieldId=""title_s""
                        className=""text-xl text-gray-800""
                    />

                    <RenderField
                        model={model}
                        fieldId=""text_t""
                        className=""text-xl text-gray-800""
                    />
                </div>
            </Model>
        </ExperienceBuilder>
    );
}

Is it possible to use Studio functionality to drag and change the positions of my two fields? For example I want to drag the text to be the first element. It seems that i only have the option to change the content, dragging is not available:

",76391913,"On CrafterCMS, you can drag & drop mainly 3 things:

Repeating group items
Component item references from an Item Selector control with a ""Components"" datasource
Media items into a media control (image or video)

The simplest way of achieving what you describe, would be to change your model to wrap with a repeat group the fields you want to reorder via drag & drop (i.e. create a repeat group and add the ""title"" input control inside of it; only 1, discard the other). Once you've done that, you'll need to update your TestComponent code to use <RenderRepeat ... /> and you should be able to reorder via drag & drop, via the contextual menu up/down buttons, or via the content form.
The rendering of the repeat with your title field, would roughly look something like this:
<RenderRepeat
  model={model}
  fieldId=""yourRepeatGroupId_o""
  componentProps={{ className: ""space-y-4"" }}
  renderItem={(item, index) => (
    <RenderField
      model={model}
      fieldId=""yourRepeatGroupId_o.title_s""
      index={index}
      className=""text-xl text-gray-800""
    />
  )}
/>

As I mentioned, you could achieve it via components (Item selector control, Components datasource), but the repeat group is simplest for this case; specially to get started and learn.
",
76394140,"Python: Comparing two lists using a for-loop, how do I print the results correctly?","import random

lst1 = []
lst2 = []
n = int(input('Step a: How many numbers in each list?  '))

for i in range(0, n):
    number1 = random.randint(1, 15)
    number2 = random.randint(1, 15)
    lst1.append(number1)
    lst2.append(number2)
    count = 0
    if lst1[i] > lst2[i]:
        count += 1
        print(f'{number1} : First List')
    else:
        print(f'{number2} : Second List')


print(f'Step b: First List{lst1}')
print(f'Step c: Second List{lst2}')
print('Step d:')
print('Larger number in each comparison:')

Program is supposed to make two lists and compare each element.
Current program prints like this:
Step a: How many numbers in each list?  5
12 : First List
14 : First List
14 : First List
15 : First List
5 : Second List
Step b: First List\[12, 14, 14, 15, 4\]
Step c: Second List\[5, 2, 8, 13, 5\]
Step d:
Larger number in each comparison:

I'm trying to get it to print like this:
Step a: How many numbers in each list?  5
Step b: First List\[12, 14, 14, 15, 4\]
Step c: Second List\[5, 2, 8, 13, 5\]
Step d:
Larger number in each comparison:
12 : First List
14 : First List
14 : First List
15 : First List
5 : Second List

Any suggestions?
I've tried to make functions out of them and kept getting errors. Very new to this and not sure what I should do.
",76394152,"Try creating a third list to store the comparison results and then printing the contents only at the end with another loop:
import random

lst1 = []
lst2 = []
comparison_results = []

n = int(input('Step a: How many numbers in each list? '))

for i in range(n):
    number1 = random.randint(1, 15)
    number2 = random.randint(1, 15)
    lst1.append(number1)
    lst2.append(number2)

    if lst1[i] > lst2[i]:
        comparison_results.append(f'{number1} : First List')
    elif lst1[i] < lst2[i]:
        comparison_results.append(f'{number2} : Second List')
    else:
        comparison_results.append(f'{number2} : In Both Lists')

print(f'Step b: First List {lst1}')
print(f'Step c: Second List {lst2}')
print('Step d: Larger number in each comparison:')
for result in comparison_results:
    print(result)


Example Output:
Step a: How many numbers in each list? 5
Step b: First List [5, 11, 9, 3, 4]
Step c: Second List [15, 11, 9, 13, 15]
Step d: Larger number in each comparison:
15 : Second List
11 : In Both Lists
9 : In Both Lists
13 : Second List
15 : Second List

",
76391153,Python Polars: Lazy Frame Row Count not equal wc -l,"Been experimenting with polars and of the key features that peak my interest is the larger than RAM operations.
I downloaded some files to play with from HERE. On the website: First line in each file is header; 1 line corresponds to 1 record.. WARNING total download is quite large (~1.3GB)! This experiment was done on AWS server (t2.medium, 2cpu, 4GB)
wget https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Shoes_v1_00.tsv.gz \
https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz \
https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Software_v1_00.tsv.gz \
https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv  .gz \
https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz 

gunzip *

Here are the results from wc -l
drwxrwxr-x 3 ubuntu ubuntu       4096 Jun  2 12:44 ../
-rw-rw-r-- 1 ubuntu ubuntu 1243069057 Nov 25  2017 amazon_reviews_us_Office_Products_v1_00.tsv
-rw-rw-r-- 1 ubuntu ubuntu   44891575 Nov 25  2017 amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv
-rw-rw-r-- 1 ubuntu ubuntu 1570176560 Nov 25  2017 amazon_reviews_us_Shoes_v1_00.tsv
-rw-rw-r-- 1 ubuntu ubuntu  249565371 Nov 25  2017 amazon_reviews_us_Software_v1_00.tsv
-rw-rw-r-- 1 ubuntu ubuntu  412542975 Nov 25  2017 amazon_reviews_us_Watches_v1_00.tsv

$ find . -type f -exec cat {} + | wc -l
8398139

$ find . -name '*.tsv' | xargs wc -l
   2642435 ./amazon_reviews_us_Office_Products_v1_00.tsv
    341932 ./amazon_reviews_us_Software_v1_00.tsv
     85982 ./amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv
   4366917 ./amazon_reviews_us_Shoes_v1_00.tsv
    960873 ./amazon_reviews_us_Watches_v1_00.tsv
   8398139 total

Now, if I count the rows using polars using our new fancy lazy function:
import polars as pl

csvfile = ""~/data/amazon/*.tsv""
(
    pl.scan_csv(csvfile, separator = '\t')
    .select( 
        pl.count()
        )
    .collect()
)
shape: (1, 1)
┌─────────┐
│ count   │
│ ---     │
│ u32     │
╞═════════╡
│ 4186305 │
└─────────┘

Wow, thats a BIG difference between wc -l and polars. Thats weird... maybe its a data issue. Lets only focus on the column of interest:
csvfile = ""~/data/amazon/*.tsv""
(
...     pl.scan_csv(csvfile, separator = '\t')
...     .select( 
...         pl.col(""product_category"").count()
...         )
...     .collect()
... )
shape: (1, 1)
┌──────────────────┐
│ product_category │
│ ---              │
│ u32              │
╞══════════════════╡
│ 7126095          │
└──────────────────┘

And with .collect(streaming = True):
shape: (1, 1)
┌──────────────────┐
│ product_category │
│ ---              │
│ u32              │
╞══════════════════╡
│ 7125569          │
└──────────────────┘

Ok, still a difference of about 1 million? Lets do it bottom up:
csvfile = ""~/data/amazon/*.tsv""
(
    pl.scan_csv(csvfile, separator = '\t') 
    .groupby([""product_category""])
    .agg(pl.col(""product_category"").count().alias(""counts""))
    .collect(streaming = True)
    .filter(pl.col('counts') > 100)
    .sort(pl.col(""counts""), descending = True)
    .select(
        pl.col('counts').sum()
    )
)
shape: (1, 1)
┌─────────┐
│ counts  │
│ ---     │
│ u32     │
╞═════════╡
│ 7125553 │
└─────────┘

Close, albeit that its once again a different count...
Some more checks using R:
library(vroom)
library(purrr)
library(glue)
library(logger)
amazon <- list.files(""~/data/amazon/"", full.names = TRUE)
f <- function(file){
     df <- vroom(file, col_select = 'product_category', show_col_types=FALSE )
     log_info(glue(""File [{basename(file)}] has [{nrow(df)}] rows""))
}

walk(amazon, f)
INFO [2023-06-02 14:23:40] File [amazon_reviews_us_Office_Products_v1_00.tsv] has [2633651] rows
INFO [2023-06-02 14:23:41] File [amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv] has [85898] rows
INFO [2023-06-02 14:24:06] File [amazon_reviews_us_Shoes_v1_00.tsv] has [4353998] rows
INFO [2023-06-02 14:24:30] File [amazon_reviews_us_Software_v1_00.tsv] has [331152] rows
INFO [2023-06-02 14:24:37] File [amazon_reviews_us_Watches_v1_00.tsv] has [943763] rows

Total: 8348462

Ok. Screw it. Basically a random number generating exercise and nothing is real.
Surely if its a data hygiene issue the error should be constant? Any idea why there might be such a large discrepancy?
",76392845,"It's usually helpful to declare the size of downloads in cases like this.

For any readers, the total size is 1.3 GB

The smallest file is https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv.gz at 17.6 MB


I tried pandas to debug this, it cannot read any of these files:
pd.read_csv('amazon-reviews/amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv', sep='\t')

ParserError: Error tokenizing data. C error: 
 Expected 15 fields in line 1598, saw 22

Line 1598:
US  3878437 R3BH4UXFRP6F8L  B00J7G8EL0  381088677   GUM Expanding Floss - 30 m - 2 pk   Personal_Care_Appliances    
4   0   0   N   Y   "" like the REACH woven that's no longer available--THAT was the wish it was a bit &#34;fluffier,&#34; like the REACH woven that's no longer available--THAT was the best    2015-08-06

The issue is the single "" character, you need to disable the default quoting behaviour.
With that change I get a total count of 8398134 each time.

polars
(pl.scan_csv('amazon-reviews/*.tsv', separator='\t', quote_char=None)
   .select(pl.count())
   .collect()
)

CPU times: user 3.65 s, sys: 2.02 s, total: 5.67 s
Wall time: 2.48 s
shape: (1, 1)
┌─────────┐
│ count   │
│ ---     │
│ u32     │
╞═════════╡
│ 8398134 │
└─────────┘


pandas
sum(
   len(pd.read_csv(file, sep='\t', quoting=3).index)
   for file in files
)

CPU times: user 57.6 s, sys: 9.78 s, total: 1min 7s
Wall time: 1min 7s
8398134


duckdb
duckdb.sql(""""""
from read_csv_auto('amazon-reviews/*.tsv', sep='\t', quote='')
select count(*)
"""""").pl()

CPU times: user 12.4 s, sys: 2.32 s, total: 14.7 s
Wall time: 5.05 s
shape: (1, 1)
┌──────────────┐
│ count_star() │
│ ---          │
│ i64          │
╞══════════════╡
│ 8398134      │
└──────────────┘


pyarrow
parse_options = pyarrow.csv.ParseOptions(delimiter='\t', quote_char=False)

sum(
   pyarrow.csv.read_csv(file, parse_options=parse_options).num_rows
   for file in files
)

CPU times: user 12.9 s, sys: 6.46 s, total: 19.4 s
Wall time: 6.65 s
8398134

",
76389541,How to upload only updated files in Google Apps Script using clasp?,"I'm developing Google Apps Script locally. I used the clasp push --watch command for pushing updated code to the original Apps Script as per the documentation.
https://www.npmjs.com/package/@google/clasp#push
The problem is that when I update the code of any one file, it uploads the whole file. I have attached the screenshot of the file structure and vs code terminal.
You can see there I had updated the code of home.html but still, it's uploaded the whole project. Because of this, we are getting time-consuming issues. It's taking a long time to upload the project. I had also attached the screen shot of clasp.json file.
Is there any way to upload the only updated file ?



",76391919,"It seems this behavior is expected based on the documentation:

Warning: Google scripts APIs do not currently support atomic nor per file operations. Thus the push command always replaces the whole content of the online project with the files being pushed.


clasp push replaces code that is on script.google.com and clasp pull
replaces all files locally. For this reason, follow these guidelines:
Do not concurrently edit code locally and on script.google.com. Use a
version control system, like git.

A new feature would be required from the Google Apps Script API to accomplish what you're looking for, it seems there is already a Feature Request here, you can vote for it, add your comments and look for any future updates.
References

Pulling & Pushing Files
Feature request: clasp push -w - add option to push only changed files
Problems when using CLASP to push client-side files from a local editor

",
76389283,How to retrieve an image from database and show it in asp image box with a click of a button?,"private void ProcessedImage()
    {
        try
        {
            if (FileUpload1.HasFile)
            {

                int length = 192;
                int width = 192;

                using (Bitmap sourceImage = new Bitmap(FileUpload1.PostedFile.InputStream))
                {
                    using (Bitmap resizedImage = new Bitmap(length, width))
                    {
                        using (Graphics graphics = Graphics.FromImage(resizedImage))
                        {
                            graphics.InterpolationMode = InterpolationMode.HighQualityBicubic;
                            graphics.SmoothingMode = SmoothingMode.HighQuality;
                            graphics.PixelOffsetMode = PixelOffsetMode.HighQuality;

                            graphics.DrawImage(sourceImage, 0, 0, length, width);
                        }

                        string resizedImagePath = Server.MapPath(""~/Images/Image.png"");
                        resizedImage.Save(resizedImagePath, ImageFormat.Png);

                        ImgPhoto.ImageUrl = ""~/Images/Image.png"";
                    }
                }
            }
        }
        catch (Exception ex)
        {
            string errorMessage = (""An error occurred "" + ex.Message);
        }
    }

    public void Save()
        {
            try
            {
                byte[] imageData;
                using (MemoryStream ms = new MemoryStream())
                {
                    using (Bitmap bitmap = new Bitmap(Server.MapPath(""~/Images/finalImage.png"")))
                    {
                        bitmap.Save(ms, ImageFormat.Png);
                        imageData = ms.ToArray();
                    }
                }
        
                using (SqlConnection con = new SqlConnection(""Data Source=127.0.0.1;Initial Catalog=Karthik;User ID=admin;Password=admin""))
                {
                    con.Open();
                    SqlCommand cmd = new SqlCommand(""INSERT INTO image_tbl (ImageID,image_data) VALUES (@ImageID,@image_data)"", con);
                    cmd.Parameters.AddWithValue(""@ImageID"", ImageID.Text.Trim());
                    cmd.Parameters.AddWithValue(""@image_data"", imageData);
                    cmd.ExecuteNonQuery();

                    Response.Write(""<script>alert('Saved Succefully')</script>"");
                }
            }
            catch (Exception ex)
            {
                string errorMessage = ""An error occurred: "" + ex.Message;
            }
        }

This is my code. I resized the image and saved it in the database. Now I want to take an input from the user for Image ID and retrieve the image corresponding to the image ID and show it in the asp text box. Is it possible to do so? Note that I am working in Visual Studio 2010. And I don't think it supports JavaScript codes.
",76391945,"Ok, first up, even vs2010 used lots of JavaScript, and in fact came with a bootstrap main menu, had the JavaScript ""helper"" library jQuery installed.
And in fact, YOUR above code even uses JavaScript here:
 Response.Write(""<script>alert('Saved Succefully')</script>"");

So, to be clear?
You have full use of JavaScript, and there is ZERO issues in regards to enjoying the use of JavaScript, and nothing stops you from using JavaScript.
Now, having stated the above, there is no need to use JavaScript, and you don't have to write any JavaScript to solve your question. But, to be crystal clear, you have full use of JavaScript in your markup, and even projects created in vs2010 have FULL support for using JavaScript.
Ok, so, lets assume we have that saved image, as you note nice and small, and you want to show the preview.
About the ONLY real issue? How many images do you need to display on a page.
Why do I ask ""how many"" images?
Answer:
Because I going to post a VERY easy bit of code to display that image from the database, but this approach NEEDS MUCH caution, since we going to stream the image to the page as raw image bytes, and this approach means the ""image"" will travel back to the server when you click on a button, or ANY thing on that page that triggers a post-back (standard round trip) of the browser will ""increase"" the size and payload of this web page.
So, for a smaller image or thumbnail type of image display, and only a few on the page, then this approach is acceptable, and VERY easy to code.
However, if there are to be many images, or the image is large, then I do NOT recommend this ""easy"" and ""simple"" code approach.
As noted, since we only are to display one image, and since it is small, then we don't care. However, if your goal was to display MANY images on a page, then I do not recommend this approach for more then a few images displayed at the same time on a single page.
Ok, I don' have your data, but this code shows how you can do this:
I have a sql server table, and one of the columns is raw byte data of the saved image.
The OTHER issue is you NEED to know what file extension your picture was in the first place. For this example, it looks like .png, but if you are to allow different kinds of images, then you MUST save the file extension, or even better yet save the so called ""mine"" type of the image. This information allows us to tell the browser what kind of image we want to display.
So, lets say we fill a dropdown list with the rows of the database (without the column that holds the data).
So, we have this markup:
        <h3>Select Fighter</h3>
        <asp:DropDownList ID=""cboFighters"" runat=""server""
            DataValueField=""ID""
            DataTextField=""Fighter""
            AutoPostBack=""true""
            OnSelectedIndexChanged=""cboFighters_SelectedIndexChanged""
            Width=""250px"">
        </asp:DropDownList>

        <br />


        <div class=""mybox"" style=""float: left"">
            <div style=""text-align: center; padding: 2px 10px 12px 10px"">

                <h3 id=""Fighter"" runat=""server""></h3>
                <asp:Image ID=""Image2"" runat=""server""
                    Width=""180"" Height=""120"" />

                <h4>Engine</h4>
                <asp:Label ID=""EngineLabel"" runat=""server"" Text="""" />

                <h4>Description</h4>
                <asp:Label ID=""DescLabel"" runat=""server"" Width=""400px""
                    Text="""" Style=""text-align: left"" Font-Size=""Large"" />
            </div>
        </div>

Note close in above, there is a image control.
So, now our code behind is this:
    protected void Page_Load(object sender, EventArgs e)
    {
        if (!IsPostBack)
        {
            cboFighters.DataSource =
                General.MyRst(""SELECT ID, Fighter FROM Fighters"");
            cboFighters.DataBind();
            cboFighters.Items.Insert(0, new ListItem(""Select Fighter"", ""0""));
        }
    }

    protected void cboFighters_SelectedIndexChanged(object sender, EventArgs e)
    {

        SqlCommand cmdSQL = 
            new SqlCommand(""SELECT * FROM Fighters WHERE ID = @ID"");
        cmdSQL.Parameters.Add(""@ID"", SqlDbType.Int).Value = cboFighters.SelectedItem.Value;

        DataRow OneFighter = General.MyRstP(cmdSQL).Rows[0];
        Fighter.InnerText = OneFighter[""Fighter""].ToString();
        EngineLabel.Text = OneFighter[""Engine""].ToString();
        DescLabel.Text = OneFighter[""Description""].ToString();

        string sMineType = OneFighter[""MineType""].ToString();
        // stringSmineType = ""image/png""  // hard coded value if all are .png

        byte[] MyBytePic = (byte[])OneFighter[""MyImage""];

        Image2.ImageUrl = 
            $@""data:{sMineType};base64,{Convert.ToBase64String(MyBytePic)}"";

    }

And the result is this:

Note the use of ""mine mapping"". Quite sure that needs .net 4.5 or later.
However, if all of your raw byte saved images in the database are .png, then you can hard code as the above commented out line shows.
Note that the above same trick works if we fill out a gridview.
However, as I stated, use ""some"" caution with this approach, since the image does not have any ""real"" URL path name to resolve, the browser will wind up sending the picture back to the server with each button click (and post back). Since there is no ""real"" URL, then the browser cannot cache such pictures. As noted, you can also consider creating a http handler, and they play much nicer then sending the picture as a base64 string.
So, if your concerned about keeping the page size small, or to leverage browser caching, then consider a custom picture handler.
However, since in your case we have a small picture of 192x192, then the above approach is fine and easy to code.
",
76394222,How to compile Packages in java,"I am new to java , how do I compile packages in Intelij
This is my directory

It is running perfectly in Main.java
To compile , I first ran
javac people/People.java
Then I compile javac Main.java, it returns Main.java:8: error: package com.amigoscode.people does not exist
Main.java
package com.amigoscode;

// Press Shift twice to open the Search Everywhere dialog and type `show whitespaces`,
// then press Enter. You can now see whitespace characters in your code.


import java.util.Scanner;
import com.amigoscode.people.People;


public class Main {

    public static void main(String[] args) {
        People p = new People();
        System.out.println(p.fullName());
    }

}

People.java
package com.amigoscode.people;

public class People {

    public String fname = ""John"";
    public String lname = ""Doe"";
    public int age = 24;


    public String fullName() {
        String fullName = fname + "" "" + lname;
        return fullName;
    }

}

Thank you very much.
",76394267,"First, navigate to the src directory using terminal. Then execute the command below to compile the People.java.
javac com/amigoscode/people/People.java

Now, to compile the Main.java execute the command below.
javac com/amigoscode/Main.java

This should work fine.
",
76389911,"CMake CMAKE_SOURCE_DIR and CMAKE_CURRENT_SOURCE_DIR to set same path, but only latter 'works'","I'm using CMake in, so far as I can tell, the same way as I always use it.  The problem is that now include_directories($(CMAKE_SOURCE_DIR)/server/include) results in my header files not being found.  When instead I use include_directories($(CMAKE_CURRENT_SOURCE_DIR)/server/include) they are found.
i.e. $(CMAKE_CURRENT_SOURCE_DIR) works, and $(CMAKE_SOURCE_DIR) doesn't.
The bizarre thing is that both of:
message(${CMAKE_SOURCE_DIR}/server/include)
message(${CMAKE_CURRENT_SOURCE_DIR}/server/include)

give:
/home/username/projects/repo/server/include

So why does one work while the other doesn't?
",76391974,"$(FOO) is not the correct syntax to refer to the cmake variable FOO. You need to use ${FOO} instead. Neither of the uses of include_directories should work, unless the build system interprets the path $(CMAKE_CURRENT_SOURCE_DIR)/server/include.
Note that you're using the proper syntax for the message commands.
The message command
message($(CMAKE_CURRENT_SOURCE_DIR)/server/include)

prints
$(CMAKE_CURRENT_SOURCE_DIR)/server/include

demonstrating the issue.
I recommend using the CMAKE_CURRENT_SOURCE_DIR instead of CMAKE_SOURCE_DIR btw in all cases except for rare exceptions where you know that the toplevel CMakeLists.txt is located in a directory with specific contents. The former version is easier to recombine when used as part of a larger project via add_subdirectory.
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/server/include)
#                    ^                        ^

Note that the use of target_include_directories usually is preferrable to the use of include_directories, since the former allows restricts the targets the include directories are applied to to the one specifically mentioned. It also would allow you to make the include directories to targets linking a cmake library target, if desired.
",
76392241,How to fix '535-5.7.8 Username and Password not accepted. Learn more at' error in Devise mail confirmation?,"I'm a beginner programmer Rails. I'll start with a problem: I'm using Devise to work with users, and I tried to enable mail confirmation. It doesn't work, unfortunately. If possible, please help!
My error:
Net::SMTPAuthenticationError in Devise::ConfirmationsController#create
535-5.7.8 Username and Password not accepted. Learn more at
My config/environments/develorment.rb:
config.action_mailer.perform_deliveries = true
config.action_mailer.raise_delivery_errors = true
config.action_mailer.perform_caching = false
config.action_mailer.default_url_options = { host: 'localhost', port: 3000 }
config.action_mailer.delivery_method = :smtp
config.action_mailer.smtp_settings = {
  address: ""smtp.gmail.com"",
  port: 587,
  authentication: ""plain"",
  enable_starttls_auto: true,
  user_name: ""xyz@gmail.com"",
  password: ""xyz"", 
  domain: ""gmail.com"",
  openssl_verify_mode: ""none"",
}

My config/initializers/devise.rb
config.mailer_sender = ""xxx@gmail.com""

config.mailer = 'Devise::Mailer'

My server starts with configurations:
=> Booting Puma
=> Rails 7.0.5 application starting in development
=> Run `bin/rails server --help` for more startup options
Puma starting in single mode...
* Puma version: 5.6.5 (ruby 3.2.0-p0) (""Birdie's Version"")
*  Min threads: 5
*  Max threads: 5
*  Environment: development
*          PID: 6074
* Listening on http://127.0.0.1:3000
* Listening on http://[::1]:3000
Use Ctrl-C to stop

And Errors in files (or what is it called):
net-smtp (0.3.3) lib/net/smtp.rb:1088:in `check_auth_response'
net-smtp (0.3.3) lib/net/smtp.rb:845:in `auth_plain'
net-smtp (0.3.3) lib/net/smtp.rb:837:in `public_send'
net-smtp (0.3.3) lib/net/smtp.rb:837:in `authenticate'
net-smtp (0.3.3) lib/net/smtp.rb:670:in `do_start'
net-smtp (0.3.3) lib/net/smtp.rb:611:in `start'
mail (2.8.1) lib/mail/network/delivery_methods/smtp.rb:109:in `start_smtp_session'
mail (2.8.1) lib/mail/network/delivery_methods/smtp.rb:100:in `deliver!'
mail (2.8.1) lib/mail/message.rb:2145:in `do_delivery'
mail (2.8.1) lib/mail/message.rb:253:in `block in deliver'
actionmailer (7.0.5) lib/action_mailer/base.rb:588:in `block in deliver_mail'
activesupport (7.0.5) lib/active_support/notifications.rb:206:in `block in instrument'
activesupport (7.0.5) lib/active_support/notifications/instrumenter.rb:24:in `instrument'
activesupport (7.0.5) lib/active_support/notifications.rb:206:in `instrument'
actionmailer (7.0.5) lib/action_mailer/base.rb:586:in `deliver_mail'
mail (2.8.1) lib/mail/message.rb:253:in `deliver'
actionmailer (7.0.5) lib/action_mailer/message_delivery.rb:119:in `block in deliver_now'
actionmailer (7.0.5) lib/action_mailer/rescuable.rb:17:in `handle_exceptions'
actionmailer (7.0.5) lib/action_mailer/message_delivery.rb:118:in `deliver_now'
devise (4.9.2) lib/devise/models/authenticatable.rb:204:in `send_devise_notification'
devise (4.9.2) lib/devise/models/confirmable.rb:121:in `send_confirmation_instructions'
devise (4.9.2) lib/devise/models/confirmable.rb:136:in `block in resend_confirmation_instructions'
devise (4.9.2) lib/devise/models/confirmable.rb:239:in `pending_any_confirmation'
devise (4.9.2) lib/devise/models/confirmable.rb:135:in `resend_confirmation_instructions'
devise (4.9.2) lib/devise/models/confirmable.rb:321:in `send_confirmation_instructions'
devise (4.9.2) app/controllers/devise/confirmations_controller.rb:11:in `create'
actionpack (7.0.5) lib/action_controller/metal/basic_implicit_render.rb:6:in `send_action'
actionpack (7.0.5) lib/abstract_controller/base.rb:215:in `process_action'
actionpack (7.0.5) lib/action_controller/metal/rendering.rb:165:in `process_action'
actionpack (7.0.5) lib/abstract_controller/callbacks.rb:234:in `block in process_action'
activesupport (7.0.5) lib/active_support/callbacks.rb:118:in `block in run_callbacks'
actiontext (7.0.5) lib/action_text/rendering.rb:20:in `with_renderer'
actiontext (7.0.5) lib/action_text/engine.rb:69:in `block (4 levels) in <class:Engine>'
activesupport (7.0.5) lib/active_support/callbacks.rb:127:in `instance_exec'
activesupport (7.0.5) lib/active_support/callbacks.rb:127:in `block in run_callbacks'
activesupport (7.0.5) lib/active_support/callbacks.rb:138:in `run_callbacks'
actionpack (7.0.5) lib/abstract_controller/callbacks.rb:233:in `process_action'
actionpack (7.0.5) lib/action_controller/metal/rescue.rb:22:in `process_action'
actionpack (7.0.5) lib/action_controller/metal/instrumentation.rb:67:in `block in process_action'
activesupport (7.0.5) lib/active_support/notifications.rb:206:in `block in instrument'
activesupport (7.0.5) lib/active_support/notifications/instrumenter.rb:24:in `instrument'
activesupport (7.0.5) lib/active_support/notifications.rb:206:in `instrument'
actionpack (7.0.5) lib/action_controller/metal/instrumentation.rb:66:in `process_action'
actionpack (7.0.5) lib/action_controller/metal/params_wrapper.rb:259:in `process_action'
activerecord (7.0.5) lib/active_record/railties/controller_runtime.rb:27:in `process_action'
actionpack (7.0.5) lib/abstract_controller/base.rb:151:in `process'
actionview (7.0.5) lib/action_view/rendering.rb:39:in `process'
actionpack (7.0.5) lib/action_controller/metal.rb:188:in `dispatch'
actionpack (7.0.5) lib/action_controller/metal.rb:251:in `dispatch'
actionpack (7.0.5) lib/action_dispatch/routing/route_set.rb:49:in `dispatch'
actionpack (7.0.5) lib/action_dispatch/routing/route_set.rb:32:in `serve'
actionpack (7.0.5) lib/action_dispatch/routing/mapper.rb:18:in `block in <class:Constraints>'
actionpack (7.0.5) lib/action_dispatch/routing/mapper.rb:48:in `serve'
actionpack (7.0.5) lib/action_dispatch/journey/router.rb:50:in `block in serve'
actionpack (7.0.5) lib/action_dispatch/journey/router.rb:32:in `each'
actionpack (7.0.5) lib/action_dispatch/journey/router.rb:32:in `serve'
actionpack (7.0.5) lib/action_dispatch/routing/route_set.rb:852:in `call'
warden (1.2.9) lib/warden/manager.rb:36:in `block in call'
warden (1.2.9) lib/warden/manager.rb:34:in `catch'
warden (1.2.9) lib/warden/manager.rb:34:in `call'
rack (2.2.7) lib/rack/tempfile_reaper.rb:15:in `call'
rack (2.2.7) lib/rack/etag.rb:27:in `call'
rack (2.2.7) lib/rack/conditional_get.rb:40:in `call'
rack (2.2.7) lib/rack/head.rb:12:in `call'
actionpack (7.0.5) lib/action_dispatch/http/permissions_policy.rb:38:in `call'
actionpack (7.0.5) lib/action_dispatch/http/content_security_policy.rb:36:in `call'
rack (2.2.7) lib/rack/session/abstract/id.rb:266:in `context'
rack (2.2.7) lib/rack/session/abstract/id.rb:260:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/cookies.rb:704:in `call'
activerecord (7.0.5) lib/active_record/migration.rb:603:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/callbacks.rb:27:in `block in call'
activesupport (7.0.5) lib/active_support/callbacks.rb:99:in `run_callbacks'
actionpack (7.0.5) lib/action_dispatch/middleware/callbacks.rb:26:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/executor.rb:14:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/actionable_exceptions.rb:17:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/debug_exceptions.rb:28:in `call'
web-console (4.2.0) lib/web_console/middleware.rb:132:in `call_app'
web-console (4.2.0) lib/web_console/middleware.rb:28:in `block in call'
web-console (4.2.0) lib/web_console/middleware.rb:17:in `catch'
web-console (4.2.0) lib/web_console/middleware.rb:17:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/show_exceptions.rb:26:in `call'
railties (7.0.5) lib/rails/rack/logger.rb:40:in `call_app'
railties (7.0.5) lib/rails/rack/logger.rb:25:in `block in call'
activesupport (7.0.5) lib/active_support/tagged_logging.rb:99:in `block in tagged'
activesupport (7.0.5) lib/active_support/tagged_logging.rb:37:in `tagged'
activesupport (7.0.5) lib/active_support/tagged_logging.rb:99:in `tagged'
railties (7.0.5) lib/rails/rack/logger.rb:25:in `call'
sprockets-rails (3.4.2) lib/sprockets/rails/quiet_assets.rb:13:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/remote_ip.rb:93:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/request_id.rb:26:in `call'
rack (2.2.7) lib/rack/method_override.rb:24:in `call'
rack (2.2.7) lib/rack/runtime.rb:22:in `call'
activesupport (7.0.5) lib/active_support/cache/strategy/local_cache_middleware.rb:29:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/server_timing.rb:61:in `block in call'
actionpack (7.0.5) lib/action_dispatch/middleware/server_timing.rb:26:in `collect_events'
actionpack (7.0.5) lib/action_dispatch/middleware/server_timing.rb:60:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/executor.rb:14:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/static.rb:23:in `call'
rack (2.2.7) lib/rack/sendfile.rb:110:in `call'
actionpack (7.0.5) lib/action_dispatch/middleware/host_authorization.rb:137:in `call'
railties (7.0.5) lib/rails/engine.rb:530:in `call'
puma (5.6.5) lib/puma/configuration.rb:252:in `call'
puma (5.6.5) lib/puma/request.rb:77:in `block in handle_request'
puma (5.6.5) lib/puma/thread_pool.rb:340:in `with_force_shutdown'
puma (5.6.5) lib/puma/request.rb:76:in `handle_request'
puma (5.6.5) lib/puma/server.rb:443:in `process_client'
puma (5.6.5) lib/puma/thread_pool.rb:147:in `block in spawn_thread'

The message is shown in the terminal.
And yes, I know it's important, e-mail exists, and the data is taken from it.
I tried many options, including the part of the code that I ended up with. Nothing helps.
",76392975,"This is related to ActionMailer configuration rather than Devise.
From the documentation authentication should be set to one of the following
:plain, :login or :cram_md5
The following is an excerpt taken from the relevant section in the documentation linked to above and is the explanation of the available options and what they do, so it all depends on your mail server as to which you wish to use however you can see that SSL is not amongst the available options

:authentication - If your mail server requires authentication, you
need to specify the authentication type here. This is a symbol and one
of :plain (will send the password in the clear), :login (will send
password Base64 encoded) or :cram_md5 (combines a Challenge/Response
mechanism to exchange information and a cryptographic Message Digest 5
algorithm to hash important information)

As requested, a typical gmail configuration would be to use SMTP like so
config.action_mailer.delivery_method = :smtp
config.action_mailer.smtp_settings = {
  address:              'smtp.gmail.com',
  port:                 587,
  domain:               'example.com',
  user_name:            '<username>',
  password:             '<password>',
  authentication:       'plain',
  enable_starttls_auto: true,
  open_timeout:         5,
  read_timeout:         5 }

Taken from the rails action mailer documentation section 5.2
Make sure to replace username x's and password x's with your credentials and NEVER post your credentials anywhere public, I suggest you amend your password on your google account immediately, I have edited your question to hide your password but it is still available to view by those with the correct privileges
",
76389742,why .gitconfig [includIf] override the default config,"I have two keys to different github account and there is the config ~/.gitconfig:
[user]
        name = example
        email = example@example.com
[pull]
        rebase = true
[rebase]
        autoStash = true
[filter ""lfs""]
        clean = git-lfs clean -- %f
        smudge = git-lfs smudge -- %f
        process = git-lfs filter-process
        required = true
[includeIf ""gitdir/i:/Users/example/Documents/github_2/""]
    [core]
        sshCommand = ""ssh -i ~/.ssh/github2_key""

then, I go to a folder that did not at /Users/example/Documents/github_2/, and run git clone github1_private_project, git told me Please make sure you have the correct access rights
It works to git clone the project from account 'github2' in /Users/example/Documents/github_2/

git version:
% git -v
git version 2.39.2 (Apple Git-143)

",76392034,"The syntax for includeIf should be (see the docs):
[includeIf ""gitdir/i:/Users/example/Documents/github_2/""]
path = </path/to/includeFile>

The syntax
[includeIf ""gitdir/i:/Users/example/Documents/github_2/""]
    [core]
        sshCommand = ""ssh -i ~/.ssh/github2_key""

actually means
[includeIf ""gitdir/i:/Users/example/Documents/github_2/""]
# No `path` hence no include

[core]
    sshCommand = ""ssh -i ~/.ssh/github2_key""

where the key core.sshCommand is always (unconditionally) defined.
",
76385174,How can I define a type in TypeScript that's a string that only should contain words from a predefined list,"I have a tricky TypeScript question.
Let say I have this Icon component with the prop size. Size can be ""2"", ""4"", ""6"". I map these values to predefined tailwind classes.
So I type it like
type SizeValues = '2' | '4' | '6';

function Icon({size = '4'}: {size: SizeValues}) {
   const sizeMap = {
     '2': 'w-2 h-2',
     '4': 'w-4 h-4',
     '6': 'w-6 h-6',
   };
 
   return <span className={sizeMap[size]}>My icon goes here</span>
}

<Icon size=""sm"" />

Everything is fine. But what if I wanna have different sizes depending on what screen size I have. So I wanna try to have like tailwinds nice syntax.
So I rewrite my Icon component to following:
type SizeValues = ???

function Icon({size = '4'}: {size: SizeValues}) {
   const sizeMap = {
     '2': 'w-2 h-2',
     '4': 'w-4 h-4',
     '6': 'w-6 h-6',
     'md:2': 'md:w-2 md:h-2',
     'md:4': 'md:w-4 md:h-4',
     'md:6': 'md:w-6 md:h-6',
     'lg:2': 'lg:w-2 lg:h-2',
     'lg:4': 'lg:w-4 lg:h-4',
     'lg:6': 'lg:w-6 lg:h-6',
   };
 
   return <span className={size.split(' ').map(s => sizeMap[s]).join(' ').trim()}>My icon goes here</span>
}

<Icon size=""2 md:4 lg:6"" />

That works fine, but how do I type it? I read TypeScript will support regex in the future. That will make it easier, but is it possible to type this now?
This is not a real component so please don't give me awesome suggestions how I can improve it. I just wanna know how I can type my size prop so it works the way I've coded it.
",76392131,"First, we need to extract sizeMap into the global scope, and const assert it to let the compiler know that this is immutable constant and restrict  it from widening types:
const sizeMap = {
  '2': 'w-2 h-2',
  '4': 'w-4 h-4',
  '6': 'w-6 h-6',
  'md:2': 'md:w-2 md:h-2',
  'md:4': 'md:w-4 md:h-4',
  'md:6': 'md:w-6 md:h-6',
  'lg:2': 'lg:w-2 lg:h-2',
  'lg:4': 'lg:w-4 lg:h-4',
  'lg:6': 'lg:w-6 lg:h-6',
} as const;

Next, we need to get a type for the keys of the sizeMap:
type SizeMap = typeof sizeMap;
type SizeMapKeys = keyof SizeMap;

Implementation:
We will create a type that accepts a string and returns it if it is valid; otherwise, return never.
Pseudo-code:
Let type accept T - string to validate, Original - original string, AlreadyUsed - union of already used keys.
If T is an empty string

return Original
Else if T starts with keys of the size map (ClassName), excluding AlreadyUsed, followed by a space and the remaining string(Rest).

Recursively call this type, passing Rest as a string to validate Original, and the AlreadyUsed with ClassName added to it.


Else if T is the key of the size map excluding AlreadyUsed

return Original
else
return never

Realization:
type _SizeValue<
  T extends string,
  Original extends string = T,
  AlreadyUsed extends string = never
> = T extends """"
  ? Original
  : T extends `${infer ClassName extends Exclude<
      SizeMapKeys,
      AlreadyUsed
    >} ${infer Rest extends string}`
  ? _SizeValue<Rest, Original, AlreadyUsed | ClassName>
  : T extends Exclude<SizeMapKeys, AlreadyUsed>
  ? Original
  : never;

We have to add a generic parameter to Item that will represent the size.
function Icon<T extends string | undefined>({
  size,
}: {
  size: _SizeValue<T>;
}) {
  return null;
}

Since, size is optional in the component, we will add a wrapper around the SizeValue which will turn string | undefined to string and pass it to _SizeValue, additionally we will add a default value for size:
type SizeValue<T extends string | undefined> = _SizeValue<NonNullable<T>>;

function Icon<T extends string | undefined>({
  size = ""2"",
}: {
  size?: SizeValue<T> | ""2"";
}) {
  return null;
}

Usage:
<Icon size=""2"" />;
<Icon size=""md:2"" />;
<Icon size=""md:2 md:6"" />;
<Icon size=""md:2 md:6 lg:6"" />;

// expected error
<Icon size=""md:2 md:6 lg:5"" />;

// no duplicates allowed
<Icon size=""2 2"" />;

playground
",
76394256,Why does __m128 cause alignment issues in a union with float x/y/z?,"I've never actually ran into this problem before, at least not that I'm aware of... But I'm working on some SIMD vector optimizations in some of my code and I'm having some alignment issues.
Here's some minimal code that I've been able to reproduce the problem with, on MSVC (Visual Studio 2022):
#include <stdio.h>
#include <stdint.h>
#include <stdbool.h>
#include <stdlib.h>
#include <string.h>
#include <xmmintrin.h>

_declspec(align(16)) typedef union
{
    struct { float x, y, z; };

#if 0
    // This works:
    float v[4];
#else
    // This does not:
    __m128 v;
#endif
} vec;

typedef struct
{
    vec pos;
    vec vel;
    float radius;
} particle;

int main(int argc, char **argv)
{
    particle *particles=malloc(sizeof(particle)*10);

    if(particles==NULL)
        return -1;

    // intentionally misalign the pointer
    ((uint8_t *)particles)+=3;

    printf(""misalignment: %lld\n"", (uintptr_t)particles%16);

    particles[0].pos=(vec){ 1.0f, 2.0f, 3.0f };
    particles[0].vel=(vec){ 4.0f, 5.0f, 6.0f };

    printf(""pos: %f %f %f\nvel: %f %f %f\n"",
           particles[0].pos.x, particles[0].pos.y, particles[0].pos.z,
           particles[0].vel.x, particles[0].vel.y, particles[0].vel.z);

    return 0;
}

I don't understand why a union with float x/y/z and float[4] works with misaligned memory addresses, but a union with the float x/y/z and an __m128 generates an access violation.
I get that the __m128 type has some extra alignment specs on it, but the overall union size doesn't change and it's also 16 byte aligned anyway, so why does it matter?
I do understand the importance of memory alignment, but the extra weird part is that I added in an aligned_malloc to my code that's allocating the offending misaligned memory (I use a slab/zone memory allocator in my code) and it still continued to crash out with an access violation, which further adds to my hair loss.
",76394286,"alignof(your_union) is 16 when it includes a __m128 member, so compilers will use movaps or movdqa because you've promised them that the data is aligned. Otherwise alignof(your_union) is only 4 (inherited from float, so they'll use movups or movdqu which has no alignment requirement.
It's still alignment undefined behaviour, as gcc -fsanitize=undefined will tell you, since you're using an address that's not even aligned by 4.
https://godbolt.org/z/6GxebxT7r shows MSVC is using movdqa stores for your code, like movdqa [rbx+19], xmm2 where RBX holds a malloc return value.  This is guaranteed to fault because malloc return values are aligned by alignof(max_align_t), which is definitely an even number and usually 16 in x86-64.
Often MSVC will only use unaligned movdqu / movups loads/stores even when you use _mm_store_ps.  (But alignment-required intrinsics will let it fold the load into a memory source operand for non-AVX instructions like addps xmm0, [rcx]).
But apparently MSVC treats aggregates differently from deref of a __m128*.
So your type has alignof(T) == 16, and thus your code has alignment UB, so it can and does compile to asm that faults.

BTW, I wouldn't recommend using this union; especially not for function args / return values since being part of an aggregate can make the calling conventions treat it less efficiently.  (On MSVC you have to use vectorcall to get it passed in a register if it doesn't inline, but x86-64 System V passes vector args in vector regs normally, if they aren't part of a union.)
Use __m128 vectors and write helper functions to get your data in/out as scalar.
Ideally don't use 1 SIMD vector to hold 1 geometry vector, that's kind of an anti-pattern since it leads to a lot of shuffling.  Better to have arrays of x, arrays of y, and arrays of z, so you can load 3 vectors of data and process 4 vectors in parallel with no shuffling.  (Struct-of-Arrays rather than Array-of-Structs).  See https://stackoverflow.com/tags/sse/info especially https://deplinenoise.wordpress.com/2015/03/06/slides-simd-at-insomniac-games-gdc-2015/
Or if you really want to do it this way, you could still improve this.  Your struct particle is 36 bytes as you've defined it, with two wasted 32-bit float slots.  It could have been 32 bytes: xyz, radius, xyz, zeroed padding, so you could have alignof(particle) == 16 without increasing the size to 48 bytes, to be able to load it efficiently (never spanning cache-line boundaries).  The radius would get loaded as high garbage along _mm_load_ps(&particle->pos_x) which gets the x,y,z positions and whatever comes next.  You might sometimes have to use an extra instruction to zero out the high element, but probably most of the time you could be shuffling in ways that don't care about it.
Actually your struct particle is 48 bytes when you have a __m128 member, since it inherits the alignof(T) from its vec pos and vec vel members, and sizeof(T) has to be a multiple of alignof(T) (so arrays work).
",
76390159,Am I using NSIS nsexec::ExecToStack Output correctly?,"I am calling nsExec::ExecToStack to get the output of a powershell command.  Based on the result, I will either do nothing, or install a windows feature.  I seem to get the result I expect from the powershell command, but the if/then logic is not doing what I expect.
Here's the code:
DetailPrint ""======================""
DetailPrint ""Checking MSMQ Services""
DetailPrint ""======================""

nsExec::ExecToStack 'powershell.exe -command ""(get-windowsfeature -name MSMQ-Services).InstallState""'
Pop $0
Pop $InstallState

DetailPrint ""MSMQ is: $InstallState""

${If} $Installstate == ""Installed""
    DetailPrint ""MSMQ is Installed""
${Else}
    DetailPrint ""========================""
    DetailPrint ""Installing MSMQ Services""
    DetailPrint ""========================""

    nsexec::exectolog 'powershell -command ""install-windowsfeature -name msmq-services""'
    
${EndIf}

I set ${DisableX64FSRedirection} in the same section, above this code, to ensure powershell is the 64-bit version and can run the command properly.
Here is what I am seeing in the Log:
======================
Checking MSMQ Services
======================
MSMQ is: Installed

========================
Installing MSMQ Services
========================

Success Restart Needed Exit Code      Feature Result                               
------- -------------- ---------      --------------                               
True    Yes            NoChangeNeeded {}                                           


I have been staring at this code, tweaking, trying different logic...
Here is what the powershell command returns:
PS C:\Windows\system32> $tom=(get-windowsfeature -name MSMQ-Services).InstallState
PS C:\Windows\system32> echo $tom
Installed

First thought was that there were leading or trailing spaces, so I did the comparison with ""X$InstallStateX"".
I tried adding the /OEM flag to ExectoStack.
I tried variations of if/then logic:
${If} $Installstate != ""Installed""
    <install command>
${EndIf}

Honestly, I'm not sure if this is a stupid logic flaw (on my part) or some funky output from exectostack that I'm not expecting.
",76392137,"It was just a CR/LF at the end of the PowerShell output.
By changing my statement to:
${If} $InstallState == ""Installed$\r$\n""

Worked like it should.
It's always something simple in the end.
",
76389426,Is there a way to warn C++ developers when they accidentally include internal implementation headers of std library?,"How to let know developers automatically that this ""bits/shared_ptr.h"" is internal to standard library (gcc and clang).
#include <bits/shared_ptr.h>

// some code using std::shared_ptr


The best would be to also inform <memory> should be used instead.
This <bits/shared_ptr.h> is just an example - I mean - how to warn about any implementation header being included.
By ""automatically"" I mean - compiler warning or static analysis like clang-tidy.
I have tried ""-Wall -Wextra -pedantic"" and all clang-tidy checks, without llvm* checks - these llvm* warns for almost every header and it is just for llvm developers, not for us, regular developers.
Any advice?
I prefer existing solution, I know I can write script for that.

Ok, I found one check in clang-tidy that I can use.
It is portability-restrict-system-includes
Just need to specify in config that ""bits"" things are not allowed:
-config=""CheckOptions: {portability-restrict-system-includes.Includes: '*,-bits/*,bitset'}""
See demo.
But, well, it is not perfect solution - one would need to maintain list of ""not allowed"" headers.
",76392179,"It seems like include-what-you-use does what you want.
It has a mapping of what names are supposed to come from what header, and it seems to know which headers are internal.
For example, when including <bits/shared_ptr.h> https://godbolt.org/z/cvq7354K6:
#include <bits/shared_ptr.h>

std::shared_ptr<int> x;

It says to remove <bits/shared_ptr.h> and add <memory>.
",
76392017,How can I fix incorrect flat lines on my PowerBI line chart with a date hierarchy?,"I have a PowerBI report with a line chart that shows average costs over a time period. The time period is based on a date that is set to use a date hierarchy for the year and the month.

When I open the report the line chart does not display correctly. The lines are incorrectly flat.

If I change the date from a date hierarchy to just the date and then back to the hierarchy, then it displays correctly.

This also happened when I published the report to the PBI service. What can be done to negate the need to do this?
I have tried resetting the field for the X-axis and restarting the application.
",76393019,"Clicking on the double arrow icon to expand the next level by default resolved my issue.
",
76384859,Create Global HTTPS load-balancer with gcloud,"I haven't kept up with changes to GCP's load-balancing, and they've introduced a new kind of global L7 load-balancer, and the ones which I am used to are now termed ""classic"".
I am not able to find ways to create these new style of load-balancers using gcloud CLI. Is there a way to do this?
",76392236,"Just to mark the question answered, I will post @John_Hanley's reply here:
Set the flag like this:
gcloud gcloud compute backend-services create --load-balancing-scheme=EXTERNAL_MANAGED
",
76394247,Create unique pair of elements in a list based on some attribute of the class/object,"I have a JSON file in the below format
[
  { ""name"": ""John"", ""team"": ""NW"", ""available"": true },
  { ""name"": ""Dani"", ""team"": ""NW"", ""available"": true },
  { ""name"": ""Lyle"", ""team"": ""NW"", ""available"": false },
  { ""name"": ""Dean"", ""team"": ""W"", ""available"": true },
  { ""name"": ""Lyle"", ""team"": ""W"", ""available"": true },
  { ""name"": ""David"", ""team"": ""W"", ""available"": true },
  { ""name"": ""George"", ""team"": ""SW"", ""available"": false },
  { ""name"": ""Luke"", ""team"": ""SW"", ""available"": false }
]

and the corresponding Java class like below:
public class Agent {
  private String name;
  private String team;
  private Boolean available;
  ...
  ...
}

I need to pair members from the list with other people on the list. The rules are that the pair has to be with a member who is available and is from a different team.
Need to list the pairs of the members along with any members which could not be paired (odd number of 'pairable' members/not enough members in other teams).
I have written the code to read the JSON in a list of 'Agent' objects using Jackson and have tried the following approaches till now

Iterating over the list and getting the first 'available' member and fetching another one from different team (used stream API/filtering to do this)
Create a map of agents (key: team, value: agent list) and iterate the EntrySet to pair the agents with agents from another team.
Naive approach of creating anew list and adding elements from the original list as and when they are mapped)

However, I am getting a lot of agents unmapped. I wanted to write the logic in such a way that it matches/pairs maximum numbers of agents and print the remaining agents in the output alongwith the agents that were paired.
Not looking for a full blown solution - any pointers will be greatly appreciated!
",76394291,"To confirm, when running you first,

Check if the agent is available.
Find a matching agent from a different team (From the map of agents)
And if a match is found, add the pair of agents to the list of pairs and mark both agents in the EntrySet, otherwise mark the agent as unmatched.

Correct?
If so, it sounds like you already have a good solution to the problem, and I would double check your code to ensure you are following these steps properly. A limitation within the data (Such as very unbalanced team sizes), could also cause you to get unbalanced results.
If you wanted another approach though, I would suggest using a 'Maximal Matching' Algorithm, like the Hopcroft-Karp algorithm or the Edmonds' Blossom algorithm to find the maximum matching.
In your case, the agents would be represented as nodes in the bipartite graph, Node set 1 representing available agents and Node set 2 representing unavailable agents. Then generate the edges between the two sets, where each node would represent a potential pairing.
An overview of the algorithm can be found here; https://www.geeksforgeeks.org/hopcroft-karp-algorithm-for-maximum-matching-set-1-introduction/
All you would need to do is generate these nodes and edges, as many implementations of the algorithm can be found online :)
",
76390788,How can I temporarily set a folder as the workspace in VS Code to run specific configurations?,"I have a tree structure with multiple different projects in Visual Studio Code (VS Code). Each project is in a separate folder within the tree structure.
Is there a way to make one of these folders the current workspace folder ad hoc, so that I can easily run the configuration specific to that folder?
Is there a way to designate a folder as the active workspace folder temporarily, so that I can execute the configurations specific to that folder without modifying the overall workspace settings?
I've come across the concept of multi-root workspaces in VS Code, but I'm not sure if it allows me to achieve what I need.
",76393048,"
Is there a way to make one of these folders the current workspace folder ad hoc, so that I can easily run the configuration specific to that folder?

As far as I'm aware, the easiest you'll get is using File: Open File... (if you've never opened the folder / workspace) before, or using File: Open Recent... (these are command palette commands and both of which have keybindings, which you can find in the command palette. They're also both available under the ""File"" menu item)


I've come across the concept of multi-root workspaces in VS Code, but I'm not sure if it allows me to achieve what I need.

Multi-root workspaces allow you to create a flat list of workspace roots that are all open at once. You can set configuration for all workspaces by putting it in the .code-workspace file, or per-workspace-root by putting it in their .vscode/settings.json files.
",
76391976,Generate a set of non repeating pairs of users in multiple groups,"I have a dataset that looks like this.
To give some context, there can multiple user groups (odd number of people in it). Each of the groups can contain multiple users in it. So, within each and every group, I needed to select pairs of users in such a fashion that,
A person must not be repeated in any of the pairs, until the entire user list is exhausted. The partial solution below starts pairing users that do not belong to the same group as well. Not sure how to tackle this grouping constraint.




group_id
user_id




1
a1


1
b1


1
c1


1
d1


2
x1




import pandas as pd
import numpy as np

df = [[1, 'a1'], 
      [1, 'b1'], 
      [1, 'c1'], 
      [1, 'd1'], 
      [2, 'x1'], 
      [2, 'y1'], 
      [2, 'z1']]
df = pd.DataFrame(df, columns=['group_id', 'user_id'])
df.head()

I have a partial solution after going through numerous questions and answers.
This solution starts pairing users that do not belong to the same group as well.
Which is not what I want:
from itertools import combinations

# Even Number of users Required
users = df['user_id'].to_list()
n = int(len(users) / 2)

stages = []
for i in range(len(users) - 1):
    t = users[:1] + users[-i:] + users[1:-i] if i else users
    stages.append(list(zip(t[:n], reversed(t[n:]))))
    
print(stages)

Not sure how to store the pairs back into a pandas data frame.

Expected Output (which was updated later on):
For group 1 and group 2, note that there can n number of groups:




group_id
combinations




1
a1-d1


1
b1-c1


1
a1-c1


1
d1-b1


1
a1-b1


2
x1-x2


2
x2-x3


2
x1-x3




Error while running @mozway's code:
This error happens for all inputs:
AssertionError                            Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in _finalize_columns_and_data(content, columns, dtype)
    981     try:
--> 982         columns = _validate_or_indexify_columns(contents, columns)
    983     except AssertionError as err:

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in _validate_or_indexify_columns(content, columns)
   1029             # caller's responsibility to check for this...
-> 1030             raise AssertionError(
   1031                 f""{len(columns)} columns passed, passed data had ""

AssertionError: 1 columns passed, passed data had 6 columns

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_14172\369883545.py in <module>
     24     return stages
     25 
---> 26 out = (df.groupby(['group_id'], as_index=False)['user_id'].apply(combine).explode('user_id'))
     27 print(out.head())

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in apply(self, func, *args, **kwargs)
   1421         with option_context(""mode.chained_assignment"", None):
   1422             try:
-> 1423                 result = self._python_apply_general(f, self._selected_obj)
   1424             except TypeError:
   1425                 # gh-20949

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in _python_apply_general(self, f, data, not_indexed_same)
   1467             not_indexed_same = mutated or self.mutated
   1468 
-> 1469         return self._wrap_applied_output(
   1470             data, values, not_indexed_same=not_indexed_same
   1471         )

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\groupby\generic.py in _wrap_applied_output(self, data, values, not_indexed_same)
   1025                 return self.obj._constructor_sliced(values, index=key_index)
   1026             else:
-> 1027                 result = self.obj._constructor(values, columns=[self._selection])
   1028                 self._insert_inaxis_grouper_inplace(result)
   1029                 return result

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy)
    719                         # ndarray], Index, Series], Sequence[Any]]""
    720                         columns = ensure_index(columns)  # type: ignore[arg-type]
--> 721                     arrays, columns, index = nested_data_to_arrays(
    722                         # error: Argument 3 to ""nested_data_to_arrays"" has incompatible
    723                         # type ""Optional[Collection[Any]]""; expected ""Optional[Index]""

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in nested_data_to_arrays(data, columns, index, dtype)
    517         columns = ensure_index(data[0]._fields)
    518 
--> 519     arrays, columns = to_arrays(data, columns, dtype=dtype)
    520     columns = ensure_index(columns)
    521 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in to_arrays(data, columns, dtype)
    881         arr = _list_to_arrays(data)
    882 
--> 883     content, columns = _finalize_columns_and_data(arr, columns, dtype)
    884     return content, columns
    885 

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in _finalize_columns_and_data(content, columns, dtype)
    983     except AssertionError as err:
    984         # GH#26429 do not raise user-facing AssertionError
--> 985         raise ValueError(err) from err
    986 
    987     if len(contents) and contents[0].dtype == np.object_:

ValueError: 1 columns passed, passed data had 6 columns```

",76393123,"updated answer
Using your code here, but applying it per group:
def combine(s):
    users = s.tolist()
    n = int(len(users) / 2)
    
    stages = []
    for i in range(len(users) - 1):
        t = users[:1] + users[-i:] + users[1:-i] if i else users
        stages.extend([f'{a}-{b}' for a,b in zip(t[:n], reversed(t[n:]))])
    return stages

out = (df.groupby('group_id', as_index=False)['user_id']
          .apply(combine).explode('user_id')
      )

Output:
   group_id user_id
0         1   a1-d1
0         1   b1-c1
0         1   a1-c1
0         1   d1-b1
0         1   a1-b1
0         1   c1-d1
1         2   x1-z1
1         2   x1-y1

original answer before question clarirication (incorrect)
You can use:
from itertools import combinations

out = [c for k, g in df.groupby('group_id')['user_id']
       for c in combinations(g, 2)]

Output:
[('a1', 'b1'),
 ('a1', 'c1'),
 ('a1', 'd1'),
 ('b1', 'c1'),
 ('b1', 'd1'),
 ('c1', 'd1'),
 ('x1', 'y1'),
 ('x1', 'z1'),
 ('y1', 'z1')]

",
76390308,Is there an equivalent of the /hierarchy API endpoint for DSpace 7.x?,"The DSpace OAI-PMH repository exposes an endpoint /hierarchy for API v6, which provides the logical structure of how communities, sub-communities and collections are related. This is documented at https://wiki.lyrasis.org/pages/viewpage.action?pageId=104566810#RESTAPIv6(deprecated)-Hierarchy
As v6 will be deprecated, is there a direct replacement for this endpoint in v7?
There's no reference to it in the documentation https://wiki.lyrasis.org/display/DSDOC7x/REST+API nor can I see anything equivalent in the live demo: https://api7.dspace.org/server/#/server/api
",76392296,"There is no exact replacement in DSpace 7 REST API.  But you can retrieve the same information as follows:

Start from the ""Top level Communities"" search endpoint here: https://github.com/DSpace/RestContract/blob/main/communities.md#search-methods

This will retrieve all the communities at the top of that hierarchy


Then, for each of those top Communities, it's possible to retrieve their sub-communities and sub-collections via the ""linked entities""

Sub-Communities: https://github.com/DSpace/RestContract/blob/main/communities.md#subcommunities
Sub-Collections: https://github.com/DSpace/RestContract/blob/main/communities.md#collections



This is how the hierarchy is achieved in the DSpace 7 User Interface when you visit the ""/community-list"" page, e.g. https://demo7.dspace.org/community-list
",
76392027,"TypeScript array intersection type: property does not exist when accessed in Array.forEach, Array.some, etc, but accessible within for loop","So, I might not have the right terminology to describe this problem, which made searching for it tricky, but I hope my example helps clear it up.
Context: I'm using query data selectors in react-query to preprocess query results and attach some properties that I need globally in the application. I'm running into an issue with the produced types, which I managed to narrow down and reproduce outside of react-query itself.
Here's the reproduction code (also on TS Playground)
// Data structure coming from third-party code (I have no control over it)
// Simplified for a more concise reproduction:
type DataStructure =
  & Array<{ a: number; b: number }>
  & Array<{ b: number; c: number }>

const structure: DataStructure = [
  {
    a: 1,
    b: 2,
    c: 3,
  },
]

structure[0].a // ok
structure[0].b // ok
structure[0].c // ok

for (const s of structure) {
  s.a // ok
  s.b // ok
  s.c // ok
}

structure.forEach(s => {
  s.a // ok
  s.b // ok
  s.c // Property 'c' does not exist on type '{ a: number; b: number; }'.(2339)
})

// If we had control over DataStructure, we'd write it like this instead:
// Array<{ a: number; b: number } & { b: number; c: number }>


The DataStructure is an intersection type of two Arrays with partially overlapping item types.
As demonstrated by the code and the comments, all 3 properties are available when the array items are accessed either by their index or inside a for loop, but inside a forEach loop (or any array method like some, every, etc,) only properties of the first array type (from the intersection) are available.
Trying to access c inside the forEach loop, TypeScript complains:
Property 'c' does not exist on type '{ a: number; b: number; }'.(2339)

Now, if I had control over the data structure definition, I'd describe it like this:
type DataStrcture = Array<{ a: number; b: number } & { b: number; c: number }>

That would indeed solve the issue. But I don't have control over that part of the code.
What I'm more interested in is understanding WHY TypeScript behaves the way it does here. That's what's baffling me the most, but if someone can offer a clean solution, too, that'd be extra amazing!
",76393168,"This is considered a design limitation of TypeScript. Intersections of array types behave strangely and are not recommended.  See microsoft/TypeScript#41874 for an authoritative answer. It says:

Array intersection is weird since there are many invariants of arrays and many invariants of intersections that can't be simultaneously met. For example, if it's valid to call a.push(x) when a is A, then it should be valid to write ab.push(x) when ab is A & B, but that creates an unsound read on (A & B)[number].
In higher-order the current behavior is really the best we can do; in zero-order it's really preferable to just write Array<A & B>, Array<A> | Array<B>, or Array<A | B> depending on which you mean to happen.

Some of the weirdness is due to the fact that intersections of functions and methods behave like overloads, and arrays are unsafely considered covariant in their element type (see Why are TypeScript arrays covariant? ), which means you suddenly have the situation with push() as described above.
Other weirdness happens when you try to iterate through them, as you've shown, and as described in microsoft/TypeScript#39693.

So the recommended approach is to avoid intersections of arrays, and instead use arrays of intersections if that's what you want.  If you can write that out directly, you should.  If you have a type with nested intersected arrays you can look at Why does the merge of 2 types with a shared property name not work when making a type with that property from the merged type? for a possible approach to writing a utility type to deal with those.
As I mentioned in the comment, if you can't control the data type, you should show this to whoever does so they can fix it.  Otherwise, you'll need to work around it by translating between the external type definition and your fixed version of it.
",
76390991,"From a list, how to get only the items with dates within a certain time period?","I have a list of items, each item has a date value:
[
  {
    ""Date Merged"": ""6/1/2023 3:46:53 PM"",
    ""PR ID"": ""470""
  },
  {
    ""Date Merged"": ""5/30/2023 2:44:25 PM"",
    ""PR ID"": ""447""
  }
]

I want to get only the PRs of the items with dates that happened in May. I think I can grab the 'PR ID' values with: map(attribute='PR ID'), but I don't know how to filter within a certain date range.
",76393253,"Given the data
  prs:
    - Date Merged: 6/1/2023 3:46:53 PM
      PR ID: '470'
    - Date Merged: 5/30/2023 2:44:25 PM
      PR ID: '447'

Q: ""Get the PRs with dates that happened in May.""
A: There are more options:

Quick & dirty. The test match ""succeeds if it finds the pattern at the beginning of the string""

  result: ""{{ prs|selectattr('Date Merged', 'match', '5')|
                  map(attribute='PR ID') }}""

gives
  result:
  - '447'


Use the filter to_datetime to get the date objects and create the list of the hashes

  format: '%m/%d/%Y %I:%M:%S %p'
  months: ""{{ prs|map(attribute='Date Merged')|
                  map('to_datetime', format)|
                  map(attribute='month')|
                  map('community.general.dict_kv', 'month') }}""

gives
  months:
  - month: 6
  - month: 5

Update the dictionaries
  prs_months: ""{{ prs|zip(months)|map('combine') }}""

gives
  prs_months:
  - Date Merged: 6/1/2023 3:46:53 PM
    PR ID: '470'
    month: 6
  - Date Merged: 5/30/2023 2:44:25 PM
    PR ID: '447'
    month: 5

Select the items and get the ids
  result: ""{{ prs_months|selectattr('month', 'eq', 5)|
                         map(attribute='PR ID') }}""

gives the same result
  result:
  - '447'



Example of a complete playbook for testing
- hosts: localhost

  vars:

    prs:
      - Date Merged: 6/1/2023 3:46:53 PM
        PR ID: '470'
      - Date Merged: 5/30/2023 2:44:25 PM
        PR ID: '447'

    result1: ""{{ prs|selectattr('Date Merged', 'match', '5')|
                     map(attribute='PR ID') }}""

    format: '%m/%d/%Y %I:%M:%S %p'
    months: ""{{ prs|map(attribute='Date Merged')|
                    map('to_datetime', format)|
                    map(attribute='month')|
                    map('community.general.dict_kv', 'month') }}""
    prs_months: ""{{ prs|zip(months)|map('combine') }}""
    result2: ""{{ prs_months|selectattr('month', 'eq', 5)|
                            map(attribute='PR ID') }}""

  tasks:

    - debug:
        var: result1

    - debug:
        var: months
    - debug:
        var: prs_months
    - debug:
        var: result2


",
76389963,WinDbg | Application memory full dump - Show file path of file handle,"I created a memory dump of an application with procdump -ma abc.exe.
The application access various files.
I run !handle 0 f FILE and get over 100 file handles.
When I get a specific handle address, I run the following command !handle 000000000000161c f which results in:
0:000> !handle 000000000000161c f
Handle 000000000000161c
  Type          File
  Attributes    0
  GrantedAccess 0x12019f:
         ReadControl,Synch
         Read/List,Write/Add,Append/SubDir/CreatePipe,ReadEA,WriteEA,ReadAttr,WriteAttr
  HandleCount   2
  PointerCount  65479
  No object specific information available

Is there a chance to retrieve the actual file path? Something like that: How get file path by handle in windbg? seems to work only in kernel mode debugging.
",76392419,"For post-mortem debugging (crash dump analysis), there's no way, except if you have a kernel dump (I can't tell you how to do that then). Windows will close handles of a process that terminated.
!handle combined with !handleex
There is handleex on Github.
Combine !handle and !handleex to get nice information.
.foreach /pS 1 /ps 1 (file {!handle 0 4 FILE}) { .echo Handle file; !handleex file; .echo }

Example output
0:007> .foreach /pS 1 /ps 1 (file {!handle 0 4 FILE}) { .echo Handle file; !handleex file; .echo }
Handle 4c
Object Type: File
Handle Name: \Device\HarddiskVolume3\Users\T

Handle 8c
Object Type: File
Handle Name: \Device\HarddiskVolume3\Windows\WinSxS\amd64_microsoft.windows.common-controls_6595b64144ccf1df_6.0.19041.1110_none_60b5254171f9507e

Handle b4
Object Type: File
Handle Name: \Device\HarddiskVolume3\Windows\System32\en-US\notepad.exe.mui

...

!handle combined with SysInternals Handle
You can combine the !handle WinDbg command and the SysInternals Handle tool.
Example notepad debugging session:
:018> !handle 0 1 FILE
Handle 4c
  Type          File
Handle 8c
  Type          File
Handle b8
  Type          File
Handle 134
  Type          File
...

Combined with the information from the console:
C:\...>handle -p notepad.exe | findstr 8C:
   8C: File          C:\Windows\WinSxS\amd64_microsoft.windows.common-controls_6595b64144ccf1df_6.0.19041.1110_none_60b5254171f9507e

C:\...>handle -p notepad.exe | findstr B8:
   B8: File          C:\Windows\System32\en-US\notepad.exe.mui

Automating it doesn't really work well:
0:007> .printf ""%d\n"",$tpid
2168
0:007> .foreach /pS 1 /ps 1 (file {!handle 0 4 FILE}) { .shell C:\handle.exe -p 2168 | findstr file: }

The process seems to be too slow and require too much user input.
",
76389602,"How to get SKTypeface.FromFamilyName to return a font for Japanese, Korean, Chinese (Android + iOS, Xamarin.Forms)","I'm using a library that uses SKTypeface.FromFamilyName internally to render font on the screen. However, as I found out if the text to display is japanese, korean or chinese, it just prints squares. I tried to add a custom font to my project but I was not able to make SKTypeface.FromFamilyName return anything but NULL with custom fonts. As I have no access to change SKTypeface.FromFamilyName to something else ( at least as far as I know because it's in a private method of a static class - https://github.com/Mapsui/Mapsui/blob/5008d3ab8b0453c27cb487fe6ad3fac87435abbe/Mapsui.Rendering.Skia/LabelRenderer.cs#L277 ), is there any way I can make it return any font for each language (or one per language) that works with these?
",76392485,"Alright, I found a solution for this. This seems to work for me:
string fontFamily;
switch (Thread.CurrentThread.CurrentUICulture.TwoLetterISOLanguageName.ToLower())
{
    case ""ja"":
        fontFamily = SKFontManager.Default.MatchCharacter('あ').FamilyName;
        break;

    case ""ko"":
        fontFamily = SKFontManager.Default.MatchCharacter('매').FamilyName;
        break;

    case ""zh"":
        fontFamily = Thread.CurrentThread.CurrentUICulture.IetfLanguageTag.ToLower() switch
            {
                ""zh-cn"" => SKFontManager.Default.MatchCharacter('实').FamilyName,
                ""zh-tw"" => SKFontManager.Default.MatchCharacter('實').FamilyName,
                _ => null
            };
        break;

    default:
        fontFamily = null;
        break;
}

",
76391957,Function return type depending on argument type,"I have the following code where the useSearchResults function (it's a React hook, but it doesn't matter) initializes the state based on the argument.
config.state can be anything defined in SearchResultsState or a function that returns anything defined in SearchResultsState.
The state returned by useSearchResults is defined as Partial<SearchResultsState> because, initially could be anything in that state type. I need state to be a partial SearchResultsState but also whatever type config.state has (an object or the object returned by a function or undefined).
I've trying and trying around this but couldn't find a solution.

type State = Record<string, any>;
interface IWidget {
  State: State;
}
interface SearchResultsState extends State {
  a: number; 
  b: string;
}

interface SearchResultsWidget extends IWidget {
  State: SearchResultsState;
}
export type StateInitializerFunction<W extends IWidget> = () => Partial<W['State']>;
export type StateInitializer<W extends IWidget> = StateInitializerFunction<W> | Partial<W['State']>;

export type WidgetInitializer<W extends IWidget> = {
  state?: StateInitializer<W>;
};

export type WidgetWrapperResult<
  W extends IWidget,
> = {
  state: Partial<W['State']>;
};


const useSearchResults = (
  config: WidgetInitializer<SearchResultsWidget> = {},
): WidgetWrapperResult<SearchResultsWidget> => {
  const state = typeof config.state === 'function' ? config.state() : config.state || {};
  
  return { state };
};

const { state: { a, b } } = useSearchResults({ state: { a: 1 } });
// or const { state: { a, b } } = useSearchResults({ state: () => ({ a: 1 }) });

const test = (a: number) => console.log(a);

// error
test(a); // a is number | undefined and I'm looking to be just number because a is number in state


You can play with it here
",76393265,"If you want a function's return type to depend on its argument type then you either need to overload it with multiple call signatures, or generic in some number of type parameters.  Overloads only work if you have a relatively small number of ways you want to call the function, while generics are more appropriate to represent an arbitrary relationship between input and output.

Overloads might work for this use case, if you only want to support the four possible cases for a and b being present or possibly absent, but it's tedious:
function useSearchResults(
  config: { state?: { a: number, b: string } | (() => { a: number, b: string }) }
): { state: { a: number, b: string } };
function useSearchResults(
  config: { state?: { a: number, b?: string } | (() => { a: number, b?: string }) }
): { state: { a: number, b?: string } };
function useSearchResults(
  config: { state?: { a?: number, b: string } | (() => { a?: number, b: string }) }
): { state: { a?: number, b: string } };
function useSearchResults(
  config: { state?: { a?: number, b?: string } | (() => { a?: number, b?: string }) }
): { state: { a?: number, b?: string } };

function useSearchResults(
  config: WidgetInitializer<SearchResultsWidget> = {},
): WidgetWrapperResult<SearchResultsWidget> {
  const state = typeof config.state === 'function' ? config.state() : config.state || {};

  return { state };
};

const { state: { a, b } } = useSearchResults({ state: { a: 1 } });
/* (property) state: {
  a: number;
  b?: string | undefined;
} */

useSearchResults({ state: () => ({ a: 1 }) }).state.a


That can quickly get out of hand, though, so you might need to use generics instead.  Once you use generics you need to make the generic type arguments inferrable from the inputs, but you can't do that with indexed access types like W[""state""] (there was a pull request at ms/TS#20126 which would have made this possible, but it's not part of the language now.  A new PR at ms/TS#53017 might be merged eventually, but for now it's not.)
So you'll need to refactor significantly to use that type directly.  Possibly as shown here:
type StateInitializerFunction<S extends State> = () => S;
type StateInitializer<S extends State> = StateInitializerFunction<S> | S;
type WidgetInitializer<S extends State> = { state?: StateInitializer<S>; };
type WidgetWrapperResult<S extends State> = { state: S; };

const useSearchResults = <S extends Partial<SearchResultsState>>(
  config: WidgetInitializer<S> = {},
): WidgetWrapperResult<S & Partial<SearchResultsState>> => {
  const state = typeof config.state === 'function' ? config.state() : config.state || {};
  return { state } as any;
};

Here what I've done is replace mentions of W with just its state property type S. I've also replaced the Partial stuff in the definitions and moved it to the function. Now the function useSearchResults accepts a WidgetInitializer<S> for some S that extends Partial<SearchResultsState>, and it returns a WidgetWrapperResults<S & Partial<SearchResultsState>>.  That intersection just helps make sure the compiler is still aware of b's existence when you don't pass it.  Let's test it:
const { state: { a, b } } = useSearchResults({ state: { a: 1 } });
/* (property) state: {
    a: number;
} & Partial<SearchResultsState> */

const test = (a: number) => console.log(a);

useSearchResults({ state: () => ({ a: 1 }) }).state.a

test(a);

Looks good!
Playground link to code
",
76389562,Excel 2016 - compare two columns for match,"I have two columns where I want to highlight differences 
I want to see if the letter in Column A exists in Column B - it should be case-sensitive, because there are C and c in column A
If there is a match I want the row preferably turn green - otherwise a OK / NOT OK in column C
Thank you in advance
",76392514,"Copy this Sub to the Worksheet code pane. For this open the Developer tab and click Visual Basic. In the left pane select the worksheet where your datas are, and doubleclick it. Insert the code.
Private Sub Worksheet_Change(ByVal Target As Excel.Range)
If Target.Column = 1 And InStr(1, Target.Offset(0, 1).Value, Target.Value) > 0 Then
Target.EntireRow.Interior.Color = vbGreen
Else
Target.EntireRow.Interior.ColorIndex = xlColorIndexNone
End If
If Target.Column = 2 Then
If InStr(1, Target.Value, Target.Offset(0, -1)) > 0 Then
Target.EntireRow.Interior.Color = vbGreen
Else
Target.EntireRow.Interior.ColorIndex = xlColorIndexNone
End If
End If
End Sub


If the content change in column A or B the respective row color will change.
",
76382415,Why does my Python code using scipy.curve_fit() for Planck's Radiation Law produce 'popt=1' and 'pcov=inf' errors?,"Covariance not estimated in SciPy's Curvefit
Here's my dataset:
frequency (Hz)  brightness (ergs/s/cm^2/sr/Hz)  brightness (J/s/m^2/sr/Hz)
float64 float64 float64
34473577711.372055  7.029471536390586e-16   7.029471536390586e-19
42896956937.69582   1.0253178228238486e-15  1.0253178228238486e-18
51322332225.44733   1.3544045476166584e-15  1.3544045476166584e-18
60344529880.18272   1.6902073280174815e-15  1.6902073280174815e-18
68767909106.5062    2.0125779972022745e-15  2.0125779972022743e-18
77780126454.10146   2.3148004995630144e-15  2.3148004995630145e-18
... ... ...
489996752265.52826  3.201319839821188e-16   3.201319839821188e-19
506039097962.6759   2.5968748350997043e-16  2.596874835099704e-19
523273092332.3638   2.0595903864583913e-16  2.0595903864583912e-19
539918248580.7806   1.7237876060575648e-16  1.7237876060575649e-19
557158231134.7507   1.3879848256567381e-16  1.3879848256567383e-19
573803387383.1646   1.0521820452559118e-16  1.0521820452559118e-19
591049358121.42 9.178609330955852e-17   9.178609330955852e-20

I tried to use CurveFit to fit this to Planck's Radiation Law:
import numpy as np
from scipy.optimize import curve_fit

h=6.626*10e-34
c=3*10e8
k=1.38*10e-23
const1=2*h/(c**2)
const2=h/k

def planck(x,v):
    return const1*(v**3)*(1/((np.exp(const2*v/x))-1))

popt,pcov= curve_fit(planck, cmb['frequency (Hz)'],cmb['brightness (J/s/m^2/sr/Hz)'])
print(popt, pcov)

Warning:
/tmp/ipykernel_2500/4072287013.py:11: RuntimeWarning: divide by zero encountered in divide
  return const1*(v**3)*(1/((np.exp((const2)*v/x))-1))

I get popt=1 and pcov=nan. Now the exponential term in the function differs by several orders of magnitude. And some of the values don't permit to approximate the law mathematically. I tried using the logarithmic form of the law but that doesn't work either. How can I overcome this problem?
",76393313,"A lot of problems here, including that your variables were swapped, you're needlessly redefining physical constants, and your expression was highly numerically unstable. You need to use exp1m instead:
import matplotlib.pyplot as plt
import numpy as np
from scipy.constants import h, c, k
from scipy.optimize import curve_fit

freq, brightness_erg, brightness_j = np.array((
    (34473577711.372055, 7.0294715363905860e-16, 7.0294715363905860e-19),
    (42896956937.695820, 1.0253178228238486e-15, 1.0253178228238486e-18),
    (51322332225.447330, 1.3544045476166584e-15, 1.3544045476166584e-18),
    (60344529880.182720, 1.6902073280174815e-15, 1.6902073280174815e-18),
    (68767909106.506200, 2.0125779972022745e-15, 2.0125779972022743e-18),
    (77780126454.101460, 2.3148004995630144e-15, 2.3148004995630145e-18),
    (489996752265.52826, 3.2013198398211880e-16, 3.2013198398211880e-19),
    (506039097962.67590, 2.5968748350997043e-16, 2.5968748350997040e-19),
    (523273092332.36380, 2.0595903864583913e-16, 2.0595903864583912e-19),
    (539918248580.78060, 1.7237876060575648e-16, 1.7237876060575649e-19),
    (557158231134.75070, 1.3879848256567381e-16, 1.3879848256567383e-19),
    (573803387383.16460, 1.0521820452559118e-16, 1.0521820452559118e-19),
    (591049358121.42000, 9.1786093309558520e-17, 9.1786093309558520e-20),
)).T


def planck(v: np.ndarray, T: float) -> np.ndarray:
    return 2*h/c/c * v**3 / np.expm1(h*v/k/T)

guess = 2.5,

(T,), _ = curve_fit(
    f=planck, xdata=freq, ydata=brightness_j, p0=guess, method='lm',
    # bounds=(0.1, np.inf),
)
print('T =', T)

fig, ax = plt.subplots()
v = np.linspace(freq.min(), freq.max(), 500)
ax.scatter(freq, brightness_j, label='data')
ax.plot(v, planck(v, *guess), label='guess')
ax.plot(v, planck(v, T), label='fit')
ax.legend()
plt.show()


",
76387822,Snowflake Flatten and parsing values,"I have a snowflake table that has VARCHAR column containing input from api. I have 300+ columns to be flattened and one of them has input like below for a particular row. I need to parse the values from the below (please refer output) and store it as a single row for a particular input row. The number of elements inside the input list might be 1 or more than 1 but the order of the keys present remain same.
Input:
[
{""active"":false,""urls"":{""6x6"":""https://url.url.com/secure/?size=xsmall&ownerId=B12345&id=123"",""4x4"":""https://url.url.com/secure/?size=small&ownerId=B12345&id=123"",""22x22"":""https://url.url.com/secure/?size=medium&ownerId=B12345&id=123"",""44x44"":""https://url.url.com/secure/?ownerId=B12345&id=123""},""displayName"":""Name1,Lname1"",""emailAddress"":"""",""key"":""B12345"",""name"":""B12345"",""self"":""https://url.url.com/rest/api/2/user?username=B12345"",""timeZone"":""India/Mumbai""},


{""active"":true,""urls"":{""6x6"":""https://url.url.com/secure/?size=xsmall&ownerId=A12345&id=456"",""4x4"":""https://url.url.com/secure/?size=small&ownerId=A12345&id=456"",""22x22"":""https://url.url.com/secure/?size=medium&ownerId=A12345&id=456"",""44x44"":""https://url.url.com/secure/?ownerId=A12345&id=456""},""displayName"":""Name1,Lname2."",""emailAddress"":""name.lname@abcdef.com"",""key"":""A12345"",""name"":""A12345"",""self"":""https://url.url.com/rest/api/2/user?username=A12345"",""timeZone"":""India/Mumbai""}

 
{""active"":true,""urls"":{""6x6"":""https://url.url.com/secure/?size=xsmall&ownerId=C12345&id=456"",""4x4"":""https://url.url.com/secure/?size=small&ownerId=C12345&id=456"",""22x22"":""https://url.url.com/secure/?size=medium&ownerId=C12345&id=456"",""44x44"":""https://url.url.com/secure/?ownerId=C12345&id=456""},""displayName"":""Name1,Lname3."",""emailAddress"":""name.lname@abcdef.com"",""key"":""C12345"",""name"":""C12345"",""self"":""https://url.url.com/rest/api/2/user?username=C12345"",""timeZone"":""India/Mumbai""}
 ]

Output I am looking for is: list of values of the keys (nested)
[ 
[false,
[https://url.url.com/secure/?size=xsmall&ownerId=B12345&id=123,
https://url.url.com/secure/?size=small&ownerId=B12345&id=123,
https://url.url.com/secure/?size=medium&ownerId=B12345&id=123,
https://url.url.com/secure/?ownerId=B12345&id=123], Name1, Lname1, ,
B12345, B12345, https://url.url.com/rest/api/2/user?username=B12345,
India/Mumbai],  

[true,
[https://url.url.com/secure/?size=xsmall&ownerId=A12345&id=456,
https://url.url.com/secure/?size=small&ownerId=A12345&id=456,
https://url.url.com/secure/?size=medium&ownerId=A12345&id=456,
https://url.url.com/secure/?ownerId=A12345&id=456], Name1, Lname2.,
name.lname@abcdef.com, A12345,
A12345,https://url.url.com/rest/api/2/user?username=B12345,
India/Mumbai] ,

[true,
[https://url.url.com/secure/?size=xsmall&ownerId=C12345&id=456,
https://url.url.com/secure/?size=small&ownerId=C12345&id=456,
https://url.url.com/secure/?size=medium&ownerId=C12345&id=456,
https://url.url.com/secure/?ownerId=C12345&id=456], Name1, Lname3.,
name.lname@abcdef.com, C12345,
C12345,https://url.url.com/rest/api/2/user?username=B12345,
India/Mumbai] 
]

Query that I tried:
SELECT  
CONCAT('[[',
REPLACE(GET(flattened.value,  'active'), '""', '') , ', ',
'[',
listagg(CASE WHEN flattened_nested.value LIKE '%https%' THEN 'https:' || REPLACE(SPLIT_PART(flattened_nested.value, ':', 2), '""', '') ELSE NULL END,',')within group (order by null)

,']', ', ',  --<== this causes issue while trying to parse urls values for each elements and to store them.
REPLACE(GET(flattened.value,  'displayName'), '""', '') , ', ',
REPLACE(GET(flattened.value,  'emailAddress'), '""', '') , ', ',
REPLACE(GET(flattened.value,  'key'), '""', '') , ', ',
REPLACE(GET(flattened.value,  'name'), '""', '')  , ', ',
REPLACE(GET(flattened.value,  'self'), '""', '') , ', ',
REPLACE(GET(flattened.value,  'timeZone'), '""', ''),   

']]')  as output_column
FROM snowflake_table  SRC

,LATERAL FLATTEN(input=>SRC.json_values:""fields"":""field_12345"") AS flattened --<== the field that contains the above input.
,LATERAL FLATTEN(input=>flattened.value:urls) AS flattened_nested
group by  flattened.value
;


The below output that I get is aggregating all the 3 urls value for a particular input and stores it as comma separated. but, it doesn't take the other values like displayname, key,name, self etc for all 3 elements. It gives me only the first occurrence.

The other issue with this approach is if I need to include other 300+ columns, I have place everything in group by clause.



[ [false,
[https://url.url.com/secure/?size=xsmall&ownerId=B12345&id=123,
https://url.url.com/secure/?size=small&ownerId=B12345&id=123,
https://url.url.com/secure/?size=medium&ownerId=B12345&id=123,
https://url.url.com/secure/?ownerId=B12345&id=123,https://url.url.com/secure/?size=xsmall&ownerId=A12345&id=456,
https://url.url.com/secure/?size=small&ownerId=A12345&id=456,
https://url.url.com/secure/?size=medium&ownerId=A12345&id=456,
https://url.url.com/secure/?ownerId=A12345&id=456,https://url.url.com/secure/?size=xsmall&ownerId=C12345&id=456,
https://url.url.com/secure/?size=small&ownerId=C12345&id=456,
https://url.url.com/secure/?size=medium&ownerId=C12345&id=456,
https://url.url.com/secure/?ownerId=C12345&id=456], Name1, Lname1, ,
B12345, B12345, https://url.url.com/rest/api/2/user?username=B12345,
India/Mumbai] ]

Can anyone please let me know how to get the desired output with any different approach irrespective of the number of input elements inside the list?
",76394313,"This won't produce the nested JSON, but it should I believe pick-out the values you want, and from there you should be able to form the JSON from it:
SELECT 
      t.value:active::BOOLEAN AS active
    , ARRAY_AGG(t.value:urls) WITHIN GROUP (ORDER BY seq) AS urls
    , SPLIT_PART(t.value:displayName::STRING, ',', 1) AS firstName
    , SPLIT_PART(t.value:displayName::STRING, ',', 2) AS lastName
    , t.value:emailAddress::STRING AS emailAddress
    , t.value:key::STRING AS key
    , t.value:name::STRING AS name
    , t.value:self::STRING AS self
    , t.value:timeZone::STRING AS timeZone
FROM snowflake_table
    , LATERAL FLATTEN(input => json_values) t
GROUP BY active, firstName, lastName, emailAddress, key, name, self, timeZone;

An alternative syntax via JSON_PARSE is also possible I feel:
SELECT 
      json_values:active::BOOLEAN AS active
    , ARRAY_AGG(json_values:urls) AS urls
    , json_values:displayName::STRING AS displayName
    , json_values:emailAddress::STRING AS emailAddress
    , json_values:key::STRING AS key
    , json_values:name::STRING AS name
    , json_values:self::STRING AS self
    , json_values:timeZone::STRING AS timeZone
FROM snowflake_table
    , LATERAL JSON_PARSE(json_values) AS json_values
GROUP BY active, displayName, emailAddress, key, name, self, timeZone;

You may want to consider both for ease of application of the final step and relative performance. It does feel like using JSON_VALUES is easier to understand but the difference in that is very a=marginal.
",
76394361,A more organic way to resolve and return a resolved promise from a JavaScript/Node function than using Promise.resolve?,"For instance, I'm writing a Mongoose utility method, and I'm wanting to return a value after the async methods resolve. This is a highly simplified example.
const testConnect = () => {
    let msg;
    mongoose.connect(mongoServer.getUri())
        .then(() => {
            msg =""Connection success!"";
        })
        .catch((err) => {
            msg = ""Connection failure! Error: "" + err;
        });
    return Promise.resolve(msg);
};

My test buildup utilizes this method in a matter like this.
beforeAll(() => {
    iMDT.testConnect()
        .then((result) => {
            console.log(result);
        })
        .catch((err) => {
            console.log(err);
        });
});

Is there a simpler, more organic way to return that value as a resolved promise - without using the prototype Promise? It feels kludgy to me.
",76394377,"What you're attempting to do won't work because your asynchronous operation is non-blocking so you will do return Promise.resolve(msg); before there's even a value in msg.  It's almost always a warning sign when you're assigning higher scoped variables inside of a .then() or .catch() handler and then trying to use those variables at the top scope.  That won't work because of timing issues (attempting to use the variable before its value is set).
Instead, you can just return the promise directly.  The general advice is to use the promises you already have and not create new ones unnecessarily.  Return the one you already have like this:
const testConnect = () => {
    return mongoose.connect(mongoServer.getUri()).then(() => {
        // set the final resolved value
        return ""Connection success!"";
    }).catch((err) => {
        // set the final rejected reason object
        throw new Error(""Connection failure! Error:""  + err.message, { cause: err });
    });
};

Note, how the failure code path returns a rejected promise that contains a human readable message and also contains the original error object as the cause.  This is the usual way to use promises, not to hide the rejection by returning a different resolved value.  This allows the caller to more easily know if the operation succeeded or failed without having to compare to specific resolved values.
And, this is also what your beforeAll() code block is expecting.  It is expecting a rejected promise if the operation fails which is not what your first code block was doing.
",
76384965,Behavior of `git push --force` when no upstream branch exists,"What's the behavior of git push --force when no upstream branch exists?
Will I get something like fatal: The current branch branch_name has no upstream branch, as would happen with a normal push, or would the upstream branch be ""forcefully"" created?
",76392645,"--force does not change the behaviour of git push without an upstream set (when no push.default and push.autoSetupRemote config is set) empirically with git 2.40.0.
$ git checkout -b dev/test
Switched to a new branch 'dev/test'
$ git push
fatal: The current branch dev/test has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin dev/test

To have this happen automatically for branches without a tracking
upstream, see 'push.autoSetupRemote' in 'git help config'.

$ git push --force
fatal: The current branch dev/test has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin dev/test

To have this happen automatically for branches without a tracking
upstream, see 'push.autoSetupRemote' in 'git help config'.

",
76390292,Oracle Date ranges that are present in parent rows but not in child rows,"Dates ranges that are found in rule1 but 'not in' rule2 E.g.,




item_no
item_type
active_from
active_to
ruleid




10001
SAR
2022-01-01
2023-05-31
rule1


10001
SAR
2023-07-01
2099-12-31
rule1


10001
SAR
2023-01-01
9999-12-31
rule2


10001
SAR
2020-12-01
2021-12-31
rule2




In this case output will be the date ranges which are in rule1 but not in rule2 is.
10001 SAR 2022-01-01 2022-12-31 

I have used connect by level which is taking more time to generate dates and compare them.. as date end is 9999-12-31.
",76392654,"Adapting my answer to your previous question, from Oracle 12, you can UNPIVOT the dates and then use analytic functions and MATCH_RECOGNIZE to process the result set row-by-row to find the consecutive rows where rule1 is active and rule2 is inactive:
SELECT item_no,
       item_type,
       active_from
       + CASE
         WHEN prev_rule2 > 0
         THEN INTERVAL '1' SECOND
         ELSE INTERVAL '0' SECOND
         END AS active_from,
       active_to
       - CASE
         WHEN next_rule2 > 0
         THEN INTERVAL '1' SECOND
         ELSE INTERVAL '0' SECOND
         END AS active_to
FROM   (
  SELECT item_no,
         item_type,
         rule_id,
         dt,
         COALESCE(
           SUM(CASE rule_id WHEN 'rule1' THEN active END) OVER (
             PARTITION BY item_no, item_type ORDER BY dt, ACTIVE DESC
           ),
           0
         ) AS rule1,
         COALESCE(
           SUM(CASE rule_id WHEN 'rule2' THEN active END) OVER (
             PARTITION BY item_no, item_type ORDER BY dt, ACTIVE DESC
           ),
           0
         ) AS rule2
  FROM   table_name
         UNPIVOT (
           dt FOR active IN ( active_from AS 1, active_to AS -1 )
         )
)
MATCH_RECOGNIZE(
  PARTITION BY item_no, item_type
  ORDER BY dt, rule1 DESC, rule2 DESC
  MEASURES
    FIRST(dt) AS active_from,
    PREV(rule1) AS prev_rule1,
    PREV(rule2) AS prev_rule2,
    NEXT(dt) AS active_to,
    NEXT(rule1) AS next_rule1,
    NEXT(rule2) AS next_rule2
  PATTERN ( active_rules+ )
  DEFINE active_rules AS rule1 > 0 AND rule2 = 0
)

Which, for the sample data from your previous question:
CREATE TABLE table_name (Item_no, item_type, active_from, active_to, rule_id) AS
SELECT 10001, 'SAR', DATE '2020-01-01', DATE '2023-01-01', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2024-01-01', DATE '9999-12-31', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2020-05-01', DATE '2021-06-01', 'rule2' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2021-01-01', DATE '2021-02-01', 'rule2' FROM DUAL;

Outputs:




ITEM_NO
ITEM_TYPE
ACTIVE_FROM
ACTIVE_TO




10001
SAR
2020-01-01 00:00:00
2020-04-30 23:59:59


10001
SAR
2021-06-01 00:00:01
2023-01-01 00:00:00


10001
SAR
2024-01-01 00:00:00
9999-12-31 00:00:00




And, for the sample data in this question:
CREATE TABLE table_name (Item_no, item_type, active_from, active_to, rule_id) AS
SELECT 10001, 'SAR', DATE '2022-01-01', DATE '2023-05-31', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2023-07-01', DATE '2099-12-31', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2023-01-01', DATE '9999-12-31', 'rule2' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2020-12-01', DATE '2021-12-31', 'rule2' FROM DUAL;

Outputs:




ITEM_NO
ITEM_TYPE
ACTIVE_FROM
ACTIVE_TO




10001
SAR
2022-01-01 00:00:00
2022-12-31 23:59:59




And, for the sample data:
CREATE TABLE table_name (Item_no, item_type, active_from, active_to, rule_id) AS
SELECT 10001, 'SAR', DATE '2022-01-01', DATE '9999-12-31', 'rule1' FROM DUAL UNION ALL
SELECT 10001, 'SAR', DATE '2022-01-01', DATE '2023-05-31', 'rule2' FROM DUAL;

Outputs:




ITEM_NO
ITEM_TYPE
ACTIVE_FROM
ACTIVE_TO




10001
SAR
2023-05-31 00:00:01
9999-12-31 00:00:00




fiddle
",
76394383,Python List formatting with small bracket,"I need to add small bracket at the start and end of a Python list (Python 3)
My list is
list1 = ['1','2','3']
List needed is [('1','2','3')]
I know the solution may be simple, but I am not getting it.
Thanks in advance
Tried fstring and join
",76394393,"Here's one way to print a list in that formatting with a combination of an f-string and str.join:
>>> list1 = ['1','2','3']
>>> print(f""[({','.join(repr(i) for i in list1)})]"")
[('1','2','3')]

Note that repr(i) adds quotes around the strings so that they print as '1' instead of 1.
Note also that this is converting the list into a particular string, not adding anything to the list.  If you wanted to add the literal strings ( and ) to list1 you could do:
>>> list1.insert(0, '(')
>>> list1.append(')')
>>> list1
['(', '1', '2', '3', ')']

But parentheses themselves aren't objects that you can add to a list.  If you want to construct an object that will render with parens, that's a tuple:
>>> list1 = ['1','2','3']
>>> [tuple(list1)]
[('1', '2', '3')]

",
76391866,How to show a list of all my todos across all the files of my project?,"I am an Emacs newbie transitioning from Neovim.
I want something that will show me a list of all of my todos across all the files of the project.
I've tried the following packages:

hl-todo
fixmee

Both of them can highlight todos, and show a list of all the todos in the current buffer.
But, what I need is a list of todos across all the files of the project and not just the current buffer. They do provide such functionality, but it is very clunky and impossible to bind to just a single key (they require choosing a path where to search and pressing enter).
The question: how can I show a list of all my todos across all the files in my project?
",76393314,"I'm not sure about todo modes, but using the rg (ripgrep) package, it is easy to add custom commands.  For example, to define a command that searches for 'TODO' or 'FIXME' in the current project,
(rg-define-search my-rg-todo
  :query ""(TODO|FIXME)""
  :format regexp
  :dir project
  :files current)

There are a lot of other similar solutions, like builtin rgrep, ag or another ripgrep/ag library.
",
76389311,How can I open a Markdown file in preview mode by default in VS Code?,"I'm trying to configure a GitPod or GitHub Codespace so that the README.md gets automatically opened in preview mode at boot time.
I managed to open the file as:
code README.md

but that opens the editor, I'd like to open it in preview only mode.
",76392934,"If you google ""github vscode issues markdown open preview by default"", you will probably find this issue ticket pretty high up in the search results: Option to automatically open markdown in preview
#54776. Quoting Matt Bernier from their comment there:

To change this, you can configure the default editor for .md files (or whatever other markdown file extension you wish). Here's an example setting:
""workbench.editorAssociations"": {
    ""*.md"": ""vscode.markdown.preview.editor""
}

Use the [View: Reopen Editor With...] command to switch back to the standard text editor.


There's also another related issue ticket about opening the Markdown preview to the side by default: Automatically Activate Markdown Preview #2766. If you're interested in that one too, give it a thumbs up to show support, and subscribe to it to get notified of discussion and progress, but please avoid noisy comments like ""+1"" or ""bump"".
",
76391982,Is there a way to represent a range of indexes in a lua table for a dissector?,"Trying to build a dissector in lua where in the ProtoField for one of the values, I map it to a table so that when passed some uint it can get said string from the list of outputs. The current issue is that I have some indexes of 1, 2, and 3, but I also need a range of numbers that correspond to one output string, and its too many to hardcode it all in. I thought about just adding some check that will add said index whenever it doesn't exist, but that would result in alot of wasted time and wanted to ensure there wasn't a better way.
Tried mapping it to a function and setting a default value with a metatable.
",76393363,"Generically, a table can contain tables as indices. These tables could each be used to represent a range of indices.
A cursory example (note that overlapping ranges are not stable):
local range_mt = {}
range_mt.__index = range_mt

function range_mt:contains(value)
    return self.lower <= value and value <= self.upper
end

local function range(lower, upper)
    return setmetatable({
        lower = lower,
        upper = upper
    }, range_mt)
end

local map = setmetatable({
    [1] = 'hello',
    [2] = 'world',
    [3] = 'goodbye',
    ranges = {
        [range(10, 20)] = 'foo',
        [range(51, 99)] = 'bar'
    }
}, {
    __index = function (self, value)
        for r, str in pairs(self.ranges) do
            if r:contains(value) then
                return str
            end
        end

        return ""DEFAULT""
    end
})

for _, value in ipairs { 9, 6, 1, 2, 16, 4, 15, 66, 51, 3, 94 } do
    print(value, '->', map[value])
end

9   ->  DEFAULT
6   ->  DEFAULT
1   ->  hello
2   ->  world
16  ->  foo
4   ->  DEFAULT
15  ->  foo
66  ->  bar
51  ->  bar
3   ->  goodbye
94  ->  bar

",
76391998,Sequelize - Is it possible to search all columns of a table with Op.or operator?,"In a project with NodeJS and ExpressJS, using Sequelize, I'm attempting to implement server-side pagination and searching
const products = await Product.findAll({
      where: {
        [Op.and]: [
          {
            status: {
              [Op.ne]: -1
            },
          },{
            [Op.or]:[
              {
                name:{
                  [Op.substring]:searchString
                }
              },{
                category:{
                  [Op.substring]:searchString
                },
              }
            ]
          }
        ]
      },
      limit: rows,
      offset: offset
    });

Now this allows me to search in the name and category columns, but Product's table has like 10 columns, Client's table has like 15 columns, is there a way to get something like
WHERE any-column LIKE ""%searchString%""
where:{
    [Op.or]:[
        {any-column}:{
            [Op.substring]:searchString
        }
    ]
}

Or is my only option to add each column manually?
",76393387,"If all columns are string type, you can use CONCAT_WS function to search in a whole combined string.
const searchCols = ['name', 'description', 'category'].map(sequelize.col);

await Product.findAll({
    where: {
        status: {
            [Op.ne]: -1
        },
        [Op.where]: Sequelize.where(Sequelize.fn('CONCAT_WS', ' ', ...searchCols), 
                                    Op.like,
                                    `%${searchString}%`)
    }
})

",
76394368,Does RailwayCLI ignore files defined in .gitignore?,"The command railway up takes your current local project and uploads it directly to railway without having to link a Github repo to your railway project.
Does RailwayCLI take into account .gitignore file like Git does? if not what is the proper way to ignore files (not upload them) when using the command.
I couldn't find anything relevant mentioned in their docs.
",76394404,"No, RailwayCLI does not take into account the .gitignore file. If you want to ignore certain files when using the railway up command, you can use the --ignore-files flag. For example, to ignore all files with the .txt extension, you would use the following command:
railway up --ignore-files .txt

Update:
The -ignore-files flag uses the same globbing patterns as .gitignore. So you can use the * character to match any number of characters, and the ? character to match any single character.
For example, the following command will ignore all files that start with the word ""app"" and have any extension:
railway up --ignore-files app*

This command will ignore all files that have the .txt or .json extensions:
railway up --ignore-files .txt,.json

This command will ignore all files in the ./test directory and its subdirectories:
railway up --ignore-files ./test/*

You can also use the -ignore-files option to ignore specific files, even if they have extensions that are not listed in the .gitignore file. For example, the following command will ignore the file app.txt, even though the .txt extension is not listed in the .gitignore file:
railway up --ignore-files app.txt

",
76394390,How to replace a string defined by starting and ending index by another string in R?,"string <- ""this is a funny cat""

I want to replace the first 15 characters of string with 'orange`. The desired output is
'orange cat'

However, using substr gives me
substr(string, 1, 15) <- ""orange""
> string
[1] ""oranges a funny cat""

which is not the desired output.
",76394420,"The output of substr should be the pattern of sub.
string <- ""this is a funny cat""

sub(substr(string, 1, 15), ""orange"", string)
[1] ""orange cat""

Or directly replace the first 15 characters in sub.
sub(""^.{15}"", ""orange"", string)
[1] ""orange cat""

",
76392186,How can I stop codelenses from breaking up indent guides in VS Code?,"I have this visual annoyance when I am using VS Code. Whenever a method or class is declared, the ""reference"" counting is causing the vertical lines that connects the curly braces to break.
It is very annoying, I tried finding the settings in user preference but could not find anything. Could someone help me?
I find the ""reference"" count is alright, but how can I keep the vertical curly braces pair lines from breaking. I don't have this problem in Visual Studio.

",76393456,"This is a known issue and as far as I know, there's not much you can do about it for now except wait for it to get handled. If you google ""github vscode issues codelens indent guide"", you should easily find Indent Guides Have Breaks Where CodeLens UI is Rendered #9604. There, you'll see that one of the maintainers, @alexdima commented:

Code lenses are implemented as view zones (same mechanism as embedded editors when you find all references or as the diff editor). In some of the view zones (such as code lens) it makes sense to render the indent guides inside view zones, while in others (such as the embedded editors or the diff editor) it does not, therefore marking this as both a feature request and a bug.

You can give that issue ticket a thumbs up reaction to show support for it getting prioritized (there's a big backlog), and subscribe to it to get notified about discussions and updates. But please don't make noisy comments like ""me too"" / ""+1"" / ""bump"".
",
76389877,"Micrometer Tracing, Spring Boot 3.0, OTLP exporter class not found error","I have Spring Boot 3.0 based project, Kotlin and Micrometer Tracing (which superseded Spring Cloud Sleuth)
Trying to connect Micrometer tracing to OTLP collector, which is part of Jaeger.
The configuration class:
@Configuration
class OpenTelemetryConfiguration(
    @Value(""\${otel.exporter.otlp.traces.endpoint:http://localhost:4317}"")
    private val tracesEndpoint: String
) {
    @Bean
    fun spanExporter(): SpanExporter =
        OtlpGrpcSpanExporter.builder().setEndpoint(tracesEndpoint).build()

    @Bean
    fun jaegerPropagator(): TextMapPropagator =
        JaegerPropagator.getInstance()

}

Dependencies in gradle:
implementation(""io.micrometer:micrometer-core:1.11.0"")
implementation(""io.micrometer:micrometer-tracing:1.1.1"")
implementation(""io.micrometer:micrometer-registry-prometheus:1.10.5"")
implementation(""io.micrometer:micrometer-tracing-bridge-otel:1.1.1"")
implementation(""io.opentelemetry:opentelemetry-sdk:1.26.0"")
implementation(""io.opentelemetry:opentelemetry-sdk-extension-autoconfigure-spi:1.26.0"")
implementation(""io.opentelemetry:opentelemetry-exporter-common:1.26.0"")
implementation(""io.opentelemetry:opentelemetry-exporter-otlp:1.26.0"")

The error when application starts:
Failed to instantiate [io.opentelemetry.sdk.trace.export.SpanExporter]: Factory method 'spanExporter' threw exception with message: io/opentelemetry/exporter/internal/otlp/OtlpUserAgent
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:171)
    at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:653)
    ... 170 common frames omitted
Caused by: java.lang.NoClassDefFoundError: io/opentelemetry/exporter/internal/otlp/OtlpUserAgent
    at io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporterBuilder.<init>(OtlpGrpcSpanExporterBuilder.java:48)
    at io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter.builder(OtlpGrpcSpanExporter.java:40)
    at com.logindex.geoservice.configuration.OtelConfiguration.spanExporter(OtelConfiguration.kt:19)
    ... 171 common frames omitted
Caused by: java.lang.ClassNotFoundException: io.opentelemetry.exporter.internal.otlp.OtlpUserAgent
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)

What dependency is missing there?
there's no such class io.opentelemetry.exporter.internal.otlp.OtlpUserAgent in the library io.opentelemetry:opentelemetry-exporter-otlp:1.26.0
",76392983,"OTel is not stable yet so not every version of the OTel SDK is compatible with every version of Micrometer Tracing due to breaking changes in the OTel SDK.
You should delete all of your version definitions and let the Spring Boot BOM define versions for you, this is what you need:
implementation 'org.springframework.boot:spring-boot-starter-actuator'
implementation 'io.micrometer:micrometer-registry-prometheus'
implementation 'io.micrometer:micrometer-tracing-bridge-otel'
implementation 'io.opentelemetry:opentelemetry-exporter-otlp'

Btw Jaeger also supports Zipkin so you can use Brave too with the Zipkin exporter.
",
76392156,How to find different between two times with sql,"I have a table contain reference time and a user check-in time. Both data is different in type.
Sample data

dtime = 2023-06-02 08:23:21
work_time = 08:00-18:00

And my code is...
SELECT substring(dtime,-8,5) AS chkin, 
SUBSTRING(work_time, 1, 5) AS wt1, 
TIMESTAMPDIFF(MINUTE, substring(dtime,-8,5), SUBSTRING(work_time, 1, 5)) AS min_diff 
FROM ta_db 
WHERE id = 13181;

As a result...

chkin = 08:23
wt1 = 08:00

Now, I want to know how many minutes different from chkin and wt with TIMESTAMPDIFF.
So I did this...
TIMESTAMPDIFF(MINUTE, substring(dtime,-8,5), SUBSTRING(work_time, 1, 5)) AS min_diff 

But it returns NULL. Please be advised.
",76393483,"Try this:
SET @dtime = '2023-06-02 08:28:21';
SET @work_time = '08:00-18:00';

SELECT substring(@dtime,-8,5) AS chkin, 
SUBSTRING(@work_time, 1, 5) AS wt1, 
TIMESTAMPDIFF(MINUTE, @dtime, CONCAT(SUBSTRING(@dtime, 1,10), ' ', SUBSTRING(@work_time, 1, 5))) AS min_diff

",
76392180,Hide views in a stack view when auto layout attempts to reduce the stack view size?,"I have a UIStackView that contains multiple arranged subviews. I want to dynamically hide certain views within the stack view when auto layout attempts to reduce the stack view's size. I'm looking for a way to achieve this behavior using auto layout and without manually manipulating the frame.
I've tried setting the isHidden property of the subviews, but it doesn't seem to update the layout of the stack view correctly. The views still take up space within the stack view.
What would be the recommended approach to hide views in a stack view when auto layout attempts to reduce its size? Are there any specific methods or techniques that should be used to achieve this behavior?
Any guidance or code examples would be greatly appreciated. Thank you!
I've try to override layoutSubviews method.
    override func layoutSubviews() {
        super.layoutSubviews()

        for view in stackView.arrangedSubviews {
            if visibilityManager.shouldBeVisible(view: view) {
                if let view = view as? MyButton {
                    if view.intrinsicContentSize.height > view.bounds.height {
                        view.isHidden = true
                    } else {
                        view.isHidden = false
                    }
                }
            }
        }
    }

Expect:
The views in the stack view will be hidden
Actual:
Views are just compressed
",76393491,"When we set .isHidden = true on a stack view's arranged subview, that subview remains in the .arrangedSubviews collection but it is removed from the hierarchy and no longer has a valid frame.
Another approach would be to set the .alpha to either 1.0 or 0.0 to ""show / hide"" the view.
We create a custom view subclass - let's call it AutoHideSubviewsView. It will have a stack view with Top / Leading / Trailing constraints, but no Bottom constraint.
When the view frame changes - gets shorter or taller - we loop through the arranged subviews and:

get the frame
convert it to the view coordinate space (it's relative to the stack view itself)
if the view's bounds contains the frame, set its .alpha = 1.0 (show it)
else, set its .alpha = 0.0 (hide it)

Here's some quick example code...
Custom View
class AutoHideSubviewsView: UIView {
    
    let stackView: UIStackView = {
        let v = UIStackView()
        v.axis = .vertical
        v.spacing = 12.0
        v.translatesAutoresizingMaskIntoConstraints = false
        return v
    }()
    
    override init(frame: CGRect) {
        super.init(frame: frame)
        commonInit()
    }
    required init?(coder: NSCoder) {
        super.init(coder: coder)
        commonInit()
    }
    private func commonInit() {
        addSubview(stackView)
        NSLayoutConstraint.activate([
            stackView.topAnchor.constraint(equalTo: topAnchor, constant: 8.0),
            stackView.leadingAnchor.constraint(equalTo: leadingAnchor, constant: 8.0),
            stackView.trailingAnchor.constraint(equalTo: trailingAnchor, constant: -8.0),
            // NO Bottom constraint
        ])
        // not strictly necessary, but let's do this anyway
        self.clipsToBounds = true
    }
    
    override func layoutSubviews() {
        super.layoutSubviews()
        
        // on first layout, the stackView's subview's frames are not set
        //  so force another layout pass
        if stackView.arrangedSubviews.first!.frame.height == 0.0 {
            stackView.setNeedsLayout()
            stackView.layoutIfNeeded()
            self.setNeedsLayout()
            self.layoutIfNeeded()
            return
        }
        for v in stackView.arrangedSubviews {
            // the frames of the arranged subviews are
            //  relative to the stack view frame, so
            //  we want to convert the frames in case
            //  the stack view TOP is not Zero
            let r = stackView.convert(v.frame, to: self)
            // animate the alpha change so we can see it
            UIView.animate(withDuration: 0.3, animations: {
                v.alpha = self.bounds.contains(r) ? 1.0 : 0.0
            })
        }
    }
    
}

Example Controller
class ViewController: UIViewController {
    
    let ahView = AutoHideSubviewsView()
    
    var hConstraint: NSLayoutConstraint!
    
    override func viewDidLoad() {
        super.viewDidLoad()
        view.backgroundColor = .systemBackground
        
        // grow and shrink buttons
        var cfg = UIButton.Configuration.gray()
        cfg.title = ""Shrink""
        let btn1 = UIButton(configuration: cfg, primaryAction: UIAction() { _ in
            if self.hConstraint.constant > 15.0 {
                self.hConstraint.constant -= 10.0
            }
        })
        cfg.title = ""Grow""
        let btn2 = UIButton(configuration: cfg, primaryAction: UIAction() { _ in
            self.hConstraint.constant += 10.0
        })

        [ahView, btn1, btn2].forEach { v in
            v.translatesAutoresizingMaskIntoConstraints = false
            view.addSubview(v)
        }

        let g = view.safeAreaLayoutGuide
        NSLayoutConstraint.activate([
            
            ahView.topAnchor.constraint(equalTo: g.topAnchor, constant: 20.0),
            ahView.leadingAnchor.constraint(equalTo: g.leadingAnchor, constant: 20.0),
            ahView.widthAnchor.constraint(equalToConstant: 200.0),
            
            btn1.topAnchor.constraint(equalTo: g.topAnchor, constant: 40.0),
            btn1.leadingAnchor.constraint(equalTo: ahView.trailingAnchor, constant: 20.0),
            btn1.trailingAnchor.constraint(equalTo: g.trailingAnchor, constant: -20.0),
            
            btn2.topAnchor.constraint(equalTo: btn1.bottomAnchor, constant: 20.0),
            btn2.leadingAnchor.constraint(equalTo: ahView.trailingAnchor, constant: 20.0),
            btn2.trailingAnchor.constraint(equalTo: g.trailingAnchor, constant: -20.0),
            
        ])
        
        // start with the view height at 320
        hConstraint = ahView.heightAnchor.constraint(equalToConstant: 320.0)
        hConstraint.isActive = true
        
        // let's add 3 labels and 3 buttons to the custom view's stackView
        let strings: [String] = [""First"", ""Second"", ""Third""]
        let colors: [UIColor] = [.cyan, .green, .yellow]
        for (str, c) in zip(strings, colors) {
            let label = UILabel()
            label.font = .systemFont(ofSize: 24, weight: .light)
            label.text = str
            label.textAlignment = .center
            label.backgroundColor = c
            label.heightAnchor.constraint(equalToConstant: 40.0).isActive = true
            var cfg = UIButton.Configuration.filled()
            cfg.title = str
            let btn = UIButton(configuration: cfg, primaryAction: UIAction() { _ in
                print(""\(str) Button Tapped"")
            })
            ahView.stackView.addArrangedSubview(label)
            ahView.stackView.addArrangedSubview(btn)
        }

        // so we can see the framing
        ahView.backgroundColor = .red
    }
    
}

Looks like this (custom view's background is red):

Tapping the ""Shrink"" button will decrease the height of the view, tapping the ""Grow"" button will increase it.



To make it really easy to see what's happening, I animate the .alpha change so the subviews ""fade"" in and out.
Here's an animation (too big to post here): https://imgur.com/a/glRRh2O
",
76390199,"How to validate and submit a modal dialog, and, when succesful, execute some Javascript","The following order of events should happen in my Oracle APEX app:

User clicks Submit button in a modal dialog.
The configured validations are executed (mostly package functions).
If validation is OK, the page is submitted (a package function is called).
If no validation error or exception during submit happened, Javascript is executed. This Javascript needs access to the item values entered by the user, so it must not be executed after a page reload.
Modal dialog is closed.

I tried many ways to accomplish this, but all failed. The obvious solution would be two dynamic actions on the button click, but I learned that the order of the actions is not guaranteed, and the Javascript is executed even after an error in the validation or submit.
Now I think I have to do it with Javascript in a single dynamic action (button click on the modal dialog) like this:
// Validate and submit.
apex.page.submit( {
    validate: true,
} );

// Other JS code on successful submit, which is accessing page items.
... $v(""myItem"") ...

// Close modal dialog.
apex.navigation.dialog.close(true);

How can I check if apex.page.submit was successful?
And will the javascript be able to access item values entered by the user?
By the way, I don't want to put this Javascript on the parent page, because there will be multiple such modal dialogs, and I prefer to keep their respective logic separate, and not turn the parent page into a ""God object"".
Thank you in advance.
",76392996,"This isn't an answer to your question, because I believe what you want is simply not possible. To understand why, you need to check how APEX processes work.

A number of processes run in a pre-rendering phase. These are server-side processes (computations, pl/sql processes, form initialization, etc).
The dom is rendered based on the components. These components (optionally) take data from the pre-rendering process. In this phase client-side actions can be executed (dynamic actions, custom javascript)
Once a page is submitted, the ""processing"" starts. The form data entered in the previous phase it sent to the server. From this point onward everything is server-side: validations, page processing, branches, etc).

Now... javascript is client side only. and only in phase 2 can client-side code be executed. Once a page is submitted, the client can no longer be accessed. So ""execute javascript"" after ""server side validations"" is simply not possible in APEX. javascript can be executed onload OR when the page is rendered via an event OR before page submit. That's it in the current version of apex (23.1).
",
76394381,React Hook Form Error on custom Input component,"I am having a problem when I use react hook form + zod in my application, in short the inputs never change value and I get the following error in the console:
Warning: Function components cannot be given refs. Attempts to access this ref will fail. Did you mean to use React.forwardRef()?

my components:
// Login.tsx

import { useForm } from ""react-hook-form"";
import { zodResolver } from ""@hookform/resolvers/zod"";
import { z } from ""zod"";

import { Button } from ""../components/Form/Button"";
import { Input } from ""../components/Form/Input"";

const loginFormValidationSchema = z.object({
  username: z.string().min(3),
  password: z.string().min(3)
});

type LoginFormFields = z.infer<typeof loginFormValidationSchema>;

export function Login() {
  const {
    register,
    handleSubmit,
    formState: { errors, isSubmitting }
  } = useForm<LoginFormFields>({
    resolver: zodResolver(loginFormValidationSchema)
  });

  function handleSignIn(data: LoginFormFields) {
    console.log(data);
  }

  return (
    <section>
      <h1>Login</h1>

      <form onSubmit={handleSubmit(handleSignIn)}>
        <Input
          type=""text""
          error={errors.username?.message}
          autoComplete=""off""
          {...register(""username"")}
        />

        <Input
          type=""password""
          label=""Senha""
          error={errors.password?.message}
          {...register(""password"")}
        />

        <Button type=""submit"" disabled={isSubmitting}>
          Entrar
        </Button>
      </form>
    </section>
  );
}

// Input.tsx

import { InputHTMLAttributes } from ""react"";

import styles from ""./Input.module.css"";

interface InputProps extends InputHTMLAttributes<HTMLInputElement> {
  name: string;
  label?: string;
  error?: string;
}

export function Input({ name, label, error, ...props }: InputProps) {
  return (
    <div className={styles.wrapper}>
      {!!label && (
        <label htmlFor={name} className={styles.label}>
          {label}
        </label>
      )}

      <input id={name} name={name} className={styles.input} {...props} />

      {!!error && <p className={styles.error}>{error}</p>}
    </div>
  );
}

I have tried some alternatives that I found in some threads, but so far I have not been successful, has anyone experienced at least or has any notion of how it is possible to get around this error?
",76394421,"I think what's happening is the {...register(""username"")}, which gets picked up as ...props in the Input component's props, is using a ref under the hood (this is how the react-hook-form library identifies inputs). You should be able to fix this by converting the Input component to use a forwardRef like this:
export const Input = React.forwardRef<HTMLInputElement, Omit<InputProps, ""ref"">>(({ name, label, error, ...props }, ref) => (
    <div className={styles.wrapper}>
      {!!label && (
        <label htmlFor={name} className={styles.label}>
          {label}
        </label>
      )}

      <input id={name} name={name} className={styles.input} ref={ref} {...props} />

      {!!error && <p className={styles.error}>{error}</p>}
    </div>
));

Input.displayName = ""Input"";

Basically what this does is it allows you to put a ref attribute on the Input component, which gets 'forwarded' to the input component. This allows the library to monitor changes.
",
76391609,Multiple AJAX calls ; get all the failed calls,"I am making multiple ajax calls as shown below. Below code works fine If all the calls succeed. But, let's say urlId 3 and 4 failed for some reason. Is it possible to get all the failed urlId's in the fail function?
var urlId = [1, 3, 4, 7]
let requests = [];
for (let i = 0; i < urlId.length; i++) {
    requests.push($.ajax(...));
}
$.when.apply($, requests).done(function () {
    $.each(arguments, function (idx, args) {
        //process args : urlId[idx]
    });
}).fail(function (jqXHR) {
    //how to get urlId's failed
});

",76393867,"You would not be able to reliably get all of the failed IDs in the .fail handler because it will fire as soon as any of the deferreds becomes rejected, regardless of the state of the other deferreds.
If you want to keep track of which requests succeeded and which failed, I think your best option would be to attach a catch handler to each $.ajax call that will catch a failure and map it to an object that has a reference to the id and to the success/failure status. Catching errors this way will mean that all of the deferreds will succeed, so the handling of success and failed states will then need to be performed in the done handler attached to the $.when. For example:
var urlId = [1, 3, 4, 7]
let requests = [];

urlId.forEach(id => {
  requests.push(
    $.ajax(/*...*/)
      .then(response => {
        return {
          id,
          success: true,
          response
        };
      })
      .catch(error => {
        return {
          id,
          success: false,
          error
        }
      })
  );
});

$.when.apply($, requests)
  .done(function (values) {
    $.each(arguments, function (idx, obj) {
      console.log(`${obj.id}: success = ${obj.success}`);
    });
 });

Here is a fiddle for reference.
",
76392242,Get the instance type of a singleton class in TypeScript,"I have a singleton class:
class Singleton {
  private static instance: Singleton;

  private constructor() {
    // Private constructor to prevent instantiation outside the class
  }

  public static getInstance(): Singleton {
    if (!Singleton.instance) {
      Singleton.instance = new Singleton();
    }
    return Singleton.instance;
  }

  someMethod(): void {
    console.log(""Singleton method called"");
  }
}

I know there is a type InstanceType, but it doesn't work with private constructors.
// Cannot assign a 'private' constructor type to a 'public' constructor type
type SingletonType = InstanceType<typeof Singleton>;

Is it possible to create a custom type that returns instance type of classes with private constructors?

EDIT
Well, I'll try to shed some light on the whole situation. I hope this clears up misunderstandings. As I wrote earlier I have a singleton class with a private constructor. I have to pass this singleton to constructor of another generic class as a parameter:
class BaseEntity<T extends typeof Singleton = typeof Singleton> {
    private singletonInstance: InstanceType<T>;
    
    constructor(instance: InstanceType<T>) {        
        this.singletonInstance = instance;        
    }
}

As far as Singleton class has a private constructor, I get an error Cannot assign a 'private' constructor type to a 'public' constructor type when trying to get its instance InstanceType<T>. So my question is, is it possible to create a custom generic type that accepts classes with ""private"" constructors and returns an instance type similar to ""InstanceType""?
",76393891,"TypeScript assumes that class constructors have a prototype property whose type is the same as the class instance type.  This isn't actually true in practice, since generally speaking class fields won't actually be present on the prototype.  But TypeScript makes that assumption as an approximation intentionally, and has declined suggestions to change this (see microsoft/TypeScript#11558 and microsoft/TypeScript#20922 for examples).
And so, if the InstanceType<T> utility type is unavailable to you because of a private constructor, you can get the same information by indexing into the constructor's type with ""prototype"":
type AlsoInstanceType<T extends { prototype: any }> =
  T[""prototype""];

type SingletonType = AlsoInstanceType<typeof Singleton>
// type SingletonType = Singleton

class Foo { a = 1 }
type Example = AlsoInstanceType<typeof Foo>
// type Example = Foo;

And thus your BaseEntity can be written that way:
class BaseEntity<T extends typeof Singleton = typeof Singleton> {
  private singletonInstance: AlsoInstanceType<T>;

  constructor(instance: AlsoInstanceType<T>) {
    this.singletonInstance = instance;
  }
}


For Singleton in particular you could write ReturnType<typeof Singleton.getInstance> using the ReturnType<T> utility type:
type AlsoSingletonType = ReturnType<typeof Singleton.getInstance>;
// type AlsoSingletonType = Singleton

type SingletonInstance<T extends typeof Singleton> = ReturnType<T[""getInstance""]>;
type AlsoAlsoSingletonType = SingletonInstance<typeof Singleton>;
// type AlsoAlsoSingletonType = Singleton

But T[""prototype""] is more flexible.
Playground link to code
",
76394041,Why is this one paragraph link showing up funky?,"I am attaching some code in here, I have recently added in a link to my portfolio website and the paragraph link is doing something weird, it seems to be aligning funky and I'm not sure what the issue is. I'm wondering if it might be stemming from some other a stylings in the CSS (like nav_links) but I'm unsure how to correct this without messing up the other a stylings.
I added a class to the a href and tried to add in some CSS styling for it, it fixed some issues (like the size and color)but that gap
",76394431,"You're making a mistake in your CSS, doing things like this:
.nav_link li, a

You need to remove those commas. I assume here you're trying to style an a within an li within an element with a class of .nav_link. But that's NOT what you're doing. Instead, this selector is applying a bunch of styles to .nav_link li and separately to all a elements. The selector should actually read like so:
.nav_link li a

You have this issue at several points in your CSS.
",
76391772,Extracting a nested type from the generic parameter of a generic class in TypeScript,"I'm working on a TypeScript project and I have a generic class AbstractDAL with a generic type parameter T_DEFAULT_RETURN. Inside the AbstractDAL class, I'm trying to extract a nested type that is specified within the T_DEFAULT_RETURN generic type parameter, but I'm facing some challenges.
Here's the simplified code structure I have:
class A {
    alpha() { }
};

class B extends A {
    beta() { }
};

abstract class AbstractDAL<
    T_DEFAULT_RETURN extends BaseEntity = BaseEntity,       
    T_DATA = T_DEFAULT_RETURN extends BaseEntity<infer D> ? D : never
> {
    get result() {
        return {} as T_DATA
    }
}

class BaseEntity<       
    T_DATA extends A = A
> { }



class TestDAL extends AbstractDAL<TestEntity> {
    delta() {
        this.result.alpha // should also be beta, not just alpha
    }
}

class TestEntity extends BaseEntity<B> { }

In the above code, the AbstractDAL class is defined with a generic type parameter T_DEFAULT_RETURN, and I'm trying to extract a nested type from this parameter. I have used a conditional type with infer and a helper type T_DATA to accomplish this. However, the inferred type for T_DATA is A instead of the expected type B.
Is there a way to correctly extract the nested type B from the T_DEFAULT_RETURN generic type parameter within the AbstractDAL class? If so, what modifications are needed in the code to achieve this?
",76393963,"The problem is that BaseEntity<T> does not depend on T structurally, and thus inference from BaseEntity<T> to T is, at best, unreliable.

TypeScript's type system is largely structural and not nominal.  That means types are compared by their structure or shape, and not by what they are named or where they are declared.  If type X and type Y are both object types with the same members, then they are the same type:
interface X { w: string; z: number; }
let x: X = { w: ""abc"", z: 123 };

interface Y { w: string; z: number; }
let y: Y = { w: ""def"", z: 456 };

x = y; // okay
y = x; // okay

In the above it doesn't matter whether you use the name X or the name Y to refer to the type.  They are interchangeable.
This extends to generics as well.  If you have a generic type that doesn't use its type parameter inside the type definition, like
interface F<T> { w: string; z: number }

then all types you create by specifying T with a type argument will be identical to each other, and thus interchangeable:
let fs: F<string> = x; // okay
let fn: F<number> = y; // okay
fs = fn; // okay
fn = fs; // okay

There is no difference between F<string> and F<number>, or between either of them and X or Y.  We don't care about the name F<string>, just the shape { w: string; z: number }.
And that's indicative of a problem in the code.  Type inference, such as what you get when you use infer in a conditional type, can only work consistently if it is inferring from structure.
Asking TypeScript ""given G<T>, what type is T"", the compiler cannot necessarily answer correctly.  It depends strongly on the definition of G.  For example, given our F<T> definition above, where F<T> is identical to X for all T, then the information you want is essentially gone.  If I asked you to tell me which T makes F<T> equal to { w: string; z: number }, there's no principled away to answer.  Indeed any answer is equally correct.  There's no principled way to say that a particular { w: string; z: number } came from F<string> vs F<number>.  Sometimes you'll find that the compiler does infer from names, but you can't rely on it.
So, in general, you should never have unused type parameters.  See the TypeScript FAQ entry Why doesn't type inference work on this interface: interface Foo<T> { }?.

The solution is therefore to add some structure to your type that depends on the generic type parameter.  A BaseEntity<T> should have something to do with T!  For example, if you add a property of type T, everything starts working:
class BaseEntity<D extends A = A> {
    declare d: D // <-- make it structurally dependent
}

class TestDAL extends AbstractDAL<TestEntity> {
    delta() {
        this.result.alpha
        //^? (property) AbstractDAL<TestEntity, B>.result: B            
    }
}
class TestEntity extends BaseEntity<B> { }

That declare field is just me telling the compiler that a BaseEntity<D> has a field of type D at key d without having to actually initialize it.  In practice you should always initialize your class fields. The compiler will allow you to write new BaseEntity<string>().d.toUpperCase() but it will explode at runtime if d wasn't initialized.  I'm more worried about typings than initialization.
That's just an example.  You should try to give BaseEntity<T> some structural dependence on T that makes sense for the actual use case, and only resort to a ""phantom"" property like declare d if you really don't have anything better.
Playground link to code
",
76390366,How can I include price filter logic in a React filter?,"I am working on multi filter checkbox in react and redux. How to add extra price filter logic when:

The price range is less than 250

The price range is between 251 and 450

The price range is greater than 450


Below is the code for the filter reducer. Tried this if else condition but the problem is if multiple checkboxes are clicked this doesn't work
case actionTypes.FILTER_PRODUCT:
      const searchParameters = data && Object.keys(Object.assign({}, ...data));
      let filterData;
      console.log('test',action.payload);
      const searchQuery = action.payload.map((val) => val.toLowerCase());
      const getItemParam = (item, parameter) =>
        (item[parameter] ?? """").toString().toLowerCase();

      if (action.payload.length === 0) {
        filterData = state.products;
      } 
      else if(action.payload.includes('0-Rs250')){
        filterData = state.products.filter((item) => {
          return item.price <= 250
        })
      }
      else if(action.payload.includes('Rs251-450')){
        data = state.products.filter((item) => {
          return item.price > 250 && item.price <= 450
        })

      }else if(action.payload.includes('Rs 450')){
        filterData = state.products.filter((item) => {
          return item.price > 450
        })
      }
      else {
        filterData = state.products.filter((item) => {
          return searchParameters.some((parameter) => {
            const itemParam = getItemParam(item, parameter);

            return searchQuery.some(
              (color) => itemParam === color)
          });
        });
      }
      console.log('test1',filterData);
      return {
        ...state,
        filteredData: filterData,
      };

",76393192,"Change the conditions from exclusive OR to inclusive OR by changing from if - else if - else to if - if - if.
Example:
Start with initially empty filtered result set, and for each filtering criteria filter and append the original state.products array.
case actionTypes.FILTER_PRODUCT:
  ...

  let filteredData = [];

  ...

  if (action.payload.length) {
    if (action.payload.includes(""0-Rs250"")) {
      filteredData.push(...state.products.filter((item) => {
        return item.price <= 250;
      }));
    }
    if (action.payload.includes(""Rs251-450"")) {
      filteredData.push(...state.products.filter((item) => {
        return item.price > 250 && item.price <= 450;
      }));
    }
    if (action.payload.includes(""Rs 450"")) {
      filteredData.push(...state.products.filter((item) => {
        return item.price > 450;
      }));
    }
  } else {
    filteredData = state.products.filter(/* filter by searchParams */);
  }

  return {
    ...state,
    filteredData,
  };

An alternative using a more inline solution:
case actionTypes.FILTER_PRODUCT:
  ...

  const filteredData = state.products.filter((item) => {
    if (action.payload.length) {
      return (
        (action.payload.includes(""0-Rs250"") && item.price <= 250) ||
        (action.payload.includes(""Rs251-450"") &&
          item.price > 250 &&
          item.price <= 450) ||
        (action.payload.includes(""Rs 450"") && item.price > 450)
      );
    }
    return /* filter by searchParams */;
  });

  return {
    ...state,
    filteredData,
  };

This second method retains the original order of products from the source array.
",
76390529,TinyMCE insertHTML modifies some HTML tags,"I have some content in tinymce 6 editor. I moved the cursor and placed it inside the content and executed
tinymce.activeEditor.insertContent(`<span class='dummy'>
                        <div><span style=""font-style:italic;font-weight:bold;text-decoration:underline;"">This Text is BOLD</span></div>
                        <div><span style=""font-style:italic;"">This text is Italic</span></div>
                        <div><span style=""text-decoration:underline;"">This Text is&#160; Underlined</span></div>
                      </span>`, {format: 'html'});

When I execute this the div elements are getting removed.
I can see the content inside the BeforeSetContent event. But on the SetContent event the content is getting modified and the div are removed. Is there any way to prevent this behaviour ?
I tried adding
valid_children: '+div[span], +span[div]' 

in the editor config.
I am expecting the html content to be added into the tinymce 6 editor without modifying the tags
",76393668,"The default behavior is to remove certain elements, such as <div>, when they are inserted using the insertContent method.
But you can override this behavior by customizing the editor's schema and adding a rule to allow the <div> element:
tinymce.init({
  selector: '#your-selector',
  // other configurations...
  setup: function (editor) {
    editor.on('BeforeSetContent', function (e) {
      // Allow the <div> element in the content
      e.content = e.content.replace(/<div>/g, '<div data-mce-bogus=""1"">');
    });
  },
  valid_elements: 'div[*]' // Allow all attributes on <div> elements
});

",
76394423,"Do I need any environment variables set to execute some code, call openai's api, and return a response?","I was going through a course in OpenAI's API using an in-browser jupyter notebook page but wanted to copy some example code from there into a local IDE. I installed Python and the jupyter extention in VS Code and the OpenAI library. My code is below:
import openai
import os

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = ""my api key is here""

def get_completion(prompt, model=""gpt-3.5-turbo""):
    messages = [{""role"": ""user"", ""content"": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message[""content""]

prompt = f""""""
Determine whether each item in the following list of \
topics is a topic in the text below, which
is delimited with triple backticks.

Give your answer as list with 0 or 1 for each topic.\

List of topics: {"", "".join(topic_list)}

Text sample: '''{story}'''
""""""
response = get_completion(prompt)
print(response)

I installed Python and imported the openai library. When I run I am getting the error:
APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

I'm assuming that's because I commented out lines 3 and 4 in the code because I am unsure what they do and do not know how to use the dotenv library. Is it simple to set this up just to make a basic call to the openai API? That's all I'm trying to do with this code right now.
",76394444,"Usually, you load your API KEY from your .env file but, as you are hardcoding it, you don't need anything else.
The error you are getting might be related to the absence of the topic_list and story definitions.
",
76385353,SQLAlchemy 2.0 mock is inserting data,"I am trying to test a SQLAlchemy 2.0 repository and I am getting the error:
sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""professions_name_key""
So, although I am mocking the test, it inserts data into the database. What should I do to the test not insert data into the database?
I am using pytest-mock.
Here is the SQLAlchemy model
# File src.infra.db_models.profession_db_model.py
import uuid
from sqlalchemy import Column, String
from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy.dialects.postgresql import UUID
from src.infra.db_models.db_base import Base


class ProfessionsDBModel(Base):
    """""" Defines the professions database model.
    """"""

    __tablename__ = ""professions""

    profession_id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name: Mapped[str] = mapped_column(String(80), nullable=False, unique=True)
    description: Mapped[str] = mapped_column(String(200), nullable=False)

Here is the repository:
# File src.infra.repositories.profession_postgresql_repository.py

from typing import Dict, Optional
import copy
import uuid
from src.domain.entities.profession import Profession
from src.interactor.interfaces.repositories.profession_repository \
    import ProfessionRepositoryInterface
from src.domain.value_objects import ProfessionId
from src.infra.db_models.db_base import Session
from src.infra.db_models.profession_db_model import ProfessionsDBModel


class ProfessionPostgresqlRepository(ProfessionRepositoryInterface):
    """""" Postgresql Repository for Profession
    """"""
    def __init__(self) -> None:
        self._data: Dict[ProfessionId, Profession] = {}

    def __db_to_entity(self, db_row: ProfessionsDBModel) -> Optional[Profession]:
        return Profession(
            profession_id=db_row.profession_id,
            name=db_row.name,
            description=db_row.description
        )

    def create(self, name: str, description: str) -> Optional[Profession]:
        session = Session()
        profession_id=uuid.uuid4()
        profession = ProfessionsDBModel(
            profession_id=profession_id,
            name=name,
            description=description
        )
        session.add(profession)
        session.commit()
        session.refresh(profession)
        if profession is not None:
            return self.__db_to_entity(profession)
        return None

Here is the test:
import uuid
import pytest
from src.infra.db_models.db_base import Session
from src.domain.entities.profession import Profession
from src.infra.db_models.profession_db_model import ProfessionsDBModel
from .profession_postgresql_repository import ProfessionPostgresqlRepository
from unittest.mock import patch


def test_profession_postgresql_repository(mocker, fixture_profession_developer):
    
    
    mocker.patch(
        'uuid.uuid4',
        return_value=fixture_profession_developer[""profession_id""]
    )
    professions_db_model_mock = mocker.patch(
        'src.infra.db_models.profession_db_model.ProfessionsDBModel')
    session_add_mock = mocker.patch.object(
        Session,
        ""add""
    )
    session_commit_mock = mocker.patch.object(
        Session,
        ""commit""
    )
    session_refresh_mock = mocker.patch.object(
        Session,
        ""refresh""
    )

    repository = ProfessionPostgresqlRepository()
    repository.create(
        fixture_profession_developer[""name""],
        fixture_profession_developer[""description""]
    )

    assert session_add_mock.add.call_once_with(professions_db_model_mock)
    assert session_commit_mock.commit.call_once_with()
    assert session_refresh_mock.refresh.call_once_with(professions_db_model_mock)

",76393734,"This solution don't need to pass the session as parameter.
The solution was to mock the Session and not its methods separately.
As an advantage, the test is more concise now!
import uuid
import pytest
from src.domain.entities.profession import Profession
from src.infra.db_models.profession_db_model import ProfessionsDBModel
from .profession_postgresql_repository import ProfessionPostgresqlRepository


def test_profession_postgresql_repository(
        mocker,
        fixture_profession_developer
):

    mocker.patch(
        'uuid.uuid4',
        return_value=fixture_profession_developer[""profession_id""]
    )
    professions_db_model_mock = mocker.patch(
        'src.infra.repositories.profession_postgresql_repository.\
ProfessionsDBModel')
    session_mock = mocker.patch(
        'src.infra.repositories.profession_postgresql_repository.Session')
    professions_db_model = ProfessionsDBModel(
        profession_id = fixture_profession_developer[""profession_id""],
        name = fixture_profession_developer[""name""],
        description = fixture_profession_developer[""description""]
    )
    professions_db_model_mock.return_value = professions_db_model
    repository = ProfessionPostgresqlRepository()
    result = repository.create(
        fixture_profession_developer[""name""],
        fixture_profession_developer[""description""]
    )
    profession = Profession(
        professions_db_model_mock.return_value.profession_id,
        professions_db_model_mock.return_value.name,
        professions_db_model_mock.return_value.description
    )
    session_mock.add.assert_called_once_with(professions_db_model_mock())
    session_mock.commit.assert_called_once_with()
    session_mock.refresh.assert_called_once_with(professions_db_model_mock())
    assert result == profession


",
76383075,How to access the Fisher Weight matrix W from glmer() fit?,"As stated in my question, I would like to access the Fisher weights used in PIRLS model fitting for GLMMs from my glmer() fit in the R package lme4. For such a simple task, I was surprised that I couldn't find any information in the documentation or on the internet at all. By looking at the structure of the glmer fit, I found two possible quantities that might correspond to what I want (but I have no way to know):
glmmfit@resp$sqrtWrkWt() and glmmfit@pp$Xwts. They seem to be the same thing (up to very small numerical error). Are these the Fisher weights, or the square root of them? Or is this something else entirely?
P.S.: Could someone also confirm that glmmfit@resp$wrkResp() gives the working responses z=G(y-mu)+eta (sometimes called pseudodata), where G is a matrix containing the derivaties of the link function? Unexpectedly, it turns out that when I do GLMM_model@resp$eta+GLMM_model@resp$wrkResids()-GLMM_model@resp$wrkResp(), having added an offset of 4 to the model, I get a vector of fours, not zeros as I would expect..
",76393989,"This is a harder question to answer than it should be, but let's try.
There is a draft paper describing the implementation of glmer (an unpublished sequel to the Bates et al. JSS paper available via vignette(""lmer"", package = ""lme4"") in this directory (PDF here), but — although it is useful as background reading — it doesn't make a direct connection with the code.

The weights are updated here via

double glmResp::updateWts() {
        d_sqrtrwt = (d_weights.array() / variance()).sqrt();
        d_sqrtXwt = muEta() * d_sqrtrwt.array();
        return updateWrss();
    }

i.e., d_sqrtrtwt is the square root of the working weights [on the linear predictor or link scale] (to be honest I'm not sure what the r signifies); d_sqrtXwt is those weights transformed back on to the response/data scale (by multiplying by dmu/deta, the derivative of the inverse-link function).
From here, sqrtWrkWt is the same as the d_sqrtXwt value computed in updateWts.
Here we can see that the weights(., type = ""working"") returns object@pp$Xwts^2, and we can even see the comment that

the working weights available through pp$Xwts should be
equivalent to: object@resp$weights*(object@resp$muEta()^2)/object@resp$variance() 
However, the unit tests in tests/glmmWeights.R suggest that this equivalence is approximate.  This may be fine, however, if the discrepancy is due to another instance of the general problem of reference class fields not being updated at the optimum, then this could cause real problems.  see for example: https://github.com/lme4/lme4/issues/166

Here we see that wrkResp is defined as (d_eta - d_offset).array() + wrkResids(); and here that wrkResids() is (d_y - d_mu).array() / muEta();
Hopefully you should be able to access all the pieces you need without poking around in the guts this way ... e.g. weights(., ""working"") should give you the weights; family(.)$mu.eta should give you the derivative of the inverse-link function; residuals(., ""working"") should give you the working residuals.
The clue to why your ""PS"" is not working is that, as you can see from the code listed above, the $eta component of the @resp slot does not include the offset ... another reason it's best to try to work with accessor methods whenever possible instead of digging around ...
",
76394283,Why does my logo not appear in the Bootstrap navbar when using Ruby on Rails?,"I'm working on Ruby on Rails and I'm trying to put a logo.png image in a bootstrap navbar.
so i used this code in app/views/home/_header.html.erb
<nav class=""navbar navbar-expand-lg bg-primary "" data-bs-theme=""dark""> 
  <div class=""container-fluid"">
   <!-- <a class=""navbar-brand"" href=""#"">Estrutecnia</a> -->
 <a class=""navbar-brand"" href=""#"">
      <img src=""logo.png"" alt=""Estrutecnia"" width=""30"" height=""24"">
    </a>


    <button class=""navbar-toggler"" type=""button"" data-bs-toggle=""collapse"" data-bs-target=""#navbarSupportedContent"" aria-controls=""navbarSupportedContent"" aria-expanded=""false"" aria-label=""Toggle navigation"">
      <span class=""navbar-toggler-icon""></span>
    </button>
    <div class=""collapse navbar-collapse"" id=""navbarSupportedContent"">
      <ul class=""navbar-nav me-auto mb-2 mb-lg-0"">
        <li class=""nav-item"">
          <a class=""nav-link active"" aria-current=""page"" href=""#"">Home</a>
        </li>
        <li class=""nav-item"">
          <a class=""nav-link"" href=""#"">Link</a>
        </li>
        <li class=""nav-item dropdown"">
          <a class=""nav-link dropdown-toggle"" href=""#"" role=""button"" data-bs-toggle=""dropdown"" aria-expanded=""false"">
            Dropdown
          </a>
          <ul class=""dropdown-menu"">
            <li><a class=""dropdown-item"" href=""#"">Action</a></li>
            <li><a class=""dropdown-item"" href=""#"">Another action</a></li>
            <li><hr class=""dropdown-divider""></li>
            <li><a class=""dropdown-item"" href=""#"">Something else here</a></li>
          </ul>
        </li>
        <li class=""nav-item"">
          <a class=""nav-link disabled"">Disabled</a>
        </li>
      </ul>
      <form class=""d-flex"" role=""search"">
        <input class=""form-control me-2"" type=""search"" placeholder=""Search"" aria-label=""Search"">
        <button class=""btn btn-outline-success"" type=""submit"">Search</button>
      </form>
    </div>
  </div>

</nav>

The problem is that the logo doesn't appear, look at the navbar

And the next errors display
Started GET ""/logo.png"" for 127.0.0.1 at 2023-06-02 20:05:47 -0500

ActionController::RoutingError (No route matches [GET] ""/logo.png""):

I tried to put the logo.png file in the root directory that I guess is the folder where the Gemfile and the README.md files are.
I expect that the logo.png appears at the navbar, but a brocken file icon appears at the left side of the navbar, like described in the image above.
",76394445,"Images are normally placed in the app/assets/images directory.
When you render a view using the ActionView image_tag helper, Rails automatically populates the image source for you:
image_tag('logo.png')
# => <img src='/assets/icon.png>

This would be the idiomatic way to handle images in Rails, but of course there's nothing that prevents you from including <image src='/assets/icon.png'> in your markup
",
76394394,React/NextJS Timer-based fade in/fade out of text not working,"I'm relatively new to React and NextJS. I've spent a good 8 hours and a fair amount of research trying to figure this out with no luck!
I'm trying to set up a component that picks a word at random from an array, fades this in, then after a delay fades it out and fades in a new word.
After hacking together several of my own failed solutions, I found a tutorial and tried their code (with some changes to shift it from TS to JS) but it still doesn't work.
I can get the text to change on a timer easily enough, but not the fading.
Any ideas?
Using NextJS 13, built on StackBlitz's basic NextJS setup
index.js
import { useEffect, useState } from 'react';
import styles from './index.module.css';

const FADE_INTERVAL_MS = 2000;
const WORD_CHANGE_INTERVAL_MS = FADE_INTERVAL_MS * 2;
const WORDS_TO_ANIMATE = [
  'Hello',
  'Ciao',
  'Jambo',
  'Bonjour',
  'Salut',
  'Hola',
  'Nǐ hǎo',
  'Hallo',
  'Hej',
  '👋🏻',
];

export default function AnimatedText() {
  const [fadeProp, setFadeProp] = useState('fadeout');
  const [wordOrder, setWordOrder] = useState(0);

  useEffect(() => {
    const fadeTimeout = setInterval(() => {
      fadeProp === 'fadein' ? setFadeProp('fadeout') : setFadeProp('fadein');
    }, FADE_INTERVAL_MS);

    return () => clearInterval(fadeTimeout);
  }, [fadeProp]);

  useEffect(() => {
    const wordTimeout = setInterval(() => {
      setWordOrder(
        (prevWordOrder) => (prevWordOrder + 1) % WORDS_TO_ANIMATE.length
      );
    }, WORD_CHANGE_INTERVAL_MS);

    return () => clearInterval(wordTimeout);
  }, []);

  return (
    <h2 className={styles.white}>
      <span className={`styles.${fadeProp}`}>
        {WORDS_TO_ANIMATE[wordOrder]}
      </span>
      , I'm bleh.
    </h2>
  );
}


index.module.css
.fadein {
  animation: fadein 0.3s;
  -moz-animation: fadein 0.3s;
  -webkit-animation: fadein 0.3s;
  -o-animation: fadein 0.3s;
}

.fadein {
  animation: fadeout 0.3s;
  -moz-animation: fadeout 0.3s;
  -webkit-animation: fadeout 0.3s;
  -o-animation: fadeout 0.3s;
}

.white {
  color: silver;
}

@keyframes fadein {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes fadeout {
  0% {
    opacity: 1;
    color: blue;
  }
  100% {
    opacity: 0;
    color: red;
  }
}


",76394468,"You are in the right path. But you need to fix a few things. Instead of doing:
 <span className={`styles.${fadeProp}`}>
      {WORDS_TO_ANIMATE[wordOrder]}
 </span>

Try it with
 <span className={fadeProp}>
      {WORDS_TO_ANIMATE[wordOrder]}
 </span>

The reason being is that you were not setting your css class name selector correclty. I think after that you can tweak it for you convenience.
------ EDIT -----
Also, you will need to create a .css file, not .module.css. I think the reason  is because the .module.css are compiled and cannot be dynamic.
Create a global.css file inside styles dir. Import it in you _app.js entrypoint inside pages dir.
If you do not have it, it looks something like this:
_app.js
import '../styles/globals.css';
 
// This default export is required in a new `pages/_app.js` file.
export default function MyApp({ Component, pageProps }) {
  return <Component {...pageProps} />;
}


Using globals.css is not the best option but you can imrove that later on. Please find more info about styling Nextjs apps here.
",
76394396,store data in memory with nestjs,"I am trying to persist some data in my nestjs server so I can then use that data in my client app through http requests.
I have created a products.service.ts file with the function getAllData() that fetch some data and creates a new array that is assigned to the variable products (which is the data I'm trying to persist). This function is called when the app is initialized (I know this works because when I run the app the console.log(this.products) inside the function shows data.
This is my products.service.ts code:
import { Injectable } from '@nestjs/common';

@Injectable()
export class ProductsService {
  private products: any[] = [];

  getProducts(): any[] {
    //empty
    console.log(this.products); 
    return this.products;
  }

  async getAllProducts(): Promise<any[]> {
    const categories = await this.getProductsCategories();
    const productsPromises = categories.categories.map(async (category) => {
        const products = await this.getProductsByCategory(category.strCategory);
        const modifiedProducts = products.meals.map((product) => {
            ....
        });
        return modifiedProducts;
      });
      
      const products = await Promise.all(productsPromises);
      const flattenedProducts = products.flat();

      this.products = flattenedProducts;
      //shows data
      console.log(this.products) 
      
      return flattenedProducts;
    }

  async getProductsCategories(): Promise<any>{
    try{
      const apiURL = 'https://www.themealdb.com/api/json/v1/1/categories.php';
      const apiResponse = await fetch(apiURL);
      const categories = await apiResponse.json(); 

      return categories;
    }
    catch(e){
      throw new Error('Error while fetching products');
    }
  }
  async getProductsByCategory(category: string): Promise<any> {
    try {
      const apiURL = `https://www.themealdb.com/api/json/v1/1/filter.php?c=${category}`;
      const apiResponse = await fetch(apiURL);
      const products = await apiResponse.json();

      return products;
    } catch (e) {
      throw new Error('Error while fetching products by category');
    }
  }
}

The function getProducts() is called in my products.controller.ts file when an http request is done in the route '/products' but the products array is empty:
import { Controller, Get, Param } from '@nestjs/common';
import { ProductsService } from './products.service';

@Controller('products')
export class ProductsController {
    constructor(private readonly ProductsService: ProductsService) {}

 @Get('/')
    async getAllProducts(): Promise<any[]> {
        const products = await this.ProductsService.getProducts();

        return products;
     }
 }

Any idea why the products variable is empty when I make the request ? It should have the data created with getAllProducts() as this function is called onModuleInit
UPDATE 1:
I'll add the products.module.ts where I call getAllProducts() onModuleInit:
import { Module, OnModuleInit } from '@nestjs/common';
import { ProductsController } from './products.controller';
import { ProductsService } from './products.service';

@Module({
    controllers: [ProductsController],
    providers: [ProductsService],
})

export class ProductsModule implements OnModuleInit {
    constructor(private readonly productsService: ProductsService) {}
  
    async onModuleInit() {
      await this.productsService.getAllProducts();
    }
}

Then I import and use this module at app.module.ts file:
import { ProductsModule } from './products/products.module';
...

@Module({
  imports: [ProductsModule],
  controllers: [AppController, ProductsController],
  providers: [AppService, ProductsService],
})

export class AppModule {}

",76394477,"Don't add the ProductsController and ProductsService to the AppModule. You essentially have two ""versions"" of the ProductsController and ProductsService being instantiated, and Nest is calling the one that didn't run the onModuleInit from the ProductsModule. Remove them from the AppModule it and should all work.
",
76390606,How can I add a chatter for many2many fields (tags) in Odoo 16?,"I want to add chatter for Tags. But odoo does not support tracking of many2many fields.
What should i do?
Any Suggestions.
I tried to put tracking=true on tag_ids field but it does not work.
I am stuck here. Please give your suggestions.
",76393898,"You can override the _track_mail function to track x2many fields.
Check the code just after # Many2many tracking comment in the project module
Example:
if len(changes) > len(tracking_value_ids):
    for changed_field in changes:
        if tracked_fields[changed_field]['type'] in ['one2many', 'many2many']:
            field = self.env['ir.model.fields']._get(self._name, changed_field)
            vals = {
                'field': field.id,
                'field_desc': field.field_description,
                'field_type': field.ttype,
                'tracking_sequence': field.tracking,
                'old_value_char': ', '.join(initial_values[changed_field].mapped('name')),
                'new_value_char': ', '.join(self[changed_field].mapped('name')),
            }
            tracking_value_ids.append(Command.create(vals))

",
76392212,Typescript return type of intersected functions is mismatched with the inferred type,"As I was trying to extract the return type of a request that is intersected, I came across the this mismatch of the return type and the inferred type. Here is the shortened url https://tsplay.dev/mAxZZN
export {}
type Foo =  (() => Promise<string>) & (() => Promise<any>) ;


type FooResult = Foo extends () => Promise<infer T> ? T : null
//   ^?

const a:Foo = async () => {
        return """";
    }

const b = await a();
//    ^?


type Foo2 =  (() => Promise<any>) & (() => Promise<string>);

type FooResult2 = Foo2 extends () => Promise<infer T> ? T : null
//   ^?

const c:Foo2 = async () => {
    return """";
}

const d = await c();
//    ^?

In these 2 above examples the results are mismatched to FooResult:any b:string and FooResult2:string d:any
To make my example more clear instead of having a Foo type I just have a intersected type like HTTPRequest & {json: () => Promise<Type>} to have a correct return type of the request json object.
Is there anyway I can make these 2 be matched correctly to same type? If so how? Thanks for the help in advance! <3
",76394011,"An intersection of function types is equivalent to an overloaded function type with multiple call signatures.  And there are known limitations when dealing with such types at the type level. Unless you're trying to use overloads and have a strong need for them, you should consider refactoring to use a single call signature instead.

When you call an overloaded function, the call is resolved with ""the most appropriate"" call signature; often the first one in the ordered list that applies:
// call signatures
function foo(x: string): number;
function foo(x: number): string;

// implementation
function foo(x: string | number) {
    return typeof x === ""string"" ? x.length : x.toFixed(1)
}

const n = foo(""abc""); // resolves to first call signature
// const n: number

const s = foo(123); // resolves to second call signature
// const s: string

So the return type will depend on the input type.

On the other hand, when you try to infer from an overloaded function type, the compiler pretty much only infers from the last call signature:
type FooRet = ReturnType<typeof foo>
// type FooRet = string
// ^^^^^^^^^^^^^^^^^^^^ not (string & number) or [string, number]

This is mentioned in the Handbook documentation for using infer in conditional types and is considered a design limitation of TypeScript, as mentioned in microsoft/TypeScript#43301.
There are some possible workarounds to try to tease apart multiple call signature information using conditional types, but they are fragile, and before you even think of using them, you should re-examine your use case.

If you've got two call signatures with the same parameter types, then they really will not behave well at all:
function bar(): { a: string };
function bar(): { b: number }; // why?
function bar() {
    return { a: """", b: 1 }
}

When you call the function you'll get the first return type:
const a = bar();
// const a: { a: string; }

But when you infer you'll get the last return type:
type BarRet = ReturnType<typeof bar>;
// type BarRet = { b: number; }

And there doesn't seem to be a reason to have multiple call signatures in such cases.  If you want to get an intersection of return types, you should just have one call signature that does that:
function baz(): { a: string } & { b: number } {
    return { a: """", b: 1 }
}
const ab = baz();
// const ab: { a: string; } & { b: number; }
type BazRet = ReturnType<typeof baz>;
// type BazRet: { a: string; } & { b: number; }


So in your case, (() => Promise<string>) & (() => Promise<any>) is equivalent to an overloaded function whose first no-arg call signature returns Promise<string> and whose second no-arg call signature returns Promise<any>.  So you'll get the first when you call and the last when you infer.  Instead you should just have a single call signature like () => Promise<string> or () => Promise<any> or whatever your desired type is (the any type is problematic itself, but I won't digress further here to talk about it).
Playground link to code
",
76394330,MobSF unable to install on windows 11,"I have a question regarding installation of MobSF on my windows 11. its been countless times i tried without any luck.
i appreciate the expert here to point out what is my problems.I have downloaded latest MobSF from git
C:\Users\User\Documents\Mobile-Security-Framework-MobSF>setup
[INSTALL] Checking for Python version 3.8+
[INSTALL] Found Python 3.9.5
[INSTALL] Found pip
Requirement already satisfied: pip in c:\users\user\appdata\local\programs\python\python39\lib\site-packages (21.1.1)
Collecting pip
  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 261 kB/s
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 21.1.1
    Uninstalling pip-21.1.1:
      Successfully uninstalled pip-21.1.1
Successfully installed pip-23.1.2
[INSTALL] Found OpenSSL executable
[INSTALL] Found Visual Studio Build Tools
[INSTALL] Creating venv
Requirement already satisfied: pip in c:\users\user\documents\mobile-security-framework-mobsf\venv\lib\site-packages (21.1.1)
Collecting pip
  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 384 kB/s
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 21.1.1
    Uninstalling pip-21.1.1:
      Successfully uninstalled pip-21.1.1
Successfully installed pip-23.1.2
[INSTALL] Installing Requirements
Collecting wheel
  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)
     ---------------------------------------- 64.5/64.5 kB 53.4 kB/s eta 0:00:00
Installing collected packages: wheel
Successfully installed wheel-0.40.0
Ignoring gunicorn: markers 'platform_system != ""Windows""' don't match your environment
Collecting Django>=3.1.5 (from -r requirements.txt (line 1))
  Downloading Django-4.2.1-py3-none-any.whl (8.0 MB)
     ---------------------------------------- 8.0/8.0 MB 2.2 MB/s eta 0:00:00
Collecting lxml>=4.6.2 (from -r requirements.txt (line 2))
  Downloading lxml-4.9.2-cp39-cp39-win_amd64.whl (3.9 MB)
     ---------------------------------------- 3.9/3.9 MB 524.7 kB/s eta 0:00:00
Collecting rsa>=4.7 (from -r requirements.txt (line 3))
  Downloading rsa-4.9-py3-none-any.whl (34 kB)
Collecting biplist>=1.0.3 (from -r requirements.txt (line 4))
  Downloading biplist-1.0.3.tar.gz (21 kB)
  Preparing metadata (setup.py) ... done
Collecting requests>=2.25.1 (from -r requirements.txt (line 5))
  Downloading requests-2.31.0-py3-none-any.whl (62 kB)
     ---------------------------------------- 62.6/62.6 kB 418.7 kB/s eta 0:00:00
Collecting bs4>=0.0.1 (from -r requirements.txt (line 6))
  Downloading bs4-0.0.1.tar.gz (1.1 kB)
  Preparing metadata (setup.py) ... done
Collecting colorlog>=4.7.2 (from -r requirements.txt (line 7))
  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)
Collecting macholib>=1.14 (from -r requirements.txt (line 8))
  Downloading macholib-1.16.2-py2.py3-none-any.whl (38 kB)
Collecting whitenoise>=5.2.0 (from -r requirements.txt (line 9))
  Downloading whitenoise-6.4.0-py3-none-any.whl (19 kB)
Collecting waitress>=1.4.4 (from -r requirements.txt (line 10))
  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)
     ---------------------------------------- 57.7/57.7 kB 1.0 MB/s eta 0:00:00
Collecting psutil>=5.8.0 (from -r requirements.txt (line 12))
  Downloading psutil-5.9.5-cp36-abi3-win_amd64.whl (255 kB)
     ---------------------------------------- 255.1/255.1 kB 1.4 MB/s eta 0:00:00
Collecting shelljob>=0.6.2 (from -r requirements.txt (line 13))
  Downloading shelljob-0.6.3-py3-none-any.whl (9.9 kB)
Collecting asn1crypto>=1.4.0 (from -r requirements.txt (line 14))
  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)
     ---------------------------------------- 105.0/105.0 kB 1.5 MB/s eta 0:00:00
Collecting oscrypto>=1.2.1 (from -r requirements.txt (line 15))
  Downloading oscrypto-1.3.0-py2.py3-none-any.whl (194 kB)
     ---------------------------------------- 194.6/194.6 kB 2.0 MB/s eta 0:00:00
Collecting distro>=1.5.0 (from -r requirements.txt (line 16))
  Downloading distro-1.8.0-py3-none-any.whl (20 kB)
Collecting IP2Location==8.9.0 (from -r requirements.txt (line 17))
  Downloading IP2Location-8.9.0-py3-none-any.whl (16 kB)
Collecting lief>=0.12.3 (from -r requirements.txt (line 18))
  Downloading lief-0.13.1-cp39-cp39-win_amd64.whl (3.1 MB)
     ---------------------------------------- 3.1/3.1 MB 2.7 MB/s eta 0:00:00
Collecting http-tools>=2.1.1 (from -r requirements.txt (line 19))
  Downloading http-tools-2.1.1.tar.gz (550 kB)
     ---------------------------------------- 550.3/550.3 kB 3.8 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting libsast>=1.5.1 (from -r requirements.txt (line 20))
  Downloading libsast-1.5.2.tar.gz (36 kB)
  Preparing metadata (setup.py) ... done
Collecting pdfkit>=0.6.1 (from -r requirements.txt (line 21))
  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)
Collecting google-play-scraper>=0.1.2 (from -r requirements.txt (line 22))
  Downloading google_play_scraper-1.2.4-py3-none-any.whl (28 kB)
Collecting androguard==3.4.0a1 (from -r requirements.txt (line 23))
  Downloading androguard-3.4.0a1-py3-none-any.whl (918 kB)
     ---------------------------------------- 918.1/918.1 kB 668.1 kB/s eta 0:00:00
Collecting apkid==2.1.4 (from -r requirements.txt (line 24))
  Downloading apkid-2.1.4-py2.py3-none-any.whl (116 kB)
     ---------------------------------------- 116.6/116.6 kB 3.4 MB/s eta 0:00:00
Collecting quark-engine==22.10.1 (from -r requirements.txt (line 25))
  Downloading quark_engine-22.10.1-py3-none-any.whl (97 kB)
     ---------------------------------------- 97.6/97.6 kB 1.4 MB/s eta 0:00:00
Collecting frida==15.2.2 (from -r requirements.txt (line 26))
  Downloading frida-15.2.2.tar.gz (11 kB)
  Preparing metadata (setup.py) ... done
Collecting tldextract==3.4.0 (from -r requirements.txt (line 27))
  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)
     ---------------------------------------- 93.9/93.9 kB 1.8 MB/s eta 0:00:00
Collecting openstep-parser==1.5.4 (from -r requirements.txt (line 28))
  Downloading openstep_parser-1.5.4-py3-none-any.whl (4.5 kB)
Collecting svgutils==0.3.4 (from -r requirements.txt (line 29))
  Downloading svgutils-0.3.4-py3-none-any.whl (10 kB)
Collecting ruamel.yaml==0.16.13 (from -r requirements.txt (line 31))
  Downloading ruamel.yaml-0.16.13-py2.py3-none-any.whl (111 kB)
     ---------------------------------------- 111.9/111.9 kB 1.6 MB/s eta 0:00:00
Collecting click==8.0.1 (from -r requirements.txt (line 32))
  Downloading click-8.0.1-py3-none-any.whl (97 kB)
     ---------------------------------------- 97.4/97.4 kB 1.4 MB/s eta 0:00:00
Collecting decorator==4.4.2 (from -r requirements.txt (line 33))
  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)
Collecting asgiref<4,>=3.6.0 (from Django>=3.1.5->-r requirements.txt (line 1))
  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)
Collecting sqlparse>=0.3.1 (from Django>=3.1.5->-r requirements.txt (line 1))
  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)
     ---------------------------------------- 41.2/41.2 kB 998.2 kB/s eta 0:00:00
Collecting tzdata; sys_platform == ""win32"" (from Django>=3.1.5->-r requirements.txt (line 1))
  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)
     ---------------------------------------- 341.8/341.8 kB 1.5 MB/s eta 0:00:00
Collecting pyasn1>=0.1.3 (from rsa>=4.7->-r requirements.txt (line 3))
  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
     ---------------------------------------- 83.9/83.9 kB 1.6 MB/s eta 0:00:00
Collecting charset-normalizer<4,>=2 (from requests>=2.25.1->-r requirements.txt (line 5))
  Downloading charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl (97 kB)
     ---------------------------------------- 97.1/97.1 kB 2.8 MB/s eta 0:00:00
Collecting idna<4,>=2.5 (from requests>=2.25.1->-r requirements.txt (line 5))
  Downloading idna-3.4-py3-none-any.whl (61 kB)
     ---------------------------------------- 61.5/61.5 kB 1.7 MB/s eta 0:00:00
Collecting urllib3<3,>=1.21.1 (from requests>=2.25.1->-r requirements.txt (line 5))
  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)
     ---------------------------------------- 123.2/123.2 kB 904.1 kB/s eta 0:00:00
Collecting certifi>=2017.4.17 (from requests>=2.25.1->-r requirements.txt (line 5))
  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)
     ---------------------------------------- 157.0/157.0 kB 1.6 MB/s eta 0:00:00
Collecting beautifulsoup4 (from bs4>=0.0.1->-r requirements.txt (line 6))
  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)
     ---------------------------------------- 143.0/143.0 kB 1.4 MB/s eta 0:00:00
Collecting colorama; sys_platform == ""win32"" (from colorlog>=4.7.2->-r requirements.txt (line 7))
  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Collecting altgraph>=0.17 (from macholib>=1.14->-r requirements.txt (line 8))
  Downloading altgraph-0.17.3-py2.py3-none-any.whl (21 kB)
Collecting mitmproxy==6.0.2 (from http-tools>=2.1.1->-r requirements.txt (line 19))
  Downloading mitmproxy-6.0.2-py3-none-any.whl (1.1 MB)
     ---------------------------------------- 1.1/1.1 MB 2.6 MB/s eta 0:00:00
Collecting markupsafe==2.0.1 (from http-tools>=2.1.1->-r requirements.txt (line 19))
  Downloading MarkupSafe-2.0.1-cp39-cp39-win_amd64.whl (14 kB)
Collecting pyyaml>=6.0 (from libsast>=1.5.1->-r requirements.txt (line 20))
  Downloading PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)
     ---------------------------------------- 151.6/151.6 kB 3.0 MB/s eta 0:00:00
Collecting networkx>=2.2 (from androguard==3.4.0a1->-r requirements.txt (line 23))
  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)
     ---------------------------------------- 2.1/2.1 MB 3.9 MB/s eta 0:00:00
Collecting pygments>=2.3.1 (from androguard==3.4.0a1->-r requirements.txt (line 23))
  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)
     ---------------------------------------- 1.1/1.1 MB 5.2 MB/s eta 0:00:00
Collecting matplotlib>=3.0.2 (from androguard==3.4.0a1->-r requirements.txt (line 23))
  Downloading matplotlib-3.7.1-cp39-cp39-win_amd64.whl (7.6 MB)
     ---------------------------------------- 7.6/7.6 MB 5.2 MB/s eta 0:00:00
Collecting pydot>=1.4.1 (from androguard==3.4.0a1->-r requirements.txt (line 23))
  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)
Collecting ipython>=5.0.0 (from androguard==3.4.0a1->-r requirements.txt (line 23))
  Downloading ipython-8.14.0-py3-none-any.whl (798 kB)
     ---------------------------------------- 798.7/798.7 kB 5.1 MB/s eta 0:00:00
Collecting yara-python-dex>=1.0.1 (from apkid==2.1.4->-r requirements.txt (line 24))
  Downloading yara_python_dex-1.0.4-cp39-cp39-win_amd64.whl (130 kB)
     ---------------------------------------- 130.2/130.2 kB 8.0 MB/s eta 0:00:00
Collecting prettytable>=1.0.0 (from quark-engine==22.10.1->-r requirements.txt (line 25))
  Downloading prettytable-3.7.0-py3-none-any.whl (27 kB)
Collecting tqdm (from quark-engine==22.10.1->-r requirements.txt (line 25))
  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)
     ---------------------------------------- 77.1/77.1 kB 4.5 MB/s eta 0:00:00
Collecting graphviz (from quark-engine==22.10.1->-r requirements.txt (line 25))
  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)
     ---------------------------------------- 47.0/47.0 kB 2.3 MB/s eta 0:00:00
Collecting pandas (from quark-engine==22.10.1->-r requirements.txt (line 25))
  Downloading pandas-2.0.2-cp39-cp39-win_amd64.whl (10.7 MB)
     ---------------------------------------- 10.7/10.7 MB 4.9 MB/s eta 0:00:00
Collecting prompt-toolkit==3.0.19 (from quark-engine==22.10.1->-r requirements.txt (line 25))
  Downloading prompt_toolkit-3.0.19-py3-none-any.whl (368 kB)
     ---------------------------------------- 368.4/368.4 kB 5.8 MB/s eta 0:00:00
Collecting plotly (from quark-engine==22.10.1->-r requirements.txt (line 25))
  Downloading plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)
     ----------------------------- ---------- 11.3/15.3 MB 32.1 kB/s eta 0:02:06
ERROR: Exception:
Traceback (most recent call last):
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 438, in _error_catcher
    yield
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 561, in read
    data = self._fp_read(amt) if not fp_closed else b""""
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 527, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
  File ""C:\Users\User\AppData\Local\Programs\Python\Python39\lib\http\client.py"", line 455, in read
    n = self.readinto(b)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python39\lib\http\client.py"", line 499, in readinto
    n = self.fp.readinto(b)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python39\lib\socket.py"", line 704, in readinto
    return self._sock.recv_into(b)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python39\lib\ssl.py"", line 1241, in recv_into
    return self.read(nbytes, buffer)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python39\lib\ssl.py"", line 1099, in read
    return self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\cli\base_command.py"", line 169, in exc_logging_wrapper
    status = run_func(*args)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\cli\req_command.py"", line 248, in wrapper
    return func(self, options, args)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\commands\install.py"", line 377, in run
    requirement_set = resolver.resolve(
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\resolution\legacy\resolver.py"", line 185, in resolve
    discovered_reqs.extend(self._resolve_one(requirement_set, req))
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\resolution\legacy\resolver.py"", line 509, in _resolve_one
    dist = self._get_dist_for(req_to_install)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\resolution\legacy\resolver.py"", line 462, in _get_dist_for
    dist = self.preparer.prepare_linked_requirement(req)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 516, in prepare_linked_requirement
    return self._prepare_linked_requirement(req, parallel_builds)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 587, in _prepare_linked_requirement
    local_file = unpack_url(
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 166, in unpack_url
    file = get_http_url(
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\operations\prepare.py"", line 107, in get_http_url
    from_path, content_type = download(link, temp_dir.path)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\network\download.py"", line 147, in __call__
    for chunk in chunks:
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\cli\progress_bars.py"", line 53, in _rich_progress_bar
    for chunk in iterable:
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_internal\network\utils.py"", line 63, in response_chunks
    for chunk in response.raw.stream(
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 622, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 587, in read
    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\venv\lib\site-packages\pip\_vendor\urllib3\response.py"", line 443, in _error_catcher
    raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.
[INSTALL] Clean Up
=======================MobSF Clean Script for Windows=======================
Running this script will delete the Scan database, all files uploaded and generated.
C:\Users\User\Documents\Mobile-Security-Framework-MobSF\scripts
Deleting all uploads
Deleting all downloads
Deleting Static Analyzer migrations
Deleting Dynamic Analyzer migrations
Deleting MobSF migrations
Deleting temp and log files
Deleting Scan database
Deleting Secret file
Deleting Previous setup files
Deleting MobSF data directory: ""C:\Users\User\.MobSF""
Done
[INSTALL] Migrating Database
Traceback (most recent call last):
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\manage.py"", line 12, in <module>
    from django.core.management import execute_from_command_line
ModuleNotFoundError: No module named 'django'
Traceback (most recent call last):
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\manage.py"", line 12, in <module>
    from django.core.management import execute_from_command_line
ModuleNotFoundError: No module named 'django'
Traceback (most recent call last):
  File ""C:\Users\User\Documents\Mobile-Security-Framework-MobSF\manage.py"", line 12, in <module>
    from django.core.management import execute_from_command_line
ModuleNotFoundError: No module named 'django'
Download and Install wkhtmltopdf for PDF Report Generation - https://wkhtmltopdf.org/downloads.html
[INSTALL] Installation Complete
[ERROR] Installation Failed!
Please ensure that all the requirements mentioned in documentation are installed before you run setup script.
Scroll up to see any installation errors.

The 'decorator==4.4.2' distribution was not found and is required by the application

Ive tried git clone
reinstall python to 3.9.5
put the environment variables for python
",76394486,"The error message says:
The 'decorator==4.4.2' distribution was not found
So have you tried simply running pip install decorator==4.4.2? According to this github issue, installing the missing package fixed a similar problem.
",
76390794,How to detect variables defined outside the function (or undefined) without running the function in python,"I would like a function (let's call it detect_undefined) which detects variables used in a function, which are not defined inside the function without running it.
Examples:
import numpy as np

def add(a):
    return np.sum(a)

print(detect_undefined(add))

Output: [""np""]
def add(a):
    import numpy as np
    return np.sum(a)

print(detect_undefined(add))

Output: []
b = 3


def add(a):
    return a + b

print(detect_undefined(add))

Output: [""b""]
def add(a, b=3):
    return a + b

print(detect_undefined(add))

Output: []
It is crucial that the algorithm works without running the function to be examined, i.e. I cannot do something like try ... except. ChatGPT suggested to use inspect and ast, but its suggestion didn't quite work.
",76394082,"I'll provide two solutions, the first one doesn't work exactly, but I'll leave the information in case someone sees this post a few years later.
Solution1
inspect.getclosurevars or func.__code__.co_names is exactly what it is for. And it should be preferred over the 2nd solution.
b = 3


def add(a):
    return a + b


print(inspect.getclosurevars(add))
# ClosureVars(nonlocals={}, globals={'b': 3}, builtins={}, unbound=set())
print(add.__code__.co_names)
# ('b',)

Unfortunately, there is a bug in it, so it won't work.
import numpy as np


def add(a):
    return np.sum(a)


print(inspect.getclosurevars(add))
# ClosureVars(nonlocals={}, globals={'np': <module 'numpy' from ...>}, builtins={'sum': <built-in function sum>}, unbound=set())
print(add.__code__.co_names)
# ('np', 'sum')
# Note: sum is incorrectly detected as a built-in function.

Solution2
Use the dis module to analyze the function in bytecode. Below is an example of the information that dis provides.
b = 3


def add(a):
    return np.sum([a, b])


print(dis.dis(add))

Result:
 10           0 LOAD_GLOBAL              0 (np)
              2 LOAD_METHOD              1 (sum)
              4 LOAD_FAST                0 (a)
              6 LOAD_GLOBAL              2 (b)
              8 BUILD_LIST               2
             10 CALL_METHOD              1
             12 RETURN_VALUE

As you can see, LOAD_GLOBAL lines are what you are looking for. Now we can parse the lines and extract the names.
To parse the above result, dis.get_instructions is more useful.
def detect_undefined(func):
    for ins in dis.get_instructions(func):
        if inspect.iscode(ins.argval):  # inner function
            yield from detect_undefined(ins.argval)
        elif ins.opname == ""LOAD_GLOBAL"":  # global variable
            yield ins.argval

Here is the complete code with some tests.
import dis
import inspect
import numpy as np


def detect_undefined(func):
    for ins in dis.get_instructions(func):
        if inspect.iscode(ins.argval):  # inner function
            yield from detect_undefined(ins.argval)
        elif ins.opname == ""LOAD_GLOBAL"":  # global variable
            yield ins.argval


def add1(a):
    return np.sum(a)

print(list(detect_undefined(add1)))
# ['np']


def add2(a):
    import numpy as np
    return np.sum(a)

print(list(detect_undefined(add2)))
# []


b = 3

def add3(a):
    return a + b

print(list(detect_undefined(add3)))
# ['b']


def add4(a, b=3):
    return a + b

print(list(detect_undefined(add4)))
# []


x = 0
y = 1
z = 2

def complex_func(*args, **kwargs):
    p = 0
    print(f""{x}"")
    def inner1(t):
        q = y
        return lambda s: x + add3(z)
    return inner1(2)

print(list(detect_undefined(complex_func)))
# ['print', 'x', 'y', 'x', 'add3', 'z']
# Note that x is used twice, so it is detected twice.

One last thing to mention is that it also detects global functions. This may not be the OP's desired result, but excluding functions would be quite a challenge. Keep in mind that in Python we can assign functions to variables (b = add) or redefine functions as variables (add = 0). Or even worse, what are we supposed to do with a tuple that contains both variables and functions.
",
76385145,"Weird behavior in linking table creation in EF Core, adding it in the DbContext but shows two versions in the migration file","I have a weird behavior happening in my code.
I've created a Place entity and a SubCategory entity.
Because it is a many to many relationship, I created a PlaceSubCategory entity with a composite key.
Now, here's what its creation looks like in the DbContext:
public DbSet<PlaceSubCategory> PlaceSubCategories { get; set; }

And here's what it creates in the migration file:
migrationBuilder.CreateTable(
    name: ""PlaceSubCategories"",
    columns: table => new
    {
        PlaceId = table.Column<Guid>(type: ""uniqueidentifier"", nullable: false),
        SubCategoryId = table.Column<Guid>(type: ""uniqueidentifier"", nullable: false),
        CreatedDateTime = table.Column<DateTime>(type: ""datetime2"", nullable: false, defaultValueSql: ""getutcdate()"")
    },
    constraints: table =>
    {
        table.PrimaryKey(""PK_PlaceSubCategories"", x => new { x.PlaceId, x.SubCategoryId });
        table.ForeignKey(
            name: ""FK_PlaceSubCategories_Places_PlaceId"",
            column: x => x.PlaceId,
            principalTable: ""Places"",
            principalColumn: ""PlaceId"");
        table.ForeignKey(
            name: ""FK_PlaceSubCategories_SubCategories_SubCategoryId"",
            column: x => x.SubCategoryId,
            principalTable: ""SubCategories"",
            principalColumn: ""SubCategoryId"");
    });

migrationBuilder.CreateTable(
    name: ""PlaceSubCategory"",
    columns: table => new
    {
        PlacesPlaceId = table.Column<Guid>(type: ""uniqueidentifier"", nullable: false),
        SubCategoriesSubCategoryId = table.Column<Guid>(type: ""uniqueidentifier"", nullable: false)
    },
    constraints: table =>
    {
        table.PrimaryKey(""PK_PlaceSubCategory"", x => new { x.PlacesPlaceId, x.SubCategoriesSubCategoryId });
        table.ForeignKey(
            name: ""FK_PlaceSubCategory_Places_PlacesPlaceId"",
            column: x => x.PlacesPlaceId,
            principalTable: ""Places"",
            principalColumn: ""PlaceId"",
            onDelete: ReferentialAction.Cascade);
        table.ForeignKey(
            name: ""FK_PlaceSubCategory_SubCategories_SubCategoriesSubCategoryId"",
            column: x => x.SubCategoriesSubCategoryId,
            principalTable: ""SubCategories"",
            principalColumn: ""SubCategoryId"",
            onDelete: ReferentialAction.Cascade);
    });

It creates the table twice, which I thought was the default behaviour in EF Core to create a background linking table and so it shows in the migration file, but I'm not sure if that's the case.
Also, here's the configuration file for PlaceSubCategory (and not PlaceSubCategories):
public class PlaceSubCategoryConfiguration : IEntityTypeConfiguration<PlaceSubCategory>
{
    public void Configure(EntityTypeBuilder<PlaceSubCategory> builder)
    {
        // define the composite key for PlaceSubCategory table
        builder.HasKey(x => new { x.PlaceId, x.SubCategoryId });

        builder.HasOne<Place>(d => d.Place)
            .WithMany(d => d.SubCategoriesLink)
            .HasForeignKey(d => d.PlaceId)
            .OnDelete(DeleteBehavior.NoAction)
            .IsRequired();

        builder.HasOne<SubCategory>(d => d.SubCategory)
            .WithMany(d => d.PlacesLink)
            .HasForeignKey(d => d.SubCategoryId)
            .OnDelete(DeleteBehavior.NoAction)
            .IsRequired();

        builder.Property(p => p.CreatedDateTime)
            .HasConversion(AppDbContext.utcConverter)
            .HasDefaultValueSql(""getutcdate()"");
    }
}

But it isn't taking it into account, as you can see by the delete behavior being non-existent in the PlaceSubCategories creation, but there's an automatically set delete behavior for
PlaceSubCategory that's not what I set in the configuration file.
And of course the configuration has been set:
modelBuilder.ApplyConfiguration(new PlaceSubCategoryConfiguration());

",76393995,"The reason was that by creating this linking entity I was doing what EF Core would do anyway which is to create a linking entity in the background that would be showing in the migration file. So my entity got created and EF Core's as well without warning.
I deleted my entity and the EF Core's automatically created linking entity got added to the migration file, which seems fine and the same thing as if I created it.
",
76391247,Accessibility Rules Check for Progress Bars,"I am working on a React eCommerce website. The designs given to me show a flow for checking someone's credit for approval. When the user hits the ""Submit Application"" button, it takes them to a page that says ""Reviewing Application"" with a progress bar that takes about 12 seconds to load. When the bar is complete, the text says, ""You're Approved"" or ""Sorry, you are not approved"".
The designs then show that the Approved screen stays for a few seconds and then they are redirected to the checkout page. My question is, does this break any accessibility rules? It feels wrong to have a user click a button and then all these actions happen and if they are not paying attention, they could miss it telling them that they are approved.
The client is very keen on having a good accessibility score and wants to make sure that they don't break any rules. If this isn't allowed, can you please add a link to where it is stated more specifically? All I keep getting when Googleing this are examples of how to build a progress bar. TIA!
",76394272,"There two WCAG checkpoints at play here.
The first is dynamic content added to the page. ""Reviewing application..."" and then ""you're approved (or not)"".  That falls under WCAG 4.1.3 Status Messages. Just make sure the ""reviewing application"" indicates that the process is running. I'm guessing you have some kind of animation during the 12 seconds or so that it takes to approve or disapprove?  You could potentially use aria-busy=""true"" during the 12 seconds then set it to false when done.
Alternatively, you can use an aria-live region and then update the contents of that region to say ""reviewing application..."" and then maybe update it every 5 seconds or so to say ""still reviewing..."".  You'll probably need aria-atomic=""true"" if you update the region with the same text (""still reviewing..."") multiple times, otherwise the live region won't think anything changed.
The second checkpoint is WCAG 2.2.1 Timing Adjustable because you are redirecting the user on your own timing, and like you said, they might miss the approval status.  Or they might not have had time to read the approval page.
There are 6 ways to fix 2.2.1 as noted in the guideline itself.  ""Extend"" is the most common and you typically see this in ""log off"" situations where you're about to be logged off due to inactivity but are given a chance to extend your session. The same would be applied to your redirect. You can show the user the approval or denial message and then have a ""you will be redirected to the checkout page in XX seconds"" message with an option to extend the time limit.
Personally, I think the redirection should be avoided altogether. Just tell the user they've been approved or denied and then have a call to action button (CTA) and let the user navigate to the checkout screen on their own time. Then you avoid the 2.2.1 issue.
",
76394452,"about the remove() function,how can i del the last element only?","I have the problem is that,When i press remove button,whole list is deleted,
for example,i add ABC and DEF,this 2 parts,
I press one time and ABC also gone!
how can i fix it?
<!DOCTYPE html>
<html>
  <head>
    <title>To-Do List</title>
  </head>
  <body>
    <h1>To-Do List</h1>`
    <input type=""text"" class=""task"" placeholder=""Enter a task"">
    <button class=""Add"">Add Task</button>
    <button class=""removeTask"">Remove</button>
    <ul class=""taskList""></ul>
  </body>
  <script>



const taskList = document.querySelector("".taskList"");
const RemoveEl = document.querySelector("".removeTask"");
const addTask = document.querySelector("".Add"");


addTask.addEventListener(""click"", (e)=>{ 
  const taskInput = document.querySelector("".task"");
const newTask = document.createElement(""li"");
let task = taskInput.value;

  if (task.trim() != """") {
    
    newTask.innerText = task;
    taskList.appendChild(newTask);
    taskInput.value = """";};
  
    if(newTask){
  RemoveEl.addEventListener(""click"",()=>{

    taskList.remove();

  })
}});
  </script>
</html>

thank you so much!
",76394488,"The issue you're facing is because in the event handler for the Remove button, you are calling remove() on the taskList, which is the ul containing all your tasks. This will remove the entire ul element from the DOM, thereby deleting all tasks.
Instead, you should remove only the last task added to the list. Here's how you can do it:
<!DOCTYPE html>
<html>
  <head>
    <title>To-Do List</title>
  </head>
  <body>
    <h1>To-Do List</h1>`
    <input type=""text"" class=""task"" placeholder=""Enter a task"">
    <button class=""Add"">Add Task</button>
    <button class=""removeTask"">Remove</button>
    <ul class=""taskList""></ul>
  </body>
  <script>
    const taskList = document.querySelector("".taskList"");
    const RemoveEl = document.querySelector("".removeTask"");
    const addTask = document.querySelector("".Add"");

    addTask.addEventListener(""click"", () => {
      const taskInput = document.querySelector("".task"");
      const newTask = document.createElement(""li"");
      let task = taskInput.value;

      if (task.trim() != """") {
        newTask.innerText = task;
        taskList.appendChild(newTask);
        taskInput.value = """";
      }
    });

    RemoveEl.addEventListener(""click"", () => {
      if (taskList.lastChild) {
        taskList.lastChild.remove();
      }
    });
  </script>
</html>

This updated script adds an event listener to the Remove button outside of the Add button's event listener. When the Remove button is clicked, it checks if there is a last child element in the task list (the last task added). If there is, it removes that element, leaving the rest of the task list intact.
",
76389619,How to solve Fatal error: Maximum execution time of 300 seconds exceeded in Drupal installation on line 171?,"Fatal error: Maximum execution time of 300 seconds exceeded on line 171
I am new in the world of drupal My port no.80 is assigned to my ASP server so I changed my Apache server from 80 to 8080 in httpd.conf , BTW I am using XAMPP server I got Error ""Fatal error: Maximum execution time of 300 seconds exceeded in C:\xampp\htdocs\MyDemoWebsite\core\modules\mysql\src\Driver\Database\mysql\Connection.php on line 171"" while install site after Set up database in core instal Set up database Image
Maximum execution time of 300 seconds exceeded in
May be I am doing somthing wrong in Set Up Database enter image description here   I have tried by changing the port no. to 8080port no. to 8080
",76394033,"Maybe you're doing it first time. So, you have confused Apache port with mysql port.

You can see, I have started Xampp, here I am running Apache, with http and https, in ports (80, and 443) now according to the availability of ports in your pc, you can change them
Now, comes the database i.e. MySQL, whose port is normally 3306, or you can change to something else, according to your availability.
During Drupal installation. The database port which you have put as 8080, will be 3306, if you have not changed it either, which you can check from your xampp control panel, as I have added screenshot.
Now, as apache is running on port 8080, then your site will run on localhost:8080/<folder-name>, by default apache port is 80 which is omitted by browser, if that changed to anything else, which requires to be mentioned on http address
",
76383192,How can I create a D3 transition animation to update a React component without re-rendering the whole plot?,"I have a D3js Plot wrapped inside a React component (v7). For example a Bar Plot with a data table and a parameter for which column to plot. On change of the plotting variable, I do not want to re-render the whole plot but instead execute a D3 transition animation to the new variable.
Right now I have tried it following the stump-code here, but first I have problems getting the initial plot to render and second I really would like to understand what the correct React hook way is to achieve this…
import * as React from ""react"";
import * as d3 from ""d3"";

export function BarPlot({
  data,
  x,
  width,
  height,
}: {
  data: DataTable;
  x: string;
  width: number;
  height: number;
}) {
  const svgRef = React.useRef<SVGSVGElement>(null);
  const svg = d3.select(svgRef.current);

  const prevX = React.useRef<string>(x);
  
  
  if (svgRef.current === null) {
    svg.selectAll(""*"").remove();
    svg
      .append(""g"")
      .selectAll()
      .data(data)
    // …
    // Normal d3 plotting code here
    // …
  }


  if (prevX.current !== x) {
    // Update the plot, animate the transition from plotting the old bars to the new bars

    prevX.current = x;
  }

  return <svg ref={svgRef} width={width} height={height} />;
}

When looking around for the correct React-way to do this, it seems useEffect is not the right choice here. I also tried to use useMemo to save the inital plot, but even then I need to manually check whether the transitionable parameters have changed…
Abstract, I think the question is how to have a React component, where part of the render code is executed intially and another part only if the already rendered component has a change in one of the props.
",76394700,"Here is an example of animated bar chart using React with D3.
Just add a useEffect on the SVG element ref and build the chart when ref is valid (when the component is mounted)


const MAX_VALUE = 200;

const BarChart = ({ data, height, width }) => {
  const svgRef = React.useRef(null);

  React.useEffect(() => {
    const svg = d3.select(svgRef.current);

    const xScale = d3.scaleBand()
      .domain(data.map((value, index) => index.toString()))
      .range([0, width])
      .padding(0.1);

    const yScale = d3.scaleLinear()
      .domain([0, MAX_VALUE])
      .range([height, 0]);

    const xAxis = d3.axisBottom(xScale)
      .ticks(data.length)
      .tickFormat((_, index) => data[index].label);

    svg
      .select(""#x-axis"")
      .style(""transform"", `translateY(${height}px)`)
      .style(""font-size"", '16px')
      .call(xAxis);

    const yAxis = d3.axisLeft(yScale);
    svg
      .select(""#y-axis"")
      .style(""font-size"", '16px')
      .call(yAxis)

    svg.selectAll('g.tick');

    const bars = svg
      .selectAll("".bar"")
      .data(data)
      .join(""g"")
      .classed(""bar"", true);
      
    bars.append(""rect"")
      .style(""transform"", ""scale(1, -1)"")
      .attr(""x"", (_, index) => xScale(index.toString()))
      .attr(""y"", -height)
      .attr(""width"", xScale.bandwidth())
      .transition()
      .delay((_, index) => index * 500)
      .duration(1000)
      .attr(""fill"", d => d.color)
      .attr(""height"", (d) => height - yScale(d.value));
      
  }, [data]);

  return (
      <svg ref={svgRef} height={height} width={width} />
  );
};

const data = [
  {value: 50, color: '#008'}, 
  {value: 100, color: '#00C'}, 
  {value: 150, color: '#00f'}
];

ReactDOM.render(
  <BarChart data={data} width={300} height={170} />, 
  document.getElementById(""chart"")
);
<script src=""https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.4/d3.min.js""></script>

<script crossorigin src=""https://unpkg.com/react@16/umd/react.development.js""></script>

<script crossorigin src=""https://unpkg.com/react-dom@16/umd/react-dom.development.js""></script>

<div id='chart'></div>



",
76390441,Procedure pointer vs procedure name as argument of Fortran procedures,"My understanding is that we can pass the information about which procedure should be used inside a Fortran procedure, either using an argument and declaring it to be a procedure name through a specific interface, or using an argument declared to be a procedure pointer.
I do not think I have grasped all pros and cons of the two alternatives. Maybe one has some limitations not present with the other? Are there efficiency issues?

Just to make clear what this question is about (if necessary), I'll briefly recall through two examples the different ways of passing information about which procedure should be used inside a procedure, through the procedure arguments.

using an argument and declaring it to be a procedure name through a specific interface

module pro
 implicit none
contains

 function myfunction(x) result(res) ! this is an actual function with the right signature to be used as argument of the subroutine prosub
    real, intent(in) :: x
    real :: res
    res = 2*x
 end function myfunction

 subroutine prosub(f,x)
   real, intent(in) :: x
   procedure(myfunction) :: f  ! the actual argument used in place of the dummy argument f must be the name of a function with the same signature as myfunction
   print*,f(x)
 end subroutine prosub
end module pro

program ppp
 use pro
 implicit none
 real :: x
 print*,' x= ?'
 read*,x
 call prosub(myfunction,x)
end program ppp



using an argument declared to be a procedure pointer

module pro
 implicit none
 abstract interface ! defines the signature of the functions the procedure pointers argument of the subroutine prosub must have
   function f(x) result(res)
     real, intent(in) :: x
     real :: res
   end function f
 end interface

contains

 function myfunction(x) result(res) ! one of the possible actual functions the procedure pointer can point to
    real, intent(in) :: x
    real :: res
    res = 2*x
 end function myfunction

 subroutine prosub(fp,x)
   real, intent(in) :: x
   procedure(f), pointer :: fp
   print*,fp(x)
 end subroutine prosub
end module pro

program ppp
 use pro
 implicit none
 real :: x
 procedure(f), pointer :: f1

 f1 => myfunction

 print*,' x= ?'
 read*,x
 call prosub(f1,x)
end program ppp 


",76394100,"A dummy procedure pointer, when contrasted with a non-pointer dummy procedure, may (depending on argument intent and the like):

be passed in as disasssociated or undefined,
have its association status tested,
have its association status changed,
pass out information to the calling context.

If these capabilities are useful to have when writing a subprogram, then there might be a need for a dummy procedure pointer.
Specifying that something is a procedure pointer or non-pointer is orthogonal to how you specify the interface.  There is no semantic difference between a declaration of the form
procedure(some_interface) :: proc

and writing out an interface body for proc that has the same characteristics as some_interface. In your first ""non-pointer"" program, you could have equally written:
subroutine prosub(f,x)
  real, intent(in) :: x
  procedure(myfunction) :: f  ! or use the abstract interface
  print*,f(x)
end subroutine prosub

and in your second ""pointer"" program, you could have written:
subroutine prosub(fp,x)
  real, intent(in) :: x
 
  interface 
    function fp(x) result(res)
      real, intent(in) :: x
      real :: res
    end function fp
  end interface
  pointer :: fp
 
  print*,fp(x)
end subroutine prosub

This choice comes down to whether you want to specify the (perhaps abstract) interface in one location, or how much you like typing.
",
76394304,Confusion regarding converting recursion to iteration using stack,"I have a big confusion on what is meant by the general method of converting recursive functions (tail and non tail) to iterative ones using stack. I made this simple example to illustrate the point:
int sigma1(int n) {
  if(n == 0) 
    return 0;
  else 
    return n + sigma1(n-1);
}

which of the following versions reflects the method correctly:
Version 1
int sigma1_iter(int n) {
  stack* s = create_stack(n);
  push(s, n);
  int res = 0;
  while(!is_empty(s)) {
    int elm = pop(s);
    if(elm == 0)
      return res;
    else {
      res += n;
      push(s, n-1);
    }
  }
}


Version 2
int sigma1_iter(int n) {
  stack* s = create_stack(n);
  push(s, n);
  int res = 0;
  while(!is_empty(s)) {
    if (n > 0) {
      n--;
      push(s, n);
    }
    else {
      int elm = pop(s);
      if(elm == 0)
        return res;
      else {
        res += n;
    }
  }
}

Also, in case of the tail recursive version of sigma with an accumulator:
int sigma2(int n, int res) {
  if (n == 0) 
    return res;
  else
    return sigma2(n-1, n+res);
}

How would one convert it? I am thinking of creating a second stack for res, but how that might work?
",76394489,"Be sure to post a minimal, reproducible example when you ask a question. That helps everyone understand the question and generate and test potential answers.
If you had created such an example, it would be evident that version 1 of your iterative solution never returns and version 2 always returns 0 making neither a good candidate for correctness.
Recursion can be challenging, but I think we can walk through a general method for converting a recursive function to an iterative function for this example.
Let's look at the recursive implementation for sigma1,
int sigma1(int n) {
    if (n == 0)
        return 0;
    else
        return n + sigma1(n - 1);
}

Under the hood, calling this function generates a new stack frame for each recursive invocation (assuming the compiler doesn't optimize it away). Each of these call frames will have the value of the argument n for that frame which will decrease by one at each level of recursion. We can think about the iterative version, in the same way, storing the value of n on our explicit std::stack<int> instead of it being handled automagically by the call stack.
When the recursion terminates and the call frames unwind, the local value of n is added to the result of the previous call creating a new result to be returned. In the same way, we can pop values from the explicit std::stack<int> and add them to the running total in the iterative version.
The function would look something like this,
int sigma1_stack(int n) {
    std::stack<int> stack;
    for (int i = n; i > 0; --i)
        stack.push(i);

    int res{};
    while (not stack.empty()) {
        res += stack.top();
        stack.pop();
    }

    return res;
}

Now, in this particular scenario, you can simplify the two loops into a single iterative loop without a stack. This, of course, will not always be the case.
int sigma1_iter(int n) {
    int res{};
    for (int i = n; i > 0; --i)
        res += i;
    return res;
}

I took the code you posted and changed it to compile using a std::stack<int> since you did not post a complete example. I also added a correct version of iteration for both sigma1 and sigma2 and the corresponding output. Hope this is helpful.
Sample Code
#include <iostream>
#include <stack>

using std::cout, std::endl;

int sigma1(int n) {
    if (n == 0)
        return 0;
    else
        return n + sigma1(n - 1);
}

int sigma1_iter_v1(int n) {
    std::stack<int> stack;
    stack.push(n);

    int res = 0;
    while (not stack.empty()) {
        int elm = stack.top();
        stack.pop();
        if (elm == 0)
            return res;
        else {
            res += n;
            stack.push(n - 1);
        }
    }

    return res;
}

int sigma1_iter_v2(int n) {
    std::stack<int> stack;
    stack.push(n);

    int res = 0;
    while (not stack.empty()) {
        if (n > 0) {
            n--;
            stack.push(n);
        } else {
            int elm = stack.top();
            stack.pop();
            if(elm == 0)
                return res;
            else {
                res += n;
            }
        }
    }

    return res;
}

int sigma1_iter(int n) {
    int res{};
    for (int i = n; i > 0; --i)
        res += i;
    return res;
}

int sigma1_stack(int n) {
    std::stack<int> stack;
    for (int i = n; i > 0; --i)
        stack.push(i);

    int res{};
    while (not stack.empty()) {
        res += stack.top();
        stack.pop();
    }

    return res;
}

int sigma2(int n, int res = 0) {
    if (n == 0)
        return res;
    else
        return sigma2(n - 1, n + res);
}

int sigma2_iter(int n) {
    int res{};
    for (int i = n; i > 0; --i)
        res += i;
    return res;
}

int main(int argc, const char *argv[]) {
    cout << ""sigma1         : "" << sigma1(20) << endl;
    cout << ""sigma1_iter   : "" << sigma1_iter(20) << endl;
    cout << ""sigma1_stack  : "" << sigma1_stack(20) << endl;
    // This never returns
    // cout << ""sigma1_iter_v1 : "" << sigma1_iter_v1(20) << endl;
    cout << ""sigma1_iter_v1 : "" << ""never returns"" << endl;
    cout << ""sigma1_iter_v2 : "" << sigma1_iter_v2(20) << endl;

    cout << endl;
    cout << ""sigma2         : "" << sigma2(20) << endl;
    cout << ""sigma2_iter    : "" << sigma2_iter(20) << endl;
    return 0;
}

Output
sigma1         : 210
sigma1_iter    : 210
sigma1_stack   : 210
sigma1_iter_v1 : never returns
sigma1_iter_v2 : 0

sigma2         : 210
sigma2_iter    : 210

",
76392206,Where to host Django project files on deployment server,"New to Django here. I have developed a minimum working django website with Postgres as database back-end and nginx/gunicorn as web server on Ubuntu linux. Currently all the files are on my laptop in ~/workspace/djangoapp/src$ in my home directory. I want to now deploy the project to GCP. Which directory, on the production server, the files would go in? It can't be my home directory on the production server. Shouldn't they go in one of the system directories like /opt?
",76395043,"If you want to deploy your project on Google Cloud Platform, you should follow GCP guidelines. There are step by step guide on how to deploy and run Django app on GCP as for example running ""Django on App Engine standard environment"".
It would be easier to follow the GCP guides for your production server.
",
76384215,How to increase Kubeflow Jupyter notebook image name to see the complete path,"I have a deployment of Kubeflow. During creation of Jupyter Notebook, we have image name tags that are longer than that can be displayed in the  Docker image name list.  See the image list in the attached image. Here the names are short, but for our customer the repository URL is a long fqdn of the harbor registry URL.   Is there any configuration that can increase the visible text ?

Looking for a configuration to increase the visible text. Tried adding tags to the docker image, but this is only a suffix for the name.
",76394322,"As I understand it, you'd like to display the repository prefix for your Jupyter images. For this, you can set to false the hideRegistry key in the Jupyter Web App ConfigMap. By default, the value of this key is true, which hides the image repository in the user interface.
Search this ConfigMap in kubeflow namespace. In Kubeflow Manifests GitHub repository you can found in spawner_ui_config.yaml file.
",
76394292,Filling NAN values in Pandas by using previous values,"I have a Pandas DataFrame in the following format.

I am trying to fill the NaN value by using the most recent non-NaN value and adding one second to the time value. For example, in this case, the program should take the most recent non-NaN value of 8:30:20 and add one second to replace the NaN value. So, the replacement value should be 8:30:21. Is there a way in Pandas to simulate this process for the entire column?
",76394541,"You can convert your data to_timedelta, ffill and add 1 second:
df['col1'] = pd.to_timedelta(df['col1'])

df['col1'] = df['col1'].ffill().add(df['col1'].isna()*pd.Timedelta('1s'))

Output:
             col1
0 0 days 08:30:18
1 0 days 08:30:19
2 0 days 08:30:20
3 0 days 08:30:21
4 0 days 08:30:22

Used input:
df = pd.DataFrame({'col1': ['8:30:18', '8:30:19', '8:30:20', np.nan, '8:30:22']})

converting back to strings
Use a custom function:
def to_str(s):
    h,m = s.dt.total_seconds().divmod(3600)
    m,s = m.divmod(60)
    return (h.astype(int).astype(str).str.zfill(2)
            +':'+
            m.astype(int).astype(str).str.zfill(2)
            +':'+
            s.astype(int).astype(str).str.zfill(2)
            )
    
df['col1'] = to_str(df['col1'])

Output:
       col1
0  08:30:18
1  08:30:19
2  08:30:20
3  08:30:21
4  08:30:22

",
76383655,docker compose 3.3 cannot reach other containers,"I have a docker-compose file version 3.3 describing a Postgres (PostGIS) database instance and a GeoServer (HTTP backend).
I run them with no dependencies, but in the same docker network.
Later once PostGIS has done its thing and GeoServer has done its thing as well, I configure a GeoServer datastore with the Postgres connection details via the GeoServer API using the Postgres Docker service name as host, but then when I test these connection details the test fails i.e. GeoServer cannot reach Postgres in the same Docker network.
Interesting fact: the Postgres container is on reach from localhost.
# this works
PGCONNECT_TIMEOUT=2 psql ""postgresql://adm_pg_user:1234abcd@localhost:6666/fgag_db"" --command ""SELECT NOW();""

It's a pure TCP/IP issue as I logged into the GeoServer running container and used the command psql to see that Postgres is not on reach, see below and notice pg-db instead of localhost:
# from within the GeoServer container this doesn't work
PGCONNECT_TIMEOUT=2 psql ""postgresql://adm_pg_user:1234abcd@pg-db:6666/fgag_db"" --command ""SELECT NOW();""


The error message is:
psql: error: connection to server at ""pg-db"" (172.19.0.3), port 6666 failed: Connection refused
    Is the server running on that host and accepting TCP/IP connections?


What's wrong with the Docker Compose YAML below?
How can I implement networking checks?

For completeness: I use this env var COMPOSE_PROJECT_NAME=fgag to be able to add my preferred prefix to docker services / docker network names. Maybe this is relevant to the networking issues.
version: ""3.3""
services:
  pg-db:
    image: postgis/postgis:15-3.3-alpine
    restart: always
    ports:
      - ""${MYPG_PORT:-5432}:5432""
    environment:
      # https://github.com/docker-library/docs/tree/master/postgres#environment-variables
      POSTGRES_USER: ""${MYPG_USER:-adm_pg_user}""
      POSTGRES_PASSWORD: ""${MYPG_PASSWORD:-1234abcd}""
      POSTGRES_DB: ""${MYPG_DB:-fgag_db}""
      PGPASSWORD: ""${MYPG_PASSWORD:-1234abcd}""
      MYDB_SCHEMA: ""${MYDB_SCHEMA:-fgag_schema}""
    volumes:
      - ./docker-volume/pg:/var/lib/postgresql/data
      - ./db-sql-init.sh:/docker-entrypoint-initdb.d/db-sql-init.sh
    networks:
      - fgagnet
  # https://github.com/geoserver/docker/blob/master/docker-compose-demo.yml
  # geoserver default admin:geoserver credentials
  #
  # to connect to the running container: `docker exec -it fgag_geoserver_1 /bin/bash`
  #
  # to be able to use `psql`:
  # apt-get update && apt-get install postgresql-client
  #
  # docker network inspect fgag_fgagnet
  geoserver:
    image:  docker.osgeo.org/geoserver:2.23.0
    ports:
      - ""${MYGS_PORT:-7777}:8080""
    environment:
      INSTALL_EXTENSIONS: ""true""
      STABLE_EXTENSIONS: wps,csw
      EXTRA_JAVA_OPTS: -Xms1G -Xmx2G
    volumes:
      - ./docker-volume/gs/geoserver_data:/opt/geoserver_data/:Z
      - ./docker-volume/gs/additional_libs:/opt/additional_libs:Z # by mounting this we can install libs from host on startup
    networks:
      - fgagnet
networks:
  fgagnet:
    driver: bridge

",76395132,"Are you sure that this command connects to the same docker container ?
# this works
PGCONNECT_TIMEOUT=2 psql ""postgresql://adm_pg_user:1234abcd@localhost:6666/fgag_db"" --command ""SELECT NOW();""

Are you sure that you have correctly configured the container to expose port 6666 ?
export MYPG_PORT=6666

",
76390662,Can't use PHPickerViewController delegate with KMM,"I'm trying to use the UiKit API PHPickerViewController using KMM and Compose for iOS.
import androidx.compose.runtime.Composable
import androidx.compose.ui.interop.LocalUIViewController
import platform.PhotosUI.PHPickerConfiguration
import platform.PhotosUI.PHPickerViewController
import platform.PhotosUI.PHPickerViewControllerDelegateProtocol
import platform.darwin.NSObject

@Composable
actual fun pickerController() {
    val uiViewController = LocalUIViewController.current
    val configuration = PHPickerConfiguration()
    val pickerController = PHPickerViewController(configuration)
    val pickerDelegate = object : NSObject(), PHPickerViewControllerDelegateProtocol {
        override fun picker(picker: PHPickerViewController, didFinishPicking: List<*>) {
            println(""didFinishPicking: $didFinishPicking"")
            picker.dismissViewControllerAnimated(flag = false, completion = {})
            uiViewController.dismissModalViewControllerAnimated(false)
        }
    }

    pickerController.setDelegate(pickerDelegate)
    uiViewController.presentViewController(pickerController, animated = false, completion = null)
}

This displays the image picker:

Unfortunately, when clicking on Cancel, the delegate callback is not called, and I get the following message on the console:
[Picker] PHPickerViewControllerDelegate doesn't respond to picker:didFinishPicking:

Is it possible to implement the callback in Kotlin?
What am I missing?
",76395154,"Since pickerDelegate is NSObject, it's lifecycle follows ObjC rules, not KMM memory model.
So as soon as the execution leaves composable block, this objects gets released - as setDelegate takes it as weak reference.
You can fix it by storing it using remember.
Also using your function is dangerous because you're gonna call presentViewController on each recomposition - e.g. if some of your reactive data changes on the calling side.
You can update it to return an action that will present it, but store delegate and the action itself using remember:
@Composable
actual fun rememberOpenPickerAction(): () -> Unit {
    val uiViewController = LocalUIViewController.current
    val pickerDelegate = remember {
        object : NSObject(), PHPickerViewControllerDelegateProtocol {
            override fun picker(picker: PHPickerViewController, didFinishPicking: List<*>) {
                println(""didFinishPicking: $didFinishPicking"")
                picker.dismissViewControllerAnimated(flag = false, completion = {})
            }
        }
    }

    return remember {
        {
            val configuration = PHPickerConfiguration()
            val pickerController = PHPickerViewController(configuration)
            pickerController.setDelegate(pickerDelegate)
            uiViewController.presentViewController(pickerController, animated = true, completion = null)
        }
    }
}

Usage:
Button(onClick = rememberOpenPickerAction()) {

}

",
76388968,print all unmet dependencies in conda,"I'm trying to install a package via conda on an M1 mac. This package has a lot of dependencies, some of which seem to be un-satisfiable due to lack of pre-built packages in conda-forge.
I know I can trigger building of packages in conda-forge by issuing a PR like shown here, but I'd prefer sending one big PR with all the packages I need built, rather than trigger building of package A, trying to install, running into dependency B, trigger building of package B, ...
Can I somehow list all unmet dependencies of a conda package?
",76394534,"Trival case: directly missing package
First, let's note that this question only has a non-trivial answer when the package in question is noarch. A noarch designation means the package itself is already compatible with osx-arm64. But if it cannot be installed with a plain mamba install, then some non-noarch (compiled) dependency(s) must be missing.
Otherwise, if the package were not noarch and does not itself have osx-arm64 builds, then requesting migration for that package would trigger both the package and all its (recursive) dependencies to be made available for osx-arm64. (Conda Forge bot is smart like that!)
I'm not assuming OP has any confusion about this, but I want to get ahead of this situation for the future visitors. Now that that's out of the way let's address OP's question proper...
Available package, but missing dependencies
We can absolutely do this with Mamba's amazing subcommand repoquery. Let's find ourselves a concrete example!
Finding an example
To illustrate, I know that Conda Forge doesn't have r-terra building for osx-arm64 right now1. We can use the mamba repoquery whoneeds command to list every noarch package that needs r-terra:
## search `conda-forge` and only consider `noarch`
$ mamba repoquery whoneeds -c conda-forge -p noarch r-terra

## abridged output, only showing r-base=4.2 packages

 Name         Version Build         Depends          Channel
────────────────────────────────────────────────────────────────────────
 r-rasterdiv  0.2_5.2 r42hc72bb7e_1 r-terra          conda-forge/noarch
 r-rastervis  0.51.2  r42hc72bb7e_1 r-terra          conda-forge/noarch
 r-biomod2    4.2_2   r42hc72bb7e_0 r-terra >=1.6_33 conda-forge/noarch
 r-biomod2    4.2_3   r42hc72bb7e_0 r-terra >=1.6_33 conda-forge/noarch
 r-rasterdiv  0.3.1   r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-rastervis  0.51.4  r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-rastervis  0.51.5  r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-spatialeco 2.0_0   r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-rasterdiv  0.2_5.2 r42hc72bb7e_1 r-terra          conda-forge/noarch
 r-rastervis  0.51.2  r42hc72bb7e_1 r-terra          conda-forge/noarch
 r-biomod2    4.2_2   r42hc72bb7e_0 r-terra >=1.6_33 conda-forge/noarch
 r-biomod2    4.2_3   r42hc72bb7e_0 r-terra >=1.6_33 conda-forge/noarch
 r-rasterdiv  0.3.1   r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-rastervis  0.51.4  r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-rastervis  0.51.5  r42hc72bb7e_0 r-terra          conda-forge/noarch
 r-spatialeco 2.0_0   r42hc72bb7e_0 r-terra          conda-forge/noarch

So, all of these are theoretically compatible with osx-arm64, but they depend on the package r-terra that isn't available yet.
Let's use for our example, r-spatialeco.
Missing dependencies of r-spatialeco
Above we used the whoneeds subcommand for a reverse dependency search; now we'll use the depends command for (forward) dependency search:
$ mamba repoquery depends -c conda-forge -p osx-arm64 r-spatialeco

Executing the query r-spatialeco

conda-forge/osx-arm64                                       Using cache
conda-forge/noarch                                          Using cache


 Name                            Version Build         Channel
─────────────────────────────────────────────────────────────────────────────
 r-spatialeco                    2.0_0   r41hc72bb7e_0 conda-forge/noarch
 r-mass                          7.3_53  r40h4d528fc_0 conda-forge/osx-arm64
 r-cluster                       2.1.0   r40h09a9d6b_4 conda-forge/osx-arm64
 r-rcurl >>> NOT FOUND <<<
 r-readr                         2.0.2   r40h8ea1354_0 conda-forge/osx-arm64
 r-sf >>> NOT FOUND <<<
 r-mgcv                          1.8_33  r40hdd02fd4_0 conda-forge/osx-arm64
 r-rann                          2.6.1   r40h39468a4_2 conda-forge/osx-arm64
 r-envstats                      2.3.1   r351_1000     conda-forge/noarch
 r-yaimpute >>> NOT FOUND <<<
 r-spdep >>> NOT FOUND <<<
 r-rms >>> NOT FOUND <<<
 r-terra >>> NOT FOUND <<<
 r-ks                            1.14.0  r41h5d63f41_0 conda-forge/osx-arm64
 r-spatstat.explore              3.0_5   r41h5d63f41_0 conda-forge/osx-arm64
 r-base                          4.1.3   hc39b4fc_7    conda-forge/osx-arm64
 r-spatialpack >>> NOT FOUND <<<
 r-spatstat.geom                 3.2_1   r42h21dc0da_0 conda-forge/osx-arm64

And there you have it: the r-spatialeco package is missing seven packages that need to be migrated to osx-arm64, as indicated by the >>> NOT FOUND <<< string.

[1]: I know this because I've taken a stab at getting it migrated multiple times and have yet to succeed. :/
",
76394525,Count exact occurrences of number in array and return True or False,"I have two arrays
Arr1 = [1,1,1,2,2,2,3,3]  and Arr2 =[1,1,2,1]
Comparing both arrays should return True as there are same occurrences of no. 1.
However If Arr2 = [1,1,2] it should return false as the no. Of occurrences of 1 or 2 don't match with the no. Of occurrences of 1 and 2 in Arr1
Even Arr2 = [1,1,2,3,1] should return True.
Thanks in advance! Cheers
I tried this but doesn't work for other instances.
function allElementsPresent(first, second) {
  return second.every((element) => first.includes(element));
}

",76394579,"I believe I understand what you want to accomplish. You want to see if the number of occurrences in the second array matches the first. If that's the case, I've used this answer as a basis


function allElementsPresent(first, second, matchAll = false) {
  if (first.length > 0 && second.length === 0) return false;
  var counts1st = {};
  var counts2nd = {};

  for (var num of first) {
    counts1st[num] = counts1st[num] ? counts1st[num] + 1 : 1;
  }
  for (var num of second) {
    counts2nd[num] = counts2nd[num] ? counts2nd[num] + 1 : 1;
  }

  for (var count in counts2nd) {
    if (matchAll && (!counts1st[count] || counts1st[count] !== counts2nd[count])) return false;
    if (!matchAll && (counts1st[count] && counts1st[count] === counts2nd[count])) return true;
  }
  return matchAll ? true : false;
}



",
76391948,IdentityServer - how to get grant_type from within a Protected API?,"I have an API protected by IdentityServer with an associated allowed scope.
I have two Identity Server clients with permission to access that allowed scope - one accepts client_credentials (for machine-machine operations), and the other accepts authorization_code (for user-machine operations).
Within the API itself, how can I determine whether a given client has been authorised by client_credentials or by authorization_code?
I can find a few references to the ""gty"" claim but this is not included in tokens generated by identity server. Is there a way to force IdentityServer to include this claim, or is there some other convention for how to identify whether a request originated from a machine client, or from a user?
",76395205,"you can have different clientID and client definitions for the different use cases (Authorization code flow cs. client credentials flow).
Then in the client definition for each one, you can add Client Claims, that will be included in the access token and will be included for any user.
See https://docs.duendesoftware.com/identityserver/v6/reference/models/client/#token
There settings there are related to this:

Claims

Allows settings claims for the client (will be included in the access token).

AlwaysSendClientClaims

If set, the client claims will be sent for every flow. If not, only for client credentials flow (default is false)

AlwaysIncludeUserClaimsInIdToken

When requesting both an id token and access token, should the user claims always be added to the id token instead of requiring the client to use the userinfo endpoint. Default is false.

ClientClaimsPrefix

If set, the prefix client claim types will be prefixed with. Defaults to client_. The intent is to make sure they don’t accidentally collide with user claims.
",
76390032,React Native Component + Navigation with Paramameters not working error undefined is not a method,"I have Reactive Component which passes data from one screen to another..., trying to call second component via Navigate(name,params) method. but it gives an error saying ""undefined method is not a method"". Copying component code below. Guid me to clear the error.
import React from 'react';
import {
  StyleSheet,
  Text,
  View,
  Pressable,
} from 'react-native';

import { NavigationContainer } from '@react-navigation/native';

export default function ScreenA(navigation:any) {
    const onPressHandler = () => {
        navigation.navigate(""Screen_B"",{ ItemName: 'Item from ScreenA', ItemId: 12 });
        // navigation.navigate(""Screen_B"");
    }

",76394730,"Destructure navigation from function parameter:
import React from 'react';
import {
  StyleSheet,
  Text,
  View,
  Pressable,
} from 'react-native';

import { NavigationContainer } from '@react-navigation/native';

export default function ScreenA({navigation}) { // wrap navigation with curly brackets
    const onPressHandler = () => {
        navigation.navigate(""Screen_B"",{ ItemName: 'Item from ScreenA', ItemId: 12 });
    }

",
76388515,Why mocking HuggingFace datasets library does not work?,"I have a Python function that uses the HuggingFace datasets library to load a private dataset from HuggingFace Hub.
I want to write a unit test for that function, but it seems pytest-mock does not work for some reason. The real function keeps getting called, even if the mock structure should be correct.
This is the main function:
def load_data(token: str):
    dataset = load_dataset(""MYORG/MYDATASET"", use_auth_token=token, split=""train"")
    return dataset

And this is the test function I wrote:
def test_data(mocker):
    # Mocked data
    token_test = ""test_token""
    mocked_dataset = [
        {'image': [[0.5, 0.3], [0.7, 0.9]], 'timestamp': datetime.date(2023, 1, 1)},
    ]
    mocker.patch('datasets.load_dataset', return_value=mocked_dataset)

    result = load_data(token_test)

    assert len(result) == 1

Could it be that there are some ""unmockable"" libraries which do stuff under the hood and make their functions impossible to stub?
",76394624,"The official Python documentation has this part: where-to-patch.
If your module is called my_module, and it does from datasets import load_dataset then you should patch mocker.patch('my_module.load_dataset' so that your module is using the mock.
Patching datasets.load_dataset might be too late, since if the import in your module happened before that instruction, it has no effect.
",
76390333,Is there an escape code to pause the terminal screen refresh?,"I have an application that displays images using 8-bit ANSI-art. This is handy for viewing images on a remote machine via SSH. When the viewer starts, it replaces all the 8-bit palette values from 16-255 with my standard values. This runs fine on the Mac terminal, but the xterm display flickers because it is trying to refresh while it is still executing these 240 commands.
I get the same thing when the program exits, and I have to reset the colours.  Here is the code for resetting the palette when the program exits.
for (int N=16; N<256; ++N) printf(""\e]104;%d\a"", N);
I can see the colours changing in the terminal as it runs.
I have not found an escape code that resets the whole palette. Sending Ctrl-C resets some things but not the palette. All the examples I have found use a loop like this. It would be nice to find a reset escape sequence if there is one, but I will also need a way to pause the screen refresh until the commands have finished.
I tried running all the escape sequences into one big string and submitting all of them at once with puts(). That did not do the trick.
I hoped to find an escape sequence that pauses the screen refresh, and another that un-pauses it. It seems like something that ought to be there, and I am not seeing it. Or, if we know for sure that no such escape sequences exist, then I can stop looking. There may be other ways of fixing this other than escape codes. However, the application is handiest when working remotely, so I want it to work on whatever terminal is running at the time, not just xterm. I can ignore the flickering if I have to.
PS:
I have a workaround. Xterm supports 24-bit Truecolor. I can use that and not change the 8-bit palette.
",76394804,"Not what the title asks for, but enabling the alternative screen buffer is a fix.
Print ""\e[?1049h to enable when the program starts and and ""\e[?1049l to disable.
The Wikipedia entry says little more than that, so it is easy to ignore. Enabling the alternative screen buffer means the application works with a terminal-sized window of characters. This is probably what an interactive terminal program needs rather than add all the screen refreshes to the current scrolling terminal buffer. Disabling restores the terminal to its previous state.
",
76391329,How do you sort lists of tuples based on the count of a specific value?,"I am working on a NER problem—hence the BIO tagging—with a very small dataset, and I am manually splitting it into train, validation, and test data. Thus, to make the first of two splits, I need to sort lists of tuples into two lists based on the count of 'B' in data.
I am shuffling data, so the output varies, but it typically yeilds what I provide below. data can be split such that a total count of 10 instances of 'B' is possible in bin_1. So it's not that data won't split this way given the way B is distributed through the lists of tuples.
How do I get the split that I am after? For this example, and the desired split, I want the total count of 'B' in bin_1 to be 10, but it's always over.
Assistance would be much appreciated.
Data:
data = [[('a', 'B'), ('b', 'I'), ('c', 'O'), ('d', 'B'), ('e', 'I'), ('f', 'O')],
        [('g', 'O'), ('h', 'O')],
        [('i', 'B'), ('j', 'I'), ('k', 'O')],
        [('l', 'B'), ('m', ''), ('n', 'B'), ('o', 'O')],
        [('p', 'O'), ('q', 'O'), ('r', 'O')],
        [('s', 'B'), ('t', 'O')],
        [('u', 'O'), ('v', 'B'), ('w', 'I'), ('x', 'O'), ('y', 'O')],
        [('z', 'B')],
        [('a', 'B'), ('b', 'I'), ('c', 'O')],
        [('d', 'O')],
        [('e', 'O'), ('f', 'O')],
        [('g', 'O'), ('h', 'B')],
        [('i', 'B'), ('j', 'I')],
        [('k', 'O')],
        [('l', 'O'), ('m', 'O'), ('n', 'O'), ('o', 'O')],
        [('p', 'O'), ('q', 'O'), ('r', 'O'), ('s', 'B'), ('t', 'O')],
        [('u', 'O'), ('v', 'B'), ('w', 'I'), ('x', 'O'), ('y', 'O'), ('z', 'B')]]

Current code:
split = 0.7
d = []
total_B = 0
bin_1 = []
bin_2 = []
counter = 0

random.shuffle(data)

for f in data:
    cnt = {}
    for _, label in f:
        if label in cnt:
            cnt[label] += 1
        else:
            cnt[label] = 1
    d.append(cnt)

for f in d:
    total_B += f.get('B', 0)

for f,g in zip(d, data):
    if f.get('B') is not None:
        if counter <= round(total_B * split):
            counter += f.get('B')
            bin_1.append(g)
        else:
            bin_2.append(g)

print(round(total_B * split))
print(sum(1 for sublist in bin_1 for tuple_item in sublist if tuple_item[1] == 'B'))
print(sum(1 for sublist in bin_2 for tuple_item in sublist if tuple_item[1] == 'B'))

Current output:
Total count of 'B' in 'bin_1' should be: 10
Total count of 'B' in 'bin_1' is': 11
Total count of 'B' in 'bin_2' is': 3

bin_1, bin_2
>>>
[[('a', 'B'), ('b', 'I'), ('c', 'O')],
  [('g', 'O'), ('h', 'B')],
  [('i', 'B'), ('j', 'I'), ('k', 'O')],
  [('u', 'O'), ('v', 'B'), ('w', 'I'), ('x', 'O'), ('y', 'O'), ('z', 'B')],
  [('s', 'B'), ('t', 'O')],
  [('l', 'B'), ('m', ''), ('n', 'B'), ('o', 'O')],
  [('a', 'B'), ('b', 'I'), ('c', 'O'), ('d', 'B'), ('e', 'I'), ('f', 'O')],
  [('i', 'B'), ('j', 'I')]],
 [[('u', 'O'), ('v', 'B'), ('w', 'I'), ('x', 'O'), ('y', 'O')],
  [('z', 'B')],
  [('p', 'O'), ('q', 'O'), ('r', 'O'), ('s', 'B'), ('t', 'O')]]

Desired output:
Total count of 'B' in 'bin_1' should be: 10
Total count of 'B' in 'bin_1' is': 10
Total count of 'B' in 'bin_2' is': 4

",76395290,"One possible solution is to get the distribution of the 'B' among indexes of your data.
Let's say data was shuffled already, make use of:

dict com prehension: https://peps.python.org/pep-0274/
enumarate: https://python-reference.readthedocs.io/en/latest/docs/functions/enumerate.html

def get_distribution(data):
    return {i: len([x for x in t if (x[1] == 'B')]) for i, t in enumerate(data) }

For data you get:
distribution = get_distribution(data)
print(distribution)
#=> {0: 2, 1: 0, 2: 1, 3: 2, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 0, 10: 0, 11: 1, 12: 1, 13: 0, 14: 0, 15: 1, 16: 2}

Now, iterate over distribution and fill your bins. You can develop a more complex algorithm, this is the simplest:
bin_1 = []
bin_2 = []
ratio = 0.7
count = 0
total = sum(distribution.values())

for k, v in distribution.items():
    if count/total < ratio:
        bin_1.append(data[k])
        count += v
    else:
        bin_2.append(data[k])

So, check:
print(bin_1)
print(bin_2)
distr_bin_1 = get_distribution(bin_1)
distr_bin_2 = get_distribution(bin_2)
print(distr_bin_1)
print(distr_bin_2)
count_bin_1 = sum(distr_bin_1.values())
count_bin_2 = sum(distr_bin_2.values())
print(count_bin_1/(count_bin_1 + count_bin_2)) # actual ratio

",
76391771,How to add the data of this table in markers (pop-up) on map?,"I imported this table from Excel into Jupyter. I was using pandas for this. But now I want to mark markers with cities from table below on my map with  popups with the data from colognes A1,A2,A3 and I don't know how to do this.
Example: I press on a definite marker and after this the popup appears with this data from colognes A1,A2,A3.
Image 1:

Image 2:

Can you tell me the corresponding code for this operation?
",76395312,"To display the pop-up in tabular form at each location, a loop process is performed on each row of the data frame, converting one row from a series to a data frame, and then transposing it. I also adjust the width of the data frame.
import pandas as pd
import io
import folium

data = '''
Name A1 A2 A3 LAT LON
""Malibu Beach"" 0.63 0.55 0.95 34.03194 -118.698387
Commerce 0.17 0.45 0.25 34.00031 -118.159770
""Long Beach"" 0.19 0.21 0.09 33.77171 -118.181310
'''
df = pd.read_csv(io.StringIO(data), delim_whitespace=True)

import folium

m = folium.Map([df.LAT.mean(), df.LON.mean()], zoom_start=8)

for i in range(len(df)):
    html = df.loc[i,['Name','A1','A2','A3']].to_frame().T.to_html(
        classes=""table table-striped table-hover table-condensed table-responsive""
    )
    popup = folium.Popup(html, max_width=500)
    folium.Marker([df.iloc[i]['LAT'], df.iloc[i]['LON']], popup=popup).add_to(m)

m


",
76391309,ffmpeg - stretched pixel issue,"Context
I'm converting a PNG sequence into a video using FFMPEG. The images are semi-transparent portraits where the background has been removed digitally.
Issue
The edge pixels of the subject are stretched all the way to the frame border, creating a fully opaque video.
Cause Analysis
The process worked fine in the previous workflow using rembg from command line however, since I started using rembg via python script using alpha_matting to obtain higher quality results, the resulting video has these issues.
The issue is present in both webm format (target) and mp4 (used for testing).
Command Used
Command used for webm is:
ffmpeg -thread_queue_size 64 -framerate 30 -i <png sequence location> -c:v libvpx -b:v 0 -crf 18 -pix_fmt yuva420p -auto-alt-ref 0 -c:a libvorbis <png output>

Throubleshooting Steps Taken

PNG Visual inspection The PNG images have a fully transparent background as desired.
PNG Alpha Measurement I have created a couple of python scripts to look at alpha level in pixels and confirmed that there is no subtle alpha level in the background pixels
Exported MP4 with AE Using the native AE renderer the resulting MP4/H.265 has a black background, so not showing the stretched pixel issue

Image of the Issue

Sample PNG Image from sequence

Code Context
rembg call via API using alpha_matting seems to generate a premultiplied alpha which uses non black pixels for 0 alpha pixels.
remove(input_data, alpha_matting=True, alpha_matting_foreground_threshold=250,
                    alpha_matting_background_threshold=250, alpha_matting_erode_size=12)

A test using a rough RGB reset of 0-alpha pixels confirms that the images are being played with their RGB value ignoring Alpha.
def reset_alpha_pixels(img):
    # Open the image file
    # Process each pixel
    data = list(img.getdata())
    new_data = []
    for item in data:
        if item[3] == 0:
            new_data.append((0, 0, 0, 0))
        else:
            new_data.append((item[0], item[1], item[2], item[3]))
        # Replace the alpha value but keep the RGB
        

    # Update the image data
    img.putdata(new_data)

    return img

Updates

Added python context to make the question more relevant within SO scope.

",76395320,"The issue is related to the video player.
Most video players doesn't support transparency, and ignores the alpha (transparency) channel.
The video player displays the rgb content of the background even if the background is supposed to be hidden (background pixels are fully according to their alpha value).
Apparently, rembg output background is not filled with solid black or white, but having the stretched effect.
When opening the PNG image, and when video in Chrome browser for example, the background is transparent (RGB values are hidden), and we can't see the ""stretched effect"".

Solving the issue using FFMPEG is challenging.
We better fix the issue in the Python code after applying rembg.
For fixing the issue, me may select a solid background color like (200, 200, 200) gray background, and apply alpha compositing between RGB channels and the background.

Extract RGB channels:

    foreground_rgb = image_after_rembg[:, :, 0:3]  # Extract RGB channels.


Extract alpha (transparency) channel and convert from range [0, 255] to [0, 1]:

    alpha = image_after_rembg[:, :, 3].astype(np.float32) / 255  # Extract alpha (transparency) channel and convert from range [0, 255] to [0, 1].
    alpha = alpha[..., np.newaxis]  # Add axis - new alpha shape is (1024, 1024, 1). We need it for scaling 3D rgb by 2D alpha channel.


Set background RGB color to light gray color (for example):

    background_rgb = np.full_like(foreground_rgb, (200, 200, 200))  # Set background RGB color to light gray color (for example).


Apply ""alpha compositing"" of rgb and background_rgb:

    composed_rgb = foreground_rgb.astype(np.float32) * alpha + background_rgb.astype(np.float32) * (1 -alpha)
    composed_rgb = composed_rgb.round().astype(np.uint8)  # Convert to uint8 with rounding.


Add the original alpha channel to composed_rgb:

    composed_rgba = np.dstack((composed_rgb, alpha_ch))


Complete Python code sample:
from PIL import Image
import numpy as np
#from rembg import remove

#image_file_before_rembg = 'input.png'
image_file_after_rembg = 'frame-00001.png'

# Assume code for removing background looks as follows:
#image_before_rembg = Image.open(image_file_before_rembg)
#image_after_rembg = remove(image_before_rembg)
#image_after_rembg.save(image_file_after_rembg)

image_after_rembg = Image.open(image_file_after_rembg)  # Skip background removing, and read the result from a file.
image_after_rembg = np.array(image_after_rembg)  # Convert PIL to NumPy array.

foreground_rgb = image_after_rembg[:, :, 0:3]  # Extract RGB channels.
alpha_ch = image_after_rembg[:, :, 3]  # Extract alpha (transparency) channel
alpha = alpha_ch.astype(np.float32) / 255  # Convert alpha from range [0, 255] to [0, 1].
alpha = alpha[..., np.newaxis]  # Add axis - new alpha shape is (1024, 1024, 1). We need it for scaling 3D rgb by 2D alpha channel.

background_rgb = np.full_like(foreground_rgb, (200, 200, 200))  # Set background RGB color to light gray color (for example).

# Apply ""alpha compositing"" of rgb and background_rgb
composed_rgb = foreground_rgb.astype(np.float32) * alpha + background_rgb.astype(np.float32) * (1 -alpha)

composed_rgb = composed_rgb.round().astype(np.uint8)  # Convert to uint8 with rounding.

composed_rgba = np.dstack((composed_rgb, alpha_ch))  # Add the original alpha channel to composed_rgb


Image.fromarray(composed_rgba).save('new_frame-00001.png')  # Save the RGBA output image to PNG file


Executing FFmpeg:
ffmpeg -y -framerate 30 -loop 1 -t 5 -i new_frame-00001.png -vf ""format=rgba"" -c:v libvpx -crf 18 -pix_fmt yuva420p -auto-alt-ref 0 out.webm


When playing with Chrome browser, the background is transparent.
When playing with VLC Player, the background is light gray:


Using FFmpeg CLI, we have to use alphaextract, overlay and alphamerge filters.
Example (5 seconds at 3fps for testing):
ffmpeg -y -framerate 3 -loop 1 -i frame-00001.png -filter_complex ""color=white:r=3[bg];[0:v]format=rgba,split=2[va][vb];[vb]alphaextract[alpha];[bg][va]scale2ref[bg0][v0];[bg0][v0]overlay=shortest=1,format=rgb24[rgb];[rgb][alpha]alphamerge"" -c:v libvpx -crf 18 -pix_fmt yuva420p -auto-alt-ref 0 -t 5 out.webm
",
76389179,find values on a user form from text box and results shown on a List box,"i have an excel sheet the has column A which the search value will be in , and it should retrieve results from column B,the code should whenever I enter a value in textbox (txtreg) get results in Listbox (txtledglist) which might be 1 result or more up to 6 .
the code that I have is this: whenever I type the search value that has just 1 result brings it fine, but when it has multiple results it gets it but takes over 5 min and somtimes excel crashes which is really unsuall. or when I delete the search value to try and enter a new one it also craches , when I check the VBA I see that the code keeps running which is causing the excel to crash.
any ideas to make the code simpler or what am I doing wrong?
thanks.
Private Sub txtreg_Change()


    Dim wb As Workbook
    Dim ws As Worksheet
    Dim lookupValue As String
    Dim results() As Variant
    Dim rng As Range
    Dim cell As Range
    Dim index As Long
    Dim count As Long
    

    Set wb = ThisWorkbook
    Set ws = wb.Sheets(""L 403"")
    

    lookupValue = txtreg.Value
    
    txtledglist.Clear
    
    Set rng = ws.Range(""A:B"")
    On Error Resume Next
    Set cell = rng.Columns(1).Find(What:=lookupValue, LookIn:=xlValues, LookAt:=xlWhole)
    On Error GoTo 0
    
    If Not cell Is Nothing Then
        count = 0
        Do
            count = count + 1
            ' Find the next match
            Set cell = rng.Columns(1).FindNext(cell)
        Loop While Not cell Is Nothing And cell.Address <> rng.Columns(1).Find(What:=lookupValue, After:=cell, LookIn:=xlValues, LookAt:=xlWhole).Address
        
        ReDim results(1 To count)
        
        Set cell = rng.Columns(1).Find(What:=lookupValue, LookIn:=xlValues, LookAt:=xlWhole)
        
        index = 1
        Do
            results(index) = rng.Columns(2).Cells(cell.Row - rng.Cells(1).Row + 1).Value ' Adjusting for header row
            index = index + 1
            Set cell = rng.Columns(1).FindNext(cell)
        Loop While Not cell Is Nothing And cell.Address <> rng.Columns(1).Find(What:=lookupValue, After:=cell, LookIn:=xlValues, LookAt:=xlWhole).Address
        
        txtledglist.List = results
       
    End If

End Sub

",76395808,"First of all, you should save the first cell you found, so that you don't have to call .Find again, which would restart the search. Also, you don't need to compare against Nothing in the loop condition.
Second, I don't clearly understand the line with the comment ""Adjusting for header row"". Anyway, there is no adjustment necessary: the two cells are in the same row.
The corrected code:
Private Sub txtreg_Change()
    Dim lookupValue As String: lookupValue = txtreg.Value
    Dim wb As Workbook: Set wb = ThisWorkbook
    Dim ws As Worksheet: Set ws = wb.Sheets(""L 403"")
    
    txtledglist.Clear
    
    Dim rng As Range: Set rng = ws.Range(""A:B"")
    On Error Resume Next
    Dim firstCell As Range: Set firstCell = rng.Columns(1).Find(What:=lookupValue, LookIn:=xlValues, LookAt:=xlWhole)
    On Error GoTo 0
    
    If Not firstCell Is Nothing Then
        Dim cell As Range: Set cell = firstCell
        Dim count As Long: count = 0
        Do
            count = count + 1
            Set cell = rng.Columns(1).FindNext(cell)
        Loop While cell.Address <> firstCell.Address
        
        Dim results() As Variant: ReDim results(1 To count)
        
        Set cell = rng.Columns(1).Find(What:=lookupValue, LookIn:=xlValues, LookAt:=xlWhole)
        
        Dim index As Long: index = 1
        Do
            results(index) = rng.Cells(cell.Row, 2).Value
            index = index + 1
            Set cell = rng.Columns(1).FindNext(cell)
        Loop While cell.Address <> firstCell.Address
        
        txtledglist.List = results
    End If
End Sub

I don't exactly know your whole application but may be it is worth avoiding .Find and .FindNext.
",
76391148,How to avoid links shifting on hover while increasing font size in a navigation bar?,"i want to create a navigation bar for my website that includes some links and a company logo upon it. The links should have customize spacing in them and means some at first and some at last of right edge . i also want to include an transformation that when the links are hovered the font size of the links increases without affecting or shifting the nearby links or content. i am having a problem as the font increases the neighbours links shifts themselves to maintain margin gap . What should i change or add in my code to do so ?
i basically tried to use hover subclass of link 'a'. it worked but not perfectly it shift the neighbour links . i am using margin left and right for each link and assigning unique margin for each. when i hover on a link each of the other links shifts themselves to maintain margin respectivley . i think is it good to use margin property in such cases or i should use float if yes then how to align them at particular distances. i am providing an online editor link of my code at end .
Or here is some part of my css code


body {
    font-family: Arial, sans-serif;
    line-height: 1.5;
    background-color: #f7f7f7;
    margin: 0;
}

header {
    background-color: black;
}

.navigation-bar {
    width: 100%;
    height: 76px;
    display: flex;
    padding: 10px;
    align-items: center;
}

.logo img {
    position: relative;
    left: 20%;
    height: 80px;
    width: auto;
}

.navigation-links {
    display: flex;
    list-style: none;
    margin: 0;
    padding: 0;
}

.navigation-links li {
    display: inline-block;
    margin-left: 90px;
    margin-right: 0px;
}

.navigation-links li a {
    font-size: 25px;
    color: white;
    text-decoration: none;
    text-align: center;
}

.navigation-links li a:hover {
    font-size: 30px;
}

/* join now class    */
.navigation-links li a.special1 {
    font-size: 25px;
    font-weight: bold;
    color: white;
    margin-left: 60px;
    margin-right: 0px;
    text-decoration: none;
    border: 2px solid rgb(226, 19, 54);
    border-radius: 50px;
    padding: 15px 10px;
    background-color: rgb(226, 19, 54);
}

/* login class */
.navigation-links li a.special2 {
    font-size: 25px;
    font-weight: bold;
    color: white;
    margin-left: 2px;
    margin-right: 0px;
    text-decoration: none;
    border: 2px solid white;
    border-radius: 50px;
    padding: 15px 40px;
    background-color: black;
}

/*resposive nature*/
@media screen and (max-width: 768px) {
    .navigation-links {
        display: none;
    }
}
<!DOCTYPE html>
<html lang=""en"">
    <link rel=""stylesheet"" href=""https://fonts.googleapis.com/css?family=Lato"">

    <head>
      <title>Navigation Bar Example</title>
      <link rel=""stylesheet"" type=""text/css"" href=""styles2.css"">
    </head>
    <body>
        <header>
            <div>
                <nav class=""navigation-bar"">
                    <div class=""logo"">
                        <img src=""rocket-g9cbacc798_1280.png"" alt=""Company Logo"" > 
                    </div>
                    <ul class=""navigation-links"">
                        <li><a href=""#"">Home</a></li>
                        <li><a href=""#"">Projects</a></li>
                        <li><a href=""#"">Services</a></li>
                        <li><a href=""#"">About</a></li>
                        <li ><a class=""special1""href=""#"">JOIN NOW</a></li>
                        <li ><a class=""special2"" href=""#"">LOG IN</a></li>
                    </ul>

                </nav>
            </div>
        </header>
        <!-- Rest of the content -->
    </body>
</html>



code link - https://codepen.io/Divyansh-Sharma-the-flexboxer/pen/JjmgJVy
",76395439,"In the example below, there many ascetic changes which are optional. The following CSS is required:
li {
  /* Start vertically and horizontally center of `<li>` when transforming */
  transform-origin: center center;
  /* Original state is at normal size */
  transform: scale(1.0); 
  /* When state changes, stretch the duration by 0.7 seconds in a easing pattern */ 
  transition: 0.7s ease /* ✥ */;
}

/* When the user hovers over a <li>... */
li:hover {
  /* ...take it out of the normal ""flow"" of the document... */
  position: relative;
  /* ...give it a higher position on the z-axis... */
  z-index: 1;
  /* ...increase it's size by 20% */
  transform: scale(1.2) /* ✥ */;
}
/* ✥ Values can vary according to preference */ 

Initially each <li> is inert but has instructions when it is hovered over. When a <li> is hovered over it is out of the normal ""flow"" of the document and it's size will not interfere with any ""static"" elements (any element that doesn't have position: relative/absolute/fixed/sticky).
Note: Please review the example in Full Page mode, viewing in the iframe doesn't render perfectly (links are too small).


:root {
  margin: 0;
  font: 5vmin/1.15 Lato;
}

body {
  min-height: 100vh;
  margin: 0;
  background-color: #f7f7f7;
}

header {
  background-color: black;
}

nav {
  display: flex;
  align-items: center;
  width: 100%;
  height: clamp(3ex 80px 10vh);
}

nav img {
  display: inline-block;
  width: 20vw;
  height: auto;
  margin-right: 1rem;
}

menu {
  display: flex;
  flex-flow: row nowrap;
  align-items: center;
  list-style: none;
  margin: 0;
  padding: 0;
}

menu li {
  margin-right: 1.5rem;
  text-align: center;
  transform-origin: center center;
  transform: scale(1.0);
  transition: 0.7s ease;
}

menu li a {
  font-size: clamp(5rem 8vw 10rem);
  color: white;
  text-decoration: none;
}

menu li:hover {
  position: relative;
  z-index: 1;
  transform: scale(1.2);
}

.btn {
  min-width: 3rem;
  margin-right: 0.75rem;
  padding: 0.25rem 0.5rem;
  border: 2px solid rgb(226, 19, 54);
  border-radius: 50px;
  font-weight: bold;
  font-variant: small-caps;
}

.join {
  background-color: rgb(226, 19, 54);
}

.login {
  background-color: black;
}

@media screen and (max-width: 300px) {
  menu {
    display: none;
  }
<!DOCTYPE html>
<html lang=""en"">

<head>
  <title>Navigation Bar Example</title>
  <link href=""https://fonts.googleapis.com/css?family=Lato"" rel=""stylesheet"">
</head>

<body>
  <header>
    <nav>
      <img src=""https://www.clipartmax.com/png/middle/31-316935_universe-rocket-icon-svg.png
"" alt=""Company Logo"">
      <menu>
        <li><a href=""#"">Home</a></li>
        <li><a href=""#"">Projects</a></li>
        <li><a href=""#"">Services</a></li>
        <li><a href=""#"">About</a></li>
        <li class=""btn join""><a href=""#"">Join Now</a></li>
        <li class=""btn login""><a href=""#"">Log In</a></li>
      </menu>
    </nav>
  </header>
  <!-- Rest of the content -->
</body>

</html>



",
76394497,RelativeLayout problem: how to set RecyclerView not to overlap a Button on bottom,"This is my activity: SearchView on top, RecyclerView on middle, and a Button at bottom
<?xml version=""1.0"" encoding=""utf-8""?>
<RelativeLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:background=""@drawable/app_background""
    tools:context="".ui.home.customer.CustomerListActivity"">

    <androidx.appcompat.widget.SearchView
        android:background=""@color/white""
        android:layout_marginLeft=""20dp""
        android:layout_marginRight=""20dp""
        android:layout_marginTop=""20dp""
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""
        android:id=""@+id/searchCustomer"" />

    <androidx.recyclerview.widget.RecyclerView
        android:layout_marginLeft=""20dp""
        android:clipToPadding=""false""
        android:layout_marginRight=""20dp""
        android:layout_marginTop=""20dp""
        android:layout_width=""match_parent""
        android:layout_height=""wrap_content""
        android:layout_below=""@id/searchCustomer""
        tools:listitem=""@layout/row_item_customer""
        android:id=""@+id/rvCustomers"" />

    <com.google.android.material.button.MaterialButton
        android:id=""@+id/btnAddCustomer""
        android:layout_marginLeft=""20dp""
        android:layout_marginRight=""20dp""
        android:layout_alignParentBottom=""true""
        android:layout_marginBottom=""20dp""
        android:textColor=""@color/text_blue_1""
        app:backgroundTint=""@color/white""
        android:layout_width=""match_parent""
        android:layout_height=""53dp""
        android:text=""Add Customer"" />

</RelativeLayout>


Turns out it doesn't work as I expected, because the RecyclerView overlaps/crosses the Button. I want the RecyclerView to ""stay"" on top of the Botton. How to fix this?
",76394646,"Just add following line in recyclerView
android:layout_above=""@id/btnAddCustomer""

",
76395850,Writing a large collection of lists to a txt file in Python,"I am trying to save links to photos in a topic on an internet forum in a txt file. I tried many ways, but the links of one page are saved in a txt file, and when the loop goes to the next page of the topic, the previous links are deleted and new links are replaced! I want to have all the links together.
This is my code:
from bs4 import BeautifulSoup
import requests

def list_image_links(url):
    response = requests.get(url)

    soup = BeautifulSoup(response.content, ""html.parser"")
    
    # Separation of download links
    image_links = []
    for link in soup.find_all('a'):
        href = link.get('href')
        if href is not None and 'attach' in href and href.endswith('image')==False:
            image_links.append(href)
    
    # Writing links in a txt file
    with open('my_file.txt', 'w') as my_file:
        my_file.write('image links:' + '\n')    
        for branch in image_links:
            my_file.write(branch + '\n')
            print('File created')
    
# Browse through different pages of the topic
i = 0
while i <= 5175:
    list_image_links(f'https://forum.ubuntu.ir/index.php?topic=211.{i}')
    i = i+15

It is clear from the comments what each section does.
Thank you in advance for your help.
",76395863,"You need to append to the file. This can be achieved by using 'a' instead of 'w' as an argument to open().
When using 'w' a file will be created if it does not exist and it will always truncate the file first, meaning it will overwrite its contents. With 'a' on the other hand the file will also be created if it does not yet exists, but it won't truncate but instead append to the end of the file if it already exists, meaning the content will not be overridden.
See Python docs.
So for your example the line
with open('my_file.txt', 'w') as my_file:

would need to be changed to:
with open('my_file.txt', 'a') as my_file:

",
76394508,How to transition text along with a circle in CSS animation?,"The code I am developing is for mobile site and the animation I expect is as following: Firstly a bigger circle appears such that it covers the whole screen looking like a splash screen and afterwards the bigger circle will transition into smaller circle towards bottom left. Along with that image inside the bigger circle will transit towards bottom left.
The Problem is circle is transitioning properly but the text is not going to bottom left properly it kind of goes to left and then goes down. Below is the code that I tried.


setTimeout(function() {
  let i = document.getElementById(""test"");
  let d = document.getElementById(""icon-img"");

  i.classList.add(""active"");
  d.classList.add(""active"");
}, 2000);
.test {
  position: fixed;
  left: 0;
  bottom: 0;
  width: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  height: 40px;
  transition: all 3s ease;
  background: gray;
  transform: scale(100);
  border-radius: 30px;
  left: 20px;
  bottom: 20px;
}

.test.active {
  transform: scale(1);
  transition: all 2s ease;
  left: 20px;
  bottom: 20px;
}

.wrapper {
  position: relative;
  display: flex;
  align-items: center;
  justify-content: center;
  height: 100vh;
}

.myclass {
  width: 100%;
  height: 100%;
  display: flex;
  align-items: center;
  justify-content: center;
}

.before {
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 2s ease-in-out;
  width: 50%;
  height: 50%;
  position: fixed;
  font-size: 50px;
}

.before.active {
  left: 20px;
  bottom: 20px;
  width: 40px;
  height: 40px;
  font-size: 15px;
  position: fixed;
  transform: translate(0, 0);
}
<div class=""wrapper"">
  <div id=""test"" class=""test""></div>
  <div class=""myclass"">
    <img src=""./logo.svg"" id=""icon-img"" class=""before""></img>
  </div>
</div>



",76394648,"A problem is that some properties do not have an initial value set so there is nothing for them to transition from. So you get a sort of jump effect.
This snippet removes the flex used for centering the image and instead uses left and bottom in conjunction with translation to center it initially.


<!DOCTYPE html>
<html>

<head>
  <meta charset=""utf-8"" />
  <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
  <title></title>
  <style type=""text/css"">
    .test {
      position: fixed;
      left: 0;
      bottom: 0;
      width: 40px;
      display: flex;
      align-items: center;
      justify-content: center;
      height: 40px;
      transition: all 3s ease;
      background: gray;
      transform: scale(100);
      border-radius: 30px;
      left: 20px;
      bottom: 20px;
    }
    
    .test.active {
      transform: scale(1);
      transition: all 2s ease;
      left: 20px;
      bottom: 20px;
    }
    
    .wrapper {
      position: relative;
      display: flex;
      align-items: center;
      justify-content: center;
      height: 100vh;
    }
    
    .myclass {
      width: 100%;
      height: 100%;
    }
    
    .before {
      display: flex;
      align-items: center;
      justify-content: center;
      transition: all 2s ease-in-out;
      width: 50%;
      height: 50%;
      position: fixed;
      font-size: 50px;
      left: 50%;
      bottom: 50%;
      transform: translate(-50%, 50%);
      background: pink;
    }
    
    .before.active {
      left: 20px;
      bottom: 20px;
      width: 40px;
      height: 40px;
      font-size: 15px;
      position: fixed;
      transform: translate(0, 0);
    }
  </style>
</head>

<body>
  <div class=""wrapper"">
    <div id=""test"" class=""test""></div>
    <div class=""myclass"">
      <img src=""./logo.svg"" id=""icon-img"" class=""before""></img>
    </div>
  </div>
  <script>
    setTimeout(function() {
      let i = document.getElementById(""test"");
      let d = document.getElementById(""icon-img"");

      i.classList.add(""active"");
      d.classList.add(""active"");
    }, 2000);
  </script>
</body>

</html>



Note: the image is given a background of pink as no actual image was supplied, just so we can see its position and size.
Two transition times are used in the question - 2s and 3s. This means the image and text arrive at different times. I've kept that in the snippet but maybe the same time was intended?
",
76383756,change database based on request origin using expressjs & prisma,"I want to change database dynamically based on request origin.
I create a globalMiddleware which is called on every routes.


// middlwares/global.middleware.js

import DBController from ""../controllers/db.controller.js"";
import { db1, db2 } from ""../prisma/prismaClient.js"";
export default (req, res, next) => {
  const dbcontroller = DBController();
  const domain = req.get(""origin"");
  switch (domain) {
    case ""http://localhost:3000"":
      dbcontroller.setDB(db1);
      break;
    case ""http://localhost:3001"":
      dbcontroller.setDB(db2);
      break;
  }
  next();
};



but when i set the db inside DBController by calling dbcontroller.setDB() method and  finally calling this.DB it is undefined.


// controller/db.controller.js

import autoBind from ""auto-bind"";
class DBController {
  constructor() {
    this.DB;
    autoBind(this);
  }
  setDB(prismaClient) {
    this.DB = prismaClient;
  }
}
export default DBController;





// conrtoller/controller.js

import { generateResponse } from ""./../util/public.util.js"";
import DBController from ""./db.controller.js"";
import autoBind from ""auto-bind"";
import createError from ""http-errors"";
class Controller extends DBController {
  constructor() {
    super();
    this.generateResponse = generateResponse;
    this.createError = createError;
    autoBind(this);
  }
}
export default Controller;





// controller/article.controller.js

import Controller from ""./controller.js"";
class ArticleController extends Controller {
  async get(req, res, next) {
    try {
      const articles = await this.DB.article.findMany(); //this.DB is undefined
      const response = this.generateResponse(""success"", articles);
      res.send(response);
    } catch (error) {
      next(error);
    }
  }
}
export default new ArticleController();



I don't know how should i set a global DB inside a top-level controller which can be used every where.
I also try js global.db vars and express app.set(""db"",db1) but i think these are not a good solution for this work.
",76395526,"finally I modify global.middleware.js file and modify request instead of setting database in a high-level controller :


import {
  prisma_aramgostar,
  prisma_karen
} from ""../prisma/prismaClient.js"";
export default async(req, res, next) => {
  const domain = await req.get(""x-forwarded-host"");
  switch (domain) {
    case ""localhost:3000"":
      req.DB = prisma_aramgostar;
      console.log(""db: aramgostar"");
      break;
    case ""127.0.0.1:3001"":
      req.DB = prisma_karen;
      console.log(""db: karen"");
      break;
  }
  next();
};



",
76388569,Codeigniter 4 unique name validation,"My problem is that I have a modal and the input contains the value from the database. When I press the update button, the unique name validation check if the name already exists in the database. However, if I don't change it, it also checks the unique name validation, which I don't want to happen.
Here is the form.
<form action=""<?= base_url('users/createRole'); ?> "" method=""post"">
<div class=""modal-body"">
    <input type=""hidden"" name=""roleID"" id=""roleID"">
    <div class=""mb-3"">
        <label for=""inputRoleName"" class=""form-label"">Rolle hinzufügen</label>
        <input type=""text"" class=""form-control"" id=""inputRoleName"" name=""inputRoleName"" placeholder=""Role Name"">
    </div>
</div>
<div class=""modal-footer"">
    <button type=""submit"" class=""btn btn-primary"">Rolle speichern</button>
    <button type=""button"" class=""btn btn-light"" data-bs-dismiss=""modal"">Schließen</button>
</div>
</form>

        $("".btnEditRole"").click(function() {
            const roleID = $(this).data('id');
            const inputRoleName = $(this).data('role');
            $('#modalTitle').html('Update Data Role');
            $('.modal-footer button[type=submit]').html('Update rolle');
            $('.modal-content form').attr('action', '<?= base_url('users/updateRole') ?>');
            $('#roleID').val(roleID);
            $('#inputRoleName').val(inputRoleName);
            $('.modal').on('hidden.bs.modal', function() {
                location.reload(); // Refresh the page when modal is closed
            });
        });

Here is the Function.
public function updateRole()
{
if (!$this->validate(['inputRoleName' => ['rules' => 'is_unique[user_role.role_name]']])) {
    session()->setFlashdata('notif_error', '<b>Das Hinzufügen eines neuen Benutzers ist fehlgeschlagen</b> Der Benutzer existiert bereits! ');
    return redirect()->to(base_url('users'));
}
$updateRole = $this->userModel->updateRole($this->request->getPost(null, FILTER_UNSAFE_RAW));
if ($updateRole) {
    session()->setFlashdata('notif_success', '<b>Benutzerdaten erfolgreich aktualisieren</b> ');
    return redirect()->to(base_url('users'));
} else {
    session()->setFlashdata('notif_error', '<b>Benutzerdaten konnten nicht aktualisiert werden</b> ');
    return redirect()->to(base_url('users'));
}
}

",76394654,"When using unique values, you check them in this way:
(this assumes your AutoIncrement/Primary is roleID
For updating values
change this:
['rules' => 'is_unique[user_role.role_name]']])) {

to this:
['rules' => 'is_unique[user_role.role_name,roleID,{roleID}]])) {

More information on placeholders can be found here:
https://codeigniter.com/user_guide/libraries/validation.html?highlight=is_unique#validation-placeholders
",
76395784,Reversing array in C with pointers,"Trying to reverse the array. And then print it
I am trying to reverse the array by indenting from last to first element of array and copy it to another array. arra will be the reversed array :
Here's my code :
#include <stdio.h>
#include <stdlib.h>

int main()
{
    int num, *arr, i;
    scanf(""%d"", &num);
    arr = (int *)malloc(num * sizeof(int));
    for (i = 0; i < num; i++) {
        scanf(""%d"", arr + i);
    }
    
    int arra[] = {};
    /* Write the logic to reverse the array. */
    arra[0] = *(arr + num);
    for (int k = 1; k == num; k++) {
        int j = 1;
        arra[j] = *(arr + num -k);
        j++;
    }

    for (i = 0; i < num; i++)
        printf(""%d "", *(arra + i));
    return 0;
}

",76395864,"For starters this declaration
int arra[] = {};

is invalid in C and C++.
You should define the array also allocating dynamically memory for it as for the first array or in the worst case you could define a variable length array (provided that the compiler supports VLAs) like
int arra[num];

This for loop
for(int k = 1; k == num; k++){

will iterate only one time when num initially is set to 1.
Also within the body of the loop the variable j is created in each iteration of the loop (provided that the condition of the loop will be updated correctly) with the value 1
    int j = 1;
    arra[j] = *(arr + num -k);
    j++;

Thus this statement
    arra[j] = *(arr + num -k);

in each iteration of the loop is equivalent to
    arra[1] = *(arr + num -k);

that does not make sense.
To copy one array into another array using pointers the for loop can look for example the following way
for ( int *src = arr, *dsn = arra + n; src != arr + n; ++src )
{
    *--dsn = *src;
}

",
76391514,Generate all possible combinations basing on binary string and under some conditions,"I have to write algorithm that will generate all possible combinations of different binary strings but under some conditions. The combinations are created by:
Replacing binary ""1"" with ""00""
Other conditions:

Input binary string, if contains 0, they are in pairs always, so ""00""
The output also can contain 0 only in pairs

Example:
Input:
11

Output:
001
100
0000
11

In above example, there is no 010, because as mentioned earlier, the ""0"" must have a pair (another ""0"")
Note that if given binary string contains ""00"", we don't change them to 1.
In other words, the algorithm should determine how many different binary strings can be created by replacing ""1"" with ""00"" (but under the conditions present above), for given binary string and returns all the possible combinations.
I tried O(n^2) algorithm, recursion but can't achieve my goal :/
That's my code:
void get_combinations(const std::string& bin, std::set<std::string>& result) {
    result.insert(bin);
    for (int i = 0; i < bin.length(); i++) {
        std::string local_combination = bin;
        for (int j = i; j < bin.length(); j++) {
            if (local_combination[j] == '1') {
                local_combination[j] = '0';
                local_combination.insert(j, ""0"");

                result.insert(local_combination);
            }
        }
    }
}

It works e.g. for simple input 10, 01. But for input 11, the output doesn't contain 0000. For ""longer"" inputs, like 1111 it gives completely bad output.
",76395561,"Fundamentally your combinations are built up like a tree. The units are either
    (0)               (1)
    / \       or      / \
   0                 1   00

where (.) signifies what was in the original binary string and the strings at the bottom are what you would add as a result.
So, like any binary search tree you can either do the equivalent of BFS (breadth-first-search): deal with all the possibilities at one level before moving to the next, or DFS (depth-first-search): recursively work down each branch to the bottom to insert a new combination string.
The two approaches are illustrated for your problem in the code below.
#include <iostream>
#include <string>
#include <set>
using namespace std;

//======================================================================

set<string> BFS( const string &str )
{
   set<string> result;
   if ( str.length() == 0 ) return result;

   result.insert( str.substr( 0, 1 ) );   if ( str[0] == '1' ) result.insert( ""00"" );
   for ( int i = 1; i < str.length(); i++ )
   {
      auto last = result;   result.clear();
      for ( const string &s : last )
      {
         result.insert( s + str[i] );
         if ( str[i] == '1' ) result.insert( s + ""00"" );
      }
   }
   return result;
}

//======================================================================

void DFS( const string &left, const string &right, set<string> &result )
{
   if ( right.length() == 0 )
   {
      result.insert( left );
   }
   else
   {
      DFS( left + right[0], right.substr( 1 ), result );
      if ( right[0] == '1' ) DFS( left + ""00"", right.substr( 1 ), result );
   }
}

//======================================================================

int main()
{
   string str;
   cout << ""Enter a binary string: "";   cin >> str;

   cout << ""BFS:\n"";
   for ( const string &s : BFS( str ) ) cout << s << '\n';

   cout << ""\nDFS:\n"";
   set<string> result;
   DFS( """", str, result );
   for ( string s : result ) cout << s << '\n';
}

Output for 1111
BFS:
00000000
0000001
0000100
000011
0010000
001001
001100
00111
1000000
100001
100100
10011
110000
11001
11100
1111

DFS:
00000000
0000001
0000100
000011
0010000
001001
001100
00111
1000000
100001
100100
10011
110000
11001
11100
1111

",
76394580,Duplicate Buttons in xml file using android studio java,"Duplicate buttons are in my xml file which is fragment_employee2
I'm only creating one xml file to list out all the list in one xml
Here is the code in my https://gist.github.com/Umen14/8d1a205f016fa970369d37f37d4bf15d
And here are the images:-
enter image description here
I tried by removing here and there but the code seems to be the same but there is a problem in the onclick listener in my EmployeeFragment.java
enter image description here
",76394668,"This is showing duplicate because you are using same xml in recyclerView item   as well as activity layout. which R.layout.fragment_employee2 in your case. You need to define different layout in EmployeeAdapter without button. This will resolve you issue.
",
76389676,Cannot connect to WebSocket controller in .Net Core Blazor WASM application,"I am trying to get a WebSocket connection going in my WASM application.
I have followed the MSDN tutorial and enabled WebSockets in my Program.cs:
app.UseWebSockets();

After that, I added a new controller like this:
[AllowAnonymous]
[ApiController]
[Route(""[controller]"")]
internal class ShellyPlusDataController : ControllerBase
{
    [HttpGet]
    [Route(""[controller]/OnDataReceived"")]
    public async Task OnDataReceived()
    {
        if (HttpContext.WebSockets.IsWebSocketRequest)
        {
            using var webSocket = await HttpContext.WebSockets.AcceptWebSocketAsync();

            var buffer = new byte[1024 * 4];
            WebSocketReceiveResult result = await webSocket.ReceiveAsync(new ArraySegment<byte>(buffer), CancellationToken.None);

            while (!result.CloseStatus.HasValue)
            {
                string raw = Encoding.UTF8.GetString(buffer, 0, result.Count);
                result = await webSocket.ReceiveAsync(new ArraySegment<byte>(buffer), CancellationToken.None);
            }

            await webSocket.CloseAsync(result.CloseStatus.Value, result.CloseStatusDescription, CancellationToken.None);
        }
        else
        {
            HttpContext.Response.StatusCode = StatusCodes.Status400BadRequest;
        }
    }
}

Opening a connection to 'wss://localhost:7220/ShellyPlusData/OnDataReceived' with PostMan doesn't work. The error displayed is: Error: Unexpected server response: 200
I have placed a breakpoint at the start of OnDataReceived(); it is never hit.
I have also tried changing the URL to ws:// or omitting the method name, but no success.
The Microsoft tutorial also suggests this in Program.cs:
app.Use(async (context, next) =>
{
    if (context.Request.Path == ""/ws"")
    {
        if (context.WebSockets.IsWebSocketRequest)
        {
            using var webSocket = await context.WebSockets.AcceptWebSocketAsync();
            await Echo(webSocket);
        }
        else
        {
            context.Response.StatusCode = StatusCodes.Status400BadRequest;
        }
    }
    else
    {
        await next(context);
    }
});

Doesn't work either.
Any suggestions?
",76395916,"So I had to figure it out on my own. There were 2 things that needed to be fixed:

Local WebSocket connection doesn't work. I don't know what needs to be configured to enable WebSockets in local debug mode, but when I deploy the application to a remote IIS server, I can establish a connection.
My controller class was internal and routing seems to work differently when working with WS connections. Here's the working class:

public class ShellyPlusDataConnectionController : ControllerBase
{
    [HttpGet(""/ShellyPlusDataConnection/OnDataReceived"")]
    public async Task OnDataReceived()
    {
        if (HttpContext.WebSockets.IsWebSocketRequest)
        {
            using var webSocket = await HttpContext.WebSockets.AcceptWebSocketAsync();

            var buffer = new byte[1024 * 4];
            WebSocketReceiveResult result = await webSocket.ReceiveAsync(new ArraySegment<byte>(buffer), CancellationToken.None);

            while (!result.CloseStatus.HasValue)
            {
                string raw = Encoding.UTF8.GetString(buffer, 0, result.Count);


                result = await webSocket.ReceiveAsync(new ArraySegment<byte>(buffer), CancellationToken.None);
            }

            await webSocket.CloseAsync(result.CloseStatus.Value, result.CloseStatusDescription, CancellationToken.None);
        }
        else
        {
            HttpContext.Response.StatusCode = StatusCodes.Status400BadRequest;
        }
    }
}

",
76395827,PySimpleGUI button event not working in Python code - why?,"Why the button event is not avalible with pysimplegui?
This is my Code.
import os
import threading
import PySimpleGUI as gui
from rsa_controller import decryptwithPrivatekey, loadPublicKey, loadPrivateKey

id = 0
target_id = 0
prikey = None


def popout(title):
    gui.popup(title)


def read_keys():
    print(""Opening key files: "" + os.getcwd() + ""\\keys\\"")
    pubkey = loadPublicKey(os.getcwd() + ""\\keys\\public.pem"")
    prikey = loadPrivateKey(os.getcwd() + ""\\keys\\private.pem"")


def recv_msg():
    global target_id
    from main import s
    while True:
        data = s.recv(1024)
        if not data:
            break
        decoded_data = data.decode('utf-8')
        if decoded_data == 'target_connected_Success':
            print(""Received message:"", decoded_data)
        elif decoded_data.startswith(""!+@""):
            target_id = decoded_data[3:]
            window2()
        elif decoded_data == 'target_connect_denied':
            gui.popup('Connection request denied')
        else:
            msg_to_recv = decryptwithPrivatekey(decoded_data, prikey)
            print(""Received message:"", msg_to_recv)


def window2():
    from main import s
    global target_id
    layout2 = [
        [gui.Text('Connecting with'), gui.Text(str(target_id), key='target_id'), gui.Text(""Establishing contact"")],
        [gui.Button('Accept and share my public key', key='accept', enable_events=True),
         gui.Button('Deny connection invitation', key='denied', enable_events=True)]
    ]
    window = gui.Window(""Connection Request"", layout2, finalize=True)
    while True:
        event2, values2 = window.read()
        if event2 == gui.WINDOW_CLOSED:
            break
        if event2 == 'Deny connection invitation':
            print(""Connection denied"")
            s.send('!x!{}'.format(target_id).encode('utf-8'))
            window.close()
        if event2 == 'Accept and share my public key':
            print(""Accepting and sharing public key"")
            # Handle the logic for accepting the connection
    window.close()


def start_GUI_progress(id):
    from main import s
    read_keys()
    layout = [
        [gui.Text('Your identification code'), gui.Text(id)],
        [gui.Text('Hint: Please enter the identification code of the person you want to connect to in the input box below and click the Connect button')],
        [gui.Input(key='target_id'), gui.Button('Connect', key='connect')]
    ]
    window = gui.Window(""RSA Encrypted Chat Software"", layout)
    host = ""localhost""
    port = 23333
    s.connect((host, port))
    print(s.recv(1024))
    t_recv = threading.Thread(target=recv_msg)
    t_recv.start()
    s.send(b""__!"" + str(id).encode('utf-8'))
    while True:
        event, values = window.read()
        if event is None:
            break
        if event == 'connect':
            print(""Client is attempting to connect to: {}"".format(values['target_id']))
            message = ""_!?{}"".format(values['target_id'])
            s.send(message.encode('utf-8'))
    window.close()



I found that the first window is intractivable,but after the window2 successfully displayed,i press buttons on it and nothing happend,what's more,when the window2 displaying, there is an error:

Exception in thread Thread-1 (recv_msg):
Traceback (most recent call last):
File ""C:\Users\bao\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 1009, in _bootstrap_inner
self.run()
File ""C:\Users\bao\AppData\Local\Programs\Python\Python310\lib\threading.py"", line 946, in run
self._target(*self._args, **self._kwargs)
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\GUIDisplay.py"", line 32, in recv_msg
window2()
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\GUIDisplay.py"", line 46, in window2
window = gui.Window(""Connecting with"", layout2, finalize=True)
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\venv\lib\site-packages\PySimpleGUI\PySimpleGUI.py"", line 9614, in init
self.Finalize()
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\venv\lib\site-packages\PySimpleGUI\PySimpleGUI.py"", line 10300, in finalize
self.Read(timeout=1)
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\venv\lib\site-packages\PySimpleGUI\PySimpleGUI.py"", line 10075, in read
results = self._read(timeout=timeout, timeout_key=timeout_key)
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\venv\lib\site-packages\PySimpleGUI\PySimpleGUI.py"", line 10146, in _read
self._Show()
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\venv\lib\site-packages\PySimpleGUI\PySimpleGUI.py"", line 9886, in Show
StartupTK(self)
File ""C:\Users\bao\PycharmProjects\RSAEncryptedChatSoftware\venv\lib\site-packages\PySimpleGUI\PySimpleGUI.py"", line 16935, in StartupTK
window.TKroot.mainloop()
File ""C:\Users\bao\AppData\Local\Programs\Python\Python310\lib\tkinter_init.py"", line 1458, in mainloop
self.tk.mainloop(n)
RuntimeError: Calling Tcl from different apartment

Process finished with exit code 0
",76395933,"    layout2 = [
        [gui.Text('Connecting with'), gui.Text(str(target_id), key='target_id'), gui.Text(""Establishing contact"")],
        [gui.Button('Accept and share my public key', key='accept', enable_events=True),
         gui.Button('Deny connection invitation', key='denied', enable_events=True)]
    ]

        if event2 == 'Deny connection invitation':
            print(""Connection denied"")
            s.send('!x!{}'.format(target_id).encode('utf-8'))
            window.close()
        if event2 == 'Accept and share my public key':
            print(""Accepting and sharing public key"")
            # Handle the logic for accepting the connection

The keys defined for buttons are different from the events used in the event loop.

keys defined for buttons:'accept' and 'denied'.
The events: 'Accept and share my public key' and 'Deny connection invitation'.


t_recv = threading.Thread(target=recv_msg)

def recv_msg():
    # ... wrong, try to call `window.write_event_value` to generate an event to call `window2` in main thread.
            window2()
    # ... wrong, try to call `window.write_event_value` to generate an event to call `gui.popup` in main thread.
            gui.popup('Connection request denied')
    # ...

GUI should be run under main thread !
",
76391959,Flutter: How to update locale in GetxController,"my splash code this
class App extends StatelessWidget {
  const App();

  @override
  Widget build(BuildContext context) {
    Get.put(SplashController());
    Get.put(ThemeController());
    Get.put(HomeController());
    Get.put(LocaleController());
    var localeController = Get.find<LocaleController>();
    print('amirrrrrrrrrrr${localeController.locale}');
    return GetMaterialApp(
      debugShowCheckedModeBanner: false,
      translations: LocaleString(),
      locale: localeController.locale,
      initialBinding: MyBindings(),
      home: Splash(),
    );
  }
}

after update locale in controller
class LocaleController extends GetxController {
  Locale locale = const Locale('fa', 'FA');

  Future<void> saveLocale(Locale newLocale) async {
    SharedPreferences prefs = await SharedPreferences.getInstance();
    await prefs.setString('languageCode', newLocale.languageCode);
    await prefs.setString('countryCode', newLocale.countryCode.toString());
    locale = newLocale;
    update();
    print('amirrrrrrr$locale');
  }

  Future<Locale> loadLocale() async {
    SharedPreferences prefs = await SharedPreferences.getInstance();
    String? languageCode = prefs.getString('languageCode');
    String? countryCode = prefs.getString('countryCode');
    Locale? locale;
    if (languageCode != null && countryCode != null) {
      locale = Locale(languageCode, countryCode);
      this.locale = locale;
    }
    update();
    print('amirrrrrrr$locale');
    return locale!;
  }
}

this code not updated
var localeController = Get.find<LocaleController>();
print('amirrrrrrrrrrr${localeController.locale}');

how to fix ?
It is updated in the LocaleController controller, but in the app class, it always returns fa_FA and does not show the updated locale.
",76395790,"As documentation mentioned you need to call;
getx
Get.changeLocale(Locale(""pt""));

You need to call this method with expected Locale.
Additionally, current locale can be checked with;
  Get.locale;

",
76395847,What is the algorithm applied to create a circular array in javascript?,"I have a code like below.
var t = [1, 2];
t[2] = t;


This creates a circular array. What is the algorithm applied to create this circular array in javascript.
",76395972,"There's no algorithm involved. It's just that your array refers to itself. When a variable or property refers to an object (arrays are objects), what's held in the variable is an object reference, which is a value that tells the JavaScript engine where that object is elsewhere in memory. You can think of it as a number that uniquely identifies a memory location where the object is. (That isn't what it is, but it's a handy way to think of it.)
Let's look at the code. You start with:
var t = [1, 2];

That creates an array and stores an object reference for it in t. That creates something somewhat like this in memory (various details omitted for clarity):

                  +−−−−−−−−−−−−−−−+
t:{Ref18465}−−−−−>|    (array)    |
                  +−−−−−−−−−−−−−−−+
                  | length: 2     |
                  | 0: 1          |
                  | 1: 2          |
                  +−−−−−−−−−−−−−−−+

The Ref18465 I've shown is a stand-in for the object reference, which we never directly see in code. We have an array, and a variable containing an object reference saying where the array is.
Then your code does this:
t[2] = t;

That adds a new element to the array containing the object reference of the array, making it refer to itself:

              +−−−−−−−−−−−−−−−−−−−−−−+
              |                      |
              \   +−−−−−−−−−−−−−−−+  |
t:{Ref18465}−−−+−>|    (array)    |  |
                  +−−−−−−−−−−−−−−−+  |
                  | length: 3     |  |
                  | 0: 1          |  |
                  | 1: 2          |  |
                  | 2: {Ref18465} |>−/
                  +−−−−−−−−−−−−−−−+

Now, both t and t[2] contain the object reference to the array. As you say, the array refers to itself. There's no particular algorithm involved, it's just a circular data structure.
",
76394621,curl_formfree not working properly return error: SegFault,"I am writing code to upload file on file server along with two other string variables as part of HTTP post request.
Idea to use multi interface here is to upload multiple file in future.
Libcurl version being used here is: 7.44
Here is my program:
#include <iostream>
#include <string>
#include <curl/curl.h>

const auto TimeoutInMS = 1000;
const auto FileDescriptorZero = 0;

bool HTTPPostSuccessful(long httpResponseCode)
{
    bool httpRequestSuccessful = false;
    if (httpResponseCode == 200)
    {
        httpRequestSuccessful = true;
    }
    return httpRequestSuccessful;
}

size_t WriteCallback(void * buffer, size_t size, size_t count, void * userp)
{
    size_t numBytes = size * count;
    static_cast<std::string*>(userp)->append(static_cast<char*>(buffer), numBytes);
    return numBytes;
}

void HTTPPost(const std::string& value1, const std::string& value2, const std::string& filePath)
{
    struct curl_slist *pHTTPRequestHeaders = nullptr;
    struct curl_httppost* pFormpost = nullptr;
    struct curl_httppost* pLastptr = nullptr;
    uint16_t httpResponseCode = 0;
    int stillSendingFile = 0;
    CURL* pCurlEasyHandle = curl_easy_init();
    CURLM *pCurlMultiHandle = curl_multi_init();
    std::string responseData{};

    
    if (pCurlEasyHandle && pCurlMultiHandle)
    {
        pHTTPRequestHeaders = curl_slist_append(pHTTPRequestHeaders, ""Content-Type: multipart/form-data"");
        curl_easy_setopt(pCurlEasyHandle, CURLOPT_HTTPHEADER, pHTTPRequestHeaders);
        curl_easy_setopt(pCurlEasyHandle, CURLOPT_URL, ""https://http_.org/logs/readers"");
        curl_easy_setopt(pCurlEasyHandle, CURLOPT_VERBOSE, 1L);
        
        curl_formadd(&pFormpost, &pLastptr,
                     CURLFORM_COPYNAME, ""fileName"",
                     CURLFORM_FILE, filePath.c_str(),
                     CURLFORM_CONTENTTYPE, ""text/csv"",
                     CURLFORM_END);
        
        curl_formadd(&pFormpost, &pLastptr,
                     CURLFORM_COPYNAME, ""Value1"",
                     CURLFORM_COPYCONTENTS, value1.c_str(),
                     CURLFORM_END);
        
        curl_formadd(&pFormpost, &pLastptr,
                     CURLFORM_COPYNAME, ""Value2"",
                     CURLFORM_COPYCONTENTS, value2.c_str(),
                     CURLFORM_END);
        
        curl_easy_setopt(pCurlEasyHandle, CURLOPT_HTTPPOST, pFormpost);
        curl_easy_setopt(pCurlEasyHandle, CURLOPT_WRITEFUNCTION, WriteCallback);
        curl_easy_setopt(pCurlEasyHandle, CURLOPT_WRITEDATA, &responseData);
        
        curl_multi_add_handle(pCurlMultiHandle, pCurlEasyHandle);
        
        do
        {
            curl_multi_perform(pCurlMultiHandle, &stillSendingFile);
            if (stillSendingFile)
            {
                curl_multi_wait(pCurlMultiHandle, nullptr, FileDescriptorZero, TimeoutInMS, nullptr);
            }
        }
        while(stillSendingFile);
        
        CURLcode res = curl_easy_getinfo(pCurlEasyHandle, CURLINFO_RESPONSE_CODE, &httpResponseCode);
        
        if (HTTPPostSuccessful(httpResponseCode) && res == CURLE_OK)
        {
            
            std::cout << ""File sent Successfully, HTTP response code: "" << httpResponseCode << "", ResponseData: ""<< responseData<< std::endl;
        }
        else
        {
            std::cerr << ""Error during request: "" << curl_easy_strerror(res) << "", Failure HTTP response code: "" << httpResponseCode << std::endl;
        }
        
        if (pCurlMultiHandle && pCurlEasyHandle)
        {
            std::cout << ""Clean up for curl_multi_remove_handle "" << std::endl;
            curl_multi_remove_handle(pCurlMultiHandle, pCurlEasyHandle);
        }
        
        if (pCurlEasyHandle)
        {
            std::cout << ""Clean up for pCurlEasyHandle "" << std::endl;
            curl_easy_cleanup(pCurlEasyHandle);
            pCurlEasyHandle = nullptr;
        }
        
        if (pCurlMultiHandle)
        {
            std::cout << ""Clean up for pCurlMultiHandle "" << std::endl;
            curl_multi_cleanup(pCurlMultiHandle);
            pCurlMultiHandle = nullptr;
        }
        
        if (pHTTPRequestHeaders)
        {
            std::cout << ""Clean up pHTTPRequestHeaders "" << std::endl;
            curl_slist_free_all(pHTTPRequestHeaders);
            pHTTPRequestHeaders = nullptr;
        }
        
        if (pFormpost)
        {
            std::cout << ""Clean up pFormpost "" << std::endl;
            curl_formfree(pFormpost);
            pFormpost = nullptr;
            pLastptr = nullptr;
        }
    }


}


bool UploadFile(const std::string& value1, const std::string& value2, const std::string& filePath)
{
    curl_global_init(CURL_GLOBAL_ALL);
    HTTPPost(value1, value2, filePath);
    curl_global_cleanup();

    return true;
}

int main ()
{
    UploadFile(""1"", ""1"", ""/tmp/UploadFIle/testDoc.txt"");
}

above code works fine in my machine, when I commit it on Jenkins build server, I get following error:
7: HTTPClient Initialization is successful.
7: * Could not resolve host: http_.org
7: * Closing connection 0
7: Error during request: No error, Failure HTTP response code: 0
7: Clean up for curl_multi_remove_handle 
7: Clean up for pCurlEasyHandle 
7: Clean up for pCurlMultiHandle 
7: Clean up pHTTPRequestHeaders 
7: Clean up pFormpost 
 7/30 Test  #7: **** ....................................***Exception: SegFault  0.29 sec

In Few cases also seeing following error:
8: HTTPClient Initialization is successful.
8: * Could not resolve host: http_.org
8: * Closing connection 0
8: Error during request: No error, Failure HTTP response code: 0
8: Clean up for curl_multi_remove_handle 
8: Clean up for pCurlEasyHandle 
8: Clean up for pCurlMultiHandle 
8: Clean up pHTTPRequestHeaders 
8: Clean up pFormpost 
8: ==11267== Invalid read of size 8
8: ==11267==    at 0x4E48018: curl_formfree (in /usr/lib/x86_64-linux-gnu/libcurl.so.4.4.0)

Can somebody please inform me what is wrong here, new to Curl.
",76394679,"The first issue resulting in UB is in WriteCallback:
static_cast<std::string*>(userp)->append(static_cast<char*>(buffer), 0, numBytes);

You have chosen the overloaded member function
basic_string& append( const basic_string& str,
                      size_type pos, size_type count )

that creates std::string from a not null-terminated data in the buffer.
You should use the other overloaded member function
basic_string& append( const CharT* s, size_type count )

therefore the correct call is
static_cast<std::string*>(userp)->append(static_cast<char*>(buffer), numBytes);

The second issue is uint16_t httpResponseCode, whereas CURLINFO_RESPONSE_CODE requires a pointer to a long value, you pass &httpResponseCode, a pointer to a short value, this is yet another UB. Particularly it corrupts data in the local variables, writes zeros in 2 or 6 bytes outside the httpResponseCode storage, probably in pFormpost bytes. It should be
long httpResponseCode = 0;

",
76391583,Spring Rest Controller not able to validate path variable when request body is also passed in addition to path variable,"I've a rest controller and one of the endpoint looks like this:
@PostMapping(value = ""/myapi/{id}"", produces = APPLICATION_JSON_VALUE, consumes = APPLICATION_JSON_VALUE)
public ResponseEntity<MyEntity> myApi(
          @Valid @PathVariable(""id"") @NotBlank String id,
          @Valid @RequestBody MyRequestPayload myRequestPayload) throws Exception)
{
  LOGGER.info(""Id is {}"",id);
  ...............
  .........................
  .............................
}

For some reason, when I call the API with an empty or null path variable and a request payload, the path variable is not failing the validation and the control comes inside the method block.
What am I doing wrong? Kindly advise.
",76395798,"@Valid validates complex objects, containing fields annotated with constraint annotations.
For this case, you need to use @Validated:

The @Validated annotation is a class-level annotation that we can use to tell Spring to validate parameters that are passed into a method of the annotated class.

So mark your controller class as @Validated, which would trigger the validation of the id path variable.
@RestController
@RequestMapping(""/my"")
@Validated
public class MyController {

  @PostMapping(value = ""/myapi/{id}"", produces = APPLICATION_JSON_VALUE, consumes = APPLICATION_JSON_VALUE)
  public ResponseEntity<MyEntity> myApi(
      @PathVariable(""id"") @NotBlank String id,
      @Valid @RequestBody MyRequestPayload myRequestPayload) throws
      Exception {
    LOGGER.info(""Id is {}"", id);
  }
}

Reference: Validation with Spring Boot
",
76394660,Fixing a function checking whether the input binary tree is ordered,"I have the following algebraic data type:
data Tree a = Empty | Node a (Tree a) (Tree a)
  deriving (Show, Eq)

Also, I have this code snippet:
fromJust :: Maybe a -> a
fromJust (Just val) = val
fromJust Nothing = error ""Cannot unpack Nothing.""

getTreeMinimum :: Ord a => Tree a -> Maybe a
getTreeMaximum :: Ord a => Tree a -> Maybe a

getTreeMaximum Empty = Nothing
getTreeMaximum (Node value l r) =
  if l == Empty && r == Empty then Just value else
  if l == Empty && r /= Empty then if value < fromJust (getTreeMinimum r) then (getTreeMaximum r) else
  if l /= Empty && r == Empty then if fromJust (getTreeMaximum l) < value then Just (value) else
  if l /= Empty && r /= Empty then if fromJust (getTreeMaximum l) < value && value < fromJust (getTreeMinimum r) then (getTreeMaximum r) else Nothing

getTreeMinimum Empty = Nothing
getTreeMinimum (Node value l r) = 
  if l == Empty && r == Empty then Just value else
  if l == Empty && r /= Empty then if value < fromJust (getTreeMinimum r) then Just (value) else
  if l /= Empty && r == Empty then if fromJust (getTreeMaximum l) < value then (getTreeMinimum l)
  if l /= Empty && r /= Empty then if fromJust (getTreeMaximum l) < value && value < fromJust (getTreeMinimum r) then (getTreeMinimum l) else Nothing

isOrderedHelper :: Ord a => Tree a -> Bool
isOrderedHelper Empty = True

isOrderedHelper (Node nodeValue leftChild Empty) = if isOrderedHelper leftChild == False then False else (fromJust (getTreeMaximum leftChild)) < nodeValue

isOrderedHelper (Node nodeValue Empty rightChild) = if isOrderedHelper rightChild == False then False else nodeValue < fromJust ((getTreeMinimum rightChild))

isOrderedHelper (Node nodeValue leftChild rightChild) =
  if isOrderedHelper leftChild == False || isOrderedHelper rightChild == False
  then False 
  else fromJust (getTreeMaximum leftChild) < nodeValue && nodeValue < fromJust (getTreeMinimum rightChild)

isOrdered :: Ord a => Tree a -> Bool
isOrdered Empty = True
isOrdered tree = isOrderedHelper tree

The above gives me:
error: parse error on input 'getTreeMinimum'
    getTreeMinimum Empty = Nothing
    ^^^^^^^^^^^^^^
Failed, no modules loaded.

I have two questions (the second one is optional):

How to fix the compile time error?
Is it possible to improve the efficiency of the function in question?

",76394688,"Since if is an expression in Haskell each if has to have exatly one then and one else but this
getTreeMaximum (Node value l r) =
  if l == Empty && r == Empty then Just value else
  if l == Empty && r /= Empty then if value < fromJust (getTreeMinimum r) then (getTreeMaximum r) else
  if l /= Empty && r == Empty then if fromJust (getTreeMaximum l) < value then Just (value) else
  if l /= Empty && r /= Empty then if fromJust (getTreeMaximum l) < value && value < fromJust (getTreeMinimum r) then (getTreeMaximum r) else Nothing

has 7 if and 7 then
but only 4 else
I would probably write that using pattern matching to avoid such a deeply nested if tree:
getTreeMaximum (Node value Empty Empty) = Just value
getTreeMaximum (Node value Empty r) = if value < fromJust (getTreeMinimum r) then (getTreeMaximum r) else Nothing
getTreeMaximum (Node value l Empty) = if fromJust (getTreeMaximum l) < value then Just (value) else Nothing
getTreeMaximum (Node value l r) = if fromJust (getTreeMaximum l) < value && value < fromJust (getTreeMinimum r) then (getTreeMaximum r) else Nothing

But I don't think the clauses other than the getTreeMaximum Empty have any reason to return Nothing at all since there's always a value, so you'll have to adjust these.
",
76395953,Regex to catch email addresses in email header,"I'm trying to parse a To email header with a regex. If there are no <> characters then I want the whole string otherwise I want what is inside the <> pair.
import re
re_destinatario = re.compile(r'^.*?<?(?P<to>.*)>?')
addresses = [
    'XKYDF/ABC (Caixa Corporativa)',
    'Fulano de Tal | Atlantica Beans <fulano.tal@atlanticabeans.com>'
]
for address in addresses:
    m = re_destinatario.search(address)
    print(m.groups())
    print(m.group('to'))

But the regex is wrong:
('XKYDF/ABC (Caixa Corporativa)',)
XKYDF/ABC (Caixa Corporativa)
('Fulano de Tal | Atlantica Beans <fulano.tal@atlanticabeans.com>',)
Fulano de Tal | Atlantica Beans <fulano.tal@atlanticabeans.com>

What am I missing?
",76395988,"You may use this regex:
<?(?P<to>[^<>]+)>?$

RegEx Demo
RegEx Demo:

<?: Match an optional <
(?P<to>[^<>]+): Named capture group to to match 1+ of any characters that are not < and >
>?: Match an optional >
$: End

Code Demo
Code:
import re
re_destinatario = re.compile(r'<?(?P<to>[^<>]+)>?$')
addresses = [
    'XKYDF/ABC (Caixa Corporativa)',
    'Fulano de Tal | Atlantica Beans <fulano.tal@atlanticabeans.com>'
]
for address in addresses:
    m = re_destinatario.search(address)
    print(m.group('to'))

Output:
XKYDF/ABC (Caixa Corporativa)
fulano.tal@atlanticabeans.com

",
76396013,MySql count with GROUP BY returns 1 even when count is 0,"I have two tables in MySql i.e., subjects and photos and I wished to count the number of photos on each subjects
SELECT a.id, a.name, count(a.id) as `refcount`, 
FROM `subjects` a 
LEFT JOIN `photos` b ON (a.id = b.subject_id)
GROUP by a.id
ORDER BY a.name"";

returns 1 even when the rowcount()=0.  How to fix it
I tried various MySql syntax including count(field), but in vain
",76396051,"You will need to count photos.id (b.id), if no photos are found for the given subject, the query will return null, count(null) = 0.
SELECT a.id, a.name, count(b.id) as `refcount`
FROM `subjects` a 
LEFT JOIN `photos` b ON a.id = b.subject_id
GROUP by a.id, a.name
ORDER BY a.name;

",
76383009,Building a Type from an object with properties of type Boolean?,"I would like to construct a type from this object:
const isSynchronized: Record<SynchronizableField, boolean> = {
    /* synchronized */
    surveyMacroEnvironments: true,
    coordinateReferenceSystemCrs: true,
    transactionType: true,
    epsgTransformation: true,
    startingAgreementDate: true,
    expirationAgreementDate: true,
    transactionTypeNotes: true,
    surveyDataType: true,
    /* not synchronized */
    surveyName: false,
    validationStateCd: false,
    legacy: false,
    notifyOnCreate: false,
    notifyOnValidate: false,
    finalReportLink: false,
    // timestamp fields
    creationDate: false,
    lastUpdate: false,
    // continent and country are handled differently
    continent: false,
    country: false,
};

where the type needs to have only the keys with values equal to true, could you please help me or give me any suggestions?
Thanks
",76396573,"As a first step we have to remove that type annotation on isSynchronized; we need the compiler to infer its type and then use that inferred type to compute the key set you're looking for.  You could use the satisfies operator instead to make sure the property types are checked against and constrained to boolean:
const isSynchronized = {
    surveyMacroEnvironments: true,
    coordinateReferenceSystemCrs: true,
    transactionType: true,
    epsgTransformation: true,
    startingAgreementDate: true,
    // ✂ ⋯ ✂
    lastUpdate: false,
    continent: false,
    country: false,
} satisfies Record<string, boolean>;

type IsSynchronized = typeof isSynchronized;

Now you can inspect IsSynchronized to get the desired type.

You're looking for an application of a type function I call KeysMatching<T, V>, as requested in microsoft/TypeScript#48992 and as discussed in
In TypeScript, how to get the keys of an object type whose values are of a given type?.  The idea is that KeysMatching<T, V> would evaluate to the union of property keys of T where the property values at those keys are assignable to V.  Specifically it looks like you want KeysMatching<IsSynchronized, true>.
There's no native KeysMatching provided by the language, but there are a number of ways to implement it yourself, with various issues and edge cases.  One approach is a distributive object type where we map over all the properties of T and then index into the result with all the keys to end up with the union of the computed property types.  Like this:
type KeysMatching<T, V> =
    { [K in keyof T]: T[K] extends V ? K : never }[keyof T]

And let's use it:
type SynchronizedKeys = KeysMatching<IsSynchronized, true>;
// type SynchronizedKeys = ""surveyMacroEnvironments"" | ""coordinateReferenceSystemCrs"" |
//   ""transactionType"" | ""epsgTransformation"" | ""startingAgreementDate"" | 
//   ""expirationAgreementDate"" | ""transactionTypeNotes"" | ""surveyDataType""

Looks good. If you don't want to keep KeysMatching around, you can inline the definition to compute SynchronizedKeys directly:
type SynchronizedKeys = {
    [K in keyof IsSynchronized]: IsSynchronized[K] extends true ? K : never
}[keyof IsSynchronized];

Playground link to code
",
76394387,Need help in string replacement in PHP,"I have a string like this:
$str = '[{""action"": ""verify_with_source"",""created_at"": ""2023-05-30T01:39:54+05:30"",""status"": ""in_progress"",""type"": ""license""}] {""address"":null,""badge_details"":null,""card_serial_no"":null,""city"":null,""cov_details"":[{""category"":""NT"",""cov"":""MCWG"",""issue_date"":""2021-03-30""},{""category"":""NT"",""cov"":""LMV"",""issue_date"":""2021-03-30""}],""date_of_issue"":""2021-03-30"",""date_of_last_transaction"":""2021-03-30"",""dl_status"":""Active"",""dob"":""1996-10-09"",""face_image"":null,""gender"":null,""hazardous_valid_till"":null,""hill_valid_till"":null,""id_number"":""DL1234567890"",""issuing_rto_name"":""MY CITY"",""last_transacted_at"":""MY CITY"",""name"":""MY NAME"",""nt_validity_from"":""2021-03-30"",""nt_validity_to"":""2036-10-08"",""relatives_name"":null,""source"":""SOURCE"",""status"":""id_found"",""t_validity_from"":null,""t_validity_to"":null}'

What I want to split the string in 2 parts - [{""action"": ""verify_with_source"",...""type"": ""license""}] and {""category"":""NT"",""cov"":""LMV"",""issue_date"":""2021-03-30""}],...,""t_validity_to"":null""}.
I removed [ and ] with -
$raw = str_replace(['[', ']'], '', $raw);

Then, I have tried-
$str = preg_replace('^/} {/', '}{', $raw);

and
$str = preg_replace('^/}\s{/', '}{', $raw);

and
$str = str_replace('} {', '{}', $raw);

Then I intend to split string $str into array of 2 strings with statement -
$arr = explode('}{', $str);

The string is not being splitted and above statement returns whole string in first element of array.
What is wrong with my script?
",76394754,"I would suggest to make it a bit easier instead of replacing all the braces.
explode can handle that for you and split the string at a place you want. So just split it at the position ] { where the license ends and the objects starts and add the missing braces afterwards to that strings.
<?php

$str = ""<your long string>""

$jsonStrings = explode('] {', $str, 1);

$jsonStrings[0] .= ']';
$jsonStrings[1] = '{' . $jsonStrings[1];


be aware of there is no error handling. if there is no ] { chars in your string, explode will not create an array with two strings and the rest of the code is failing.
",
76395778,"MongoDB insertMany skip the same _id field to avoid 'code: 11000,'","await testCollection.insertMany(testArray, { ordered: false });

I have this code. I found that putting { ordered: false } will prevent getting E11000 error code. But, it looks like, it does not. Is there any way that I can avoid this error? I want to skip and do not insert a doc that has the same _id code.
",76396057,"You can't avoid error, but you can proceed with inserting. See here. For example:
    MongoDB Enterprise replset:PRIMARY> db.products.insert(
    ...    [
    ...      { _id: 20, item: ""lamp"", qty: 50, type: ""desk"" },
    ...      { _id: 21, item: ""lamp"", qty: 20, type: ""floor"" },
    ...      { _id: 21, item: ""lamp"", qty: 20, type: ""floor"" },
    ...      { _id: 22, item: ""bulk"", qty: 100 }
    ...    ],
    ...    { ordered: false }
    ... )
    BulkWriteResult({
            ""writeErrors"" : [
                    {
                            ""index"" : 2,
                            ""code"" : 11000,
                            ""errmsg"" : ""E11000 duplicate key error collection: newdb1.products index: _id_ dup key: { _id: 21.0 }"",
                            ""op"" : {
                                    ""_id"" : 21,
                                    ""item"" : ""lamp"",
                                    ""qty"" : 20,
                                    ""type"" : ""floor""
                            }
                    }
            ],
            ""writeConcernErrors"" : [ ],
            ""nInserted"" : 3,
            ""nUpserted"" : 0,
            ""nMatched"" : 0,
            ""nModified"" : 0,
            ""nRemoved"" : 0,
            ""upserted"" : [ ]
    })
    MongoDB Enterprise replset:PRIMARY> db.products.find()
    { ""_id"" : 20, ""item"" : ""lamp"", ""qty"" : 50, ""type"" : ""desk"" }
    { ""_id"" : 21, ""item"" : ""lamp"", ""qty"" : 20, ""type"" : ""floor"" }
    { ""_id"" : 22, ""item"" : ""bulk"", ""qty"" : 100 }

you may see, that if you remove { ordered: false }, the only inserted records will be records before first error accured:
MongoDB Enterprise replset:PRIMARY> db.products.find()
{ ""_id"" : 20, ""item"" : ""lamp"", ""qty"" : 50, ""type"" : ""desk"" }
{ ""_id"" : 21, ""item"" : ""lamp"", ""qty"" : 20, ""type"" : ""floor"" }

",
76391858,how to generate relationship between two enumerate with double cycle,"Thanks a lot in advance .  trouble in how to generate relationship between two enumerate .
%-------------------------------
enum weeks = {w1,w2,w3,w4};
array[weeks] of 1..10 : weekShiftQty = [4,5,7,4]; % total shift = 20
enum shift = _(1..20);
array[shift] of weeks : shiftWeek
= [
% how to generate shiftWeek relationship %
  % just like following : calculate shift belog to which week base on weekShiftQty
  % shiftWeek[1] = w1 , ... , shiftWeek[4] = w1 , 
  % shiftWeek[5] = w2 ,...  , shiftWeek[9] = w2 ,
  % shiftWeek[10] = w3 ,...  , shiftWeek[16] = w3 ,
  % shiftWeek[17] = w4 ,...  , shiftWeek[20] = w4 ,
  | s in shift
];

%-------------------------------
",76397188,"Here's a solution, i.e. using [ w | w in weeks, _ in 1..weekShiftQty[w]] to generate the shiftWeek array:
enum weeks = {w1,w2,w3,w4};
array[weeks] of 1..10 : weekShiftQty = [4,5,7,4]; % total shift = 20
enum shift = _(1..20);

array[shift] of weeks: shiftWeek = [
                                     w
                                     | w in weeks,
                                       _ in 1..weekShiftQty[w]
                                     ];
output [
        ""shiftWeek: \(shiftWeek)\n""
        ];


The output is
shiftWeek: [w1, w1, w1, w1, w2, w2, w2, w2, w2, w3, w3, w3, w3, w3, w3, w3, w4, w4, w4, w4]

",
76394627,Why are the curves of the data samples connected by a line?,"I am attaching my txt file, graph, and code. Can you please tell me what to change in this code, or why this third straight line is coming in my graph, because I only need two curve lines. In other software like xmgrace it's showing two curves only.
import numpy as np
import matplotlib.pyplot as plt

deformation, potential = np.loadtxt(""poten-Rf259.txt"", unpack=True)
plt.subplot(1,3,1)
plt.plot(deformation, potential, linewidth=1, color='b')
plt.xlabel(""$deformation$"")
plt.ylabel(""potential"")
plt.yscale(""log"")
plt.show()


0.60     1996.95397779
0.61     1995.35525840
0.62     1993.86701437
0.63     1992.48491231
0.64     1991.19969171
0.65     1990.00485364
0.66     1988.89605689
0.67     1987.86875188
0.68     1986.91599808
0.69     1986.03420343
0.70     1985.21922699
0.71     1984.46631470
0.72     1983.77171151
0.73     1983.13117131
0.74     1982.53983312
0.75     1981.99532235
0.76     1981.49445904
0.77     1981.03536392
0.78     1980.61563713
0.79     1980.23282155
0.80     1979.88357050
0.81     1979.56605195
0.82     1979.27790062
0.83     1979.01687947
0.84     1978.78288516
0.85     1978.57209950
0.86     1978.38399957
0.87     1978.21704688
0.88     1978.06886192
0.89     1977.93793974
0.90     1977.82377479
0.91     1977.72453886
0.92     1977.64028757
0.93     1977.56966186
0.94     1977.51247702
0.95     1977.46734438
0.96     1977.43305218
0.97     1977.40866283
0.98     1977.39347411
0.99     1977.38486567
1.00     1977.38198044
1.01     1977.38578333
1.02     1977.39435293
1.03     1977.40721066
1.04     1977.42425865
1.05     1977.44721979
1.06     1977.47357918
1.07     1977.50320577
1.08     1977.53535737
1.09     1977.56925776
1.10     1977.60496583
1.11     1977.64155671
1.12     1977.67899730
1.13     1977.71674843
1.14     1977.75457845
1.15     1977.79221495
1.16     1977.82838072
1.17     1977.86261243
1.18     1977.89473051
1.19     1977.92436517
1.20     1977.95062327
1.21     1977.97352605
1.22     1977.99387754
1.23     1978.01047267
1.24     1978.02233261
1.25     1978.03018886
1.26     1978.03434229
1.27     1978.03431686
1.28     1978.02782602
1.29     1978.01489388
1.30     1977.99620759
1.31     1977.97164846
1.32     1977.94011930
1.33     1977.90108165
1.34     1977.85474975
1.35     1977.80116069
1.36     1977.73920528
1.37     1977.66702371
1.38     1977.58440415
1.39     1977.49291347
1.40     1977.39203011
1.41     1977.28170103
1.42     1977.16108588
1.43     1977.02866014
1.44     1976.88410316
1.45     1976.72847328
1.46     1976.56259216
1.47     1976.38386074
1.48     1976.20047863
1.49     1976.00349668
1.50     1975.79062208
1.51     1975.56427272
1.52     1975.32369916
1.53     1975.06525907
1.54     1974.78917861
1.55     1974.49730653
1.56     1974.19357061
1.57     1973.87528322
1.58     1973.54134915
1.59     1973.19101633
1.60     1972.82834582
1.61     1972.45110208
1.62     1972.05553210
1.63     1971.64144442
1.64     1971.20783885
1.65     1970.75735680
1.66     1970.29140464
1.67     1969.81219620
1.68     1969.31582059
1.69     1968.80021418
1.70     1968.26283644
1.71     1967.70510563
1.72     1967.13020137
1.73     1966.53988789
1.74     1965.93070810
1.75     1965.30035432
1.76     1964.64993511
1.77     1963.98214335
1.78     1963.29607120
1.79     1962.58677705
1.80     1961.85278252
1.81     1961.10290054
1.82     1960.33687553
1.83     1959.54927874
1.84     1958.74001445
1.85     1957.90788391
1.86     1957.05397043
1.87     1956.17614396
1.88     1955.26999324
1.89     1954.33375080
1.90     1953.37236582
1.91     1952.38212458
1.92     1951.35983354
1.93     1950.30220457
1.94     1949.20603907
1.95     1948.05714844
1.96     1946.84779027
1.97     1945.58008527
1.98     1944.23420931
1.99     1942.79805826
2.00     1941.26241606
2.01     1939.59635092
2.02     1937.76411055
2.03     1935.72007499
2.04     1933.39927526
2.05     1930.72795563
2.06     1927.60977402
2.07     1923.90784077
2.08     1919.45804354
2.09     1914.31285484

0.60     1998.69655342
0.61     1997.10205960
0.62     1995.61774169
0.63     1994.23972037
0.64     1992.95788603
0.65     1991.76641764
0.66     1990.66090368
0.67     1989.63677721
0.68     1988.68674207
0.69     1987.80810777
0.70     1986.99582463
0.71     1986.24553783
0.72     1985.55344269
0.73     1984.91507367
0.74     1984.32536754
0.75     1983.78293400
0.76     1983.28383613
0.77     1982.82671380
0.78     1982.40862045
0.79     1982.02768397
0.80     1981.67975264
0.81     1981.36364770
0.82     1981.07656995
0.83     1980.81699835
0.84     1980.58410256
0.85     1980.37438858
0.86     1980.18712107
0.87     1980.02126097
0.88     1979.87363292
0.89     1979.74345272
0.90     1979.62995824
0.91     1979.53135472
0.92     1979.44772023
0.93     1979.37757311
0.94     1979.32104058
0.95     1979.27599816
0.96     1979.24203421
0.97     1979.21798198
0.98     1979.20293129
0.99     1979.19417215
1.00     1979.19165696
1.01     1979.19541560
1.02     1979.20395824
1.03     1979.21615707
1.04     1979.23320721
1.05     1979.25599428
1.06     1979.28171220
1.07     1979.31101088
1.08     1979.34264374
1.09     1979.37618654
1.10     1979.41145970
1.11     1979.44782160
1.12     1979.48476237
1.13     1979.52196027
1.14     1979.55904133
1.15     1979.59588083
1.16     1979.63104564
1.17     1979.66430513
1.18     1979.69566099
1.19     1979.72388637
1.20     1979.74966849
1.21     1979.77150493
1.22     1979.79086274
1.23     1979.80610500
1.24     1979.81670006
1.25     1979.82369958
1.26     1979.82686535
1.27     1979.82547672
1.28     1979.81769448
1.29     1979.80330511
1.30     1979.78339939
1.31     1979.75736288
1.32     1979.72409901
1.33     1979.68352105
1.34     1979.63566920
1.35     1979.58069993
1.36     1979.51670517
1.37     1979.44233796
1.38     1979.35779846
1.39     1979.26468767
1.40     1979.16177621
1.41     1979.04976449
1.42     1978.92676326
1.43     1978.79213218
1.44     1978.64555037
1.45     1978.48837015
1.46     1978.32011092
1.47     1978.14017085
1.48     1977.95500440
1.49     1977.75460116
1.50     1977.53929775
1.51     1977.31056663
1.52     1977.06737359
1.53     1976.80606527
1.54     1976.52721955
1.55     1976.23286883
1.56     1975.92682410
1.57     1975.60574409
1.58     1975.26801315
1.59     1974.91469758
1.60     1974.54947451
1.61     1974.16923372
1.62     1973.77070376
1.63     1973.35355306
1.64     1972.91665268
1.65     1972.46280943
1.66     1971.99372187
1.67     1971.51115360
1.68     1971.01142423
1.69     1970.49285180
1.70     1969.95210065
1.71     1969.39103370
1.72     1968.81236301
1.73     1968.21808006
1.74     1967.60511707
1.75     1966.97077175
1.76     1966.31642577
1.77     1965.64473808
1.78     1964.95488565
1.79     1964.24169727
1.80     1963.50348643
1.81     1962.74968760
1.82     1961.97913235
1.83     1961.18709132
1.84     1960.37331514
1.85     1959.53686374
1.86     1958.67882118
1.87     1957.79696651
1.88     1956.88661558
1.89     1955.94582437
1.90     1954.97949954
1.91     1953.98399101
1.92     1952.95610229
1.93     1951.89377598
1.94     1950.79298520
1.95     1949.63969262
1.96     1948.42603486
1.97     1947.15347382
1.98     1945.80171537
1.99     1944.35916231
2.00     1942.81707081
2.01     1941.14446913
2.02     1939.30723411
2.03     1937.25876976
2.04     1934.93414618
2.05     1932.25762526
2.06     1929.13222293
2.07     1925.42169053
2.08     1920.96503220
2.09     1915.81720928

",76394768,"It's simply because you are plotting one line for one set of x,y data, not two lines; hence the one line has to join all points. Try
 p.scatter(deformation, potential, s = 2,  color='b')

and see your data as it should be shown. As these are data points this is more appropriate anyway.
",
76396037,Check in range of multiple float variables,"User input scores of subjects and I need to check if these scores are valid (0->10, step is 0.1 because for exp: 5.25 or 5.1 is acceptable). Below is my code:
def Task_22():
    mathematics = float(input(""Input mathematics score: ""))
    literature  = float(input(""Input literature score: ""))
    english     = float(input(""Input english score: ""))
    # check valid score
    if all(i in range(0,11) for i in [mathematics, literature, english]):
        print(""OK"")
    else: print(""NOK"")

but when input as below, the result is not as my expected:
Input mathematics score: 2
Input literature score: 5
Input english score: 7.25
NOK

",76396113,"The range() function lets you create an iterator so you can loop over some integers. When you use i in range(0, 11), you're essentially asking ""will range(0, 11) eventually iterate over i"", not ""is i within the upper and lower bounds of range(0, 11)"". Because range() only works with integers, a float will be iterator over by range, and thus a float will never be in a range.
What you really want to do is check if the number is greater or equal to a lower bound and less than or equal to an upper bound, using operators like >= and <=.
def Task_22():
    mathematics = float(input(""Input mathematics score: ""))
    literature  = float(input(""Input literature score: ""))
    english     = float(input(""Input english score: ""))
    # check valid score
    if all(0 <= i <= 10 for i in [mathematics, literature, english]):
        print(""OK"")
    else: print(""NOK"")

",
76396082,"I'm trying to do a search by title and by content, but it gives an error syntax error at or near \""OR\","I'm trying to do a search by title and by content, but it gives an error
Typeorm
select
p.*,
from post p
left join vote v on p.id = v.post_id and v.user_id = $1
where p.is_published = true AND p.title ilike OR p.context::text ilike $2
limit 5 offset $3

const posts = await AppDataSource.query(`
  select
  p.*,
  from post p
  left join vote v on p.id = v.post_id and v.user_id = $1
  where p.is_published = true AND p.title ilike OR p.context::text ilike $2
  limit 5 offset $3
 `, [req.user.id, `%${req.query.q}%`, req.query.skip]
)

",76396144,"There's a matching pattern missing:
p.title ilike OR p.context::text ilike $2
             ^ HERE

",
76390635,"resampling/""upscaling"" using scipy.interpolate.RectBivariateSpline no diffrence","I am testing out scipy.interpolate.RectBivariateSpline for a project where I want to upscale some data to achieve better resolution. My attempt at using both scipy.interpolate.RectBivariateSpline and scipy.interpolate.interp2d results in no interpolation actually happening to the data; I just end up with a bigger matrix filled with more zeros. I have looked at some examples as well, but I am unable to see what I have done differently from them. And i would also expect my orgianal data to be centerd. Any help is appreciated
code
n = 10
smile = np.zeros((n,n))
a = 0.5
smile[2,2] = a
smile[3,2] = a
smile[2,7] = a
smile[3,7] = a
smile[6,2] = a
smile[6,7] = a
smile[7,3:7] = a


plt.imshow(smile)
plt.show()

#RECTBIVARIATESPLINE
#making interpolation function
x = np.arange(n)
y = x 
z = smile

interpolation_funk = scipy.interpolate.RectBivariateSpline(x,y,z)

#using interpolation
x_new = np.arange(2*n)
y_new = x_new
Z_new = interpolation_funk(x_new,y_new)


#plotting new funtion
plt.imshow(Z_new)


",76397231,"Note x_new = np.arange(2*n) : you are evaluating the interpolant outside of the original data range ([0, 9] x [0, 9]) and you get all zeros for the extrapolation.
Use x_new = np.arange(2*n) / 2 or some such to actually interpolate between the data points.
",
76396174,Why is my SQL query not updating user properties other than 'user' in Node.js and MySQL?,"I have a Service method to update a user from the database:
const updateUser = async (user) => {
    const {firstName, lastName, email, password, phone, dob, countryid, gender, address, role, id} = user;
    const sql = `UPDATE user set user_firstName = ?, user_lastName = ?, user_email = ?, user_password = ?, user_phoneNumber = ?, user_dob= ?, user_countryId = ?, user_gender = ?, user_address = ?, user_role= ? WHERE client_id = ?`;
    try {
        await db.query(sql, [firstName, lastName, email, password, phone, dob, countryid, gender, address, role, id]);
        return { message: ""records updated successfully."" }
    } catch (error) {
        return { message: ""Failed to updated"" }
    }
}

whenever I log user alone it can read it, but it's not reading any other properties ( firstName etc...) and getting this error: Bind parameters must not contain undefined. To pass SQL NULL specify JS null.
preview of console.log(user):

my route is working perfectly, the same function has been used before without any issues
",76396198,"The user argument you're printing out isn't an object representing a user, it's an array with a single such element. In addition, the object itself doesn't have the firstName and lastName properties you're trying to use, it has firstname and lastname (notice the lowercase ns). Since they aren't there, they get undefined values
Assuming you intended to pass an array to the function, you should probably iterate over all the users an update each of them. Regarding the properties, if you replace firstName and lastName with firstname and lastname, respectively, you should be OK:
const updateUser = async (users) => {
    users.forEach(user => {
        const {firstname, lastname, email, password, phone, dob, countryid, gender, address, role, id} = user; // Here
        const sql = `UPDATE user set user_firstName = ?, user_lastName = ?, user_email = ?, user_password = ?, user_phoneNumber = ?, user_dob= ?, user_countryId = ?, user_gender = ?, user_address = ?, user_role= ? WHERE client_id = ?`;
        try {
            await db.query(sql, [firstname, lastname, email, password, phone, dob, countryid, gender, address, role, id]); // And here!
            return { message: ""records updated successfully."" }
        } catch (error) {
            return { message: ""Failed to updated"" }
        }
    }
});

",
76394699,Conditionally project a field value in mongodb,"**I have a mongo document as below.
**
{
  ""metadata"": {
    ""docId"": ""7b96a""
  },
  ""items"": {
    ""content"": ""abcd"",
    ""contentWithInfo"": ""content with additional info""
  }
}

I want to project content field based on the condition whether contentWithInfo field is present or not. If contentWithInfo is present, it's value should be projected as content field value and contentWithInfo should be empty. Otherwise content should be projected as is. Is it possible?
I tried the following shell query:
db.collection1.aggregate([
  {
    ""$match"": {
      ""metadata.docId"": {
        ""$in"": [
          ""7b96a""
        ]
      }
    }
  },
  {
    ""$unwind"": ""$items""
  },
  {
    ""$project"": {
      ""metadata"": 1,
      ""items.content"": {
        ""$cond"": {
          ""if"": {
            ""$eq"": [
              ""$items.contentWithInfo"",
              null
            ]
          },
          ""then"": ""$items.content"",
          ""else"": ""$items.contentWithInfo""
        }
      }
    }
  }
])


If contentWithInfo is present, it is returning the following:
{
  ""metadata"": {
    ""docId"": ""7b96a""
  },
  ""items"": {
    ""content"": ""content with additional info""
  }
}

If contentWithInfo is not present, it is returning the following:
{
  ""metadata"": {
    ""docId"": ""7b96a""
  },
  ""items"": {}
}


whereas I expect it to return
{
  ""metadata"": {
    ""docId"": ""7b96a""
  },
  ""items"": {
    ""content"": ""abcd""
  }
}

",76394770,"Approach 1
Instead of checking items.contentWithInfo is null, check whether the items.contentWithInfo is missing with $type operator.
db.collection.aggregate([
  {
    ""$match"": {
      ""metadata.docId"": {
        ""$in"": [
          ""7b96a""
        ]
      }
    }
  },
  {
    ""$unwind"": ""$items""
  },
  {
    ""$project"": {
      ""metadata"": 1,
      ""items.content"": {
        ""$cond"": {
          ""if"": {
            ""$eq"": [
              {
                $type: ""$items.contentWithInfo""
              },
              ""missing""
            ]
          },
          ""then"": ""$items.content"",
          ""else"": ""$items.contentWithInfo""
        }        
      }
    }
  }
])

Demo Approach 1 @ Mongo Playground
Another approach, if you want items.content as default value if items.contentWithInfo is missing or null, you can use $ifNull operator.
db.collection.aggregate([
  {
    ""$match"": {
      ""metadata.docId"": {
        ""$in"": [
          ""7b96a""
        ]
      }
    }
  },
  {
    ""$unwind"": ""$items""
  },
  {
    ""$project"": {
      ""metadata"": 1,
      ""items.content"": {
        $ifNull: [
          ""$items.contentWithInfo"",
          ""$items.content""
        ]
      }
    }
  }
])

Demo Approach 2 @ Mongo Playground
",
76396065,"""You have requested more vCPU capacity"", when launching G instance","I have this error when launching g4dn.xlarge instance
You have requested more vCPU capacity than your current vCPU limit of 0 allows for the instance bucket that the specified instance type belongs to. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit.

However my Service Quota is like this below

All G and VT Spot Instance Requestss is 8
And I don't launch another G instance at the same time.
Where should I check?
Or is it relevant?(EC2 -> Limit, It shows  Limits page deactivated)
(I can jump to the service quota page from the link in this page though)

",76396319,"Ensure you are requesting a spot instance when you configure the launch of your vm. There is a check box under the Advanced section of the launch wizard.
",
76390618,How to perform XML Element substitution in config.wxi using Replace Tokens?,"I am building an Azure Devops Release pipeline and I have a WIX config file in my source code with the following structure
<?xml version=""1.0"" encoding=""utf-8""?>
<Include>
  <?define Key =""MyValue""?>
</Include>

I would like to replace ""MyValue"" with ""MyNewValue"" using the Replace Tokens package
(link here)
Following the answer given here.
I have <Include> as my token prefix and </Include> as my token suffix. I've added the following pipeline variable:
enter image description here
When I build my pipeline it finds the file correctly, but doesn't find any variables to replace, am I missing something to link the variables step to the pipeline variables?
alternatively I was thinking the '<?' tags could be throwing it off or perhaps it's just not compatible with the WIX xml files?
",76397387,"Why not just pass the preprocessor variable via the .wixproj or command-line (whichever way you are building). That way you avoid the include file completely and have a much simpler solution.
I cover this technique in Episode 12 of the Deployment Dojo - All the Ways to Change. Variables and Variables. Directories and Properties:
<Project Sdk=""WixToolset.Sdk/4.0.0"">
  <PropertyGroup>
   <DefineConstants>Key=MyValue</DefineConstants>
  </PropertyGroup>
</Project>

Then you can easily override the value with msbuild -p:Key=MyNewValue.
",
76394516,creating a function looping through multiple subsets and then grouping and summing those combinations of subsets,"I am attempting to build a function that processes data and subsets across two combinations of dimensions, grouping on a status label and sums on price creating a single row dataframe with the different combinations of subsets of the summed prices as output.
edit
to clarify, what I'm looking for is to subset on two different combinations of dimensions; a time delta and an association label.
I'm then looking to group on a different status label (which is different from the association label) and sum those on price.
Combinations of subsets:

the association labels are in the ""Association Label"" column and the three of interest are [""SDAR"", ""NSDCAR"", ""PSAR""] there are others in the column/data but they can be ignored
the time interval are [7, 30, 60, 90, 120, None] and are in the ""Status Date"" column

What's being grouped and summed as per those combination of subsets:

The Status Labelled are transaction statuses which are to be grouped on as per the different combinations of the above subsets from time deltas and association labels. They include [""Active"",""Pending"",""Sold"",Withdrawn"",""Contingent"",""Unknown""] (this is not an exhaustive list but just an example)
And finally ['List Price (H)'] which is to be summed per each of those status labelled and as per each combination of the fist two subsets.

So example columns of desired output would be something like PSAR_7_Contingent_price or SDAR_60_Withdrawn_price
This builds off of this question and answer which worked fantastic for value counts, but I'm having difficulty modifying it for summing on a price variable.
The code I used to build off of is
def crossubsets(df):
    labels = [""SDAR"", ""NSDCAR"", ""PSAR""]
    time_intervals = [7, 30, 60, 90, 120, None]
    group_dfs = df.loc[
        df[""Association Label""].isin(labels)
    ].groupby(""Association Label"")

    data = []
    for l, g in group_dfs:
        for ti in time_intervals:
            s = (
                g[g[""Status Date""] > (pd.Timestamp.now() - pd.Timedelta(ti, ""d""))]
                if ti is not None else g
            )
            data.append(s[""Status Labelled""].value_counts().rename(f""counts_{l}_{ti}""))

    return pd.concat(data, axis=1) #with optional .T to have 18 rows instead of cols

# additional code to flatten the output to a (1, 180) dataframe

counts_processeed = counts_processeed.unstack().to_frame().sort_index(level=1).T
counts_processeed .columns = counts_processeed.columns.map('_'.join)

This worked great for the value_counts per Status Labelled, but now I'm looking to sum the associated price per those that Status Labelled, and across those dimensions of subsets. I naively attempted to modify the above function with:
def crossubsetsprice(df):
    labels = [""SDAR"", ""NSDCAR"", ""PSAR""]
    time_intervals = [7, 30, 60, 90, 120, None]
    group_dfs = df.loc[
        df[""Association Label""].isin(labels)
    ].groupby(""Association Label"")

    data = []
    for l, g in group_dfs:
        for ti in time_intervals:
            s = (
                g[g[""Status Date""] > (pd.Timestamp.now() - pd.Timedelta(ti, ""d""))]
                if ti is not None else g
            )
            data.append(s['List Price (H)'].sum().rename(f""price_{l}_{ti}""))

    return pd.concat(data, axis=1) #with optional .T to have 18 rows instead of cols

But that throws and error AttributeError: 'numpy.float64' object has no attribute 'rename' and I don't think makes much sense or would get the desired output anyway.
The alternative I want to avoid, but I know would work, is creating 18 distinct functions for each of combination of subsets then concatinating the output. An example would be:
def price_PSAR_90(df):
    subset_90 = df[df['Status Date'] > (datetime.now() - pd.to_timedelta(""90day""))]
    subset_90_PSAR= subset_90[subset_90['Association Label']==""PSAR""]  

    grouped_90_PSAR = subset_90_PSAR.groupby(['Status Labelled'])

    price_summed_90_PSAR = (pd.DataFrame(grouped_90_PSAR['List Price (H)'].sum()))
    price_summed_90_PSAR.columns = ['Price']
    price_summed_90_PSAR = price_summed_90_PSAR.reset_index()
    price_summed_90_PSAR = price_summed_90_PSAR.T
    price_summed_90_PSAR = price_summed_90_PSAR.reset_index()
    price_summed_90_PSAR.drop(price_summed_90_PSAR.columns[[0]], axis=1, inplace=True)
    price_summed_90_PSAR_header = price_summed_90_PSAR.iloc[0] #grab the first row for the header
    price_summed_90_PSAR = price_summed_90_PSAR[1:] #take the data less the header row
    price_summed_90_PSAR.columns = price_summed_90_PSAR_header


    return price_summed_90_PSAR

The last code snippet works, but without looping would need to be repeated with the time delta and association label being changed for each combination, and then relabelling the output columns and concatenated them together, which I want to avoid if possible.
",76394776,"Maybe you can try to use a dict for data instead of a list. Something like:
def crossubsetsprice(df):
    labels = [""SDAR"", ""NSDCAR"", ""PSAR""]
    time_intervals = [7, 30, 60, 90, 120, None]
    group_dfs = df.loc[
        df[""Association Label""].isin(labels)
    ].groupby([""Association Label"", 'Status Labelled'])

    data = {}  # HERE
    for (l1, l2), g in group_dfs:
        for ti in time_intervals:
            s = (
                g[g[""Status Date""] > (pd.Timestamp.now() - pd.Timedelta(ti, ""d""))]
                if ti is not None else g
            )
            data[(l1, l2, ti)] = s['List Price (H)'].sum()  # HERE

    names = ['Association Label', 'Status Labelled', 'Time Interval']
    return pd.Series(data, name='Price').rename_axis(names)  # HERE

Output:
>>> crossubsetsprice(df)
Association Label  Status Labelled  Time Interval
NSDCAR             Active           7.0               1393
                                    30.0              6090
                                    60.0             11397
                                    90.0             16540
                                    120.0            21660
                                                     ...  
SDAR               Withdrawn        30.0              3167
                                    60.0              8897
                                    90.0             15768
                                    120.0            21806
                                    NaN              28379
Name: Price, Length: 108, dtype: int64

Minimal Reproducible Example:
import pandas as pd
import numpy as np

N = 1000
rng = np.random.default_rng(42)
labels = rng.choice([""SDAR"", ""NSDCAR"", ""PSAR""], N)
status = rng.choice([""Active"", ""Pending"", ""Sold"", ""Withdrawn"", ""Contingent"", ""Unknown""], N)
today = pd.Timestamp.today()
start = pd.Timestamp('2023-01-01 00:00:00')
offsets = rng.integers(0, (today - start).total_seconds(), N)
dates = start + pd.to_timedelta(offsets, unit='S')
prices = rng.integers(1, 1001, N)
df = pd.DataFrame({'Association Label': labels,
                   'Status Date': dates,
                   'Status Labelled': status,
                   'List Price (H)': prices})

",
76397512,ggplot - ordering legends with guides changes continuous legend to discrete,"I'm running ggplot2 v3.4.1. I created this 2 legend plot that by default it is placing the year2 size legend below the cty color legend. However, I would like the size legend to be on top.
library(tidyverse)

mpg$year2 = factor(mpg$year)
values = c(2,4); names(values) = c(""1999"", ""2008"")
p = mpg %>%
  ggplot(aes(x = cty, y = hwy, color = cty, size = year2)) + 
  geom_point() +
  scale_size_manual(name = ""year2"", values = values)
p


Therefore, I used guides() to specify the legend ordering but it changes the continuous color legend cty to discrete
p + guides(size = guide_legend(order = 1), 
           color = guide_legend(order = 2))


I saw this post ggplot guide_legend argument changes continuous legend to discrete but am unable to figure out how to use guide_colorbar() when you have 2 or more legends.
How do I change my code to keep the cty legend as continuous? Thx
",76397558,"It's simply
p + guides(size = guide_legend(order = 1), 
           color = guide_colorbar(order = 2))


",
76396169,"python3, converting integer to bytes: which are the alternatives to using to_bytes() for small integers?","Is there a more efficient method to convert an integer, in the range 0..255 (in C uint8), to one byte?
x = 100
x.to_bytes(1, ""big"")

",76396390,"More efficient method for a single integer? Probably not.
Here's a comparison of the common ones:
python -m timeit -s ""import struct"" ""struct.pack('<B', 100)""
2000000 loops, best of 5: 101 nsec per loop
python -m timeit ""(100).to_bytes(1)""                     
5000000 loops, best of 5: 81.1 nsec per loop
python -m timeit ""bytes([100])""     
2000000 loops, best of 5: 199 nsec per loop

If you're talking about multiple, bytes([integers]) will probably be the most efficient.
For some premature optimization you can cache the function, which will net you a few nanoseconds:
python -m timeit -s ""tb = int.to_bytes"" ""tb(100,1)"" 
5000000 loops, best of 5: 78.6 nsec per loop

And if you want the most efficient possible, it would probably be using a tuple:
python -m timeit -s ""b=tuple(map(int.to_bytes, range(256)))"" ""b[100]""
10000000 loops, best of 5: 20.9 nsec per loop

But personally I find it disgusting.
",
76394463,Simulate decaying function,"Lets t be the time tick i.e. 1,2,3,4,5....
I want to calculate and plot a cumulative decaying function f(inits[],peaks[],peak-ticks,zero-ticks).
Preferably in python
Where :
- inits[] is a list of points at time/tick t where a new 'signal' is introduced
- peaks[] is a list of values which must be reached after peak-ticks. (corresponding to inits)
- peak-ticks is how many ticks it takes to reach the next peak value
- zero-ticks is how many ticks it takes to reach zero from the peak

For example :
 f(inits=[10,15,18], peaks=[1,1,1], peak-ticks=1, zero-ticks=10)

in this case decay takes 10 ticks i.e. 0.1 per tick.
at tick:
 10! result is 0
 11. = 1
 12. = 0.9
 .....
 15! = 0.6 + 0 = 0.6
 16. = 0.5 + 1 = 1.5
 17. = 0.4 + 0.9 = 1.3
 18! = 0.3 + 0.8 + 0 = 1.1
 19. = 0.2 + 0.7 + 1 = 1.9
 20. = 0.1 + 0.6 + 0.9 = 1.6
 .....

PS> As a complication, what if the decay is exponential like 1/x ?
",76394795,"For the base case you mentioned, it is actually pretty simple, you just need to define a triangular function that returns the contribution of a specific singal at the current tick t.
Then, just sum the contribution of all signals at tick t, that is your answer.
In the code below, I implemented the decaying function as an infinite generator, so I have to use islice to define how many ticks to compute (or maybe the start and end ticks). You could also implement it as a normal function, you'd just have to pass in the start and end ticks.
from itertools import count, islice

def fdecay(inits, peaks, ptks, ztks):
  for t in count():
    yield sum(triang(p, i, i+ptks, i+ptks+ztks, t) for i, p in zip(inits, peaks))

def triang(ymax, xa, xb, xc, x):
  if x < xa: return 0
  if x < xb: return ymax * (x-xa) / (xb-xa)
  if x < xc: return ymax * (xc-x) / (xc-xb)
  return 0

x = list(islice(fdecay([10,15,18], [1,1,1], 1, 10), 30))
print(x)
# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.9, 0.8, 0.7, 0.6, 1.5, 1.3, 1.1, 1.9, 1.6, 1.3, 1.1, 0.9, 0.7, 0.5, 0.3, 0.2, 0.1, 0]


If you want exponential decay, just switch the triangular function to the exponential function at time t (with the appropriate params).
",
76396125,Rails 7 - Accessing joined model generate new query,"I have Rails 7 project where I've got tables
Pipelines
class Pipeline < ApplicationRecord
  has_many :states, inverse_of: :pipeline, dependent: :destroy  
end

States
class State < ApplicationRecord
  belongs_to :pipeline, inverse_of: :states 
  has_many :items, inverse_of: :state
end

Items
class Item < ApplicationRecord
  belongs_to :state, inverse_of: :items
end

I wrote a query to limit only specific items on state
@pipeline = Pipeline.joins(states: :items)
                    .where(states: { items: { name: ""Lorem"" } })
                    .find_by(id: '123')

query:
Pipeline Load (0.7ms)  SELECT ""pipelines"".* FROM ""pipelines"" INNER JOIN ""states"" ON ""states"".""pipeline_id"" = ""pipelines"".""id"" INNER JOIN ""items"" ON ""items"".""state_id"" = ""states"".""id"" WHERE ""items"".""name"" = $1 AND ""pipelines"".""id"" = $2 LIMIT $3  [[""name"", ""Lotem""], [""id"", ""123""], [""LIMIT"", 1]]

but once I call states from pipeline it generates new query without limitation:
puts @pipeline.states.inspect

State Load (0.4ms)  SELECT ""states"".* FROM ""states"" WHERE ""states"".""pipeline_id"" = $1 /* loading for inspect */ LIMIT $2  [[""pipeline_id"", ""123""], [""LIMIT"", 11]]

I'm expecting that it should grab states from collection it already has loaded,
am I missing something ?
",76396459,"
but once I call states from pipeline it generates new query without
limitation

yeah, that's how joins work, it doesn't do more than that so the docs even don't bother adding more information. If you want to access the states within a single query (really it'll end up being 2 queries) you could use includes instead;
ActiveRecord::QueryMethods#includes;

Specify relationships to be included in the result set. It allows you
to access the address attribute of the User model without firing an
additional query.

and your query would slightly change to;
Pipeline
  .includes(states: :items)
  .where(items: { name: ""Lorem"" })
  .find('123')

The fact that invoking states on a Pipeline instance makes a new query is because you're invoking an instance method on that object which by definition makes a new query. Active Record won't know that you're referring to something that was already created in the query unless is specified otherwise (by using a custom select clause or similar).

Notice
Pipeline.joins(states: :items).where(items: { name: ""Lorem"" }) is the same as Pipeline.joins(states: :items).where(states: { items: { name: ""Lorem"" } }), you don't need to nest items under states.
",
76397544,"When injecting the DbContext using DI and using its private field in a query, should you still use the ""using"" scope?","I wanted to know if when you inject your DbContext with DI in a class, within the methods calling that context, should you use the using scope as well ? Or does DI knows about disposal after the method has been executed and/or does transient by default and it's safe ?
",76397591,"
When you inject a DbContext using dependency injection (DI), you generally don't need to use the using scope explicitly within the methods calling that context.
In the case of a transient registration for your DbContext, a new instance of the context will be created for each method call, and the DI container will automatically dispose of it once the method execution is complete. Therefore, you don't need to manually dispose of the context using the using statement.

 Microsoft Docs
",
76395882,How can I generate signed URLs for accessing Firebase Storage images?,"I am making a web service with sveltekit and firebase.
By the way, when users save images on firebase storage and other users try to use these images, I want to create a signed url and show it on the page to prevent Hotlink.
I searched and found that there is a function called getSignedUrl that generates a signed url, but there is no official document page that describes it in the firebase document.
Where can I get some example functions or information related to this?
",76396461,"The Firebase SDK for Cloud Storage uses a different type of URL, called a download URL. You can generate a download URL by calling getDownloadURL with/on a reference to the file, as shown in the documentation on downloading data through a URL.
",
76396176,"ngx-image-cropper , roundCropper = ""true"" not working , its showing this error - Type 'string' is not assignable to type 'boolean'","<image-cropper [imageChangedEvent]=""imageChangedEvent"" [maintainAspectRatio]=""true"" [aspectRatio]=""4 / 4""
    format=""jpg"" (imageCropped)=""imageCropped($event)"" roundCropper = ""true"">
</image-cropper>

[screenshot attached for your reference]
I used roundCropper = ""True"". But its not working and throwing the error:

Type 'string' is not assignable to type 'boolean'.

If a try to run the same code on stackblitz then its working. I have also tried it with roundCropper = true , but its giving same error.
I want to use round cropper in my ngx-image-cropper.
",76396467,"Try to surround roundCropper with []
[roundCropper] = ""true""
",
76387496,ASP.Net Core can't connect to MySql database in docker,"I have error in the fifth line when asp.net try to connect db.
2023-06-02 09:36:59 warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]
2023-06-02 09:36:59       Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed.
2023-06-02 09:37:00 info: Microsoft.EntityFrameworkCore.Infrastructure[10403]
2023-06-02 09:37:00       Entity Framework Core 6.0.8 initialized 'AppIdentityDbContext' using provider 'Pomelo.EntityFrameworkCore.MySql:6.0.2' with options: ServerVersion 0.0-mysql 
2023-06-02 09:37:00 fail: Microsoft.EntityFrameworkCore.Database.Connection[20004]
2023-06-02 09:37:00       An error occurred using the connection to database '' on server 'db2'.
2023-06-02 09:37:00 Unhandled exception. System.InvalidOperationException: An exception has been raised that is likely due to a transient failure. Consider enabling transient error resiliency by adding 'EnableRetryOnFailure()' to the 'UseMySql' call.
2023-06-02 09:37:00  ---> MySqlConnector.MySqlException (0x80004005): Unable to connect to any of the specified MySQL hosts.
2023-06-02 09:37:00    at MySqlConnector.Core.ServerSession.ConnectAsync(ConnectionSettings cs, MySqlConnection connection, Int32 startTickCount, ILoadBalancer loadBalancer, IOBehavior ioBehavior, CancellationToken cancellationToken) in /_/src/MySqlConnector/Core/ServerSession.cs:line 433
2023-06-02 09:37:00    at MySqlConnector.MySqlConnection.CreateSessionAsync(ConnectionPool pool, Int32 startTickCount, Nullable`1 ioBehavior, CancellationToken cancellationToken) in /_/src/MySqlConnector/MySqlConnection.cs:line 926
2023-06-02 09:37:00    at MySqlConnector.MySqlConnection.OpenAsync(Nullable`1 ioBehavior, CancellationToken cancellationToken) in /_/src/MySqlConnector/MySqlConnection.cs:line 406
2023-06-02 09:37:00    at MySqlConnector.MySqlConnection.Open() in /_/src/MySqlConnector/MySqlConnection.cs:line 369
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnection(Boolean errorsExpected)
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenInternal(Boolean errorsExpected)
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open(Boolean errorsExpected)
2023-06-02 09:37:00    at Pomelo.EntityFrameworkCore.MySql.Storage.Internal.MySqlRelationalConnection.Open(Boolean errorsExpected)
2023-06-02 09:37:00    at Pomelo.EntityFrameworkCore.MySql.Storage.Internal.MySqlDatabaseCreator.<>c__DisplayClass18_0.<Exists>b__0(DateTime giveUp)
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.<>c__DisplayClass12_0`2.<Execute>b__0(DbContext c, TState s)
2023-06-02 09:37:00    at Pomelo.EntityFrameworkCore.MySql.Storage.Internal.MySqlExecutionStrategy.Execute[TState,TResult](TState state, Func`3 operation, Func`3 verifySucceeded)
2023-06-02 09:37:00    --- End of inner exception stack trace ---
2023-06-02 09:37:00    at Pomelo.EntityFrameworkCore.MySql.Storage.Internal.MySqlExecutionStrategy.Execute[TState,TResult](TState state, Func`3 operation, Func`3 verifySucceeded)
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, TState state, Func`2 operation, Func`2 verifySucceeded)
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.ExecutionStrategyExtensions.Execute[TState,TResult](IExecutionStrategy strategy, TState state, Func`2 operation)
2023-06-02 09:37:00    at Pomelo.EntityFrameworkCore.MySql.Storage.Internal.MySqlDatabaseCreator.Exists(Boolean retryOnNotExists)
2023-06-02 09:37:00    at Pomelo.EntityFrameworkCore.MySql.Storage.Internal.MySqlDatabaseCreator.Exists()
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.Migrations.HistoryRepository.Exists()
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.Migrations.HistoryRepository.GetAppliedMigrations()
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.GetAppliedMigrations(DatabaseFacade databaseFacade)
2023-06-02 09:37:00    at Microsoft.EntityFrameworkCore.RelationalDatabaseFacadeExtensions.GetPendingMigrations(DatabaseFacade databaseFacade)
2023-06-02 09:37:00    at ClothingShop.Models.IdentitySeedData.EnsurePopulated(IApplicationBuilder app) in /src/Models/IdentitySeedData.cs:line 16
2023-06-02 09:37:00    at System.Threading.Tasks.Task.<>c.<ThrowAsync>b__128_1(Object state)
2023-06-02 09:37:00    at System.Threading.QueueUserWorkItemCallbackDefaultContext.Execute()
2023-06-02 09:37:00    at System.Threading.ThreadPoolWorkQueue.Dispatch()
2023-06-02 09:37:00    at System.Threading.PortableThreadPool.WorkerThread.WorkerThreadStart()
2023-06-02 09:37:00    at System.Threading.Thread.StartCallback()

I have an asp.net core project and two databases. I am creating a docker compose file.
version: '3.8'

services:
  web:
    build: .
    ports:
      - ""8000:80""
    depends_on:
      - db1
      - db2

  db1:
    image: mysql:8.0.30
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: clothingshop1
    volumes:
      - ./db1:/docker-entrypoint-initdb.d
    ports:
      - ""3307:3306""

  db2:
    image: mysql:8.0.30
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: identity
    volumes:
      - ./db2:/docker-entrypoint-initdb.d
    ports:
      - ""3308:3306""

I am using generated file docker
#See https://aka.ms/customizecontainer to learn how to customize your debug container and how Visual Studio uses this Dockerfile to build your images for faster debugging.

FROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:6.0 AS build
WORKDIR /src
COPY [""ClothingShop.csproj"", "".""]
RUN dotnet restore ""./ClothingShop.csproj""
COPY . .
WORKDIR ""/src/.""
RUN dotnet build ""ClothingShop.csproj"" -c Release -o /app/build

FROM build AS publish
RUN dotnet publish ""ClothingShop.csproj"" -c Release -o /app/publish /p:UseAppHost=false

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT [""dotnet"", ""ClothingShop.dll""]

In programm.cs i have:
builder.Services.AddDbContext<StoreDbContext>(opts =>
{
    opts.UseMySql(builder.Configuration.GetConnectionString(""ClothingShopConnection""), new MySqlServerVersion(new Version()), b => b.EnableRetryOnFailure());
});

and
builder.Services.AddDbContext<AppIdentityDbContext>(options =>
 options.UseMySql(builder.Configuration.GetConnectionString(""IdentityConnection""), new MySqlServerVersion(new Version())));

In appsettings.json i have:
  ""AllowedHosts"": ""*"",
  ""ConnectionStrings"": {
    ""ClothingShopConnection"": ""server=db1;port=3307;user=root;password=root;Pooling=true;Max Pool Size=200;database=clothingshop1"",
    ""IdentityConnection"": ""server=db2;port=3308;user=root;password=root;database=identity""
  },

I run
docker-compose up -d in cmd. And i have this result Created container but asp won't start.
I try edit appsettings.json to:
    ""ClothingShopConnection"": ""server=localhost;port=3306;user=root;password=root;Pooling=true;Max Pool Size=200;database=clothingshop1"",
    ""IdentityConnection"": ""server=localhost;port=3306;user=root;password=root;database=identity""

    ""ClothingShopConnection"": ""server=localhost;port=3307;user=root;password=root;Pooling=true;Max Pool Size=200;database=clothingshop1"",
    ""IdentityConnection"": ""server=localhost;port=3308;user=root;password=root;database=identity""

    ""ClothingShopConnection"": ""server=db-1;port=3307;user=root;password=root;Pooling=true;Max Pool Size=200;database=clothingshop1"",
    ""IdentityConnection"": ""server=db-2;port=3308;user=root;password=root;database=identity""

but to no avail.
An interesting fact is that it mysql worckbanch connector sees these databases and makes it possible to connect to them through localhost: 3307 and 08, respectively.Successful connection. Db created correct.
I've looked at similar threads but haven't found an answer.
",76394801,"I solved my problem by configuring the connection string like this:
""ConnectionStrings"": {
""ClothingShopConnection"": ""server=dbClothingShop;port=3306;user=root;password=root;Pooling=true;Max Pool Size=200;database=clothingshop1"",
""IdentityConnection"": ""server=dbIdentity;port=3306;user=root;password=root;database=identity""
}

And apparently part of the problem was due to incorrect reading of tables from the database. Tables with capital letters were expected, but in the table they are with small letters (apparently something broke when importing and exporting the database).
I solved it with attributes DataAnnotation:
[Table(""size"")]
public class Size
{
   //SizeID...
}

Thanks a lot for the great questions @QiangFu
",
76396262,Plotly: auto resize height,"I am creating a bar graph using plotly express inside dash application. The graph is getting displayed but I am having an issue with height.Currently I am using default height and width.
Now for eg:

dataframe having field column contain 3 entires, the graph looks ok.

dataframe having field column contain 10 entires, the bar width is reduced auto and height remains the same and graph looks congested and hard to read.


figure = (
    px.bar(
        data_frame=dataframe,
        x=""size"",
        y=""field"",
        title=""Memory Usage"",
        text=""size"",
        # width=400,
        # height=400,
        orientation=""h"",
        labels={""size"": ""size in byte(s)""},
        template=template,
    ).update_traces(width=0.4)
    .update_layout(autosize=True)
)
        
dcc.Graph(id=""memory_bar"", figure=figure, className=""dbc"")

Is it possible depending on number of entires, the height can be auto-resized? Also I am using orient as horrizontal. I tried autosize=true but got no effect on height it remains same.
",76396484,"It is possible to define a dynamic width and height based on the dataframe:

The number of categories can be found with dataframe['field'].nunique() (assuming you are using pandas). It will impact the height of the figure (since the bar chart is horizontal)
The number of entries can be found with dataframe.shape[0] and will impact the width of the figure. You could be more precise if you use dataframe.groupby(""field"").count()[""size""].max() instead. It returns the maximum entry per category.

Then, we can define two methods for computing height and width of the figure:
def num_fields_based_height(num_fields: int) -> int:
    padding = 150 # arbitrary value depending on legends
    row_size = 100 # arbitrary value
    return padding + row_size * num_fields

def num_entries_based_width(num_entries: int) -> int:
    padding = 150 # arbitrary value depending on legends
    entry_size = 100 # arbitrary value
    return padding + entry_size * num_entries

Then call this methods when declaring the figure:
figure = (
    px.bar(
        data_frame=dataframe,
        x=""size"",
        y=""field"",
        title=""Memory Usage"",
        text=""size"",
        width=num_entries_based_width(dataframe.shape[0]),
        height=num_fields_based_height(dataframe['field'].nunique()),
        orientation=""h"",
        labels={""size"": ""size in byte(s)""},
    )
)

Now you need to find the right parameters (padding, entry_size, row_size) for your scenario.
",
76397539,Rendering a Polygon in Pyglet using GL_TRIANGLES - KeyError 'v' Error,"I am trying to render a polygon using the GL_TRIANGLES mode for pyglet.graphics.draw but have been running into issues.
I have been attempting to render it like I've seen people do in many other places
def draw(self):
    pyglet.graphics.draw(
       size=int(len(self.coords) / 2), 
       mode=pyglet.gl.GL_TRIANGLES,
       position=('v2f', self.coords),
    )

but have been running into the following error:
  File ""C:\Python311\Lib\site-packages\pyglet\graphics\__init__.py"", line 52, in draw
    gl_type = vertexdomain._gl_types[fmt[0]]
              ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^
KeyError: 'v'

Did the usage change? Not sure exactly what I'm doing wrong here.
",76397621,"v2f is the old format specification. In Pyglet Version 2.0, this has changed. See the documentation of draw(size, mode, **data). The first parameter is the number of vertices and each vertex has to have 3 components. The format is just f for float. e.g.:
self.coords = [x0, y0, z0, x1, y1, z1, ...]

pyglet.graphics.draw(
    size = len(self.coords) // 3,
    mode = pyglet.gl.GL_TRIANGLES,
    position = ('f', self.coords)
)

Note that this will draw only black triangles. Set the colors  attribute to draw a colorful shape.
Minimal example:

import pyglet

window = pyglet.window.Window(800, 600)

vertices = [100, 100, 0, 300, 100, 0, 200, 300, 0]
colors = [255, 0, 0, 255, 0, 255, 0, 255, 0, 0, 255, 255]

@window.event
def on_draw():
    window.clear()
    pyglet.graphics.draw(
        size = 3, 
        mode = pyglet.gl.GL_TRIANGLES, 
        position = ('f', vertices),
        colors = ('Bn', colors))
    
pyglet.app.run()

",
76394514,Switching between Tkinter screens with withdraw() method on button click,"Everything here works just fine but I haven't found a way to make it so that when I press the start button   it withdraws the Main Menu screen and displays the next one. It already creates the second screen when I run the code and it withdraws the MainMenu Screen before I even click the start button.
https://replit.com/@AbdullahKaran/ConfusedSophisticatedFolders#main.py
",76394813,"work on how you ask questions, as this can lead to your account being blocked. I've seen this happen before, so it's good advice.
Now, you can do this by using the tkraise() method of the Frame widget. To learn more, please visit this website: https://www.pythontutorial.net/tkinter/tkraise/
",
76396417,Dependent names vs non-dependent names,"In the following example:
template<typename T>
struct MyList {
    typedef std::list<T> type1;
};

template<typename T>
class MyList2 {
    typename MyList<T>::type1 type2;
};

I thought that both type1 and type2 are dependent name since both of their types depend on the template parameter. However, why is the first one considered non-dependent since I can use typedef with it?
",76396495,"A dependent name is, in C++ terms, a name whose grammatical properties are dependent on a template parameter.
Names can mean a lot of different things in C++, and the grammatical location where a name can be used depends on certain properties of that name. In order to understand what X x; is trying to do, the compiler needs to know what X is at this point in the program. If X names a type, then this is declaring a variable named x. The compiler doesn't need to know everything about X, but grammatically, in order to even begin to make sense of this text, the compiler does need to know that X here names a type.
std::list is the name of a class template; the compiler can easily see that because that's how it is declared. It doesn't need to have the body of the declaration; template<typename T> class list; is sufficient. std::list<T> is an instantiation of a class template, so just from the declaration, the compiler knows that this is the name of a type.
However, MyList<T>::type1 now requires more than just the declaration of the MyList template. It requires actually instantiating that template. And if T is itself a template parameter of the current code (as is the case for MyList2), then instantiation at the point of initially compiling the code is impossible. That instantiation must be delayed until MyList2 is itself given a concrete T to work with.
But the compiler still has to make sense of this code: typename MyList<T>::type1 type2;. And to do that, it needs some idea of what MyList<T>::type1 actually is. Without instantiating MyList<T>.
Which is why you have to tell it that it is a typename.
",
76397594,Get parameters from JavaThread* before a call takes place,"I am currently working on a c++ project that interacts with the JVM, more specifically I have a block of code that will run before some Java function ( lets call it funcABC ) runs, within this block of code I am able to read/write the registers and stack of the JVM.
I am able to get a JavaThread* ptr out of a register, and so far I have successfully been able to get important data from that JavaThread instance such as the JNIEnv and the thread state.
This is done by reconstructing the JavaThread structure in ReClass.NET, allowing me to access the variables stored within the structure.
I would like to also get the parameters that are being passed to funcABC, I am told that they are stored somewhere within the JavaThread structure, so far I have not been able to find them, I dont see anything within the jdk sources that would suggest where they might be.
Does anyone know how and where they are stored in the JavaThread?
As an alternative I tried getting the parameters directly from the stack, only to find that there is no consistent layout, the parameters will be in a random order, sometimes on the stack and sometimes within registers like rdi, r9, and r8, or both, their positions/order also change during runtime.
By printing all the registers and a large chunk of the stack I was able to find them, they are all there, but using them in this current state is beyond impractical unless there is some sort of field somewhere that specifics where everything is located in that particular call, but I cant find anything like that.
",76397628,"In the JVM, the method parameters are typically passed on the stack or stored in registers, depending on the platform's calling convention. However, the exact layout and location of these parameters can be complex and implementation-dependent. As you have observed, the layout may change during runtime, making it challenging to reliably extract the parameters directly from the stack or registers.
To access the parameters passed to funcABC, you may need to dig deeper into the JVM implementation and examine the bytecode interpretation and method invocation process. The JVM uses a bytecode instruction set, and the parameters are usually pushed onto the stack before the method is invoked. The JVM then retrieves the parameters from the stack and places them in appropriate locations, such as registers or stack frames, for the method to access.
Since you are already working with the JavaThread structure and have access to the JNIEnv and thread state, you might consider examining the thread's stack frames. The stack frames store local variables, including method parameters, and their layout can be platform-specific. You may need to navigate through the stack frames to locate the specific frame corresponding to funcABC and extract the parameters from there. However, keep in mind that accessing stack frames directly can be challenging and error-prone.
Another option is to leverage JVM debugging interfaces, such as the Java Debug Interface (JDI), which provides a higher-level abstraction for inspecting and manipulating JVM internals. The JDI allows you to programmatically interact with the JVM, inspect variables, and retrieve method arguments using a more convenient API.
I would recommend consulting JVM-specific documentation, such as the OpenJDK source code or documentation for the particular JVM implementation you are working with. Additionally, exploring JVM debugging and profiling tools might provide insights into how to access method parameters at runtime.
Keep in mind that manipulating JVM internals at such a low level can be fragile and might lead to unpredictable behavior. Exercise caution and thoroughly test your code to ensure it works reliably across different JVM versions and configurations.
",
76394498,"handling 401 in angular, how does pipe work really?","I am using Angular 14 and making a post request to my json server using the pattern like this:
myApiCall(data:any): Observable<any> {
  return this.http.post<any>(url, data).pipe(catchError(this.handleError));
}

( as specified in https://angular.io/guide/http)
And then wanted to add 401 (unauthorized) handling to handleError
so I added this block
  if (error.status===401){//UnAthorized
    this.router.navigate(['login']);
  }

to my handleError like so:
private handleError(error: HttpErrorResponse) {
  if (error.status===401){//UnAthorized
    this.router.navigate(['login']);
  }
...
  return throwError(
    () => new Error('Something bad happened; please try again later.')
  );

}
I can see in debugger that this.router.navigate(['login']); is hit, but it does not navigate to the login screen.  same code does work in another place.
The error in chrome console is :
ERROR TypeError: Cannot read properties of (has file and line see below for details) undefined (reading 'router')
    at handleError (line where this.router.navigate(['login']); is)
    at catchError.js:10:39
    at OperatorSubscriber._error (OperatorSubscriber.js:23:21)
    ...

The top in call stack in the error is the .subscribe after myApiCall...
So what happens to handleError when I call
this.router.navigate(['login']);

Does it cause a return? And if so what gets returned ?
UPDATE
After Igor's suggestion of adding
return of([]) 

AND declaring handleError with ()=>{} syntax instead of normal, it worked.
I wonder why the second part is needed. I still suspect it has something to do with how pipe works.
This works:
private handleError = (error: HttpErrorResponse) => { 
  if (error.status===401)//UnAthorized
  {
    this.router.navigate(['login']);
    return of([]);
  } 
  ...
  return throwError(
    () => new Error('Something bad happened; please try again later.')
  );  
}

This does not
private handleError(error: HttpErrorResponse){ 
  if (error.status===401)//UnAthorized
  {
    this.router.navigate(['login']);
    return of([]);
  } 
  ...
  return throwError(
    () => new Error('Something bad happened; please try again later.')
  );  
}

After this.router.navigate instead of hitting return, control is transferred to the caller that results in an error that mentions router.
Why?
SECOND UPDATE:
Actually the first part (return of...) is not necessary to make it work, only the second (making it a lambda expression) is enough.
",76394825,"Try this:
myApiCall(data:any): Observable<any> {
  return this.http.post<any>(url, data).pipe(
    catchError((err) =>
        if (err.status===401){
          // Redirect if unhautorized
          this.router.navigate(['login']);
        }
        ...
    );
}

Note the lambda syntax of the error handler declaration. It is required in order to pass in the context (this). Without it the 'router' is not known hence the error. More details here
",
76396165,How to create a spatial gridlines using Latitude and Longitude in R,"How to create a gridline of 7*7 sqkm using Latitude and Longitude values. These values should be the centroid value of a single square in the grid. I am not sure if I am doing it in the right way. I tried st_make_grid from sf (Simple Features) library but that shows me an empty plot.
MyGrid <- st_make_grid(DF, cellsize = c(0.07, 0.07), square = TRUE, crs = 4326) 

Below is my example DF
DF <- structure(list(lat = c(43.25724, 43.25724, 43.25724, 43.25616, 
43.25616, 43.25616), lon = c(-96.01955, -95.98172, -95.92336, 
-96.40973, -96.25733, -96.17735)), class = ""data.frame"", row.names = c(NA, 
6L))

## > DF
##        lat       lon
## 1 43.25724 -96.01955
## 2 43.25724 -95.98172
## 3 43.25724 -95.92336
## 4 43.25616 -96.40973
## 5 43.25616 -96.25733
## 6 43.25616 -96.17735

Thanks
",76396510,"from the documentation of st_make_grid:

Create a square or hexagonal grid covering the
bounding box of the geometry of an sf or sfc object


so you need to convert your dataframe of point coordinates to an sf-object ""the_points"" (and reproject to a projection accepting metric length units):

library(sf)

the_points <- 
    st_sf(geometry = DF[c('lon', 'lat')] |>
              as.matrix() |>
              st_multipoint() |>
              st_sfc() |>
              st_cast('POINT'),
          crs = 4326 ## geographic data (in degrees)
          ) |>
    ## convert to projected coordinates (to specify dimensions in m
    ## take Google Mercator as first guess (EPSG-code 3857)
    st_transform(3857)


create grid (note that your points have only about 100 m latitudinal range):

the_grid <- 
    st_make_grid(n = c(10, 1), cellsize = 7e3 ## 7000 km)


inspect result:

plot(the_grid)
plot(the_points, add = TRUE)


",
76397550,incorrect array index being rendered using react tsx upon deletion of an index,"basically I'm creating a flashcards kind of application, where you can either go through the flashcards (which is an array) or you can edit them. in the editing phase, 2 input boxes get rendered with a button next to them where you can edit the text or you can delete that entire index. the problem lies with deletion, once the delete button is clicked, in the the correct index is deleted in the array which I verified using console logs, but in the UI the bottom index gets removed. so like lets say I have an array with 4 indexes, and i delete the 2nd, the UI will display 1 2 3, but the array it self will have 1 3 4. not really sure how to fix it but here is my code:
const [flashcardsState, setFlashcardsState] = useState(flashcards);

const handleDeleteClick = (index: number) => {
        const updatedFlashcards = [...flashcardsState];
        updatedFlashcards.splice(index, 1);
        setFlashcardsState(updatedFlashcards);
}
const handleCardChange = (cardIndex: number, fieldIndex: number, newValue: string) => {
        const updatedFlashcards = [...flashcardsState];
        updatedFlashcards[cardIndex][fieldIndex] = newValue;
        setFlashcardsState(updatedFlashcards);
};

const renderEdit = () =>{
        return (
            <div className=""card-container"">
                <div className=""inputcardstext"">
                    <h4>Front</h4>
                    <h4>Back</h4>
                </div>
                {flashcardsState.map((flashcardsState, index) => (
                    <div key={index}>
                        <div className=""inputcards"">
                            <input
                                type=""text""
                                className='front-back'
                                defaultValue={flashcardsState[0]}
                                onChange={(e) => handleCardChange(index, 0, e.target.value)}

                            />
                            <input
                                type=""text""
                                className='front-back'
                                defaultValue={flashcardsState[1]}
                                onChange={(e) => handleCardChange(index, 1, e.target.value)}
                            />
                            <button onClick={() => handleDeleteClick(index)}></button>
                        </div>
                    </div>
                ))}
            </div>
        );
}

",76397649,"It's deleting the right element, your issue is that you're using the array index as they key in React (<div key={index}>), which is a no-no. You're seeing this issue because of a combination of the array key as an index and defaultValue.
TL;DR chose a different key, like
  const flashcards = [
    [""key"", ""a"", ""1""],
    [""key"", ""b"", ""2""],
    [""key"", ""c"", ""3""]
  ];
  const [flashcardsState, setFlashcardsState] = useState<string[][]>(
    flashcards
  );

  //...

        {flashcardsState.map((flashcardsState, index) => (
          <div key={flashcardsState[2]}>
            ...
          </div>
        ))}

See this example on Codepen.
Or, you can see your removal is working correctly if you change defaultValue to value (I explain why below).
You should also install ESLint which will warn you against using an array key as an index.
What's happening is you've told React each element's unique identifier is the array index. The first element in your array has index 0. React says ""ok, I'll render element with id 0, and I'll remember what I rendered, to compare it later.""
Then you remove element at index 0, but your array still has an element at index 0. So you give your new array back to React. React then says:

""Ok I have the same element here with id 0, so I only need to change it if the new render of id 0 is different from what I already have
React re-renders element with ""id"" 0, and even though it's a different element in the array, the render output is identical. This is because defaultValue is only set on first render, not on subsequent ones.
React says ""ok, nothing in id 0 has changed, I'll move on and not update the DOM
React gets to ""id"" of 2, the last element in the array, and sees ""ah, there is no longer an id of 2, I shall remove the last component in the list for you.

",
76395825,Efficient way to change the class of several matrices in R,"I have several matrices and I would like to apply something like
class(matrix) <- ""numeric"" to all of them at once, i.e. the class of all matrices should be changed to numeric.
Do you know how to do this?
dput(matrix[1:3,]) results in structure(c(285.789361223578, 282.564165145159, 273.633228540421,  256.789452806115, 260.808130130172, 241.718192100525, 266.765343174338,  267.881099879742, 250.710165724158, 284.365977942944, 281.670583188534,  268.735618144274, 264.118778035045, 262.856532484293, 254.31867428124,  286.250801086426, 284.585711210966, 268.984649181366, 286.17267370224,  284.429456442595, 267.478255555034, 275.10055847466, 274.141056537628,  259.477523118258, 246.454664766788, 252.470473349094, 232.699362188578,  284.998321458697, 283.73363442719, 269.555955678225, 0, 0, 0), dim = c(3L,  11L), dimnames = list(NULL, c("""", """", """", """", """", """", """", """",  """", """", ""vec"")))
",76396539,"In this examples all matrix variables of the current environment are converted to numeric.
See the warning in the case where matrix cannot be converted to numeric.
var1 <- matrix(1:10, 5, 2)
var2 <- matrix(as.character(5:13), 3,3)
var3 <- letters[1:5]
var4 <- matrix(letters[1])

print(sapply(mget(ls()), typeof))
#>        var1        var2        var3        var4 
#>   ""integer"" ""character"" ""character"" ""character""

for (i in ls()[sapply(mget(ls()), is.matrix)])
  assign(i, as.numeric(get(i)))
#> Warning in assign(i, as.numeric(get(i))): NAs introduced by coercion

print(sapply(mget(ls()), typeof))
#>           i        var1        var2        var3        var4 
#> ""character""    ""double""    ""double"" ""character""    ""double""

Created on 2023-06-03 with reprex v2.0.2
",
76396002,Authentication between Azure Web PubSub and server,"There is a simple ASP.NET app as an event handler Server:
WebApplicationBuilder builder = WebApplication.CreateBuilder(args);
builder.Services
    .AddWebPubSub(options => options.ServiceEndpoint = new ServiceEndpoint(WebPubSubConnectionString))
    .AddWebPubSubServiceClient<WebPubSubHub>();

WebApplication app = builder.Build();

app.MapWebPubSubHub<WebPubSubHub>(""/eventhandler"");
app.Run();

The Azure Web PubSub has the following configuration of Webhook URL:

I need to protect this endpoint somehow /eventhandler because it's public and anyone can call it. One of the options that Azure suggests is using a simple authentication code.

Help me to understand where I should verify that code in my ASP.NET app?
Let's say I configured the URL template in WPS like
https://a9e5-92-253-212-316.ngrok-free.app/eventhandler?code=RRRRR

then in the server code
app.MapWebPubSubHub<WebPubSubHub>(""/eventhandler?code=RRRRR"")

result is exception

Microsoft.AspNetCore.Routing.Patterns.RoutePatternException: 'The
literal section 'eventhandler?code=RRRRR' is invalid. Literal sections
cannot contain the '?' character.'

",76396544,"One possible way is to use AddEndpointFilter
app.MapWebPubSubHub<WebPubSubHub>(""/eventhandler"").AddEndpointFilter(new ApiKeyFilter(builder.Configuration));

the implementation could look like this:
public class ApiKeyFilter : IEndpointFilter
{
    private readonly string _apiKey;

    public ApiKeyFilter(IConfiguration configuration)
    {
        _apiKey = configuration.GetValue<string>(""ApiKey"");
    }

    public async ValueTask<object> InvokeAsync(EndpointFilterInvocationContext context, EndpointFilterDelegate next)
    {
        if (!context.HttpContext.Request.Query.TryGetValue(""code"", out var extractedApiKey))
        {
            return Results.Unauthorized();
        }

        if (!_apiKey.Equals(extractedApiKey))
        {
            return Results.Unauthorized();
        }

        return await next(context);
    }
}

another option is to use
app.MapWebPubSubHub<WebPubSubHub>`(""/eventhandler"").RequireAuthorization(builder => builder.AddRequirements(new ApiKeyRequirement))

where ApiKeyRequirement is your implementation of IAuthorizationRequirement
",
76394476,How to convert milliseconds to seconds units and display them in textView?,"How to convert milliseconds to seconds units?
For example the seekBar values are from 100 to 1000 and I want to display the units if they are under 1000(second) as 0.1 0.2 0.3....0.9 then 1
private var counter: Long = 0

    @RequiresApi(Build.VERSION_CODES.O)
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        val textView = findViewById<TextView>(R.id.textView)
        val seek = findViewById<SeekBar>(R.id.seekBar)
        seek.min = 100
        seek.max = 1000
        seek.progress = 500
        counter = seek.progress.toLong()

        seek.setOnSeekBarChangeListener(
            object : SeekBar.OnSeekBarChangeListener {
                override fun onProgressChanged(
                    seekBar: SeekBar?,
                    progress: Int,
                    fromUser: Boolean
                ) {
                    counter = progress.toLong()
                    textView.text = (counter/1000).toString()

The line:
textView.text = (counter/1000).toString()
make it that if the bar is on the right side it will display 1 but then to the left it will display only 0.
and I want that on the right end it will display 1 but then when moving it to the left it will display 0.9 then 0.8 then 0.7.....0.1
I tried:
textView.text = (counter/1000).toString()
",76394841,"Change the data type of counter from Long to Float or Double. Long only allows integer values.
private var counter: Float = 0.0F
...
counter = seek.progress.toFloat()

",
76396150,Unbound module Sqlite3,"I am trying to learn some OCaml, I created a very simple example to open and then close a sqlite file.
let () =
  let db = Sqlite3.db_open ""test.db"" in
  Sqlite3.db_close db

I did an opam install sqlite3 and I see the .opam/default/lib/sqlite3 files in place. I would expect this means that the package is installed.
When I run ocaml sqltest.ml I would expect this to execute. Instead I get a Error: Unbound module Sqlite3
Even when I make it ocaml -I +sqlite3 sqltest.ml I get the same result. It's kind of hard to search for info on ocaml and most of the stuff I find is using dune or ocamlbuild or ocamlfind or something... I think that's fine, but in the interest of starting with the basics and building from there I would like to understand how all of the pieces are working. I'd imagine the basic ocaml or ocamlc should work in this simple case.
How do I make it understand where these libs are and use them? Why does it not understand it's own ""default libs""?
",76396550,"Using ocamlfind to compile a trivial demonstration program that opens the Sqlite3 module.
$ cat test.ml
open Sqlite3

let () = print_endline ""hello""
$ ocamlc test.ml
File ""test.ml"", line 1, characters 5-12:
1 | open Sqlite3
         ^^^^^^^
Error: Unbound module Sqlite3
$ ocamlfind ocamlc test.ml -package sqlite3 -linkpkg
$ ./a.out
hello

Alternative use of ocamlfind:
$ ocamlc -I `ocamlfind query sqlite3` sqlite3.cma test.ml
$ ./a.out
hello

As a side note, your code will not compile even with proper use of the compiler and ocamlfind because Sqlite3.db_close returns a boolean, you're trying to bind that expression to (). This is a type mismatch.
And you've opened Sqlite3 so you don't need to use the fully qualified names. You might want to use let& to avoid the db_close.
open Sqlite3

let () =
  let& db = db_open ""test.db"" in
  print_endline ""hello""

",
76397627,ggplot - reorder_within - Order month,"I am plotting a bar graph. However, the axes x show in different month order. Using the command reoder_within, doesn't work for the purpose.
Follow below the ME.
ggplot(de, aes(fill=Cidade, y = Leitura  , x = Mes ))+geom_bar(position='dodge', stat='identity')

Generate the follow plot:
Plot
My purpose is modify the axes x to: Jan, Fev, March .....
The set of data is:
  Cidade    Mes       Leitura
  <chr>     <chr>       <dbl>
1 Petrolina Janeiro     74.2 
2 Petrolina Fevereiro   73.2 
3 Petrolina Março       68.7 
4 Petrolina Abril       42.9 
5 Petrolina Maio         9.84
6 Petrolina Junho        8.02

",76397651,"We can use fct_inorder here:
ggplot will order x axis alphapetically. To get the order in your table use fct_inorder:
library(ggplot2)
library(forcats)
library(dplyr)

de %>% 
  mutate(Mes = fct_inorder(Mes)) %>% 
  ggplot(aes(fill = Cidade, y = Leitura, x = Mes)) +
  geom_bar(position = 'dodge', stat = 'identity')


",
76396027,How to define a command alias in PowerShell for filtering output?,"I need to define an alias for this:
Select-String -NotMatch -Pattern ""^[\t ]+\d""
so that I can use the alias instead of writing that long string each time.
After googling for 5 minutes and doing some experiments I came up with this:
filter foo {
    $_ | Select-String -NotMatch -Pattern ""^[\t ]+\d""
}

So now my script looks like this:
command1 | foo
command2 | foo
command3 | foo
command4 | foo

This is apparently working as expected, but I'm concerned about the efficiency implications of doing this.
Is the foo filter acting as a transparent alias of the longer command line, or is it creating an entire new pipe or buffer or something?
",76396561,"
Is the foo filter acting as a transparent alias of the longer command line, or is it creating an entire new pipe or buffer or something?

The latter, your current implementation is invoking Select-String per pipeline input object instead of invoking it once and processing all input. If you care about performance you should change your implementation for a steppable pipeline:
function steppablefoo {
    param([Parameter(ValueFromPipeline)] $InputObject)

    begin {
        $pipe = { Select-String -NotMatch -Pattern '^[\t ]+\d' }.GetSteppablePipeline()
        $pipe.Begin($PSCmdlet)
    }
    process {
        $pipe.Process($InputObject)
    }
    end {
        $pipe.End()
    }
}

You can test it for yourself with this performance comparison:
$tests = @{
    Filter = {
        0..300kb | foo
    }
    SteppablePipeline = {
        0..300kb | steppablefoo
    }
}

$tests.GetEnumerator() | ForEach-Object {
    [pscustomobject]@{
        Test              = $_.Key
        TotalMilliseconds = (Measure-Command { & $_.Value }).TotalMilliseconds
    }
}

",
76396418,Why do I receive an access violation error in my SFML C++ project when passing object and function pointers?,"I'm currently learning c++ and I'm working on my first game using SFML/TGUI. I have tried making a function that creates a button, which in turn calls on a function when pressed. In an effort to make the button creator smarter and more versitile, I have made it so that it takes an object pointer and a function pointer as parameters. When the button then gets pressed, I want it to be able to call a member function from the object that was passed to it. It should also be noted that I got it working for a while, but then I changed something and now it doesn't work.
In my main.cpp, I create a gui and a pointer to this gui. I then pass the pointer into my Utility class, which consists of static variables and functions:
//Create gui object
tgui::GuiSFML gui{ window };

//Create pointer to gui object
tgui::GuiSFML* guiPointer = &gui;

//Pass guiPointer into Utility class
Utility::setup_getGuiPointer(guiPointer);

This is what is in my Utility class:
//Static gui pointer in Utility
static tgui::GuiSFML* guiPointer;

//Passes gui pointer from main into Utility
static void Utility::setup_getGuiPointer(tgui::GuiSFML* passedGuiPointer)
{
    guiPointer = passedGuiPointer;
}

The create button function in my utility class looks like this,
//Takes arguments: ObjectPtr, FunctionPtr, Size, Pos and Text to create button
template<typename Class>
inline tgui::Button::Ptr& Utility::u_createButton(Class* object, void(Class::* functionPointer)(tgui::GuiSFML* guiPointer, tgui::Button::Ptr& button), tgui::Layout2d size, tgui::Layout2d position, std::string buttonText)
{
        //Creates the button
    tgui::Button::Ptr button = tgui::Button::create(buttonText);

        //Sets size and position
    button->setSize(size);
    button->setPosition(position);

        //Adds button to gui
    guiPointer->add(button);

        //Sets onPress on the button to call the passed function from the passed objects,
        //passing the gui pointer and button pointer into this function.
    button->onPress([&]() { (object->*functionPointer)(guiPointer, button); });

        //Returns button pointer if it's needed before onPress.
    return button;
}

Now, I can then create a custom button from inside any class using the following line:
Utility::u_createButton(this, &ClassName::FunctionName, size, position, text;

And I create a button inside one of my classes, and the function that I pass a pointer to is just:
void ClassName::FunctionName(tgui::GuiSFML* guiPointer, tgui::Button::Ptr& button)
{
    std::cout << ""Button Pressed"" << std::endl;

        //The reson behind the passed arguments is so that the function itself can delete
        //the button after use if needed using the following line:
    //guiPointer->remove(button);
}

When I run this, it creates a button but gives the following error when I press it:
Unhandled exception at 0x85EEB600 in GUI_SFML_Template.exe: 0xC0000005: Access violation executing location 0x00000000.

The current stack frame was not found in a loaded module. Source cannot be shown for this location.

I don't understand why I get this error, though I believe it has something to do with my objects and functions trying to write onto the gui without the proper access to it. I would be very grateful if anybody could help me.
",76396580,"Your lambda is capturing by reference, all the local variables it captures references to will cause undefined behaviour when the button is pressed as the local variables will have gone out of scope. You should capture by value instead:
button->onPress([object, functionPointer, guiPointer, button]() { (object->*functionPointer)(guiPointer, button); });

Generally lambdas capturing all by reference are only safe to use within the function they're declared in and shouldn't be stored for long term use.
",
76394772,Remove whitespace from an array of string,"Remove whitespace from an array of string.
While creating an array of string from a sentence, I'm encountering multiple spaces and need to remove them in order to create a new reverse sentence from the given sentence. How do I remove extra spaces in the middle of two words?
Example: a good   example
How do I remove two spaces in between good and example ?
Here is the code I'm using:
public class reverseString {
    public static void main(String[] args) {
        String s = ""a good   example"";
        String ans = revString(s);
        System.out.println(ans);
    }
    static String revString(String s) {
        String[] arr = s.split("" "");
        List<String> list = new ArrayList<String>();
        StringBuilder ans = new StringBuilder();
        for (int i = arr.length - 1; i >= 0; i--) {
            if (arr[i] == null) {
                continue;
            } else {
                ans.append(arr[i].trim());
                // ans.append("" "");
            }
        }
        String is = ans.toString();
        return is.trim();
    }
}

The expected output was: example good a
The output I'm getting is: example   good a
Comparing the string at the I-th index with "" "" should result in continuation of the loop. Instead it jumps to the else condition.
",76394849,"The parameter to method split (of class java.lang.String) is a regular expression. Just add a + (i.e. ""plus"" symbol – which means one or more) to the value of the parameter.
import java.util.ArrayList;
import java.util.List;

public class ReverseString {

    public static void main(String[] args) {
        String s = ""a good   example"";
        String ans = revString(s);
        System.out.println(ans);

    }

    static String revString(String s) {
        String[] arr = s.split("" +""); // CHANGE HERE - added '+'
        List<String> list = new ArrayList<String>();
        StringBuilder ans = new StringBuilder();
        for (int i = arr.length - 1; i >= 0; i--) {
            if (arr[i] == null) {
                continue;
            }
            else {
                ans.append(arr[i].trim());
                ans.append("" "");
            }
        }
        String is = ans.toString();
        return is.trim();
    }
}

Running the above code produces the following output:
example good a

",
76381579,Is there a way to listen for changes in p-columnFilter match options and call a function in Angular and PrimeNG?,"Is it possible to bind a function that gets called every time user chooses between matchOption of p-columnFilter.
<p-columnFilter type=""date"">
...
</p-columnFilter>


example: https://stackblitz.com/edit/owuvzd?file=src%2Fapp%2Fdemo%2Ftable-filter-menu-demo.html
",76394860,"I was able to find a workaround to this, by creating my own matchModeOptions and their filter functions.
To do this I made use of FilterService
public dateIsFilter: string = ""is-date"";

public matchModeOptions: any[] = [
  { label: ""Date is"", value: this.dateIsFilter }
]

constructor(private filterService: FilterService){
}

ngOnInit(): void {
  this.filterService.register(this.dateIsFilter, (value: any, filter: any) => {
  // code here
  return this.filterService.filters.dateIs(value, filter)
});
}

Then I changed p-columnFilter to use custom matchModeOptions.
<p-columnFilter type=""date"" [matchModeOptions]=""this.matchModeOptions"">
...
</p-columnFilter>


",
76397575,Laravel issue creating new user,"I want to fix an error regarding an unassigned variable, but I don't know where to define it:

Use of unassigned variable '$userInfo'PHP(PHP1412)

The error is occuring in this line $accessToken = $userInfo->createToken(uniqid())->plainTextToken;.
This is my code:
public function createUser(Request $request)
{
    try {
        // Validate

        $validateUser = Validator::make($request->all(), [
            'avatar' => 'required',
            'type' => 'required',
            'open_id' => 'required',
            'name' => 'required',
            'email' => 'required|email|unique:users,email',
        ]);

        if ($validateUser->fails()) {
            return response()->json([
                'status' => false,
                'message' => 'validation error',
                'errors' => $validateUser->errors()
            ], 401);
        }

        // validated, will have all user field values
        // we can save in the database
        $validated = $validateUser->validated();

        $map = [];
        // email, phone, facebook, apple
        $map['type'] = $validated['type'];
        $map['open_id'] = $validated['open_id']; // if there is user in our database
        $user = User::where($map)->first();

        $user = User::where($map)->first();

        // whether user have already logged in or not
        // empty means does not exist
        // this token is user id
        if (empty($user->id)) {
            // this certain user has never been in our website
            // our job is to assign the user to database
            // this token is user id
            $validated[""token""] = md5(uniqid().rand(10000,99999));
            // user first time created
            $validated['created_at'] = Carbon::now();
            // returns the id of row after saving
            $userID = User::insertGetId($validated); // we return the userid
            // users all info will be here
            $userInfo = User::where(""id"", ""="", $userID)->first();

            $accessToken = $userInfo->createToken(uniqid())->plainTextToken;

            $userInfo->access_token = $accessToken;

            return response()->json([
                'status' => true,
                'message' => 'User Created Successfully',
                'data' => $userInfo
            ], 200);
        }

        $accessToken = $userInfo->createToken(uniqid())->plainTextToken;
        $userInfo->access_token = $accessToken;
        return response()->json([
            'status' => true,
            'message' => 'User Logged In Successfully',
            'token' => $userInfo
        ], 200);
    } catch (\Throwable $th) {
        return response()->json([
            'status' => false,
            'message' => $th->getMessage()
        ], 500);
    }
}

",76397661,"The variable $userInfo is not defined before the line where the error is occurring. To fix this issue, you need to define $userInfo before using it. Replace the variable $userInfo with $user in the last part of the code.
public function createUser(Request $request)
{
    try {
        // Validated
        $validateUser = Validator::make($request->all(), [
            'avatar' => 'required',
            'type' => 'required',
            'open_id' => 'required',
            'name' => 'required',
            'email' => 'required|email|unique:users,email',
        ]);

        if ($validateUser->fails()) {
            return response()->json([
                'status' => false,
                'message' => 'validation error',
                'errors' => $validateUser->errors()
            ], 401);
        }

        // Validated, will have all user field values
        // We can save in the database
        $validated = $validateUser->validated();

        $map = [];
        $map['type'] = $validated['type'];
        $map['open_id'] = $validated['open_id'];
        $user = User::where($map)->first();

        // Whether user has already logged in or not
        // Empty means does not exist
        // This token is user id
        if (empty($user->id)) {
            // This certain user has never been on our website
            // Our job is to assign the user to the database
            // This token is user id
            $validated[""token""] = md5(uniqid() . rand(10000, 99999));
            // User first time created
            $validated['created_at'] = Carbon::now();
            // Returns the id of row after saving
            $userID = User::insertGetId($validated);
            // Users all info will be here
            $userInfo = User::where(""id"", ""="", $userID)->first();

            $accessToken = $userInfo->createToken(uniqid())->plainTextToken;
            $userInfo->access_token = $accessToken;

            return response()->json([
                'status' => true,
                'message' => 'User Created Successfully',
                'data' => $userInfo
            ], 200);
        }

        $accessToken = $user->createToken(uniqid())->plainTextToken;
        $user->access_token = $accessToken;

        return response()->json([
            'status' => true,
            'message' => 'User Logged In Successfully',
            'token' => $user
        ], 200);

    } catch (\Throwable $th) {
        return response()->json([
            'status' => false,
            'message' => $th->getMessage()
        ], 500);
    }
}

",
76394425,How can I convert an array with key-value pairs to an array with separate sub-arrays for the keys and values in PHP?,"Converting an array [fields => values] to one [fields, values]
If I have an array like this:
[
field1 => value1,
field2 => value2,
field3 => value3
];

and I want to convert it to an array like this:
[
[field1, field2, field3],
[value1, value2, value3]
];

now I just took the keys and values and put them in another array:
[
array_keys($array),
array_values($array)
];

Is there a more elegant way to do this?
",76394893,"Before considering ""elegant"", first consider output consistency.  If you always want to create an array with two first level elements, your approach is suitable.
However, if you want an empty output array when your input array is empty, you'll need a different approach where the result array is only deepened when necessary such as...
Code: (Demo)
$result = [];
foreach ($array as $k => $v) {
    $result[0][] = $k;
    $result[1][] = $v;
}
var_export($result);


If you always want a two-element result, a body-less loop can avoid making any function calls in a concise way. (Demo)
$result = [[], []];
foreach ($array as $result[0][] => $result[1][]);
var_export($result);


Functional programming with array_reduce() can become ugly/verbose for this task. (Demo)
var_export(
    array_reduce(
        array_keys($array),
        function ($result, $k) use ($array) {
            $result[0][] = $k;
            $result[1][] = $array[$k];
            return $result;
        },
        []
    )
);

And if you are considering array_walk(), that is just going to work like a foreach() anyhow because it doesn't deliver its result payload as a returned value.

If you consider functional programming to be elegant, bear in mind that double-transposing with array-map() calls will not work when there is one element or less in your input array. Proof
",
76396496,How to have child div not trigger hover effect of parent div?,"I am trying to find out how to have a child element that has position: absolute that is positioned outside of its parent element that does not trigger the parent's :hover effect.
Since the parent has a hover effect, the child elements will  trigger the parent element, even though that child is outside of the parent element's boundary.
Is there an attribute I am missing, or is this just the way inheritance in HTML works?
Picture Example:

In this image, my mouse cursor is inside the child div, but outside of the parent div.


body {
  display: flex;
  align-items: center;
  justify-content: center;
}

.container {
  background-color: black;
  width: 800px;
  aspect-ratio: 1 / 1;
}

.container:hover {
  background-color: darkorange;
}

.child {
  position: absolute;
  background-color: red;
  width: 100px;
  height: 100px;
  transform: translate(-50px, 0px);
}
<div class=""container"">
  <div class=""child""></div>
</div>



",76396581,"You can use following solutions for this.

You can use pointer-events: none on the child element. But remember that this will block all types of pointer events on that, and not just hover event. So any sort of click events will also not work on that child element.

Another option is to use :has() method in the css. :has() allows you to target an element that meets the conditions passed to it. You can do it like this -


  .container:not(:has(.child:hover)):hover {
    background-color: darkorange;
  }
        

This will prevent the hover effect on container when hovered over its child element, which is specified in the :has() method.
Here is the JSFiddle example demonstrating this method.
Read more about :has() in here
",
76392269,Executing RXJS Functions in Sequence Defined By Array,"I am attempting to execute multiple functions consecutively by defining them in an array (specifically for an Angular APP_INITIALIZER function).
I have this array:
const obsArray = [
    myService1.init(),
    myService2.init(),
    ...
    myServiceN.init()
]

Each of these init() methods returns Observable<void>. Here's an example:
    init() : Observable<void> {
         return this.http.get(this.apiUrl)
             .pipe(
                 // do something with the response
                 switchMap(() => EMPTY)
             );
    }

The ``switchMapstatement ensures that the return type isObservable```.
I've tried this:
    const init$ : Observable<void> = forkJoin(obsArray);

But at best it executes the function but not the HTTP call inside. There is no subscribe() call since this is inside an Angular factory function assigned to the APP_INITIALIZER token.
I've also tried concatMap() with no luck.
Is there an rxjs function that will execute each of these consecutively, while waiting for the previous to complete?
I have followed this up with this question.
",76397689,"You can also put all the observables from the array into an observable, essentially converting Array<Observable<T>> to Observable<Observable<T>> and then use the higher order function concatAll:
from(obsArray).pipe(concatAll())

Here's a live demo.
",
76396395,"How can I extract text from single quotes, even if the text itself contains single quotes, using regex in Python?","I'm trying to extract data from a .txt file and while my regex did work for the most part, it fails when it comes across single quotes within the text I'm trying to extract.
{'pro_id':'1692423', 'pro_model':'SKUF42051', 'pro_category':'accessories', 'pro_name':'Gants tactiques Escalade en plein air Gants antidérapants résistants à l'usure Formation Gants de moto d'équitation', 'pro_current_price':'27.99', 'pro_raw_price':'27.99', 'pro_discount':'36', 'pro_likes_count':'11'}

This is what my text in the .txt file looks like.
I'm looping through and creating dicts from them. I do that by extracting the content from within the single quotes and appending the ""key"" and ""value"" pairs to a dict.
I've first extracted the content from within the curly brackets, then split that at "", "" to get the ""items"" in a list, after which I looped through the list and used the regex in the command key, value = re.findall(r""\'([^']+)\'"", element) to extract the ""key"" and ""value"".
I'm a regex as well as a programming novice, so I could use some expert help.
I did ask ChatGPT for a regex '([^']+(?:\\'[^']+)*?)':'([^']+(?:\\'[^']+)*?)' but that fails too.
I want to get a list that holds
['pro_name', 'Gants tactiques Escalade en plein air Gants antidérapants résistants à l'usure Formation Gants de moto d'équitation'] from re.findall
but instead I get
['Gants tactiques Escalade en plein air Gants antidérapants résistants à l', 'équitation'].
",76396624,"Your string is malformed. Strings containing literal single quotes should be enclosed in double quotes, else it can't be parsed correctly.
It is extremely difficult to use regex to sort this out, and also by using a for loop.
But I have discovered a way, I have found simple patterns. Since all strings are enclosed in single quotes, and the key value pairs are separated by commas followed by a space, and the keys are separated from values by single colons, it is easy to identify key value pairs by first split the string by ""', '"", then split each substring by ""':'"".
You can then convert it to dict, with cleanup if necessary.
Example:
import re

text = ""{'pro_id':'1692423', 'pro_model':'SKUF42051', 'pro_category':'accessories', 'pro_name':'Gants tactiques Escalade en plein air Gants antidérapants résistants à l'usure Formation Gants de moto d'équitation', 'pro_current_price':'27.99', 'pro_raw_price':'27.99', 'pro_discount':'36', 'pro_likes_count':'11'}""
arr = [i.split(""':'"") for i in text.split(""', '"")]
def clean(s):
    return re.sub(""^[{']+|[}']+$"", '', s)

{clean(a): clean(b) for a, b in arr}

The result is:
{'pro_id': '1692423',
 'pro_model': 'SKUF42051',
 'pro_category': 'accessories',
 'pro_name': ""Gants tactiques Escalade en plein air Gants antidérapants résistants à l'usure Formation Gants de moto d'équitation"",
 'pro_current_price': '27.99',
 'pro_raw_price': '27.99',
 'pro_discount': '36',
 'pro_likes_count': '11'}

Wrap it in a function:
def dictify(text):
    arr = [i.split(""':'"") for i in text.split(""', '"")]
    return {clean(a): clean(b) for a, b in arr}

I assume you have many more strings like the above in your text file, since I don't know the exact format, I can only demonstrate how to convert the file to a list of dicts as if it is newline separated.
with open('/path/to/file', 'r') as f:
   text = f.read()
[dictify(row) for row in text.split('\n')]

You need to change the file path placeholder to the actual path. The above won't work if your file isn't newline separated.
And my method won't work if your string deviates from the format, for example if there are spaces after the key-value delimiting colons, or there aren't spaces after the commas that separate key-value pairs.
If that is the case I cannot help you, you need to figure out a different method, but my example does work on the example you have given.
",
76394871,Cubic-bezier fixes exiting animation but breaks entering animation,"I'm trying to animate a collapsing list with React/Joy-UI
Here is my Transition element
     <Transition nodeRef={nodeRef} in={browseOpen} timeout={1000}>
      {(state: string) => (<List
        aria-labelledby=""nav-list-browse""
        sx={{
          '& .JoyListItemButton-root': { p: '8px' },
          transition: '1000ms',
          transitionProperty: 'max-height',
          overflow: 'hidden',
          ...{
            exiting: { maxHeight: '0px'},
            exited: { maxHeight: '0px'},
            entering: { maxHeight: '500px'},
            entered: { maxHeight: '500px'},
          }[state],
        }}
      >

With this the list expands fine, but there appears to be a delay on collapsing.
I then added ""cubic-bezier(0, 1, 0, 1)"" to the ""transition"" property, which fixes the collapsing animation, but the expand animation then seems to break entirely.
How can I get both the entering and exiting working?
Before adding cubic bezier (sorry for the loss-y gifs):

After adding cubic bezier:

",76394902,"Found the answer:
The transition needed to be set like this:
transition: `1000ms ${state === ""exiting"" ? ""cubic-bezier(0, 1, 0, 1)"" : ""ease-out""}`

For any other novices out there, note the backticks `` around the string rather than ''
",
76394248,Can I publish client ID?,"I've coded beyond my ability and managed to get MSAL authentication working for my Browser Extension. I'm ready to push code to GitHub.
Is it ok to push code with the Client ID in it? Can someone else use my Client ID? If I can't safely push this to a public GitHub, how do I handle it?
// Microsoft Authentication Library (MSAL) instance
// Client ID is the Application (client) ID GUID from the Azure portal
const msalInstance = new msal.PublicClientApplication({
    auth: {
        authority: ""https://login.microsoftonline.com/common/"",
        clientId: ""ffffffff-ffff-ffff-ffff-ffffffffffff"",
        redirectUri: redirectUri,
        postLogoutRedirectUri: redirectUri
    },
    cache: {
        cacheLocation: ""localStorage""
    }
});

",76394918,"Technically, it should be okay.

Is it ok to push code with the Client ID in it?

The question you should ask yourself is, what information does the client ID give away? According to the Microsoft identity platform and the OAuth 2.0 client credentials flow a client_id is,

The Application (client) ID that the Azure portal – App registrations experience assigned to your app.

It is essentially like your name. It won't be end of the world if you give it away but you probably should not where there is no reason to.
Quoting, Should the OAuth client_id be kept confidential?

Your client_id is like your username or e-mail address you use to authenticate your application/service to OAuth. It's not exactly top-secret, but putting it out in the public domain might also be undesirable.

Further,

Can someone else use my Client ID?

Yes, but even in the confidential client apps, ""The client ID is exposed through the web browser.""
Note that we can actually see the Thunderbird's client id exposed in their source code. It isn't really meant to be private information.

If I can't safely push this to a public GitHub, how do I handle it?

You can use a password manager if you are the only user of your extension.
",
76397713,Mixing grid auto layout with a fixed row-column position,"I have the following HTML with a CSS grid
<div id=""grid"">
    <div class=""main-item"">Main</div>
</div>

#grid {
    display: grid;
    grid-template-columns: repeat(5, auto);
    text-align: center;
}

#grid > * {
    border: 1px solid blue;
    padding: 20px;
}

#grid > .main-item {
    grid-row: 3;
    grid-column: 3 / 5;
    background: rgba(0, 0, 0, 0.2);
}

The important part is .main-item has a fixed position in the grid.
I now add 25 cells to the grid.
const grid = document.querySelector(""#grid"");

for (let i = 0; i < 25; i++) {
    const item = document.createElement(""div"");
    item.innerText = i.toString();
    grid.append(item);
}

The problem is that I want these elements to ignore the position of the .main-item (treat it as if it wasn't there). However the CSS currently corrects for this and flows the elements around .main-item. I want the secondary behaviour below:

I can correct by setting style.gridRow and style.gridColumn in the JavaScript
item.style.gridRow = (Math.floor(i / 5) + 1).toString();
item.style.gridColumn = ((i % 5) + 1).toString();

Is there a way to do this without setting every other element in JS? Is there a CSS to prevent fixed element affecting the flow correction?
Codepen Link
",76397738,"You can give the grid a relative position and position that specific grid item absolutely.


const grid = document.querySelector(""#grid"");
for (let i = 0; i < 25; i++) {
    const item = document.createElement(""div"");
    item.innerText = i;
    grid.append(item);
}
#grid {
    display: grid;
    grid-template-columns: repeat(5, auto);
    text-align: center;
    position: relative;
}

#grid > * {
    border: 1px solid blue;
    padding: 20px;
}

#grid > .main-item {
    position: absolute;
    left: 0;
    right: 0;
    grid-row: 3;
    grid-column: 3 / 5;
    background: rgba(0, 0, 0, 0.2);
}
<div id=""grid"">
    <div class=""main-item"">Main</div>
</div>



",
76394958,How to pass curly bracket ({) as a string in f-string python?,"due to these bracket { and } in Hourly Status','{',''),'}','') causing the syntax error in query .How to pass these bracket as string in a f-string format?
query = f""""""Select fd.serial_number,txidkey,cast(replace(replace(data->>'Hourly Status','{',''),'}','') as text) as description,TO_TIMESTAMP(TIME/1000+19800) as date_time,time,total_min from filter_data fd , total_sum ts
where fd.serial_number = ts.serial_number
and time between {yesterday10PM*1000} and {today6AM*1000}'''

",76394976,"just use double curly braces {{:
query = f""""""\
Select fd.serial_number, txidkey, cast(\
replace(replace(data->>'Hourly Status','{{',''),'}}','') as text) \
as description,TO_TIMESTAMP(TIME/1000+19800) as date_time,time,total_min \
from filter_data fd , total_sum ts \
where fd.serial_number = ts.serial_number \
and time between {yesterday10PM*1000} and {today6AM*1000}
'''

You can also read about formatted strings here:
https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals
",
76396569,Calculating Collective Count of departments on individual dates from a given date range,"I have the following table




Function
Department
Start Date
End Date




Const
Const 1
2023-03-01
2023-03-05


Const
Const 2
2023-03-02
2023-03-03


Mining
Mining 1
2023-03-02
2023-03-05


Mining
Mining 2
2023-03-01
2023-03-06


Const
Const 1
2023-03-03
2023-03-07


Const
Const 2
2023-03-02
2023-03-05


Mining
Mining 1
2023-03-06
2023-03-09


Mining
Mining 2
2023-03-05
2023-03-08




I want to get per date the total count in each department. Both start date and end date and included in counting.
It would be nice to have an intermediate output as follows




Function
Department
Date
Count




Const
Const1
2023-03-01
1


Const
Const1
2023-03-02
1


Const
Const1
2023-03-03
2


Const
Const1
2023-03-04
2


Const
Const1
2023-03-05
2


Const
Const1
2023-03-06
1


Const
Const1
2023-03-07
1


Const
Const1
2023-03-08
0


Const
Const1
2023-03-09
0


Const
Const1
2023-03-10
0


Const
Const2
2023-03-01
0


Const
Const2
2023-03-02
2


Const
Const2
2023-03-03
2


Const
Const2
2023-03-04
1


Const
Const2
2023-03-05
1


Const
Const2
2023-03-06
0


Const
Const2
2023-03-07
0


Const
Const2
2023-03-08
0


Const
Const2
2023-03-09
0


Const
Const2
2023-03-10
0


Mining
Mining 1
2023-03-01
0


Mining
Mining 1
2023-03-02
1


Mining
Mining 1
2023-03-03
1


Mining
Mining 1
2023-03-04
1


Mining
Mining 1
2023-03-05
1


Mining
Mining 1
2023-03-06
1


Mining
Mining 1
2023-03-07
1


Mining
Mining 1
2023-03-08
1


Mining
Mining 1
2023-03-09
1


Mining
Mining 1
2023-03-10
0


Mining
Mining 2
2023-03-01
1


Mining
Mining 2
2023-03-02
1


Mining
Mining 2
2023-03-03
1


Mining
Mining 2
2023-03-04
1


Mining
Mining 2
2023-03-05
2


Mining
Mining 2
2023-03-06
2


Mining
Mining 2
2023-03-07
1


Mining
Mining 2
2023-03-08
1


Mining
Mining 2
2023-03-09
0


Mining
Mining 2
2023-03-10
0




The desired final output is a pandas df as follows




Date
Const 1
Const 2
Mining 1
Mining 2




2023-03-01
1
0
0
1


2023-03-02
1
2
1
1


2023-03-03
2
2
1
1


2023-03-04
2
1
1
1


2023-03-05
2
1
1
2



",76396655,"df['Start Date'] = pd.to_datetime(df['Start Date'])
df['End Date'] = pd.to_datetime(df['End Date'])

dates = pd.date_range(df['Start Date'].min(), df['End Date'].max()) #get the complete dates in data set
final_df = pd.DataFrame({'Date': dates})
final_df = final_df.set_index('Date')

departments = df['Department'].unique()

for department in departments:
    mask = (df['Department'] == department)
    department_counts = (df.loc[mask, 'Start Date'].value_counts() + df.loc[mask, 'End Date'].value_counts()).sort_index()
    final_df[department] = department_counts

final_df = final_df.fillna(0)

",
76397698,Displaying colored text in windows console by linux codes (C++),"I'm writing game for project in VS Community 2017 in c++. I can see that coloring text works well in windows terminal, but I'm not sure if it'll be working on every windows compiler?  Is there any safer way to print colored text?
Example code:
#include <iostream>
#include <Windows.h>

using namespace std;

int main()
{
    cout<<""\033[34m""<<""Hello World in blue\n"";
    cout<<""\033[31m""<<""Hello World in red\n"";
    cout<<""\033[35m""<<""Hello World in purple\n"";
    return 0;
}

",76397757,"The compiler has little (if anything) to do with this. Your code just sends data. It's up to the terminal program to do something with that data.
There are some terminal programs that do, and others that don't interpret them well. Prior to Windows 10, the default Windows console didn't, so if you care about supporting older versions of Windows, it's not a great choice.
So, it depends on what you about. If you want portability to (current) Windows and Linux, what you're doing is fine. If you want portability to older versions of Windows...not so much.
If you need something that works for older Windows and don't care about Linux, you can Windows' console functions (e.g., WriteConsoleOutputAttribute or FillConsoleOutputAttribute).
My own advice would be to use some manipulators, so you'd do something like:
console << blue << ""Hello world in blue\n"";
console << red << ""Hello world in red\n"";

...and when/if you need to move code to a different platform, you can rewrite just those manipulators. For Linux can current Windows you can send the ANSI escape sequences you already know about.
Supporting manipulators like this on older versions of Windows isn't trivial, at least using actual cout. You need a Windows handle to the console. But it's pretty easy to do that, with an ostream, so writing to it works about like you'd normally expect.
For example:
// winbuf.hpp
#pragma once
#include <ios>
#include <ostream>
#include <windows.h>

class WinBuf : public std::streambuf
{
    HANDLE h;

public:
    WinBuf(HANDLE h) : h(h) {}
    WinBuf(WinBuf const &) = delete;
    WinBuf &operator=(WinBuf const &) = delete;

    HANDLE handle() const { return h; }

protected:
    virtual int_type overflow(int_type c) override
    {
        if (c != EOF)
        {
            DWORD written;
            WriteConsole(h, &c, 1, &written, nullptr);
        }
        return c;
    }

    virtual std::streamsize xsputn(char_type const *s, std::streamsize count) override
    {
        DWORD written;
        WriteConsole(h, s, DWORD(count), &written, nullptr);
        return written;
    }
};

class WinStream : public virtual std::ostream
{
    WinBuf buf;

public:
    WinStream(HANDLE dest = GetStdHandle(STD_OUTPUT_HANDLE))
        : buf(dest), std::ostream(&buf)
    {
    }

    WinBuf *rdbuf() { return &buf; }
};

class SetAttr
{
    WORD attr;

public:
    SetAttr(WORD attr) : attr(attr) {}

    friend WinStream &operator<<(WinStream &w, SetAttr const &c)
    {
        WinBuf *buf = w.rdbuf();
        auto h = buf->handle();
        SetConsoleTextAttribute(h, c.attr);
        return w;
    }

    SetAttr operator|(SetAttr const &r)
    {
        return SetAttr(attr | r.attr);
    }
};

class gotoxy
{
    COORD coord;

public:
    gotoxy(SHORT x, SHORT y) : coord{.X = x, .Y = y} {}

    friend WinStream &operator<<(WinStream &w, gotoxy const &pos)
    {
        WinBuf *buf = w.rdbuf();
        auto h = buf->handle();
        SetConsoleCursorPosition(h, pos.coord);
        return w;
    }
};

class cls
{
    char ch;

public:
    cls(char ch = ' ') : ch(ch) {}

    friend WinStream &operator<<(WinStream &os, cls const &c)
    {
        COORD tl = {0, 0};
        CONSOLE_SCREEN_BUFFER_INFO s;
        WinBuf *w = os.rdbuf();
        HANDLE console = w->handle();

        GetConsoleScreenBufferInfo(console, &s);
        DWORD written, cells = s.dwSize.X * s.dwSize.Y;
        FillConsoleOutputCharacter(console, c.ch, cells, tl, &written);
        FillConsoleOutputAttribute(console, s.wAttributes, cells, tl, &written);
        SetConsoleCursorPosition(console, tl);
        return os;
    }
};

extern SetAttr red;
extern SetAttr green;
extern SetAttr blue;
extern SetAttr intense;

extern SetAttr red_back;
extern SetAttr blue_back;
extern SetAttr green_back;
extern SetAttr intense_back;

Along with:
// winbuf.cpp
#include ""winbuf.hpp""

#define WIN32_LEAN_AND_MEAN
#include <Windows.h>

SetAttr red { FOREGROUND_RED };
SetAttr green { FOREGROUND_GREEN };
SetAttr blue { FOREGROUND_BLUE };
SetAttr intense { FOREGROUND_INTENSITY };

SetAttr red_back { BACKGROUND_RED };
SetAttr blue_back { BACKGROUND_BLUE };
SetAttr green_back { BACKGROUND_GREEN };
SetAttr intense_back { BACKGROUND_INTENSITY };

...and a quick demo:
#include ""winbuf.hpp""

int main() {
    WinStream w;

    auto attrib = intense | blue_back;

    w << attrib << cls() << ""Some stuff"";
    w << gotoxy(0, 4) << (green | blue_back) << ""This should be green"";
    w << gotoxy(0, 5) << attrib << ""And this should be grey"";
}

",
76394719,Extract predefined URL text (starts with the same prefix) into an Array,"I have files URLs text like these examples:
http://xxxxx.pdf http://xxxxxxxxxxx.doc http://xxxxxxxxxxxxx.xls
The delimiter between each URL is not a space, it may be separated by vbTab , vbLf or what ever.
But in all cases the URLs start with the same prefix ""http:"" and end with a dot+three characters.
I need to extract these predefined URLs into an Array to manipulate it later.
I made the below code as a workaround, by using two arrays and also I had to start looping from the second element from the first array because I found there is extra ""http"" on the second array.
My question, Is there another neater code?
Sub Split_URLs_to_Array()

  Dim wDoc As Word.Document, rngSel As String
  Dim arrS, arrD, i As Long
  
  Set wDoc = Application.ActiveInspector.WordEditor
  
  rngSel = Trim(wDoc.Windows(1).Selection.Text)

  arrS = Split(rngSel, ""http"")
  
  ReDim arrD(0 To UBound(arrS))
  
   For i = 1 To UBound(arrS)
       arrD(i) = ""http"" & arrS(i)
     Debug.Print arrD(i)
   Next

End Sub 
 

",76394978,"
You asked (yesterday) about a way to extract URLs from a string, separated by no any separator... The next function will do it:

Function SplitByStartOfString(strTxt As String, strDelim As String) As Variant
  Dim arr: arr = Split(strTxt, strDelim)
  
  arr(0) = ""@#$%^"": arr = filter(arr, ""@#$%^"", False) 'eliminate the first empty element
  SplitByStartOfString = Split(strDelim & Join(arr, ""|"" & strDelim), ""|"")
End Function

It can be tested in the next way:
Sub testSplitByStartOfString()
   Dim x As String: x = ""https://myurl1/x.pdfhttps://myurl2/y.xlsxhttps://myurl3/z.docx""
   Dim arr
   arr = SplitByStartOfString(x, ""https:"")
   Debug.Print Join(arr, ""||"") 'just to visually see the array result.
End Sub

Of course, you can use what strings you know, the delimiter will be the common prefix of each of them...
And end of line separators or VbTab exists, the above code will also work, but these separators will be included of the end of the string. Which, for URL in Outlook will not count. They will only be arranged using the respective separator.
1.1 Another function, using FilterXML may be the next one:
Function splitXMLByStartOfString(strText As String, strDelim As String) As Variant
    Dim XML As String: XML = ""<t><s>"" & VBA.Replace(strText, strDelim, ""</s><s>"" & strDelim) & ""</s></t>""
    splitXMLByStartOfString = Application.FilterXML(XML, ""//s[position()>1]"")  'nodes starting from the second one...
    'splitXMLByStartOfString = Application.FilterXML(XML, ""//s[count(node())>0]"")   'another working way (all not empty nodes)
    'splitXMLByStartOfString = Application.FilterXML(XML, ""//s[starts-with(., '"" & strDelim & ""')]"") 'working way, too (nodes starting with strDelim)
End Function

It can be tested using the next sub:
Sub TestFilterXMLHttp()
   Dim x As String: x = ""https://myurl1/x.pdfhttps://myurl2/y.xlsxhttps://myurl3/z.docx""
   Dim arr: arr = splitXMLByStartOfString(x, ""https:"") 'It returns a 2D, 1 column array...
  
   Debug.Print Join(Application.Transpose(arr), ""||"")
End Sub

The above function works well and fast, but in Excel. The question does not mention that it should be used in Outlook VBA (but I knew about that...). So, the next solution uses an automation from Outlook, and use Excel.Application it this way. This version needs an Excel session open, but it can be easily adapted to open a new one if nothing open:
Function splitXMLByStartOfString(strText As String, strDelim As String, objEx As Object) As Variant
   
    Dim XML As String: XML = ""<t><s>"" & VBA.Replace(strText, strDelim, ""</s><s>"" & strDelim) & ""</s></t>""
    splitXMLByStartOfString = objEx.FilterXML(XML, ""//s[position()>1]"")  'nodes starting from the second one...
    'splitXMLByStartOfString = Application.FilterXML(XML, ""//s[count(node())>0]"")   'another working way (all not empty nodes)
    'splitXMLByStartOfString = Application.FilterXML(XML, ""//s[starts-with(., '"" & strDelim & ""')]"") 'working way, too (nodes starting with strDelim)
End Function

And the sub to test it, using the above mentioned automation:
Sub TestFilterXMLHttp()
   Dim objEx As Object: Set objEx = GetObject(, ""Excel.application"")
   Dim x As String: x = ""https://myurl1/x.pdfhttps://myurl2/y.xlsxhttps://myurl3/z.docx""
   Dim arr: arr = splitXMLByStartOfString(x, ""https:"", objEx) 'It returns a 2D, 1 column array...
  
   Debug.Print Join(objEx.Transpose(arr), ""||"")
End Sub


Extracting an array from string elements separated by one from more supposed separators can be extracted using the next function:

Function extractFromStringSep(strText As String) As Variant
   Dim arrC: arrC = Array(vbTab, vbLf) 'you can extend the supposed separators...
   Dim El
   
   For Each El In arrC
        If InStr(strText, El) > 0 Then
            extractFromStringSep = Split(strText, El): Exit Function
        End If
   Next El
End Function

Of course, the string to be extracted must NOT contain the supposed separators...
It can be tested like in the next sub:
Sub TestextractFromStringSep()
   Dim x As String: x = ""https://myurl1/x.pdf"" & vbTab & ""https://myurl2/y.xlsx"" & vbTab & ""https://myurl3/z.docx""
          'x = ""https://myurl1/x.pdf"" & vbLf & ""https://myurl2/y.xlsx"" & vbLf & ""https://myurl3/z.docx""
   Dim arr
   arr = extractFromStringSep(x)
   Debug.Print Join(arr, ""||"") 'just to visually see the array result.
End Sub


And a last version, also allowing part of the url string as separator (only for didactic purpose):

Function extractFromStrAndSep(strText As String, strDelim As String) As Variant
     Dim arrC: arrC = Array(vbTab, vbLf, ""myur"") 'you can extend the supposed separators...
     Dim El
     
     For Each El In arrC
        If InStr(strText, El & strDelim) > 0 Then
            extractFromStrAndSep = Split(strText, El & strDelim): Exit Function
        End If
   Next El
End Function

Tested with the next (adapted) sub:
Sub TestextractFromStrAndSep()
   Dim x As String: 'x = ""https://myurl1/x.pdf"" & vbTab & ""https://myurl2/y.xlsx"" & vbTab & ""https://myurl3/z.docx""
          'x = ""https://myurl1/x.pd"" & vbLf & ""https://myurl2/y.xlsx"" & vbLf & ""https://myurl3/z.docx""
          x = ""https://myurl1/x.pd"" & ""myur"" & ""https://myurl2/y.xlsx"" & ""myur"" & ""https://myurl3/z.docx""
   Dim arr
   arr = extractFromStrAndSep(x, ""https:"")
   Debug.Print Join(arr, ""||"") 'just to visually see the array result.
End Sub

",
76387096,Why are the fitting results of the Arima() and glm() function different?,"I am confused about the difference in the fitting results of the Arima() function and glm() function.
I want to fit an AR(1) model with an exogeneous variable. Here is the equation:
$$
x_{t} = \alpha_{0} + \alpha_{1}x_{t-1} + \beta_{1}z_{t} + \epsilon_{t}
$$

Now I estimate this model using the Arima() function and glm() function and compare the results, but the results turned out to be quite different!
Here is the sample data. x denotes the time-series variable, and z denotes the exogeneous variable, as shown in the equation above.
library(forecast)
library(tidyverse)
data(""Nile"")
df <- 
  Nile %>% 
  as_tibble() %>% 
  mutate(x = as.numeric(x)) %>% 
  mutate(z = rnorm(100))

Then fit the model using the Arima() and glm() and compare the results.
fit_arima <- Arima(df$x, order = c(1, 0, 0), include.mean = TRUE, xreg = df$z)
tibble(Parameters = c(""x lag"", ""intercept"", ""z""),
       Coefficients = coef(fit_arima),
       Standard_Errors = sqrt(diag(vcov(fit_arima))))  
fit_glm <- glm(df$x ~ lag(df$x) + df$z) 
tibble(Parameters = c(""intercept"", ""x lag"", ""z""),
       Coefficients = coef(fit_glm),
       Standard_Errors = summary(fit_glm)$coefficients[, ""Std. Error""])

The results are displayed as follows.
Arima() function:
# A tibble: 3 × 3
  Parameters Coefficients Standard_Errors
  <chr>             <dbl>           <dbl>
1 x lag             0.510          0.0868
2 intercept       920.            29.4   
3 z                 5.02          12.1    

glm() function:
# A tibble: 3 × 3
  Parameters Coefficients Standard_Errors
  <chr>             <dbl>           <dbl>
1 intercept       444.            83.4   
2 x lag             0.516          0.0896
3 z                 8.95          13.9 

The estimated coefficient and standard error of x lag are quite close, but the values of other two variables are very different. I find this puzzling because both the Arima() and glm() function use the maximum likelihood estimator. Could you please explain why this difference happens and how can I fix this?
",76394984,"First, Arima() does not fit the model given in your equation. It fits a regression with ARIMA errors like this:
x_{t} = \alpha_{0} + \beta_{1}z_{t} + \eta_{t}

where
\eta_t = \phi_{1}\eta_{t-1}+\varepsilon_{t}.

We can rearrange this to give
x_{t} = (1-\phi_{1})\alpha_{0} + \phi_{1}x_{t-1} + \beta_{1}z_{t} - \beta_{1}\phi_{1}z_{t-1} + \varepsilon_{t}

This explains the major differences in the two results.
But even if you specified exactly the same model, they would give slightly different results because Arima() uses the true likelihood whereas glm() will use a conditional likelihood because of the initial missing value due to the lag() function.
See https://robjhyndman.com/hyndsight/arimax/ for a discussion of the different model specifications.
",
76397650,Is there a better/faster way to fill one vector with contents of another (smaller) vector than using a for loop?,"I'm trying to figure out a better solution to fill a vector with another vector whill avoiding loops. Is it even possible? Maybe using address range or something else?
This is my working code which does exactly what I need but slowly:
#include <iostream>
#include <vector>
#include <string>

typedef std::vector<std::vector<int>> Vec2i;

void Vec2DPrinter(Vec2i vec)
{
    size_t vec_h = vec.size();

    for (size_t y = 0; y < vec_h; y++)
    {
        size_t vec_w = vec.at(y).size();

        for (size_t x = 0; x < vec_w; x++)
        {
            std::cout << vec.at(y).at(x);
        }

        std::cout << std::endl;
    }
}

void Vec2DFiller(Vec2i &dest, Vec2i src, int dest_x, int dest_y)
{
    int src_h = (int)src.size();

    // Loop to fill dest vector with elements of src vector at specific coords
    for (int y = 0; y < src_h; y++)
    {
        int src_w = (int)src.at(y).size();
        int dest_h = (int)dest.size();

        if ((y + dest_y) >= dest_h)
        {
            break;
        }

        if ((y + dest_y) < 0)
        {
            continue;
        }

        int dest_w = (int)dest.at(y + dest_y).size();

        for (int x = 0; x < src_w; x++)
        {
            if (x + dest_x >= dest_w)
            {
                break;
            }

            if (x + dest_x < 0)
            {
                continue;
            }

            dest.at(y + dest_y).at(x + dest_x) = src.at(y).at(x);
        }
    }
}

int main(void)
{
    // This is just visual example (i know how to fill vectors)
    Vec2i dest =
    {
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }
    };

    Vec2i src =
    {
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 }
    };

    std::cout << ""Destination contents (before change):"" << std::endl;
    Vec2DPrinter(dest);
    std::cout << ""\nSurce contents:"" << std::endl;
    Vec2DPrinter(src);

    // Destination coords
    int dest_x = 3;
    int dest_y = 4;

    // Key function to fill one vector with another
    Vec2DFiller(dest, src, dest_x, dest_y);

    std::cout << ""\nDestination contents (after change):"" << std::endl;
    Vec2DPrinter(dest);

    std::string exit = """";
    std::getline(std::cin, exit);

    return 0;
}

Key part:
// Destination coords
int dest_x = 3;
int dest_y = 4;

int src_h = (int)src.size();

// Loop to fill dest vector with elements of src vector at specific coords
for (int y = 0; y < src_h; y++)
{
    int src_w = (int)src.at(y).size();
    int dest_h = (int)dest.size();

    if ((y + dest_y) >= dest_h)
    {
        break;
    }

    if ((y + dest_y) < 0)
    {
        continue;
    }

    int dest_w = (int)dest.at(y + dest_y).size();

    for (int x = 0; x < src_w; x++)
    {
        if (x + dest_x >= dest_w)
        {
            break;
        }

        if (x + dest_x < 0)
        {
            continue;
        }

        dest.at(y + dest_y).at(x + dest_x) = src.at(y).at(x);
    }
}

This is the best solution I could come with. (Not the same but similar. The original one has single but wide if statement so it wouldn't fit on a single line here so that is why I changed it a bit.)
The reason why I want to do this is pretty simple.
I'm doing this thousand times a second and on a bigger scale, that is why.
I was searching on the web and found things like std::fill and std::copy but none of them meets my requirements. (std::fill is not what I need and std::copy is an algorithm that consists of loop.)
",76397766,"As with all performance questions, the solution is going to depend heavily on the hardware and software being used and the particulars of the test conditions. In this case, for example, the dataset size is relevant.
With that caveat, I was able to improve the performance by over 4x on my platform (M1, MacOSX, clang). Your mileage will vary. I moved the conditional checks out of the inner loop and used std::copy for the actual work. For larger dataset sizes, I suspect the speedup will be greater.
I used the timer library for measuring performance.
Performance could be improved further by using a native 2D storage structure like a matrix or std::mdspan instead of a std::vector of std::vector.
Sample Code
#include <iostream>
#include <vector>
#include <string>
#include ""core/timer/timer.h""

using std::cout, std::endl;

typedef std::vector<std::vector<int>> Vec2i;

void Vec2DPrinter(Vec2i vec) {
    size_t vec_h = vec.size();
    for (size_t y = 0; y < vec_h; y++) {
        size_t vec_w = vec.at(y).size();
        for (size_t x = 0; x < vec_w; x++)
            std::cout << vec.at(y).at(x);
        std::cout << std::endl;
    }
}

void Vec2DFiller(Vec2i &dest, Vec2i src, int dest_x, int dest_y) {
    int src_h = (int)src.size();

    // Loop to fill dest vector with elements of src vector at specific coords
    for (int y = 0; y < src_h; y++) {
        int src_w = (int)src.at(y).size();
        int dest_h = (int)dest.size();

        if ((y + dest_y) >= dest_h)
            break;

        if ((y + dest_y) < 0)
            continue;

        int dest_w = (int)dest.at(y + dest_y).size();

        for (int x = 0; x < src_w; x++) {
            if (x + dest_x >= dest_w)
                break;

            if (x + dest_x < 0)
                continue;

            dest.at(y + dest_y).at(x + dest_x) = src.at(y).at(x);
        }
    }
}

void Vec2DFiller2(Vec2i &dest, const Vec2i& src, int dest_x, int dest_y) {
    int ylen = std::min(src.size(), dest.size() > dest_y ? dest.size() - dest_y : 0);
    if (ylen == 0)
        return;

    int xlen = std::min(src.at(0).size(), dest.at(0).size() - dest_x);
    if (xlen == 0)
        return;

    for (auto ydx = 0; ydx < ylen; ++ydx) {
        auto& ydest = dest.at(ydx + dest_y);
        const auto& ysrc = src.at(ydx);
        std::copy(ysrc.data(), ysrc.data() + xlen, ydest.data() + dest_x);
    }
}

int main(void)
{
    // This is just visual example (i know how to fill vectors)
    Vec2i dest =
    {
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
        { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }
    };

    Vec2i src =
    {
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 },
        { 1, 1, 1, 1, 1 }
    };

    std::cout << ""Destination contents (before change):"" << std::endl;
    Vec2DPrinter(dest);
    std::cout << ""\nSurce contents:"" << std::endl;
    Vec2DPrinter(src);

    // Destination coords
    int dest_x = 3;
    int dest_y = 4;

    // Key function to fill one vector with another
    auto ns = core::timer::Timer().run(1'000'000, [&]() {
        Vec2DFiller(dest, src, dest_x, dest_y);
    }).elapsed_per_iteration();
    cout << ns << "" ns / op"" << endl;

    ns = core::timer::Timer().run(1'000'000, [&]() {
        Vec2DFiller2(dest, src, dest_x, dest_y);
    }).elapsed_per_iteration();
    cout << ns << "" ns / op"" << endl;

    std::cout << ""\nDestination contents (after change):"" << std::endl;
    Vec2DPrinter(dest);

    return 0;
}

Relevant Output
142.167 ns / op
30.4702 ns / op

",
76394895,Problems with Alamofire and Swift closures,"I have the following class with a function that returns a boolean value using alamofire.
class Productos: Codable{
    
    // MARK: Propiedades de los productos
    var id_producto : String = """"
    var producto_es : String = """"
    var id_seccion : String = """"
   
    init(id_producto : String = """", producto_es : String = """", id_seccion : String = """"){
        self.id_producto = id_producto
        self.producto_es = producto_es
        self.id_seccion = id_seccion
    }

    func hayProductosNuevos(ultimoProductoEnBd: Int, completion: @escaping (Bool) -> Void) {
        let urlWS = Globales().urlWS//Url para la webservice
      
        AF.request(urlWS+""actualiza_productos.php?ultimo_producto=\(ultimoProductoEnBd)"",method: .get).responseDecodable(of: [Productos].self) { responseConsulta in
            switch responseConsulta.result{
                case .success(let actualizacionDeProductos):
                    if (actualizacionDeProductos.isEmpty){
                        completion(false)
                    } else {
                        completion(true)
                    }
                case .failure(let error):
                    print(""hay un error \(String(describing: error.errorDescription))"")
            }
         }
    }
}

My problem is how to call it and control what is returned to me from my view:

I know that within a view I cannot call it that. The idea is if I return true, perform a product update and display a ProgressView.
I have tried everything I have been reading, using its init() in the view. But I get another error.

I think my problem is that from Alamofire I need to know if I have to update my application's data or not, that's why it returns true or false, but I'm not clear about the use of closures, so I don't know how I have to call Alamofire from my view so I can control the data.
",76395000,"This approach is wrong, you cannot put the model which is used in an array   containing multiple instances and the controller to load the data in the same object. Well actually you can do it but then you have to create static APIs which is bad practice though.
First of all declare the object representing a product as struct named in singular form and without the logic to load the data.
struct Producto: Decodable {
    
    // MARK: Propiedades de los productos
    var id_producto : String 
    var producto_es : String
    var id_seccion : String
}

And it's highly recommended to use camelCase struct member names, but this is beyond the question. And if the values will never change declare the struct members as constants (let).

Then create a class conforming to ObservableObject for the logic to load the data with a @Published property for the products and a method to load the data. A completion handler is not needed, assign (or append) the result to the array. Actually AF is not needed either for a simple GET request, built-in URLSession and JSONDecoder can do the same.
@MainActor
class ViewModel : ObservableObject {
    @Published var productos = [Producto]()

    func hayProductosNuevos(ultimoProductoEnBd: Int) {
        let urlWS = Globales().urlWS //Url para la webservice
      
        AF.request(urlWS+""actualiza_productos.php?ultimo_producto=\(ultimoProductoEnBd)"",method: .get).responseDecodable(of: [Producto].self) { responseConsulta in
            switch responseConsulta.result {
                case .success(let actualizacionDeProductos):
                    self.productos = actualizacionDeProductos
                    // or self.productos.append(contentsOf: actualizacionDeProductos)
                case .failure(let error):
                    print(""hay un error"", error)
            }
         }
    }
}


In the view create an instance of ViewModel as @StateObject and load the data in onAppear
struct ActualizarProductos : View {
    @StateObject private var viewModel = ViewModel()
    @State private var ultimoProductoEnBd = 0

    var body: some View {
        NavigationView {
          // UI stuff
        }
        .onAppear {
            viewModel.hayProductosNuevos(ultimoProductoEnBd: ultimoProductoEnBd)
        }
    }
}

The ultimoProductoEnBd is not handled in the question, it's just an example adding a @State property.
",
76396627,Why am I getting a syntax error when using modulo on floats in my fragment shader?,"I'm new to using the fragment shader. Why does my code raise a syntax error? I can't understand why it doesn't work. This is the piece of code that raises the error:
if ((int) pos.y % 9 == 1) shade = 1;

Both pos.y and shade are floats. I've put (int) before pos.y so that I could use modulo on it. The error message says this:
ERROR: 0:17: ')' : syntax error syntax error

I'm taking the error to mean I have too many or to few brackets, but I checked that and my code seems fine. Why is there an error, and what can I do to solve it?
",76396665,"GLSL is not C. As stated in the GLSL specification:

There is no typecast operator; constructors are used instead.

As such, (int) is not a thing in GLSL. If you want to convert some float to an int, you use constructor syntax: int(pos.y).
",
76397759,How to find a specific class that contains another class?,"I have the following code:
<div class=""accordion mobile ui-accordion ui-widget ui-helper-reset"" role=""tablist"">
    <h3 class=""accord-**19** **ui-state-active**""><span class=""ui-accordion-header-icon ui-icon ui-icon-triangle-1-s"">Title 1</h3>
        <div style="""" class=""ui-accordion-content ui-corner-bottom ui-helper-reset ui-widget-content ui-accordion-content-active"" id=""ui-id-2"" aria-labelledby=""ui-id-1"" role=""tabpanel"" aria-hidden=""false"">
            Content for title 1
        </div>
        <h3 class=""accord-41"">Title 2</h3>
        <div style=""display: none;"" class=""ui-accordion-content ui-corner-bottom ui-helper-reset ui-widget-content"" id=""ui-id-4"" aria-labelledby=""ui-id-3"" role=""tabpanel"" aria-hidden=""true"">
            Content for title 2
        </div>
        <h3 class=""accord-47"">Title 3</h3>
        <div style=""display: none;"" class=""ui-accordion-content ui-corner-bottom ui-helper-reset ui-widget-content"" id=""ui-id-6"" aria-labelledby=""ui-id-5"" role=""tabpanel"" aria-hidden=""true"">
            Content for title 3
        </div>        
</div>

What I am trying to do is:

Get the h3 tag that contains the class ui-state-active
and then
Get the unique id  (in this example, 19) that comes after the class name that starts with ""accord-"".

Note, the above is churned out by a plugin I am using, which means I cannot set and ""id"" tag for h3 to be unique.
I have tried:
var classname = $(""div.accordion h3.ui-state-active"").find('class^=""accord-""');
To get the name of the class, before I substring it.
However, this doesn't work.
",76397790,"When you tried to retrieve the class name .find  was not the correct way to do it. To find the attributes of an element you should use .attr, and then specify .attr(""class"") to find classes. Then from there use regex to find the correct accord-id.
var classname = $(""div.accordion h3.ui-state-active"").attr(""class"");
var id = classname.match(/accord-(\d+)/)[0];

",
76396594,Nested for loop - model.id in parent for loop does not match model.id in nested for loop (django),"I am trying to access data from a parent to a child via a foreign key.
WHAT WORKS - the views
The data in the child is not ""ready to be used"" and need to be processed, to be represented in a progress bar in %.
The data processing is handled in the views. When I print it on the console, it seems to work and stored into a variable reward_positions.
Reward positions = [(<Venue: Venue_name>, reward_points, reward_position_on_bar)]

So this part works.
The plan is therefore to access reward_position_on_bar by calling {{reward_positions.2}}
WHAT DOESNT WORK - the template
But something is not working to plan in the template.
The template renders the last child_model (thats rewardprogram) objects of the last parent_id (thats venue) irrespective of the actual parent_id processed in the for loop.
TEST RESULT & WHERE I THINK THE PROBLEM IS
I think my problem lies in my nested forloop. The parent_id in the parent forloop does not match the '{{reward_position.0}}' in the nested forloop.
Doing a verification test, {{key}} should be equal to {{reward_position.0}} as they both go through the same parent forloop.
However, if {{key}} does change based on venue.id (parent forloop id), {{reward_position.0}} is stuck to the same id irrespective of the parent forloop id.
Can anyone seem what I am doing wrong?
THE CODE
models
class Venue(models.Model):
    name = models.CharField(verbose_name=""Name"",max_length=100, blank=True)

class RewardProgram(models.Model):
    venue = models.ForeignKey(Venue, null = True, blank=True, on_delete=models.CASCADE, related_name=""venuerewardprogram"")
    title = models.CharField(verbose_name=""reward_title"",max_length=100, null=True, blank=True)
    points = models.IntegerField(verbose_name = 'points', null = True, blank=True, default=0)

views
def list_venues(request):
    venue_markers = Venue.objects.filter(venue_active=True)
    
    #Progress bar per venue
    
    bar_total_lenght = 100
    rewards_available_per_venue = 0
    reward_position_on_bar = 0
    venue_data = {}
    reward_positions = {}
    
    for venue in venue_markers:
        print(f'venue name ={venue}')
                
        #list all reward programs
        venue.reward_programs = venue.venuerewardprogram.all()
        reward_program_per_venue = venue.reward_programs
        
        #creates a list of reward points needed for each venue for each object
        reward_points_per_venue_test = []
        
        #appends the points to empty list from reward program from each venue
        for rewardprogram in reward_program_per_venue:
            reward_points_per_venue_test.append(rewardprogram.points)
        
        #sorts list in descending order
        reward_points_per_venue_test.sort(reverse=True)
        
        #set position of highest reward to 100 (100% of bar length)
        if reward_points_per_venue_test:
            highest_reward = reward_points_per_venue_test[0]

        if not reward_program_per_venue:
            pass
        else:    
            #counts reward program per venue
            rewards_available_per_venue = venue.reward_programs.count()
                    
            if rewards_available_per_venue == 0:
                pass
            else:   
                #position of reward on bar  
                reward_positions = []
                for rewardprogram in reward_program_per_venue:
                    #list all points needed per reward program objects
                    reward_points = rewardprogram.points
  
                    #position each reward on bar
                
                    reward_position_on_bar = reward_points/highest_reward
                    reward_positions.append((venue, reward_points, reward_position_on_bar))
                    #reward_positions[venue.id] = reward_position_on_bar
                reward_positions = reward_positions
                print(f'Reward positions = {reward_positions}')

    context = {'reward_positions':reward_positions,'venue_data':venue_data,'venue_markers':venue_markers}
    
    return render(request,'template.html',context)

template
            {%for venue in venue_markers%}
    
            {%for key, value in venue_data.items%}
            {%if key == venue.id%} #venue.id = 3
                                  {% for reward_position in reward_positions %}#test result
                                  {{reward_position.0.id}} # = id = 7 (thats not the good result)
{{key}} #id = 3 (thats the good result)
                                  {% endfor %}
                                  <div class=""progress-bar bg-success"" role=""progressbar"" style=""width: {{value}}%"" aria-valuenow=""{{value}}"" aria-valuemin=""0"" aria-valuemax=""100""></div>
    
            {%endif%}
            {%endfor%}
       
            {%endfor%}

",76396668,"You are overcomplicating things, which makes the template very complex (which often will only introduce extra bugs), and makes it even quite slow because of an N+1 problem in the view.
def list_venues(request):
    venues = (
        Venue.objects.filter(venue_active=True)
        .annotate(
            total_points=Sum('venuerewardprogram__points'),
            rewards_available=Count('venuerewardprogram'),
        )
        .prefetch_related('venuerewardprogram')
    )
    for venue in venues:
        for rewardprogram in venue.venuerewardprogram.all():
            rewardprogram.position_on_bar = round(
                100 * rewardprogram.reward_points / venue.total_points
            )
    return render(request, 'template.html', {'venues': venues})
then in the template, we can render this as:
{%for venue in venues %}
  <h1>{{ venue.name }}</h1>
  {% for rewardprogram in venue.venuerewardprogram.all %}
    <div class=""progress-bar bg-success"" role=""progressbar"" style=""width: {{ rewardprogram.position_on_bar }}%"" aria-valuenow=""{{ rewardprogram.position_on_bar }}"" aria-valuemin=""0"" aria-valuemax=""100""></div>
  {% endfor %}
{% endfor %}
this will thus take the .position_on_bar attributes we added to the rewardPrograms that we prefetched for the Venues.
",
76394285,Show Tool Tip When a certain Button is Held down & Removetool Tip when button is Released / Up,"need help creating an example script of tool tip
Scenario : when i held down / press down CTRL a tooltip will appear , the tooltip will only dissappear if

the CTRL modifier is released
when CTRL key is held down then ANY KEYS is triggered / combine ( ex. Ctrl + A B C so forth ) or any modifier keys like ( Ctrl + SHIFT  ) ( ctrl + alt ) ( ctrl + alt + shift ) so forth

thank you
",76395019,"~LControl::
    ToolTip, LControl,,, 1
    SetTimer RemoveTooltip1, 50
return

~LControl Up:: 
    SetTimer RemoveTooltip1, off
    ToolTip,,,, 1
return

RemoveTooltip1:
    If (A_PriorKey != ""LControl"")
        ToolTip,,,, 1
return

For all modifier keys:
; List of keys to bind
keys := ""LControl,RControl,LShift,RShift,LAlt,RAlt,LWin,RWin""

; bind the keys to keypress_handler
Loop, parse, keys, `,
{
    hotkey, ~%A_Loopfield%, keypress_handler1
    hotkey, ~%A_Loopfield% Up, keypress_handler2
}

keypress_handler1:
    ThisKeyLabel := StrReplace(A_ThisHotkey, ""~"")
    ToolTip, %ThisKeyLabel%,,, 1
    SetTimer RemoveTooltip1, 50
return

keypress_handler2:
    SetTimer RemoveTooltip1, off
    ToolTip,,,, 1
return

RemoveTooltip1:
    ThisKeyLabel := StrReplace(A_ThisHotkey, ""~"")
    If (A_PriorKey != ThisKeyLabel)
        ToolTip,,,, 1
return

",
76397770,Is it possible to replace a word used within an argument to a preprocessor macro?,"Say I have a macro:
#define SUBST(MAGIC, ...) __VA_ARGS__  /* this won't work*/

And I want to call it like:
SUBST(int, MAGIC a = 1);

// expected output:
int a = 1;

Is there some set of wild indirections and expansions that I can use to force the expansion of MAGIC within the second argument with a value which depends on the call to the macro?
This comes up when the other arguments come from another layer and the SUBST macro is repeated many times with different values of MAGIC.
For example:
#define WRAP(function, MAGIC, stuff, ...) \
  void function(__VA_ARGS__) { stuff };
#define SUPERWRAP(function, stuff, ...) \
    WRAP(function, uint8_t, stuff##_u8, __VA_ARGS__) \
    WRAP(function, uint16_t, stuff##_u16, __VA_ARGS__)

SUPERWRAP(example, EXAMPLE_IMPL, template_t<MAGIC>, int)

Which cannot be solved in templates because:
#define WRAP(function, stuff, ...) \
  template<class MAGIC>
  int function(__VA_ARGS__) { stuff }
WRAP(example, /*...*/, template_t<MAGIC>, int)

doesn't allow for specialised implementaion, and and taking the type as a templatea argument in the specialisation is a partial specialisation which is also illegal.
",76397841,"
Is it possible to replace a word used within an argument to a preprocessor macro?

No, it is not possible to replace a word used within an argument to a preprocessor macro.

SUBST(int, MAGIC a = 1);
// expected output:
int a = 1;


That is not possible.

Is there some set of wild indirections and expansions that I can use to force the expansion of MAGIC within the second argument with a value which depends on the call to the macro?

No.

For example: #define WRAP(function, MAGIC, stuff, ...) \

Using C macros to generate function definition results in hard to read, maintain and debug code that no one is ever able to fix or do anything with later. Consider writing the stuff that you want line by line.
You could consider using a better preprocessor, like jinja2, m4 or php.
",
76396589,Add namespace Microsoft.Exchange.Data.Mime to project?,"Sorry as I know I must sound stupid here but how do I add/import a namespace to my project? Microsoft.Exchange.Data.Mime is what i need but I cannot find the way to add in visual studio?
Thanks
",76396678,"You have to install the 'Microsoft.Exchange.Data.Common' nuget package first. Then try to import it into your file. I would suggest using MimeKit, which is a more sophisticated Mime tool for .NET.
",
76396616,Wrapping text around a box in the corner but keep scroll possibility,"I have some text which should wrap around a toolbox in the top right corner.
The total height should be limited, therefore the text should be scroll-able.
The corner box should stay in the corner, not scrolling with the text.
How can I achieve this?
I have a box and some text, but when adding overflow-y: scroll it will break like this:


.box {
  width: 100px;
  height: 40px;
  float: right;
  clear: both;
}

#blue {
  background-color: blue;
}

#text {
  height: 120px;
  overflow-y: scroll;
}
<div id='blue' class='box'></div>
<div id='text'>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor
  in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipiscing elit,
  sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore
  eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
</div>



",76396679,"You can add a div as a parent for the box and text and set the blue box position to sticky position: sticky;
I hope This is what you want


.box {
    width:100px;
    height:40px;
    float:right;
    clear: both;
}

#blue {
    background-color:blue;
    position: sticky;
    top: 0;
}

#text {
}
.container{
 height: 120px;
    overflow-y: auto;
}
<div class=""container"">
 <div id='blue' class='box'></div>
  <div id='text'>
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
  </div>
</div>



",
76394915,Convert to ISO time format in Angular using moment.js,"I'm trying to format date in this below format

20230201T103000Z

i tried this method in angular
moment(utctime).format('YYYYMMDDTHHmmsssZ')

this is returning following date format
20230523T1900000+05:30

",76395044,"Z' is a token witch will show time zone. You can add your 'Z' as part of string
https://momentjs.com/docs/#/displaying/format/
console.log(moment().format('YYYYMMDDTHHmmss') + 'Z');

The second option is Escaping characters using []
console.log(moment().format('YYYYMMDDTHHmmss[Z]'));

",
76397802,SQL MAX mixes up lines,"I have a little table where i like to get the latest content for each groupId.
This is the Table with my data:

This is my try to get it.
SELECT *, MAX(highest_datetime.creattionDate)
FROM highest_datetime
GROUP BY highest_datetime.groupId;


The MAX(highest_datetime.creattionDate) includes the latest content witch is a success. 
But the complete rest of the line does not match the MAX(highest_datetime.creattionDate). 
The rest of the row got mixed up.
How do i prevent this?
Do i use it wrong?
here is the data for the Database so you can create it yourself:
CREATE TABLE `test`.`highest_datetime` (
  `id` INT NOT NULL AUTO_INCREMENT,
  `groupId` INT NOT NULL,
  `creationDate` DATETIME NOT NULL,
  `Content` VARCHAR(100) NOT NULL,
  PRIMARY KEY (`id`),
  INDEX (`groupId`)
) ENGINE = InnoDB;




INSERT INTO `test`.`highest_datetime` (`groupId`, `creationDate`, `Content`)
VALUES
(1, '2023-05-02 00:00:00', 'Newest content'),
(1, '2023-01-01 00:00:00', 'Middle Content'),
(2, '2023-03-01 00:00:00', 'Middle Content'),
(2, '2022-11-01 00:00:00', 'Old Content'),
(2, '2023-06-01 00:00:00', 'New Content'),
(1, '2022-11-20 00:00:00', 'Old Content'),
(2, '2024-11-01 00:00:00', 'Future Content');

",76397843,"You are probably running MySQL in its notorious cheat mode, i.e. you haven't SET sql_mode = 'ONLY_FULL_GROUP_BY', and the DBMS allows invalid aggregation queries.
You have:
SELECT *, MAX(highest_datetime.creationDate)
FROM highest_datetime
GROUP BY highest_datetime.groupId;

So you want one result row per groupid, with the maximum creation date. You also want all columns, but you don't tell the DBMS from which row. For example you select the content, but which? The first in alhabet or the last? You don't state this, and so this query is invalid.
This would raise an error in about every DBMS. In MySQL's old cheat mode however, the DBMS silently applies ANY_VALUE on all non-aggregated columns. In other words it picks the content and other columns from arbitrary rows.
What you want instead is to get the rows with the maximum creation dates for their group. One way to do this:
SELECT id, groupid, creationdate, content
FROM highest_datetime
WHERE (groupid, creationdate) IN
(
  SELECT groupid, MAX(creationdate)
  FROM highest_datetime
  GROUP BY groupid
);

Another approach:
SELECT id, groupid, creationdate, content
FROM
(
  SELECT *, MAX(creationdate) OVER (PARTITION BY groupid) as max_grp_date
  FROM highest_datetime
) with_max_date
WHERE creationdate = max_grp_date;

Add an ORDER BY clause to the queries, if you want your result rows in a particular order.
Demo: https://dbfiddle.uk/FzP5oloZ
",
76397864,How do I add multiple lines of text to tabs in Swift Playgrounds?,"My app in Swift Playgrounds has tabs. I want to add multiple lines of text to one tab, but I don’t know how. I can’t find an answer anywhere.
 Text(""Developer"").tabItem { Text(""Settings"").tag(5)
                                        VStack {
                                            Image(systemName: ""gear"")
                                                .font(.title)
                                            Text(""Settings"")
                                        }
                                    }

I tried it the normal way, but it just spaced out the specific tab more than all of the other tabs.
",76397894,"I think you need to specify the text you would like to add, and where you would like to add it. This can work to add more text within the main content area of the tab:
TabView {
    VStack {
        Text(""Line 1"")
        Text(""Line 2"")
        Text(""Line 3"")
    }
    .tabItem {
        VStack {
            Image(systemName: ""gear"")
                .font(.title)
            Text(""Settings"")
        }
    }
    .tag(5)

    // ... other tabs ...
}

",
76396628,Cast lambda expression to class object whose constructor accept that lambda (Brief lambda),"I am figuring out a way to program a kinda of FunctionalInterface - Java's similar ones - in C++ (although it already provides these features).
I am practicing with Predicate for the moment. Actually I have already done it:
namespace _utils{

    namespace _functional{

        template <typename T>
        class Predicate: public FunctionalI{
            protected:
                std::function<bool(T)> predicate;

            public:
                Predicate(std::function<bool(T)> predicate){
                    this->predicate = predicate;
                }

                inline bool test(T t){
                    return this->predicate(t);
                }
        };
    }
}

And this is main.cpp:
bool foo(_functional::Predicate<int> predicate){
    return predicate.test(2);
}

int main(){
    _functional::Predicate<int> tmp = _functional::Predicate<int>([] (int x) ->  bool{ return x == 2; });

    std::cout << foo(tmp);

    return 0;
}

Now, of course it works!
My question is about how could I make a brief of that lambda expression? I mean, is there any way to pass ""directly"" the lambda expression (without give it to the Predicate constructor) and then let it construct the Predicate object with right that lambda expression.
I have taken a look at using aliasing and even at functor with operator(), but they did not helped me (maybe I have not been able to do it)
An example of main.cpp which I wish to reach:
bool foo(_functional::Predicate<int> predicate){
    return predicate.test(2);
}

int main(){
    // To much stuff to write
    //std::cout<<foo(_functional::Predicate<int>( [] (int x) -> bool{ return x == 2; } ));

    //What I whish to reach
    foo([] (int x) -> bool{return x == 2;} );

    return 0;
}

Another question to make a brief of lambda expr is: is there a way to make implicit the return type of lambda and even the keyword return?
",76396680,"The problem with foo( [] (int x) -> bool{return x == 2;} ); is that it tries to perform two implicit conversions (lambda to std::function and std::function to Predicate). C++ doesn't allow you to do that. Instead, you can state explicitly that you are initialising Predicate by adding braces:
foo( { [] (int x) -> bool{return x == 2;} } );
//   ^ initialise Predicate object        ^

Or create a template constructor as suggested in comments:
class Predicate : public FunctionalI {
protected:
    std::function<bool(T)> predicate;

public:
    template <typename PredType>
    Predicate(PredType predicate) : predicate{ std::move(predicate) }
    {
        // use member init list instead of reassigning object in constructor
    }
    // ...
};

",
76394826,"Matching records between EventA and the first EventB before the next EventA, in a specific order","I have the following data (fiddle),




id
datec
event




1
2022-09-19 12:16:38
EVENTA


2
2022-09-19 12:16:38
A


3
2022-09-19 12:21:08
B


4
2022-09-19 12:21:12
EVENTD


5
2022-09-19 12:25:18
C


6
2022-09-19 12:25:18
D


7
2022-09-19 12:25:28
E


8
2022-09-19 12:25:29
F


9
2022-09-19 12:25:38
EVENTA


10
2022-09-19 12:25:39
G


11
2022-09-19 12:25:40
H


12
2022-09-19 12:25:48
I


13
2022-09-19 12:27:18
EVENTD


14
2022-09-19 12:29:08
J




I can't figure out how to select values between two others, but in a specific order. Only events between EVENTA and EVENTD should be returned, in that order.
So that results should be the rows with id 1 to 4 and 9 to 13
Tried to do something like the following, but it is giving me id 1,4,9 and 13 omitting what is between them.
SELECT id, datec, event 
FROM table1 
WHERE event BETWEEN 'EVENTA' AND 'EVENTD';

I then tried to use this,
SELECT id, datec, event 
FROM table1 
WHERE (id BETWEEN (SELECT id 
                   FROM table1 
                   WHERE event BETWEEN 'EVENTA' AND 'EVENTD' 
                   LIMIT 1) 
              AND (SELECT id 
                   FROM table1 
                   WHERE event BETWEEN 'EVENTA' AND 'EVENTD' 
                   LIMIT 1,1)) 
   OR (id BETWEEN (SELECT id 
                   FROM table1 
                   WHERE event BETWEEN 'EVENTA' AND 'EVENTD' 
                   LIMIT 2,1) 
              AND (SELECT id 
                   FROM table1 
                   WHERE event BETWEEN 'EVENTA' AND 'EVENTD' LIMIT 3,1));

And it gives me the results but I have many rows in my table.
Can please someone guide me on how to repeat this till the end as i'm sure there is a way to do this but i can't figure out how?
Regards,
pierre
",76395095,"Here's one approach:

compute running counts of armed events and disarmed events, ordering by date
compute a ranking order of records for each armed event count, by ordering on the amount of disarmed events

At this point you should note that this ranking value we generated, assumes value 0 when there's not yet an EventD in our armed_event partition. And it gets value 1 when the first EventD is found, till the successive EventD.
So you can just filter accordingly inside a WHERE clause, when this ranking value is either 0 or is 1 and event is exactly ""EventD"".
WITH cte AS (
    SELECT *, SUM(`event`='EVENTA') OVER(ORDER BY datec, id) AS armed_events,
              SUM(`event`='EVENTD') OVER(ORDER BY datec, id) AS disarmed_events
    FROM Table1
), cte2 AS (
    SELECT *, DENSE_RANK() OVER(PARTITION BY armed_events ORDER BY disarmed_events) -1 AS rn
    FROM cte
)
SELECT `id`, `datec`, `event` 
FROM cte2
WHERE rn = 0 OR (rn = 1 AND `event` = 'EVENTD')
ORDER BY id

Output:




id
datec
event




1
2022-09-19 12:16:38
EVENTA


2
2022-09-19 12:16:38
A


3
2022-09-19 12:21:08
B


4
2022-09-19 12:21:12
EVENTD


9
2022-09-19 12:25:38
EVENTA


10
2022-09-19 12:25:39
G


11
2022-09-19 12:25:40
H


12
2022-09-19 12:25:48
I


13
2022-09-19 12:27:18
EVENTD




Check the demo here.
Note: The last ORDER BY clause is not necessary. It's there just for visualization purposes.
",
76394665,How to pass SQLX connection - a `&mut Trait` as a fn parameter in Rust,"I need to generalize functions to pass an Executor trait for SQLX code. In the code below with a concrete &mut SqliteConnection parameter, main can call process, or it can call process_twice which calls process 2x times.  All sqlx functions require arg type E: Executor.
I need to make my code generic so that conn: &mut SqliteConnection arg is also written with some generic, but so i can use it more than once.
Inside Sqlx, multiple structs implement Executor trait on a mutable reference, e.g.
impl<'c> Executor<'c> for &'c mut SqliteConnection {...}

I was able to convert a SINGLE call (the fn process), but not the  fn process_twice - because executor is not copyable.
async fn process<'a, E>(conn: E) -> anyhow::Result<()>
where E: sqlx::Executor<'a, Database = sqlx::Sqlite> {...}

Full example
// [dependencies]
// anyhow = ""1.0""
// futures = ""0.3""
// sqlx = { version = ""0.6"", features = [ ""sqlite"", ""runtime-tokio-native-tls""] }
// tokio = { version = ""1.28.2"", features = [""macros""] }
//
//  //// TO RUN, must set env var:
// DATABASE_URL='sqlite::memory:' cargo run

use futures::TryStreamExt;
use sqlx::SqliteConnection;
use sqlx::{query, Connection};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let mut conn = SqliteConnection::connect(""sqlite::memory:"").await?;
    process(&mut conn).await?;
    process_twice(&mut conn).await?;
    Ok(())
}

async fn process(conn: &mut SqliteConnection) -> anyhow::Result<()> {
    let sql = query!(""SELECT name FROM sqlite_master"");
    let mut rows = sql.fetch(conn);
    while let Some(row) = rows.try_next().await? {
        println!(""{row:?}"")
    }
    Ok(())
}

async fn process_twice(conn: &mut SqliteConnection) -> anyhow::Result<()> {
    process(conn).await?;
    process(conn).await?;
    Ok(())
}

Similar questions: this
",76395111,"The trick is to not parametrize the whole E but just the type behind the reference:
async fn process_twice<T>(conn: &mut T) -> anyhow::Result<()>
where
    for<'e> &'e mut T: Executor<'e, Database = Sqlite>,
{
    process(&mut *conn).await?;
    process(conn).await
}

That way you can still reborrow the reference. That does mean that you can't take Pool<DB> any more because it only implements Executor for &Pool but should work for your usecase.
",
76397789,Issue with loading jquery after bootstrap,"I tried to research before asking here, but I coun't find any answers. Just for context, I'm using old version of bootstrap v2.0.2. I doubled checked if my paths are correct, and they are.

This is the order of imported scripts that produce the errors above. If I load bootstrap.min.js first, I will get a different error.
<head>
   <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
        
   <script src=""lib/jquery/jquery-3.7.0.min.js""></script>
   <script src=""lib/bootstrap/js/bootstrap.min.js""></script>
        
   <link rel=""stylesheet"" href=""lib/bootstrap/css/bootstrap.min.css"">
   <link rel=""stylesheet"" href=""lib/bootstrap/css/bootstrap-responsive.min.css"">
   <link rel=""stylesheet"" href=""css/app.css"">

   <script src=""lib/angular/angular.min.js""></script> 
   <script src=""js/app.js""></script>
</head>

I've tried to import bootstrap.min.js first, but this the error message I get, and I'm not sure why.
<head>
   <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
        
   <script src=""lib/bootstrap/js/bootstrap.min.js""></script>
   <script src=""lib/jquery/jquery-3.7.0.min.js""></script>
        
   <link rel=""stylesheet"" href=""lib/bootstrap/css/bootstrap.min.css"">
   <link rel=""stylesheet"" href=""lib/bootstrap/css/bootstrap-responsive.min.css"">
   <link rel=""stylesheet"" href=""css/app.css"">

   <script src=""lib/angular/angular.min.js""></script> 
   <script src=""js/app.js""></script>
</head>

I expected it that this small change would resolve the issue, but it didn't.

",76397903,"You cannot use jQuery versions starting with 1.9 with Bootstrap 2.0.2.
jQuery 1.8.3 does work as shown below (note that jQuery must still be loaded before Bootstrap's JavaScript):


<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/1.8.3/jquery.min.js"" integrity=""sha512-J9QfbPuFlqGD2CYVCa6zn8/7PEgZnGpM5qtFOBZgwujjDnG5w5Fjx46YzqvIh/ORstcj7luStvvIHkisQi5SKw=="" crossorigin=""anonymous"" referrerpolicy=""no-referrer""></script>
<script src=""https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/2.0.2/bootstrap.min.js"" integrity=""sha512-jv0RCzX/cFYIF5KkvheT4Xk06YMyEmYAJ8mxZ0pgzwHMgIdT/KGedWD2dK2modKDyaK6DSDwh6ptzTHtfJ0Nng=="" crossorigin=""anonymous"" referrerpolicy=""no-referrer""></script>
<link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/2.3.0/css/bootstrap-responsive.min.css"" integrity=""sha512-c4E36NNKxeFBnGTk/nFCssF4B7gXdZ9hqBnR0fec+DiZ7EnCB2RA42O8HPdgKHFnseb4s6ieQtmT8iksW+QfWw=="" crossorigin=""anonymous"" referrerpolicy=""no-referrer"" />



",
76396498,Finding first empty cell in a row problem,"I would like to clear everything in each sheet starting from the first empty cell in the 10th row (Microsoft Excel) but .End property skip the empty cell between filled ones.
I tried such code:
Sub clearRange()
    
    Dim ws As Worksheet
    Dim lastColumn As Long, i As Long
    
    For i = 2 To Worksheets.Count - 1
        Set ws = Worksheets(i)
        lastColumn = ws.Cells(10, ws.Columns.Count).End(xlToLeft).Column
        ws.Range(ws.Cells(10, lastColumn + 1), ws.Range(""CZ200000"")).ClearContents
    Next i
    
End Sub

Cell D10 is skipped:
Sheet screenshot
Thank you for help in advance.
",76396696,"The statement
lastColumn = ws.Cells(10, ws.Columns.Count).End(xlToLeft).Column

will take you to column J as the last column in your picture (lastColumn = 10), NOT to column D.
To find the first empty cell, start in column A and use end-right, if A10:B10 are both non-empty. If either can be empty, you'll have to address them individually.
Sub clearRange()
    Dim ws As Worksheet
    Dim lastColumn As Long, i As Long
    
    For i = 2 To Worksheets.Count - 1
        Set ws = Worksheets(i)
        lastColumn = ws.Cells(10, 1).End(xlToRight).Column + 1  ' Do this
        ws.Range(ws.Cells(10, lastColumn + 1), ws.Range(""CZ200000"")).ClearContents
    Next i
    
End Sub

If this does or doesn't work for you, let us know the result.
And in future requests, please include a table we can copy. We can't copy a screenshot to a worksheet.
",
76396524,Execution failed while running a Kotlin module in Android Studio,"I am getting following error in Build Output when running a Kotlin module :-
Execution failed for task ':Test:compileKotlin'.
> 'compileJava' task (current target is 1.7) and 'compileKotlin' task (current target is 17) jvm target compatibility should be set to the same Java version.
Consider using JVM toolchain: https://kotl.in/gradle/jvm/toolchain

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --scan to get full insights.

The build.gradle file for this module has the following code:-
plugins {
    id 'java-library'
    id 'org.jetbrains.kotlin.jvm'
}

java {
    sourceCompatibility = JavaVersion.VERSION_1_7
    targetCompatibility = JavaVersion.VERSION_1_7
}

Kindly help me find the error and its resolution.
",76396697,"try to make java version 17 not 1.7 like this:
plugins {
id 'java-library'
id 'org.jetbrains.kotlin.jvm'
}

java {
sourceCompatibility = JavaVersion.VERSION_17
targetCompatibility = JavaVersion.VERSION_17
}

",
76391066,"Altair: only show related field on hover, not all of them","This is a follow-up question to a previous (solved) question about Altair with the same toy dataset.
In the code below, we have a dataset that can be read as: ""two cooks cook1, cook2 are doing a competition. They have to make four dishes, each time with two given ingredients ingredient1, ingredient2. A jury has scored the dishes and the grades are stored in _score.
My code is working well already:

on the y-axis the score is given, both for cook1 and cook2
in the legend, cook1 and cook2 are separate
a vertical line is drawn over the nearest x-axis item when hovering

I would like to make changes to the legend.
Now it shows the following properties:

dish (x-axis number)
ingredient 1, ingredient 2
cook1's dish, cook2's dish
score of the cook that we're hovering closest to
cookX_score (cook1_score, cook2_score) of the cook that we're hovering closest to

Instead, I would like to change it so that the dishes (cook1 and cook2) are not both shown but that, just like with score, only the respective dish is included. So if I hover closer to a point of cook1_score I only want to show cook1 and not also cook2.
My attempts to restrict this have failed, but the reason is simply because Altair does not know that cook1 and cook1_score, and cook2 and cook2_score are linked. But I am not sure how I can tell Altair that so that it only shows the relevant fields when hovering close to a point.
import altair as alt
import pandas as pd

alt.renderers.enable(""altair_viewer"")
df = pd.DataFrame({
    ""ingredient1"": [""potato"", ""onion"", ""carrot"", ""beet""],
    ""ingredient2"": [""tomato"", ""pepper"", ""zucchini"", ""lettuce""],
    ""dish"": [1, 2, 3, 4],
    ""cook1"": [""cook1 dish1"", ""cook1 dish2"", ""cook1 dish3"", ""cook1 dish4""],
    ""cook1_score"": [0.4, 0.3, 0.7, 0.9],
    ""cook2"": [""cook2 dish1"", ""cook2 dish2"", ""cook2 dish3"", ""cook2 dish4""],
    ""cook2_score"": [0.6, 0.2, 0.5, 0.6],
})


value_vars = [c for c in df.columns if c.endswith(""_score"")]
cook_names = [c.replace(""_score"", """") for c in value_vars]
id_vars = [""dish"", ""ingredient1"", ""ingredient2""] + cook_names
df_melt = df.melt(id_vars=id_vars, value_vars=value_vars, var_name=""cook"", value_name=""score"")

nearest_dish = alt.selection(type=""single"", nearest=True, on=""mouseover"", fields=[""dish""], empty=""none"")

# Main chart with marked circles
chart = alt.Chart(df_melt).mark_circle().encode(
    x=""dish:O"",
    y=""score:Q"",
    color=""cook:N"",
    tooltip=id_vars + [""score"", ""cook""]
).add_selection(
    nearest_dish
)

# Draw a vertical rule at the location of the selection
vertical_line = alt.Chart(df_melt).mark_rule(color=""gray"").encode(
    x=""dish:O"",
).transform_filter(
    nearest_dish
)

# Combine the chart and vertical_line
layer = alt.layer(
    chart, vertical_line
).properties(
    width=600, height=300
).interactive()

layer.show()

Note: I'm stuck on Altair<5.
",76397917,"I ended up modifying my DataFrame so that every row contains the dish name and the scores separately as well. That way I could target those items within the tooltip.
import altair as alt
import pandas as pd

alt.renderers.enable(""altair_viewer"")
df = pd.DataFrame({
    ""ingredient1"": [""potato"", ""onion"", ""carrot"", ""beet""],
    ""ingredient2"": [""tomato"", ""pepper"", ""zucchini"", ""lettuce""],
    ""dish_id"": [1, 2, 3, 4],
    ""cook1"": [""cook1 dish1"", ""cook1 dish2"", ""cook1 dish3"", ""cook1 dish4""],
    ""cook1_score"": [0.4, 0.3, 0.7, 0.9],
    ""cook2"": [""cook2 dish1"", ""cook2 dish2"", ""cook2 dish3"", ""cook2 dish4""],
    ""cook2_score"": [0.6, 0.2, 0.5, 0.6],
})


value_vars = [c for c in df.columns if c.endswith(""_score"")]
cook_names = [c.replace(""_score"", """") for c in value_vars]
id_vars = [""dish_id"", ""ingredient1"", ""ingredient2""] + cook_names
df_melt_score = df.melt(id_vars=[""dish_id"", ""ingredient1"", ""ingredient2""], value_vars=value_vars, var_name=""cook_score"", value_name=""score"")
df_melt_score[""cook""] = df_melt_score[""cook_score""].str.replace(""_score"", """")
df_melt_score = df_melt_score.drop(columns=[""cook_score""])
df_melt_dish = df.melt(id_vars=[""dish_id"", ""ingredient1"", ""ingredient2""], value_vars=[""cook1"", ""cook2""], var_name=""cook"", value_name=""dish_name"")

df_concat = pd.merge(df_melt_score, df_melt_dish, on=[""dish_id"", ""cook"", ""ingredient1"", ""ingredient2""], how=""left"").reset_index()

nearest_dish = alt.selection(type=""single"", nearest=True, on=""mouseover"", fields=[""dish_id""], empty=""none"")

# Main chart with marked circles
chart = alt.Chart(df_concat).mark_circle().encode(
    x=""dish_id:O"",
    y=""score:Q"",
    color=""cook:N"",
    tooltip=[""dish_id"", ""ingredient1"", ""ingredient2"", ""cook"", ""dish_name""]
).add_selection(
    nearest_dish
)

# Draw a vertical rule at the location of the selection
vertical_line = alt.Chart(df_melt_score).mark_rule(color=""gray"").encode(
    x=""dish_id:O"",
).transform_filter(
    nearest_dish
)

# Combine the chart and vertical_line
layer = alt.layer(
    chart, vertical_line
).properties(
    width=600, height=300
).interactive()

layer.show()


",
76396585,How to pass a div ID to a PHP function using jQuery AJAX?,"PHP Post doesn't receive jQuery ajax post value
I have created an ajax POST with jQuery to obtain the ID of a div and pass it to a PHP function. The function of php is in a folder phpFunction taht contains the file phpFunc.php in the same root of index.php. The post request is:
$(document).ready(function() {
            $('.openModal').click(function() {
                var divId = $(this).attr('id');
                // Invia l'ID al server tramite richiesta AJAX
                $.ajax({
                    type: 'POST',
                    url: 'http://localhost/icollegati/phpFunction/phpFunc.php',
                    data: {divId: divId},
                    success: function(response) {
                        // Gestisci la risposta del server
                        console.log(""Risposta"",response);
                    }
                });
            });
        });

the PHP function is:
function FetchIdNumber(){
    if ($_SERVER['REQUEST_METHOD'] === 'POST') {
    // Verifica se il parametro ""divId"" è stato inviato tramite POST
    if (isset($_POST['divId'])) {
        $id = $_POST['divId'];
        echo ""ID: "" . $id;
        return $id;
    }
  }
};

Thanks in adavance for the help.
",76396706,"you need to call the FetchIdNumber function in your PHP code.
function FetchIdNumber(){
    if ($_SERVER['REQUEST_METHOD'] === 'POST') {
        // Verifica se il parametro ""divId"" è stato inviato tramite POST
        if (isset($_POST['divId'])) {
            $id = $_POST['divId'];
            echo ""ID: "" . $id;
            return $id;
        }
    }
}

// Call the function
FetchIdNumber();

Make sure to place the function call FetchIdNumber(); after the function definition.
Additionally, please ensure that the path to your PHP file is correct. In your AJAX code, you have specified the URL as http://localhost/icollegati/phpFunction/phpFunc.php. Make sure that this path is accurate and matches the actual location of the PHP file on your server.
",
76396634,Numpy interval of indices around max value on the last dimension,"I have an array arr in which I would like to zero the interval (over the last dim) around every max value.
Here's what I'm generally trying to do (this code doesn't work):
assume that interval_size is some positive odd integer.
argmax = np.argmax(arr, axis=-1)

half_interval = (interval_size - 1) // 2  # assume the interval is of odd size
left_index = argmax - half_interval
right_index = argmax + half_interval
# fix overflow
left_index[left_index < 0] = 0
right_index[right_index >= arr.shape[-1]] = arr.shape[-1] - 1

# this is just illustrative and doesn't work:
arr[left_index:right_index] = 0  # zero the interval in-place

",76396739,"Given that the intervals could be different sizes, I'd go for boolean indexing:
import numpy as np

arr = np.random.randint(10, size=(8, 8))
half_interval = 2

# Use keepdims=True for easy broadcasting
argmax = arr.argmax(axis=-1, keepdims=True)

# ""Normal"" indices for the last axis
idx = np.arange(arr.shape[-1])

# True when ""normal"" index is between
# [argmax - half_interval, argmax + half_interval]
mask = (argmax - half_interval <= idx) & (idx <= argmax + half_interval)

arr[mask] = 0
# or out = np.where(mask, 0, arr) if you don't want to modify arr

Results:
>>> arr
array([[6, 8, 5, 4, 6, 9, 3, 6],
       [9, 4, 9, 3, 9, 0, 3, 6],
       [7, 9, 7, 1, 0, 3, 0, 0],
       [9, 9, 4, 8, 3, 8, 3, 2],
       [9, 7, 6, 9, 2, 0, 9, 6],
       [1, 7, 9, 7, 6, 2, 9, 4],
       [8, 0, 8, 9, 6, 5, 0, 9],
       [2, 0, 9, 5, 9, 0, 2, 4]])
>>> argmax
array([[5],
       [0],
       [1],
       [0],
       [0],
       [2],
       [3],
       [2]], dtype=int64)
>>> mask
array([[False, False, False,  True,  True,  True,  True,  True],
       [ True,  True,  True, False, False, False, False, False],
       [ True,  True,  True,  True, False, False, False, False],
       [ True,  True,  True, False, False, False, False, False],
       [ True,  True,  True, False, False, False, False, False],
       [ True,  True,  True,  True,  True, False, False, False],
       [False,  True,  True,  True,  True,  True, False, False],
       [ True,  True,  True,  True,  True, False, False, False]])
>>> out
array([[6, 8, 5, 0, 0, 0, 0, 0],
       [0, 0, 0, 3, 9, 0, 3, 6],
       [0, 0, 0, 0, 0, 3, 0, 0],
       [0, 0, 0, 8, 3, 8, 3, 2],
       [0, 0, 0, 9, 2, 0, 9, 6],
       [0, 0, 0, 0, 0, 2, 9, 4],
       [8, 0, 0, 0, 0, 0, 0, 9],
       [0, 0, 0, 0, 0, 0, 2, 4]])

The above should work for arrays with more than 2 axes as well due to broadcasting. But only because you argmax over the final axis.
",
76395036,Converting np.int16 to torch.ShortTensor,"I have many NumPy arrays of dtype np.int16 that I need to convert to torch.Tensor within a torch.utils.data.Dataset.  This np.int16 ideally gets converted to a torch.ShortTensor of size torch.int16 (docs).
torch.from_numpy(array) will convert the data to torch.float64, which takes up 4X more memory than torch.int16 (64 bits vs 16 bits).  I have a LOT of data, so I care about this.
How can I convert a numpy array to a torch.Tensor minimizing memory?
",76395145,"Converting a numpy array to torch tensor:
array = np.ones((1000, 1000), dtype=np.int16)
print(""NP Array size: {}"".format(array.nbytes))
t = torch.as_tensor(array) # as_tensor avoids copying of array 
print(""Torch tensor type: {}"".format(t.dtype))
print(""Torch tensor size: {}"".format(t.storage().nbytes()))

NP Array size: 2000000
Torch tensor type: torch.int16
Torch tensor size: 2000000

",
76397906,"In sveltekit, what's the convention for splitting client side code between route files?","Both +page.js/ts and the <script><script/> tags within +page.svelte files are for processing client side code.
What's the typical convention for choosing between these 2 files?
",76397930,"The question has been clearly answered in the Documentation. The separate file is for loading and constructing data that is required by your route, the script tag is for DOM manipulation and client side logic.
That is the convention as per the docs but you are free to use both methods as per your preference but +page.server.js/ts is strictly  for server side logic like authentication and database calls.
",
76396256,How to send a widget object created later to callback in Tk?,"I am new to Perl Tk.
I have option menu.  In its callback, I want to pass a table object which is created later. As per the selection of the option, I want to render specific data into the table.
Since my optionmenu is created before, how can I send the table object to it?
In the code, I have commented it as 'HOW TO PASS ?
use Tk;
use Tk::Table;
use Tk::NoteBook;
use Tk::Optionmenu;

my $data = { 'Jan' =>  { 'id' => 1, 'product' => 'new'} , 'Feb' => {'id'=>2, 
'product'=> 'latest'} }; 

&tk_gui(); 

sub tk_gui {

my $mw = MainWindow->new;
$mw->geometry(""500x500"");
my $f = $mw->Frame()->pack(-side=>'top');
my ($var, $tvar);
my $opt = $f->Optionmenu(
-options => [[jan =>1], [feb =>2]],
-command => sub { 
        my @nums = @_;
        #HOW TO PASS ?
        &show_table_data($nums[0], $table)

        
    },
-variable => \$var,
-textvariable => \$tvar,
)->pack(-side => 'left', -anchor => 'n',);  

my $f2 = $mw->Frame()->pack(-side=>'bottom');
my $table = $f2->Table( -columns => 2);
my @col = qw(id product);

foreach my $c (0 .. 1) {
if ($c) {
    my $t = $table->Label(-text => 'product'); 
    $table->put(0, $c, $t); 
} else {
    my $t = $table->Label(-text => 'id'); 
    $table->put(0, $c, $t); 

 }

}

$table->pack();

MainLoop;

}

sub show_table_data {
    my $num = shift;
    my $table = shift;
}

",76396740,"
Since my optionmenu is created before, how can I send the table object to it?

Just declare the $table variable before it is used in the callback. Even if the variable has not been defined when the callback is compiled, it will be defined when the callback is called at runtime. Example:
use feature qw(say);
use strict;
use warnings;
use Tk;
use Tk::Table;
use Tk::NoteBook;
use Tk::Optionmenu;

{    
    tk_gui(); 
}

sub tk_gui {
    my $mw = MainWindow->new;
    $mw->geometry(""500x500"");
    my $f = $mw->Frame()->pack(-side=>'top');
    my ($var, $tvar);
    my $table;  # <-- declare the variable here..
    my $opt = $f->Optionmenu(
        -options => [[jan =>1], [feb =>2]],
        -command => sub { 
            my @nums = @_;
            show_table_data($nums[0], $table);                
        },
        -variable => \$var,
        -textvariable => \$tvar,
    )->pack(-side => 'left', -anchor => 'n',);  

    my $f2 = $mw->Frame()->pack(-side=>'bottom');
    $table = $f2->Table( -columns => 2);

    foreach my $c (0 .. 1) {
        if ($c) {
            my $t = $table->Label(-text => 'product'); 
            $table->put(0, $c, $t); 
        } else {
            my $t = $table->Label(-text => 'id'); 
            $table->put(0, $c, $t); 

        }
    }
    $table->pack();
    MainLoop;
}

sub show_table_data {
    my $num = shift;
    my $table = shift;
    return if !defined $table;
    say ""Num = $num"";
    say ""table = $table"";
}

",
76394556,Android need fullwidth drawerlayout. it should appear and disappear when hamburger icon is clicked,"xml code
    <?xml version=""1.0"" encoding=""utf-8""?>
<androidx.constraintlayout.widget.ConstraintLayout
    xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:background=""@drawable/app_background_01""
    tools:context="".FullscreenActivity"">
    <androidx.constraintlayout.widget.ConstraintLayout
        android:id=""@+id/topbar""
        android:layout_width=""match_parent""
        android:layout_height=""50dp""
         app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toTopOf=""parent"">
        <ImageView
            android:id=""@+id/actionDrawer""
            android:layout_width=""28dp""
            android:layout_height=""28dp""
            android:layout_marginStart=""16dp""
            app:layout_constraintBottom_toBottomOf=""parent""
            app:layout_constraintStart_toStartOf=""parent""
            app:layout_constraintTop_toTopOf=""parent""
            app:srcCompat=""@drawable/ic_hamburger"" />

        <ImageView
            android:id=""@+id/actionHome""
            android:layout_width=""28dp""
            android:layout_height=""28dp""
            android:layout_marginRight=""16dp""
            app:layout_constraintBottom_toBottomOf=""parent""
            app:layout_constraintEnd_toEndOf=""parent""
            app:layout_constraintTop_toTopOf=""parent""
            app:srcCompat=""@drawable/ic_home"" />
    </androidx.constraintlayout.widget.ConstraintLayout>

    <androidx.constraintlayout.widget.ConstraintLayout
        android:id=""@+id/containerFragments""
        android:layout_width=""match_parent""
        android:layout_height=""0dp""
        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toBottomOf=""@id/topbar"">

        <TextView
            android:layout_width=""match_parent""
            android:layout_height=""match_parent""
            android:fontFamily=""sans-serif-condensed""
            android:text=""This is Home page""
            android:textAlignment=""center""
            android:textColor=""#FFFFFF""
            android:textSize=""40sp""
            app:layout_constraintBottom_toBottomOf=""parent""
            app:layout_constraintEnd_toEndOf=""parent""
            app:layout_constraintStart_toStartOf=""parent""
            app:layout_constraintTop_toTopOf=""parent"">

        </TextView>
    </androidx.constraintlayout.widget.ConstraintLayout>

    <androidx.drawerlayout.widget.DrawerLayout
        android:id=""@+id/drawer""
        android:layout_width=""match_parent""
        android:layout_height=""match_parent""
        android:layout_marginTop=""50dp""
        app:layout_constraintBottom_toBottomOf=""parent""
        app:layout_constraintEnd_toEndOf=""parent""
        app:layout_constraintStart_toStartOf=""parent""
        app:layout_constraintTop_toBottomOf=""@+id/topbar"">
            <LinearLayout
                android:layout_width=""match_parent""
                android:layout_height=""match_parent""
                app:layout_constraintEnd_toEndOf=""parent""
                app:layout_constraintStart_toStartOf=""parent""
                android:background=""@drawable/app_background_01""
                android:orientation=""vertical""
                android:layout_gravity=""left"">
                <androidx.constraintlayout.widget.ConstraintLayout
                    android:layout_width=""match_parent""
                    android:layout_height=""wrap_content""
                    app:layout_constraintEnd_toEndOf=""parent""
                    app:layout_constraintStart_toStartOf=""parent"">
                    <ImageView
                        android:id=""@+id/imageView2""
                        android:layout_width=""50dp""
                        android:layout_height=""50dp""
                        app:layout_constraintBottom_toBottomOf=""parent""
                        app:layout_constraintStart_toStartOf=""parent""
                        app:layout_constraintTop_toTopOf=""parent""
                        app:srcCompat=""?android:attr/textSelectHandle"" />
                </androidx.constraintlayout.widget.ConstraintLayout>
            </LinearLayout>
    </androidx.drawerlayout.widget.DrawerLayout>
</androidx.constraintlayout.widget.ConstraintLayout>

kotlin code
package com.example.testdrawer

import androidx.appcompat.app.AppCompatActivity
import android.os.Bundle
import android.view.Gravity
import androidx.constraintlayout.widget.ConstraintLayout
import androidx.core.view.GravityCompat
import com.example.testdrawer.databinding.ActivityFullscreenBinding

class FullscreenActivity : AppCompatActivity() {
    private lateinit var binding: ActivityFullscreenBinding

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        binding = ActivityFullscreenBinding.inflate(layoutInflater)
        setContentView(binding.root)
        setListeners()
    }
    private fun setListeners() {
        binding.actionDrawer.setOnClickListener {
            if (binding.drawer.isDrawerOpen(GravityCompat.START)) {
                binding.drawer.closeDrawer(Gravity.LEFT)
            } else {
                binding.drawer.openDrawer(GravityCompat.START)
            }
        }
    }
}

Behaviour Required
1- Home screen comes in start
2- When click on hamburger icon the drawerlayout should come with full height and width also containing hamrbuger and home icons.
3- When click on hamburger icon the drawerlayout should close and the home screen should come.
Issues
1- drawerlayout is not full width and required functionality is not achievable.
The below is the homescreen

Want the drawerlayout to fully occupy the screen as in the design its having full background and icons on top.

When I tried Drawerlayout constraint in xml toptotopof parents I was not able to click on hamburger icon
",76395167,"you can add negative margin for the sides of your drawer layout to fill the whole screen.
android:layout_marginRight=""-64dp""

",
76397897,How do I disable bpython colored output?,"bpython by default has a really nice blue theme, but it doesn't go well with light backgrounds. How can I disable its colour output and have it give just plain black (on Arch Linux)?
",76397934,"You could create a theme that only uses black as an output color. That might look like:
[syntax]
keyword = k
name = k
comment = k
string = k
error = k
number = K
operator = K
punctuation = k
token = K
paren = K

[interface]
background = d
output = k
main = k
prompt = k
prompt_more = k
right_arrow_suggestion = K

This is based on the default theme, replacing all the colors with k (for normal black) or K (for bold black).
Save this in ~/.config/bpython/light.theme and then set color_scheme = light in ~/.config/bpython/config.
",
76396610,Remove everything in brackets at the end,"I'd like to removing everything that is is brackets () if (and only if) they are at the end and not do match the following pattern \(\d{4}\p{Pd}\d{4}\) *. This pattern is nothing more than date range in brackets, eg. (1920-2988).
For instance, I'd like to match/capture (for removing, ie. string.replaceAll(my_regex_here, """")):

foo bar (blah)

foo bar (blah) blah (blah)


I don't like to match:

some (blah) data
some date following (1920-1921).

I have the following regex: \s*(.+?)\s*$. It tends to match too much:

some data (..) match (match)
some data (1920-1977)

",76396741,"Use negative look ahead to avoid date ranges, then actually match it using [^()]+?:
\s*\(                  # Match 0+ spaces, a '(',
(?!\d{4}\p{Pd}\d{4}\)) # which is not followed by a date range and a ')',
[^()]+                 # 1+ non-parenthesis characters and
\)\s*$                 # ')' then 0+ spaces right before the end of line.

Try it on regex101.com.
The above regex will not match:
parentheses with no content ()
years with more than 4 digits (1234-56789)
or less than 4 (123-4567)
nested ((brackets))
mismatched (brackets

",
76397679,TrackBar: no Scroll events in the list,"I replaced my TrackBar with another TrackBar but when I look for the Scroll event of the new TrackBar I can't find it.
Here is the cole of the old TrackBar:
private void TrackBar_Scroll(object sender, ScrollEventArgs e)

How can I create the Scroll event for the new TrackBar, as it doesn't exist in the event list?
",76397939,"Looking at the code of the HZH_Controls in their Github, it would appear you need to handle the ValueChanged event.
/// <summary>
/// Occurs when [value changed].
/// </summary>
[Description(""值改变事件""), Category(""自定义"")]
public event EventHandler ValueChanged;

Other parts of the code suggest that this event is raised Scrollbar is moved
",
76394788,Reorder data from multiplexed column values to rows,"I have a table organization as below where temperature measurements are all stored in one column and there are 14 different temperature samples at different depths per timestamp.
select * from water_temp order by unixtimestamp desc limit 28;





id
unixtimestamp
depthname
depth
temperature




481042727209
1685770037
600ft
600
38.5910000


481042727208
1685770037
500ft
500
38.6960000


481042727207
1685770037
400ft
400
38.6570000


481042727206
1685770037
300ft
300
38.9040000


481042727205
1685770037
200ft
200
39.3400000


481042727204
1685770037
150ft
150
39.5030000


481042727203
1685770037
100ft
100
40.2140000


481042727202
1685770037
80ft
80
41.3850000


481042727201
1685770037
60ft
60
44.1530000


481042727200
1685770037
50ft
50
46.2590000


481042727199
1685770037
40ft
40
47.2420000


481042727198
1685770037
30ft
30
49.7390000


481042727197
1685770037
20ft
20
55.3010000


481042727196
1685770037
10ft
10
61.8840000


481041534602
1685769380
600ft
600
38.5910000


481041534601
1685769380
500ft
500
38.6400000


481041534600
1685769380
400ft
400
38.7130000


481041534599
1685769380
300ft
300
38.9040000


481041534598
1685769380
200ft
200
39.2840000


481041534597
1685769380
150ft
150
39.5600000


481041534596
1685769380
100ft
100
40.2700000


481041534595
1685769380
80ft
80
41.4410000


481041534594
1685769380
60ft
60
44.3220000


481041534593
1685769380
50ft
50
46.3150000


481041534592
1685769380
40ft
40
47.3540000


481041534591
1685769380
30ft
30
49.9070000


481041534590
1685769380
20ft
20
56.1450000


481041534589
1685769380
10ft
10
62.5030000




Hoping to get output in this format:




unixtimestamp
depth10
depth20
depth30
...




1685769380
62.5
56.14
49.91
...


1685770037
61.8
55.3
49.74
...


...
...
...
...
...




I have inherited this table format. I have tried using distinct but can't find a way to have a distinct timestamp and depth. Thinking I may have to dump the data and use python to read and reorganize in a one row per timestamp.
Is there a straightforward SQL query to get the data to list in one row per timestamp with each depth temperature as columns? I want to output the data in csv format where each row is one timestamp.
Also is there some reason to organize data in a DB in the manner? I am tying to understand. Seems to me the more intuitive/useful way would be to have one column per temperature depth and one row per timestamp.
Thanks in advance.
",76395179,"
Is there some reason to organize data in a DB in the manner? I am tying to understand. Seems to me the more intuitive/useful way would be to have one column per temperature depth and one row per timestamp.

You organize data in a way so that your queries are faster or to make data more consumable for your purposes. If having pivoted data make them more consumable for your needs, you should go for it.
Just to give you an example, if you had to get depth values when the temperature went higher than 40, attempting to do it with the output you're requiring, would likely force you to go through unpivoting, while it would be straight simple if you used your original table.
Conversely, if you need to do some kind of reporting, and observe only temperatures for specific depths (say depth 10 and depth 100), using the input table would require you a filtering, while using the output table can give you that instantly.

What you're looking for is called ""pivot"", and allows you to change rows into columns. Since in the output there are 14 columns dedicated to depth (from the data I assume they're always 14 for each timestamp), you should expect to put 14 computed fields inside the SELECT statement, like:
SELECT unixtimestamp,
       ... AS depth10,
       ... AS depth20,
       ... AS depth30,
       ... AS depth40,
       ...
       ... AS depth600
FROM water_temp 

Then you realize that you actually need to select your temperatures, but only when the depth corresponds to your selected depth. Something you can do with a CASE expression (or an IF, if we want):
SELECT unixtimestamp,
       CASE WHEN depth =  10 THEN temperature END AS depth10,
       CASE WHEN depth =  20 THEN temperature END AS depth20,
       CASE WHEN depth =  30 THEN temperature END AS depth30,
       CASE WHEN depth =  40 THEN temperature END AS depth40,
       ...
       CASE WHEN depth = 600 THEN temperature END AS depth600
FROM water_temp 

And when you attempt to do that, you realize you will have a kind of squared matrix, for which unique timestamp has now 14 records, each of which with one non-null value (on the corresponding depth column).
Since you wanted one record only, for each of your timestamps, one of the best options for reducing the size of tables is aggregation (as long as it ""aggregates"" data). You need to aggregate by grouping on your timestamps (because you want one record for each timestamp), and using an aggregate function that can compress one non-null value with other null values, and that's either MIN or MAX. Your code then becomes:
SELECT unixtimestamp,
       MAX(CASE WHEN depth =  10 THEN temperature END) AS depth10,
       MAX(CASE WHEN depth =  20 THEN temperature END) AS depth20,
       MAX(CASE WHEN depth =  30 THEN temperature END) AS depth30,
       MAX(CASE WHEN depth =  40 THEN temperature END) AS depth40,
       ...
       MAX(CASE WHEN depth = 600 THEN temperature END) AS depth600
FROM water_temp 
GROUP BY unixtimestamp

which gives you exactly what you were looking for.
",
76396158,"""instagram api"" business discovery api on python return 'Invalid OAuth access token - Cannot parse access token' error","I'm writing python code to use instagram graph api/business discovery to get data.
But I get ""Invalid OAuth access token"" error but I could not figure out how to solve this problam, please enlighten me.
Following code works fine on Graph Api Explorer
IgUserId?fields=business_discovery.username(bluebottle){followers_count,media_count,name}

But when I operate following code on Python file in Jupyter lab
import requests
import pandas as pd
pd.set_option('display.max_rows', None)

# information
business_account_id = ""same as IgUserId""
token = ""one that generated on Graph Api Explorer""
fields = ""followers_count,media_count,name""
version = ""v17.0""
username = ""bluebottle""

def user_info(version, igUserId,token,username,fields):
    request_url = ""https://graph.facebook.com/{version}/{igUserId}?fields={fields}&access_token={access-token}""
    response = requests.get(request_url)
    return response.json()

print(user_info(business_account_id,token,username,fields,version))

I get following error

{'error': {'message': 'Invalid OAuth access token - Cannot parse access token', 'type': 'OAuthException', 'code': 190, 'fbtrace_id': 'Just for case I'm deleteing this bit'}}

To avoid my misunderstanding to confuse anybody, following is picture of Graph Api Explorer where I got Access Token which I just copy and pasted from top black shadowed bit.

",76396782,"I am not a Python guy, but just to help you I modified your Python script and while doing that, I saw many issues with your script. Use the below script which I tested with my IG User ID and access_token and is working perfectly fine.
import requests
import pandas as pd
pd.set_option('display.max_rows', None)

# information
business_account_id = ""YOUR_IG_USER_ID""
access_token = ""YOUR_ACCESS_TOKEN""
fields = ""followers_count,media_count,name""
version = ""v17.0""
username = ""bluebottle""

def user_info(version, igUserId,access_token,username,fields):
    request_url = f""https://graph.facebook.com/{version}/{igUserId}?fields=business_discovery.username({username}){{{fields}}}&access_token={access_token}""
    print(request_url);
    response = requests.get(request_url)
    return response.json()

print(user_info(version,business_account_id,access_token,username,fields))

Output:-
{'business_discovery': {'followers_count': 442462, 'media_count': 2076, 'name': 'Blue Bottle Coffee', 'id': '17841401441775531'}, 'id': '17841448XXXXX'}

I used Python 3.10.6 to test this script.
Hope this helps.
",
76397787,Renaming dataframe columns via User Defined Function via Pandas,"i'm trying to rename dataframe columns through User Defined Function without success.
In particular it seems that doesn't exist ""rename"" method when called inside a UDF.
Following an example to better explain my problem:
import pandas as pd`

data = {'Age': [21, 19, 20, 18,80,90],'Stream': [88, 65,99, 765,65,55],'Percentage': [88, 92, 95, 70,55,47]}

df = pd.DataFrame(data, columns=['Age', 'Stream', 'Percentage'])

print(""Given Dataframe :n"", df)

def modname(frame):


    frame=frame.rename(columns={'Age':'aa','Stream':'bb','Percentage':'cc'})
    
    return frame

df=df.apply(modname)

print(df)

`
I greatly appreciate your help
i tried the same code without UDF and it works
",76397970,"Instead of doing:
df=df.apply(modname)

You should do:
df = modname(df)

",
76395115,How to create comment block in visual studio fast?,"In Eclipse I can just type /* and enter, and it would form a perfect comment block for complex comments above the code:
/*
 *
 */

How to do the same or similar in Visual Studio?
",76395187,"What language? I'm going to assume C++.
Tools > Options > Text Editor > C/C++ > Advanced > Brace Completion > Complete Multiline Comments > True
This setting will do exactly what you describe. Typing /* will auto-complete the */ and any new line entered after /* will auto-insert a new * at the beginning of the line.
Alternatively:

Download and install Visual Assist: https://www.wholetomato.com/
Highlight code to be commented.
Shift + 8 or * on the keypad.
Profit.

",
76396291,Error encountered when trying to plot individual trees from cforest() forest using the partykit package in R,"I am doing conditional inference tree analysis using the partykit package in R. I want to plot any tree that is extracted from the forest grown by cforest(). But I got an error message when I am trying with the plot function. The following is a chunk of codes that may produce the like error message with the iris data.
Code:
library(partykit)

cf <- cforest(Species ~., data=iris)
tr <- gettree(cf, tree=1)
plot(tr)

Errors in console:

Error in Summary.factor(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, :
‘min’ not meaningful for factors

I am expecting plots for individual trees in the cforest() result.
",76396797,"I have got an answer after researching into it.
Since plot works well with ctree() objects, I compared the extracted tree from cforest and the tree generated by ctree() and found the following difference in their data structure.
For ctree object, which can be plotted:
$ fitted:'data.frame':  150 obs. of  3 variables:
  ..$ (fitted)  : int [1:150] 2 2 2 2 2 2 2 2 2 2 ...
  ..$ (weights) : num [1:150] 1 1 1 1 1 1 1 1 1 1 ...
  ..$ (response): Factor w/ 3 levels ""setosa"",""versicolor"",..: 1 1 1 1 1 1 1 1 1 1 ...

but for a tree from cforest() result, which cannot be plotted:
$ fitted:'data.frame':  150 obs. of  4 variables:
  ..$ idx       : int [1:150] 1 2 3 4 5 6 7 8 9 10 ...
  ..$ (response): Factor w/ 3 levels ""setosa"",""versicolor"",..: 1 1 1 1 1 1 1 1 1 1 ...
  ..$ (weights) : int [1:150] 0 1 1 1 1 1 1 1 0 0 ...
  ..$ (fitted)  : int [1:150] 2 2 2 2 2 2 2 2 2 2 ...

Please note that the variables (response), (weights) and (fitted) are in different columns in the data structure of ""fitted"" dataframe in these two trees.
Therefore, I use the following manipulation to adjust the ""fitted"" dataframe structure in the cforest tree object, and plotting is successful:
library(partykit)
cf <- cforest(Species ~., data=iris)
tr <- gettree(cf, tree=1)
nfitted <- data.frame(tr$fitted$`(fitted)`,tr$fitted$`(weights)`,tr$fitted$`(response)`)
colnames(nfitted) <- c('(fitted)', '(weights)', '(response)')
tr$fitted <- nfitted
plot(tr)

Hope this will help those who encounter the same problem with the plotting of trees from cforest() in the partykit package.
",
76397687,SQL ORDER BY next,"Please tell me how to sort in PostgreSQL (15+) such a table:
id |  name  | next_id
1  | Test01 |  2
2  | Test02 |  3
3  | Test03 |  6
4  | Test04 |  5
5  | Test05 |  null
6  | Test06 |  4
7  | Test07 |  null

It takes an SQL query to order by the next_id column - it's a pointer to the next row. The result I expect is:
id |  name  | next_id
1  | Test01 |  2
2  | Test02 |  3
3  | Test03 |  6
6  | Test06 |  4
4  | Test04 |  5
5  | Test05 |  null
7  | Test07 |  null

It is necessary to read the next_id field for each record, find this record and display it after the existing one. After all, this field can change, so sorting in ascending order will not work and other tricks.
So far, there are no ideas how to do it gracefully. Except for loops and temporary tables.
",76397971,"The following demonstrates an approach to sorting as requested:
WITH RECURSIVE t(id, name, next_id) AS (
  VALUES (1  , 'Test01',   2),
         (2  , 'Test02',   3),
         (3  , 'Test03',   6),
         (4  , 'Test04',   5),
         (5  , 'Test05',   NULL),
         (6  , 'Test06',   4),
         (7  , 'Test07',   NULL)),
cte AS (
  SELECT t.id, t.name, t.next_id, t.id AS root_id, 1 AS depth
    FROM t
   WHERE NOT EXISTS (SELECT * FROM t t2 WHERE t.id = t2.next_id)
  UNION ALL
  SELECT t.id, t.name, t.next_id, cte.root_id, cte.depth + 1 AS depth
    FROM t
    JOIN cte ON (t.id = cte.next_id))
SELECT cte.id, cte.name, cte.next_id
  FROM cte
 ORDER BY cte.root_id, cte.depth, cte.id;

This query assumes that there are no cycles in the path defined by next_id.
The query first initializes the recursive CTE by identifying the rows that are not referenced by any next_id, memoizing the root id, and initializing depth. The recursive portion of the CTE finds the rows referenced by these root rows and incrementing depth. The recursive part repeats using the rows discovered by the prior iteration until no rows are found.
The final step sorts by root_id, depth, and id. Including id in the sort is done to create a deterministic order for the case of ids with multiple next_ids (a tree structure instead of linear paths): it can be left out if only linear paths are allowed.
",
76394209,Creating AWS Security Group for a range of ports in Terraform,"I want to create the following AWS security group in Terraform, using Hashi Corp Language. In this configuration the second ingress rule contains the range of ports, but such a syntax is not supported by Terraform.
In current implementation Terraform treats port's range as the math expression and computes the difference which gives the negative value. If I close the port range in parenthesis it fails because it requires number format.
What is the appropriate way to create ingress rule for a large range of ports?
  resource ""aws_security_group"" ""sg_nx"" {
  name   = ""sg_nx""
  vpc_id = aws_vpc.vpc.id
  ingress {
    from_port   = 4000
    to_port     = 4000
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }
  ingress {
    from_port   = 4011-4999
    to_port     = 4011-4999
    protocol    = ""udp""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

Using list does not work either. I know Terraform allows dynamicly generated resources, how to implement it in this case?
",76395234,"If you look at AWS docs https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html they support port range. The way to do it with terraform is:
from_port = 4011 and to_port = 4999
See also the terraform docs:
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group#from_port
",
76396441,Best practice for allowing multiple Vulkan instances as part of 'renderer' objects,"I'm writing a library in C to abstract multiple graphics APIs including Vulkan behind the same interface.
Currently, the library allows for the creation of multiple 'renderer' objects, which contain a whole 'state' of a graphics API - as it stands, this translates to having one Vulkan instance per renderer, for example.
However, I am using volk to load Vulkan symbols. This library loads global functions based on the extensions enabled for a Vulkan instance, and problems arise when attempting to create multiple instances, potentially with different layers/extensions enabled.
For example, if I create an instance with debug utils enabled, the associated functions like vkCreateDebugUtilsMessengerEXT are loaded as expected. If I then create another instance without debug utils enabled, these functions seem to be unloaded, which sets their function pointers to NULL, causing a segfault when the instance that enabled them attempts to call them.
At the end of the day, supporting multiple Vulkan instances seems to be introducing a lot of issues - is there perhaps a better way of abstracting this design, or should I just limit the user to creating one Vulkan renderer (and therefore one instance) per program?
",76396804,"Function pointers obtained through a particular VkInstance object (through vkGetInstanceProcAddr) cannot be used with different instance objects. This is just as true of the Vulkan SDK as it is of Volk. That is, a function pointer obtained as the result of calling volkLoadInstance(some_instance) can only be used on other some_instance objects or those derived from them (like VkDevices).
Note that Volk doesn't hand you function pointers; it initializes global function pointers. Therefore, if you call volkLoadInstance, all of the global functions it hands you are bound to that instance. You can call it again for a different instance, but you're just changing which global pointers you're talking about.
That is, Volk is specifically designed to be used with a single instance. It has functionality for easily using it with multiple devices without dynamic dispatch logic (volkLoadDeviceTable returns a table of function pointers specific to a device), but it has no similar construct for instances.
As such, trying to allow each ""renderer"" to have its own VkInstance object is going to run into problems. volkLoadInstance is not a fast function.
The reason Volk is written this way is because creating multiple instances like this is generally... not useful. Vulkan instance extensions tend to be about the interaction between your code and the operating system.
Consider VK_KHR_surface. This is an instance extension that acts as glue between your program and an OS window. If you want to render into a window, you need this instance extension. But the glue between the OS window (represented by a VkSurfaceKHR) and a Vulkan device is governed by VK_KHR_swapchain, which is a device extension. A device created from an instance doesn't have to use that extension. If it doesn't, then it won't be able to render into a surface.
But it doesn't harm the device in any way to have been created from an instance that uses VK_KHR_surface. So you should get your ""renderers"" together and set down a list of instance extensions that they could rely on, and then require them for the one VkInstance that they all share.

Broadly speaking, separate VkInstance objects exist primarily for DLLs, where there is a hard barrier between the global state of two pieces of code that happen to exist in the same program. One DLL can use a Vulkan instance, while the main program uses a separate one. The separate instances, and attendant global instance function pointers, make it easy for these two pieces of code to co-exist without stomping on each other.
",
76397780,"Compose WebView - avoid closing application when I press button back, but save ability go back in web view","I am experiencing an issue with the BackButton in a Compose WebView. The current code works well with the WebView backstack. However, when the backstack is empty and the back button is pressed, it causes the application to close, which is not the desired behavior.
@Composable
fun StatScreen(zeusPrefManager: ZeusPrefManager){

    var fPCallback: ValueCallback<Array<Uri>>? by remember {
        mutableStateOf(null)
    }

    val launcher = rememberLauncherForActivityResult(
        contract = ActivityResultContracts.GetMultipleContents()
    ) {
        fPCallback!!.onReceiveValue(it.toTypedArray())
    }

    var backEnabled by remember { mutableStateOf(false) }
    var webView: WebView? = null


    var gameStat by remember {
        mutableStateOf("""")
    }

    LaunchedEffect(key1 = ""goStat""){
        val destination = zeusPrefManager.zeusStatDataFlow.first()
        gameStat = destination
    }

    if (gameStat != """"){
        AndroidView(factory = { context ->
            WebView(context).apply {
                layoutParams = ViewGroup.LayoutParams(
                    ViewGroup.LayoutParams.MATCH_PARENT,
                    ViewGroup.LayoutParams.MATCH_PARENT
                )
                settings.javaScriptEnabled = true
                settings.domStorageEnabled = true
                settings.loadWithOverviewMode = true
                settings.userAgentString = ZeusConstan.converter007(settings.userAgentString)

                webViewClient = object: WebViewClient(){

                    override fun onPageStarted(view: WebView, url: String?, favicon: Bitmap?) {
                        backEnabled = view.canGoBack()
                    }

                    override fun onPageFinished(view: WebView?, url: String?) {
                        super.onPageFinished(view, url)

                    }
                }

                webChromeClient = object : WebChromeClient(){
                    override fun onShowFileChooser(
                        webView: WebView?,
                        filePathCallback: ValueCallback<Array<Uri>>?,
                        fileChooserParams: FileChooserParams?
                    ): Boolean {

                        if (fPCallback != null){
                            fPCallback!!.onReceiveValue(null)
                        }
                        fPCallback = filePathCallback
                        launcher.launch(""image/*"")
                        return true
                    }
                }
                loadUrl(gameStat)
            }
        }, update = {
            webView = it
        })

        BackHandler(enabled = backEnabled) {
            if (webView!!.canGoBack()){
                webView?.goBack()
            } else {
                //Do nothing but webView!!.canGoBack() is always true, and close the app
            }

        }
    }
}

I didn't override onBackPressed() in Activity.
I need to avoid closing application when I press button back
",76397994,"you can make use of the onBackPressedDispatcher in your activity. Change your code as below-
    @Composable
fun StatScreen(zeusPrefManager: ZeusPrefManager, onBackPressed: () -> Unit) {
    
    BackHandler(enabled = backEnabled) {
        if (webView!!.canGoBack()) {
            webView?.goBack()
        } else {
            onBackPressed()
        }
    }
}

In your activity, override the onBackPressed() method and pass it to the StatScreen composable function:
override fun onBackPressed() {
    // Handle the back button press here
}

",
76395225,Happend multiple .csv file from Drive Folder to Unique Sheet in Google,"Hello to everyone guys!
I've got a problem with the code below; my purpose is to retrieve all the .csv file in a google drive folder and put them all in sequence in a specific sheet in Google.
The error code that it get is: ""Exception: Cannot convert 'function () { [native code] }1' to int.""
Could you please help me solving that issue?
Many thanks in advance
function happendCSV() {
  const folder = DriveApp.getFolderById(""ID_Folder"");
  const sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(""Sheet_Name"");
  const files = folder.getFiles();
  while (files.hasNext()) {
    let file = files.next();
    //let fn = file.getName();
    let fileID = file.getId();
    let fileType = file.getMimeType();
    if (fileType === ""text/csv"") {
      let csvData = Utilities.parseCsv(file.getBlob().getDataAsString(), "","");
      sheet.getRange(sheet.getLastRow +1, 1, csvData.length, csvData[0].length).setValues(csvData);
    }
  }
}

",76395250,"Modification points:

In your script, sheet.getLastRow  of sheet.getRange(sheet.getLastRow +1, 1, csvData.length, csvData[0].length).setValues(csvData); should be sheet.getLastRow(). I thought that this might be the reason for your current issue of Exception: Cannot convert 'function () { [native code] }1' to int..
In your situation, I thought that getFilesByType might be useful.
When setValues is used outside of the loop, the process cost becomes low a little.

When these points are reflected in your script, it becomes as follows.
Modified script:
function happendCSV() {
  const folder = DriveApp.getFolderById(""ID_Folder"");
  const sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(""Sheet_Name"");
  const files = folder.getFilesByType(MimeType.CSV);
  let values = [];
  while (files.hasNext()) {
    let file = files.next();
    let csvData = Utilities.parseCsv(file.getBlob().getDataAsString(), "","");
    values = [...values, ...csvData];
    console.log(`Now: ${file.getName()}`); // For confirming the current processing file.
  }
  sheet.getRange(sheet.getLastRow() + 1, 1, values.length, values[0].length).setValues(values);
}

Note:

If your CSV data is large, Sheets API might be suitable instead of Spreadsheet service (SpreadsheetApp).

References:

getFilesByType(mimeType)
getLastRow()

",
76397874,Get all appointments for the month (including recurring appointments that are scheduled in previous months),"I am having some issues getting all the appointments from specific month. Problem is with appointments that has been scheduled with recurring setting. Is there some way to get all appointments present in outlook calendar recurring and not recurring?
The best so far I was able to achieve is to check for is appointment.IsRecurring and if true I can get some parameters from recurrencePattern. However it seems to work only for appointments that has been set in this specific month with recurring parameter. If appointment with recurrence has been set earlier, then current code does not seem to work.
Is there some way to get just all the appointments from specific month?
private void GetAllItems(DateTime startDate, DateTime endDate)
{
  Outlook.Application outlookApp = new Outlook.Application();
  Outlook.NameSpace outlookNamespace = outlookApp.GetNamespace(""MAPI"");
  Outlook.MAPIFolder calendarFolder = outlookNamespace.GetDefaultFolder(Outlook.OlDefaultFolders.olFolderCalendar);
  Outlook.Items calendarItems = calendarFolder.Items;

  string restriction = $""[Start] >= '{startDate.ToShortDateString()}' AND [Start] <= '{endDate.ToShortDateString()}'"";
  Outlook.Items filteredItems = calendarItems.Restrict(restriction);

  foreach (Outlook.AppointmentItem appointment in filteredItems)
  {
    DataModel dataModel = new DataModel
    {
      Subject = appointment.Subject,
    };
    this.Data.Add(dataModel);

    if (appointment.IsRecurring)
    {
      Outlook.RecurrencePattern recurrencePattern = appointment.GetRecurrencePattern();

      DataModel recurringDataModel = new DataModel
      {
        Subject = appointment.Subject,
      };
      this.Data.Add(recurringDataModel);
    }
  }
}

",76398005,"You are almost there - you need to set the Items.IncludeRecurrences property to true and call Items.Sort on the Start property to tell Items.Restrict to expand the recurrences. See https://learn.microsoft.com/en-us/office/vba/api/outlook.items.includerecurrences for more details and an example
  Outlook.Items calendarItems = calendarFolder.Items;
  calendarItems.Sort(""[Start]"");
  calendarItems.IncludeRecurrences = true;
  string restriction = $""[Start] >= '{startDate.ToShortDateString()}' AND [Start] <= '{endDate.ToShortDateString()}'"";
  Outlook.Items filteredItems = calendarItems.Restrict(restriction);

",
76396574,Qt QObject::connect receiver and slot of different classes,"I have two classes: first one is the main QMainWindow class, and the second one is my custom class. For example, I want to make a connection in the constructor of my custom class where when I press a TestButton (which is a part of the ui of main class), it calls a function from my custom class.
Here are code:
Program.h:
class Custom;

class Program : public QMainWindow
{
    Q_OBJECT

    friend class Custom;

public:
    Program(QWidget *parent = nullptr);
    ~Program();

private:
    Ui::ProgramClass ui;
}

Program.cpp:
#include ""Program.h""
#include ""Custom.h""

Program::Program(QWidget *parent)
    : QMainWindow(parent)
{
    ui.setupUi

    Custom custom = Custom(this);
}

Custom.h:
#include ""Program.h""

class Custom : public QObject {
    Q_OBJECT
public:
    Custom(Program* program);
    ~Custom();

public slots:
    void foo();

private:
    Program* m_program;
}

and finally Custom.cpp:
#include ""Custom.h""
#include ""Program.h""

Custom::Custom(Program* program) {
    m_program = program;

    /* Here's the main problem */
    QObject::connect(m_program->ui.TestButton, &QPushButton::clicked, m_program, &Custom::foo);
}

/* Here just changing text of the button and set flat true or false every time button is clicked */
void Custom::foo() {
    QPushButton* button = m_program->ui.TestButton;

    button->setFlat(!button->isFlat());
    button->setText(button->isFlat() ?
        ""If you see the text changing when clicking the button, it means test is working correctly"" :
        ""Text changed"");
}

The main part is in the Custom constructor, where I typed connect function.
Error: cannot convert argument 3 from 'Program *' to 'const Custom *.
So, a pointer to receiver and to function foo must be the same classes.
So I tried this:
QObject::connect(m_program->ui.TestButton, &QPushButton::clicked, this, &Custom::foo);
No error, but there is actually no connection to the main program and when I click the button - nothing changing.
The only working variants are:

make foo function as a method of Program class, BUT I don't want to make Program class have a lot of functions which are actually should be methods of separated classes. And if I want to modify some other fields of these separated classes, this variant won't work;
type lambda-function inside QObject::connection, BUT I have some big functions, and I need to call some of them as a slot in QObject::connect oftently.

So, how can I make a proper connection to the whole program but leave foo function as the method of Custom class?
",76396831,"The line  Custom custom = Custom(this); creates a local object which ""dies immediately"" after exit from Program constructor, that's not what you want to do. This is what you have to do for an instance of Custom to persist:
 Custom *custom = new Custom(this);

You can even make pointer named custom a member variable if you want access it later. The constructor of Custom must be:
 Custom::Custom(Program* program) : QObject(program) 
 {
     QObject::connect( m_program->ui.TestButton, &QPushButton::clicked,
                       this, &Custom::foo );
 }

This constructor passes pointer to constructor of QObject, which registers Custom as a ""child"" of provided object, a Program in our case. In Qt terms Program will be responsible for Custom instance's destruction.
Is what you meant to do? To connect a button to an instance of Custom? Frankly, using m_program->ui.TestButton here invades Programs personal space and relies on implementation, but it's an offtopic here.
But let's make a step aside and take a look why actually what you did, even if that was utterly wrong, didn't work?
Let's put aside functions and slots. What you'd do if had to do  this with ""normal"" classes and to call a method foo() of class B using a pointer of distinct class A?
Right, class B should be derived from A and foo() should be a virtual method first declared in A. THis allows a kind of type erasure where  a pointer  or reference to B can be passed as pointer to A. Functions can be passed by pointer too, but not if they are members of a class. For that a special kind of pointer exist.
#include <iostream>

class A {
public:
   virtual void foo() = 0;
};

class B : public A {
public:
   virtual void foo() { std::cout << ""Hello from foo()!\n""; }
};

// A registering\calling class
class C {
public:

   void connect(A* p) {  
         cptr = p;
         f_ptr = &A::foo;
   }

   void call() {  (cptr->*f_ptr)(); }
private:
   A    *cptr;
   void (A::*f_ptr)();
};

int main() {
    B  b;
    C  c;
    c.connect(&b);
    c.call();
}

Now, you must understand what C::connect does there: it saves value of pointer to the object A* p and a pointer to the member. The expression &A::foo, a pointer to a member of A, is legal and correct for overridden &B::foo if we will use it with a pointer to B.
The pointer-to-member could be made a parameter of C::connect like  in Qt, but to make it work with any member function we need to create a template and another level of type erasure to save those values. I left it out for brevity.
It's almost same what happens with Qt signal\slot system, at least if you use direct connection and new connect syntax. You need a class instance in order to call its member, that's why connect got such syntax. Even if you had succeed in performing connection, you would have invoked an Undefined Behavior by clicking connected button. Thankfully, Qt handles it gracefully by disconnecting destroyed objects, therefore nothing actually happens.
That's why all classes that use signal\slot have to be descendants of QObject. Signals and slots are just functions. The difference between them is that meta object compiler generates an implementation for signals.
For type erasure to work, you can do either of those:

have to pass a pointer of type QObject* to an instance of Custom and cast it to class Custom in order for QObject::connect to work. The cast is unnecessary if signal or virtual slot is declared in QObject.
Or you have to pass a pointer to some base class which already got the slot void foo() declared.

You can declare a slot as virtual and you don't need to do so for signals.
public slots:
    virtual void foo();  // can be pure in abstract class

It appears to me that to have a QMainWindow as a base class is a bad idea, you'd needs some third class. In simplest cases it creates too verbose code and that's why another overload was introduced, which allows a lambda or functor object.
",
76397857,React Router DOM: Custom Route component not working,"My application in total has 5 routes, out of these 5 for four of them I need a condition to be true first to let user access those routes. So I wanted to create checks for said routes.
I made PublicRoute and PrivateRoute Components
import { Route } from 'react-router-dom';


const PublicRoute = ({ component: Component, ...rest }) => (
    <Route {...rest} render={(props) => <Component {...props} />} />
);  

export default PublicRoute

import { Route, Redirect } from 'react-router-dom';

const PrivateRoute = ({ component: Component, isAuthenticated, ...rest }) => (
  <Route
    {...rest}
    render={(props) =>
      isAuthenticated ? (
        <Component {...props} />
      ) : (
        <Redirect to=""/"" />
      )
    }
  />
);

export default PrivateRoute

And my app.js
import { Routes } from ""react-router-dom""
import PublicRoute from ""./components/PublicRoute""

const App = () => {
  return (
    <>
      <Routes>

        <PublicRoute
          exact
          path=""/""
          component={Com}
        />

      </Routes>
    </>
  )
}

export default App

const Com=()=>(
  <>
  <h1>Hi</h1>
  </>
)

And currently I am getting this error
 not a <Route> component. All component children of <Routes> must be a <Route> or <React.Fragment>
    at invariant (history.ts:48

Given I am returning a Route Component I can't quite understand why this error pops up. Eventually I want a way to perform checks and keep code clean, If there's another efficient way please let me know!
",76398007,"The Routes component needs a literal Route component as the child.
This worked for me -
PublicRoute
import { Route } from ""react-router-dom"";

const PublicRoute = ({ component: Component, ...rest }) => (
  <Component {...rest} />
);

export default PublicRoute;


App.js
import { BrowserRouter, Routes, Route } from ""react-router-dom"";
import PublicRoute from ""./PublicRoute"";

const App = () => {
  return (
    <BrowserRouter>
      <Routes>
        <Route
          exact
          path=""/""
          element={
            <PublicRoute component={Com} msg={""there""} person={""sandy""} />
          }
        />
      </Routes>
    </BrowserRouter>
  );
};

export default App;

const Com = ({ msg, person }) => (
  <>
    <h1>
      Hi {msg} {person}
    </h1>
  </>
);

",
76395133,To increase the font size of label that appears on hover in chart.js,"I want increase font size of label that appears on hover in chart.js,I am trying to give custom text to label but am not able to increase it's font.
 var myPieChart = new Chart(ctxP, {
                            type: 'pie',
                            data: {
                                labels: datas.labels,
                                datasets: [{
                                    label: 'Dataset 1',
                                    data: datas.value,
                                    backgroundColor: datas.colour
                                }],
                                others: datas.others
                            },
                            options: {
                                hover: {
                                    mode:'index'
                                },
                                legend: {
                                    display: true,
                                    position: ""right"",
                                    ""labels"": {
                                        ""fontSize"": 20,
                                    }
                                },
                                tooltips: {
                                    ""fontSize"": 20, bodyFont:20,
                                    callbacks: {                                       
                                        label: function (tooltipItem, data) {
                                            let label = data.labels[tooltipItem.index];
                                            let value = data.datasets[tooltipItem.datasetIndex].data[tooltipItem.index];
                                            let otherdata = data.others[tooltipItem.index];
                                            return ' ' + label + ': ' + value + '%  ' + otherdata;
                                        }
                                    }
                                }, hover: {
                                    mode: 'index',
                                    ""label"": {
                                        ""fontSize"": 20,
                                    }
                                }
                            }
                        })

i tried hover: {                  mode: 'index', ""label"": { ""fontSize"": 20, } } but it didn't work
this is how it looks right now
",76395272,"To change the font size of tooltips in Chart.js 3.x, just use options.plugins.tooltip.titleFont.size or options.plugins.tooltip.bodyFont.size.


let ctxP=document.getElementById('mychart').getContext('2d');
let datas={
    value: [10, 15, 20, 25],
    labels: ['A', 'B', 'C', 'D']
};
var myPieChart=new Chart(ctxP, {
    type: 'pie',
    data: {
        labels: datas.labels,
        datasets: [{
            label: 'Dataset 1',
            data: datas.value,
            backgroundColor: datas.colour
        }],
        others: datas.others
    },
    options: {
        hover: {
            mode: 'index'
        },
        legend: {
            display: true,
            position: ""right"",
            ""labels"": {
                ""fontSize"": 20,
            }
        },
        tooltips: {
            // ""fontSize"": 20, bodyFont: 20,
            callbacks: {
                label: function (tooltipItem, data) {
                    let label=data.labels[tooltipItem.index];
                    let value=data.datasets[tooltipItem.datasetIndex].data[tooltipItem.index];
                    let otherdata=data.others[tooltipItem.index];
                    return ' '+label+': '+value+'%  '+otherdata;
                }
            }
        },
        hover: {
            mode: 'index',
            ""label"": {
                ""fontSize"": 20,
            }
        },
        plugins: {
            tooltip: {
                titleFont: {
                    size: 20
                },
                bodyFont: {
                    size: 20
                }
            }
        }
    }
})
<script src=""https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.9.1/chart.min.js""></script>
<canvas id=""mychart""></canvas>



",
76396771,Cypress - How to check if a checkbox is checked or not?,"I have some code below to check if a checkbox is checked or not. Even if the checkbox is checked, it says that its not. Why does the code fail and how to fix it?
function isChecked(locator : string) : boolean {
 let checked : boolean = false;
 cy.get(locator).then(el){
  if(el.val() === ""on""){
   checked = true;//Is actually true when element is checked.
  }
 }
 return checked;//Is actually false when element is checked.
}

",76396836,"function isChecked(locator) : Promise<boolean>{
  return new Cypress.Promise((resolve) => {
    cy.get(locator).then((el) => {
      resolve(el.prop('checked'));
    });
  });
}

isChecked('#myCheckbox').then((checked) => {
  if (checked) {
    // Checkbox is checked
  } else {
    // Checkbox is not checked
  }
});

",
76397886,Query to find difference between today and yesterday's data along with a pseudo column,"There is a table that stores account data for everyday. I want to find out the difference in data between today and yesterday. The query for table creation and insert statements are below :
CREATE TABLE daily_account_data (id varchar(6), Name varchar (20), DS_DW_Id varchar(4), flag_1 varchar(5), flag_2 varchar(5), Insert_date date );

INSERT INTO daily_account_data VALUES('A01R11', 'Gene Graham',      'PT12', 'TRUE', 'FALSE', '2023-06-01');
INSERT INTO daily_account_data VALUES('A01R16', 'Molly Ringwald',   'PT15', 'TRUE', 'TRUE',  '2023-06-01');
INSERT INTO daily_account_data VALUES('A01R19', 'John Doe',         'PT24', 'FALSE','TRUE',  '2023-06-01');
INSERT INTO daily_account_data VALUES('A01R34', 'Jane Doe',         'PT26', 'TRUE', 'FALSE', '2023-06-01');
INSERT INTO daily_account_data VALUES('A01R11', 'Gene Wilder',      'PT12', 'TRUE', 'FALSE', '2023-06-02');
INSERT INTO daily_account_data VALUES('A01R16', 'Molly Ringwald',   'PT15', 'TRUE', 'TRUE',  '2023-06-02');
INSERT INTO daily_account_data VALUES('A01R19', 'John Doe',         'PT24', 'TRUE', 'TRUE',  '2023-06-02');
INSERT INTO daily_account_data VALUES('A01R34', 'Jane Doe',         'PT26', 'TRUE', 'FALSE', '2023-06-02');

I have the query to find the difference in the data of the 2 days.
SELECT id, name, DS_DW_Id, flag_1, flag_2 FROM daily_account_data WHERE Insert_date = '2023-06-02'
EXCEPT
SELECT id, name, DS_DW_Id, flag_1, flag_2 FROM daily_account_data WHERE Insert_date = '2023-06-01';

But I can't figure out to get the data in the pseudo column. The last column is changed data. On 1st June data the name of the id A01R11 is Gene Graham and on 2nd it is Gene Wilder. The pseudo column should display ""Name change"".
Similarly for id A01R19 (John Doe) the value for flag_1 has changed to TRUE. The pseudo column should display ""flag_1 change"".
The output should look like :




id
Name
DS_DW_Id
flag_1
flag_2
Data Change




A01R11
Gene Wilder
PT12
TRUE
FALSE
Name Change


A01R19
John Doe
PT24
TRUE
TRUE
flag_1 Change



",76398014,"You can join the table and subtract the date.
If the order of the record is correct (the previous day must be the previous record, you can use the window function(LEAD ))
select 
         a.id
        ,a.Name
        ,a.DS_DW_Id
        ,a.flag_1
        ,a.flag_2

        ,iif(a.Name=b.Name ,'',' Name Change') 
        +iif(a.DS_DW_Id=b.DS_DW_Id ,'',' DS_DW_Id Change')
        +iif(a.flag_1=b.flag_1 ,'',' flag_1 Change')
        +iif(a.flag_2=b.flag_2 ,'',' flag_2 Change') AS [Data Change]

from daily_account_data a
inner join(
            select *
            from daily_account_data b
)b on   a.id=b.id 
and DATEADD(day,-1, b.Insert_date)=a.Insert_date
where a.Name<>b.Name 
        or  a.DS_DW_Id<>b.DS_DW_Id   
        or    a.flag_1<>b.flag_1
        or    a.flag_2<>b.flag_2


",
76394491,Why is my .LockAspectRatio=msoTrue showing as an error,"I want to load data including an image from a Userform onto an excel worksheet.  I want to specify the image width and to maintain the aspect ratio.
The line .LockASpectRatio = MsoTrue shows as an error.  Can anyone help with the syntax?
'Save image to Cell

    Dim FileNAme As Variant
    Dim Img As Picture

If FileNAme <> """" Then
    
     FileNAme = Me.TextBox_5
    
With ws
     Set Img = .Pictures.Insert(FileNAme)

With Img
    .Placement = xlMove
    .Width = 150
    .LockAspectRatio = msoTrue
    .Left = ws.Cells(lr, ""D"").Left
    .Top = ws.Cells(lr, ""D"").Top
           
End With

End With

End If

",76395303,"I'm sure it should be
.ShapeRange.LockAspectRatio = msoTrue
Another observation,
If FileNAme <> """" Then    
 FileNAme = Me.TextBox_5

You are checking if Textbox is not empty before you set the text box variable. Therefore FileNAme  would always empty.
",
76396783,How to use 'if' and 'else' function to distinguish answer 'yes' from 'no' in python3,"I`ve just started to learn my first coding language, python.
And I want to give to print() informations in my program, depending from user choice.
I am very fresh so pls be forgiving.
the code:
answer = input(""Do You like it?"" + "" Yes/No"")
    if answer = (yes)
    print(""That`s great!\nThank You"" + name + ""."")
    else answer == (no)
print(""Sorry to hear that\nPerhaps we could use other names?"")

I want to give to print() informations in my program, depending from user choice.
I am very fresh so pls be forgiving.
",76396846,"you can try and modify this code. hope this helps
def ifelse():
    entername = input() #first user input
    print('Do you like it?') #first prompt 
    answer = input()
    
    if answer == 'yes': 
        print(""That`s great!"") 
        print(""Thank You name"" )
    else:  # you can also use 'elif answer == 'no'
        #but since you only have two choices consider this a shortcut
        print(""Sorry to hear that"")  
        print(""Perhaps we could use other names?"")
        
ifelse()

ps. you can format the print statement so that the user input 'entername' can be printed along side it [ you can also do this w/o using def fcns]
",
76395256,"I have a trouble in counting amount of ""items"" in binary file. C","I want to count the amount of ""items"" in binary file using feof and fseek functions.
I have a binary file with names of people in chars and their salaries in floats.
before each name there is an int that represents the amount of chars in the name.
for example my file could look like this (but without spaces and without \0's):
5danny5000.00 4lena2500.50
one item is ""4lena2500.50"" for example.
in my code, the while loop does not stops.
what can I do to repair the problem?
thanks!
int count_items(FILE* file)
{
    int count=0;
    int curr_name_len;
    while (!feof(file))
    {
        fread(&curr_name_len, sizeof(int), 1, file);
        fseek(file, (curr_name_len * sizeof(char)) + sizeof(float), SEEK_CUR);
        count++;
    }
    rewind(file);
    return count;
}

",76395340,"feof doesn't check whether the file is at the EOF, it checks whether the eof-indicator of the file was set on a previous operation. fseek allows to seek to an arbitrary position (if the operating system and the file system supports this) to allow for example to write with holes inside of the file, which is useful if you intend to write things inbetween.
Thus the eof-indicator is set after the fread-call, but is cleared after your fseek-call.
So this should work:
int count_items(FILE* file)
{
    int count=0;
    int curr_name_len;
    for (fread(&curr_name_len, sizeof(int), 1, file);
         !feof(file);
         fread(&curr_name_len, sizeof(int), 1, file))
    {
        fseek(file, (curr_name_len*sizeof(char))+sizeof(float), SEEK_CUR);
        count++;
    }
    rewind(file);
    return count;
}

or if you don't like that style:
int count_items(FILE* file)
{
    int count=0;
    int curr_name_len;

    fread(&curr_name_len, sizeof(int), 1, file);
    while (!feof(file))
    {
        fseek(file, (curr_name_len*sizeof(char))+sizeof(float), SEEK_CUR);
        count++;
        fread(&curr_name_len, sizeof(int), 1, file);
    }
    rewind(file);
    return count;
}

or less structured, but more clearly:
int count_items(FILE* file)
{
    int count=0;
    int curr_name_len;

    while (true);
    {
        if (sizeof(int) != fread(&curr_name_len, sizeof(int), 1, file)))
        {
            break;
        }

        fseek(file, (curr_name_len*sizeof(char))+sizeof(float), SEEK_CUR);
        count++;
    }

    if (feof(file))
    {
        rewind(file);
        return count;
    }

    if (ferror(file))
    {
        /* error handling */
    }
    else
    {
        /* fatal error handling */
    }
}

",
76396806,draggable position:absolute div shrinking after hitting edge of position:relative div?,"I have two draggable divs with position:absolute positioned inside of a position:relative div. My problem is that when I drag my divs to the edge of the position:relative parent they start to shrink. I need my draggable divs to stay the same size when they leave the parent's container, but I have no idea how to fix this issue.
Here's my codepen with the problem
    <div id=""all"">
        <div class=""move txtbox"">
            <div class=""topper"">test test</div>
            <span id=""test"">test test etst test test test</span>
        </div>
        <div class=""move txtbox"">
            <div class=""topper"">test test</div>
            <span id=""test"">test test etst test test test</span>
        </div>
    </div>
    <script src=""move.js""></script>

* {
    box-sizing: border-box;
    font-family: Arial, Helvetica, sans-serif;
    line-height: 1.1;
    margin: 0;
}

#all {
    position: relative;
    margin: 0 auto;
    width: 50%;
    height: 100vh;
}

.move {
    cursor: move;
    position: absolute;
}

.txtbox, .topper {
    background-color: lightgrey;
}

.txtbox {
    min-height: 70px;
    max-width: 250px;
}

.topper {
    font-size: .625em;
    border-bottom: 1px solid black;
    padding: 2px;
}

const els = document.querySelectorAll("".move"");
els.forEach((name) => {
  dragElement(name);
});

function dragElement(elmnt) {
  var pos1 = 0,
    pos2 = 0,
    pos3 = 0,
    pos4 = 0;
  elmnt.onmousedown = dragMouseDown;

  function dragMouseDown(e) {
    e = e || window.event;
    e.preventDefault();
    // get the mouse cursor position at startup:
    pos3 = e.clientX;
    pos4 = e.clientY;
    document.onmouseup = closeDragElement;
    // call a function whenever the cursor moves:
    document.onmousemove = elementDrag;
  }
  function elementDrag(e) {
    e = e || window.event;
    e.preventDefault();
    // calculate the new cursor position:
    pos1 = pos3 - e.clientX;
    pos2 = pos4 - e.clientY;
    pos3 = e.clientX;
    pos4 = e.clientY;
    // set the element's new position:
    elmnt.style.top = elmnt.offsetTop - pos2 + ""px"";
    elmnt.style.left = elmnt.offsetLeft - pos1 + ""px"";
  }
  function closeDragElement() {
    /* stop moving when mouse button is released:*/
    document.onmouseup = null;
    document.onmousemove = null;
  }
}

",76396874,"That's totally fine since an absolute positioned element wraps its content according its relative/absolute parent. Just set the width manually:
elmnt.style.width = elmnt.offsetWidth + 'px';

CODEPEN
",
76383000,Creating a WHILE Loop in Visio ShapeSheet,"I'm aware that this can be easily done using VBA, but I'd like a macroless solution if possible.
I have 2 User rows in a Shape's ShapeSheet: User.Count and User.Loop. User.Count will simply store a number, and the While loop will be performed by User.Loop using the following basic conditional:
User.Loop = IF(User.Count < 1000, SETF(GETREF(User.Count), User.Count + 1), ""Loop complete!"")  + DEPENDSON(User.Count)

This executes to User.Count = 41 (these pulses of 41 are consistent). Is performing this type of loop possible?
",76398016,"
because if the answer ever becomes 42 then that is the end of Life, The Universe and Everything!

Paul you are right ! :)

changing the loop limit to 40 and

Sad, but this trick dont works…

My experiment results.

My example file
My loop can't make it to the 1000 mark. It has to stop at 80. You can continue the loop through the context menu Go to 1000 limit.
",
76394237,incorrect number of arguments to numeric-type constructor,"So I've been following this tutorial to make a portal shader and I understand by looking elsewhere that I'm apparently missing the expected number of arguments. Error locations indicated""
        fixed4 frag (v2f i) : SV_Target
        {
            fixed2 uvs = fixed2(i.uv.x, i.uv.y + (_Time.y * _Speed)); <ERROR 1>
            fixed4 col = tex2D(_MainTex, i.uv);

            UNITY_APPLY_FOG(i.fogCoord, col);
            return fixed4(col.rgb * _Color.rgb, col.a * i.uv.y * _Intensity); <ERROR 2>
        }

At line 58 (first error) there are only two arguments requested by the fixed2 call. It's only calling for two things, which I would expect would be fulfilled by i.uv.x and i.uv.y, and that the addition shouldn't affect the number of arguments..
At line 62 (second error), from what I can tell it's returning the arguments R,G,B (multiplied by user input in the _Color variable), and A (multiplied by the Intensity variable), so that should be working, too, right?
I should note that the second error is only thrown in the absence of line 58, and that these errors are thrown even if I change them to fixed1, fixed2, fixed3, or fixed4, just as experiments.
Seeing as I've reproduced his code exactly, I can't begin to know where to look that would be causing this error.
The whole code is as follows (I've commented out the problem lines, and it works fine without them, just without the desired functionality):


Shader ""Unlit/Shader_PortalVortex""
{
    Properties
    {
        _MainTex (""Texture"", 2D) = ""white"" {}
        _Color (""Color"", Color) = (1, 1, 1, 1)
        _Intensity (""Intensity"", Range(0, 1)) = 1
        _Speed (""Speed"", Range(0, 1)) = 0.5
    }
    SubShader
    {
        Tags { ""RenderType""=""Opaque"" ""Queue""=""Transparent+2""}
        ZTest Greater
        Blend SrcAlpha OneMinusSrcAlpha
        Cull Front
        LOD 100

        Pass
        {
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            // make fog work
            #pragma multi_compile_fog

            #include ""UnityCG.cginc""

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };

            struct v2f
            {
                float2 uv : TEXCOORD0;
                UNITY_FOG_COORDS(1)
                float4 vertex : SV_POSITION;
            };

            sampler2D _MainTex;
            float4 _MainTex_ST;
            float4 _Color;
            float4 _Intensity;
            float4 _Speed;

            v2f vert (appdata v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.uv = TRANSFORM_TEX(v.uv, _MainTex);
                UNITY_TRANSFER_FOG(o,o.vertex);
                return o;
            }

            fixed4 frag (v2f i) : SV_Target
            {
                //fixed2 uvs = fixed2(i.uv.x, i.uv.y + (_Time.y * _Speed));
                fixed4 col = tex2D(_MainTex, i.uv);

                UNITY_APPLY_FOG(i.fogCoord, col);
                //return fixed4(col.rgb * _Color.rgb, col.a * i.uv.y * _Intensity);
            }
            ENDCG
        }
    }
}
  



Thanks in advance.
",76395389,"Could be happening because of trying to do calculations of mismatching types.
Most likely it’s because you’re multiplying ‘_Time.y’ by ‘float4 _Speed’.
float4 _Intensity; 
float4 _Speed; 

// vs

// Incorrect arguments because 
// you’re implicitly calling: ‘fixed2(x, y + (a * (p, q, r, s));’ // error
fixed2 uvs = fixed2(i.uv.x, i.uv.y + (_Time.y * _Speed));

Same thing for line 62 and Intensity

Cast: Try casting to native HLSL types such as ‘float2’. They are more precise than ‘fixed’, which only usually takes 11 bits or make Speed and Intensity float

Debugging: Try removing ’Time’ and ‘Speed’ variables one by one and running each time to see if one of them makes the error or both.


HLSL in Unity reference
",
76397881,Returning a Task vs using await,"I am trying to follow the guidance in this blog about not using await when not needed. And first off, if using { ... } is involved, then yes - use await.
Ok, so for the following code, why does DoItTask() not work? Is returning a Task and not using await only work if there are no uses of await in the method?
    private static async Task<string> ReadOne()
    {
        return await readerOne.ReadAsync();
    }

    private static async Task<string> ReadTwo()
    {
        return await readerTwo.ReadAsync();
    }

    private static async Task<string> ReadThree()
    {
        return await readerOne.ReadAsync();
    }

    private static async Task<string> ReadCombinedAsync(string one, string two, string three)
    {
        return await reader.CombineAsync(one, two, three);
    }

    private static Task<string> ReadCombinedTask(string one, string two, string three)
    {
        return reader.CombineAsync(one, two, three);
    }

    private static async Task<string> DoItAwait()
    {
        string one = await ReadOne();
        string two = await ReadOne();
        string three = await ReadOne();

        return await ReadCombinedAsync(one, two, three);
    }

    private static Task<string> DoItTask()
    {
        string one = await ReadOne();
        string two = await ReadOne();
        string three = await ReadOne();

        return ReadCombinedAsync(one, two, three);
    }

ps - I initially asked this question with a very sloppy example. Apologies for that.
",76398036,"The following doesn't work, because you cannot use await without async. These keywords always go together when using the Task Parallel Library (TPL):
private static Task<string> DoItTask()
{
    //problem: method is not async - compiler error
    string one = await ReadOne(); 
    string two = await ReadOne();
    string three = await ReadOne();

    // OK, because method is not async
    return ReadCombinedAsync(one, two, three);
}

Asynchronous methods with the async Task signature create a state machine under the hood, which then executes the asynchronous operation without blocking the calling thread. The calling method gets suspended while any Task that is being executed hasn't finished yet. Inside of an asynchronous method, you can only await other asynchronous Tasks or start Tasks without returning them.
This is different when you have a synchronous method. There, the Task can actually be returned directly:
public Task GetSomeTaskAsync()
{
    return SomethingElseAsync();
}

public async Task AwaitTaskAsync()
{
    await GetSomeTaskAsync();
}

This only is useful when you really can just pass along a Task without doing anything else related to that operation, so that you can save some resources, because the state machines of async Task create some overhead. The downside is that you'll lose some stack trace information when exceptions occur.
You can find more useful information here: Why use async and return await, when you can return Task directly?
Coming back to your scenario, your only two options here are:
private static async Task<string> DoItTask()
{
    string one = await ReadOne();
    string two = await ReadOne();
    string three = await ReadOne();

    return await ReadCombinedAsync(one, two, three);
}

or
private static async Task<Task<string>> DoItTask()
{
    string one = await ReadOne();
    string two = await ReadOne();
    string three = await ReadOne();

    return ReadCombinedAsync(one, two, three);
}

The latter actually returns a Task<string> as the result of the async Task.
Note: I don't recommend doing this, it will be very confusing for other developers as this is unexpected. Tasks should be awaited whenever possible. If you return a Task, you'll lose parts of the call stack in the stack trace when an exception occurs and it makes debugging quite nasty if you do this in many places, especially when wrapping Tasks in Tasks like this.
",
76395346,Manage edge's weight and attributes with Netoworkx,"I'm facing on a trouble related to how I'm managing the edges and their weight and attributes in a MultiDiGraph.
I've a list of edges like below:
[
(0, 1, {'weight': {'weight': 0.8407885973127324, 'attributes': {'orig_id': 1, 'direction': 1, 'flip': 0, 'lane-length': 3181.294317920477, 'lane-width': 3.6, 'lane-shoulder': 0.0, 'lane-max-speed': 50.0, 'lane-typology': 'real', 'lane-access-points': 6, 'lane-travel-time': 292.159682258003, 'lane-capacity': 7200.0, 'lane-cost': 0.8407885973127324, 'other-attributes': None, 'linestring-wkt': 'LINESTRING (434757.15286960197 4524762.33387408, 434267.30180536775 4525511.90463009, 436180.7891782945 4526762.385413274)'}}}), 
(1, 4, {'weight': {'weight': 0.6659876355281887, 'attributes': {'orig_id': 131, 'direction': 1, 'flip': 0, 'lane-length': 2496.129360921626, 'lane-width': 3.6, 'lane-shoulder': 0.0, 'lane-max-speed': 50.0, 'lane-typology': 'real', 'lan...

That list is used to add weight and attributes to a MultiDiGraph previous mentioned:
 graph = ntx.MultiDiGraph(weight=None)
 graph.add_weighted_edges_from(edge_list)

Trying to read the properties of a single edge(graph.edges.data()) I see this:
(0, 1, {'weight': {'weight': 0.8407885973127324, 'attributes': {'orig_id': 1, 'direction': 1, 'flip': 0, 'lane-length': 3181.294317920477, 'lane-width': 3.6, 'lane-shoulder': 0.0, 'lane-max-speed': 50.0, 'lane-typology': 'real', 'lane-access-points': 6, 'lane-travel-time': 292.159682258003, 'lane-capacity': 7200.0, 'lane-cost': 0.8407885973127324, 'other-attributes': None, 'linestring-wkt': 'LINESTRING (434757.15286960197 4524762.33387408, 434267.30180536775 4525511.90463009, 436180.7891782945 4526762.385413274)'}}})

Every edge is builded in that way: [node[0], node[1], {'weight': weight, 'attributes': attributes}].
If I use this way: [node[0], node[1], weight], I see the right use of the weight but I need to use also the attributes.
[(0, 1, {'weight': 0.8407885973127324}), (1, 4, {'weight': 0.6659876355281887}), (1, 46, {'weight': None}), (4, 5, {'weight': 1.2046936800705539}), (4, 6, {'weight': 0.4469496439663275})....

What is the correct way to manage in the same time both weight and attributes?
",76395409,"Using add_weighted_edges_from does not have an option to add independent edge attributes. It takes a list of triples (u,v,w) and consider w as the weight. That's why, you find a nested dictionary in the weight attribute of the node.
You can add shared attribute for the bunch by specifying keyword argument:
graph.add_weighted_edgees_from([...], attr1=..., attr2=...)

but you will find the same attribute values for all edges in the bunch.
Instead, you can directly use the add_edge method in a for loop:
for u, v, attr in [...]:
   graph.add_edge(u, v, **attr)

which will add your edges with individual attributes.
",
76396049,PySpark: Create a new column in dataframe based on another dataframe's cell values,"I have PySpark dataframe dhl_price of the following form:
+------+-----+-----+-----+------+
|Weight|    A|    B|    C|     D|
+------+-----+-----+-----+------+
|     1|16.78|17.05|20.23|  40.1|
|     2|16.78|17.05|20.23| 58.07|
|     3|18.43|18.86| 25.0| 66.03|
|     4|20.08|20.67|29.77| 73.99|

So you can get the delivery price based on the category (i.e. the columns A, B, C, D) and the weight of your parcel (i.e. the first column Weight) and for weights larger than 30, we have prices specified only for 30, 40, 50 etc.
I also have PySpark dataframe requests, one row for each request by a customer. It includes columns product_weight, Type (the category that is in dhl_price). I want to create a new column in requests delivery_fee based on dhl_price dataframe. In particular, for each row in dhl_price column, I want to get a cell value in dhl_price where column is the one specified in column Type and row is the one specified in column product_weight of requests dataframe.
So far I could code it in pandas:
def get_dhl_fee(weight, type):
    if weight <= 30:
        price = dhl_price.loc[dhl_price[""Weight""] == weight][type].values[0]
    else:
        price = dhl_price.loc[dhl_price[""Weight""] >= weight].reset_index(drop = True).iloc[0][type].values[0]
    return price
    
new_requests[""dhl_fee""] = new_requests.apply(lambda x: get_dhl_fee(x[""product_weight_g""], x[""Type""]), axis = 1)

How can I do the same with PySpark? I tried to use PySpark's UDF:
# Define the UDF (User-Defined Function) for calculating DHL fee
@fn.udf(returnType=DoubleType())
def get_dhl_fee(product_weight_g, calculate_way):
    broadcast_dhl_price = fn.broadcast(dhl_price)

    if weight <= 30:
        price = broadcast_dhl_price.filter(dhl_price[""Weight""] == weight).select(calculate_way).first()[0]
    else:
        price = broadcast_dhl_price.filter(dhl_price[""Weight""] >= weight).select(calculate_way).first()[0]

    return price

# Register the UDF
sc.udf.register(""get_dhl_fee"", get_dhl_fee)

# Apply the UDF to calculate dhl_fee column
requests = requests.withColumn(""dhl_fee"", get_dhl_fee(fn.col(""product_weight""), fn.col(""Type"")))

but it returns error SPARK-5063:

""It appears that you are attempting to reference SparkContext from a
broadcast variable, action, or transformation. SparkContext can only
be used on the driver, not in code that it run on workers.""

",76396875,"Setup
dhl_price.show()
+------+-----+-----+-----+-----+
|Weight|    A|    B|    C|    D|
+------+-----+-----+-----+-----+
|     1|16.78|17.05|20.23| 40.1|
|     2|16.78|17.05|20.23|58.07|
|     3|18.43|18.86| 25.0|66.03|
|     4|20.08|20.67|29.77|73.99|
|    30|20.08|20.67|29.77|73.99|
|    40|21.08|21.67|30.77|74.99|
+------+-----+-----+-----+-----+

requests.show()
+--------------+----+
|product_weight|type|
+--------------+----+
|             1|   B|
|             2|   D|
|             4|   A|
|            30|   A|
|           100|   C|
+--------------+----+

Code
Create a column of map type which maps type to corresponding price for a given weight
c = dhl_price.columns[1:]
dhl_price_map = dhl_price.select('Weight', F.map_from_arrays(F.array(*map(F.lit, c)), F.array(*c)).alias('price'))
dhl_price_map.show()


+------+--------------------+
|Weight|               price|
+------+--------------------+
|     1|{A -> 16.78, B ->...|
|     2|{A -> 16.78, B ->...|
|     3|{A -> 18.43, B ->...|
|     4|{A -> 20.08, B ->...|
|    30|{A -> 20.08, B ->...|
|    40|{A -> 21.08, B ->...|
+------+--------------------+

Assign a unique identifier for each row in the requests dataframe
df = requests.withColumn('id_', F.monotonically_increasing_id())
df.show()
+--------------+----+-----------+
|product_weight|type|        id_|
+--------------+----+-----------+
|             1|   B| 8589934592|
|             2|   D|25769803776|
|             4|   A|34359738368|
|            30|   A|51539607552|
|           100|   C|60129542144|
+--------------+----+-----------+

Join the two dataframes on weight condition then use the indexing to yank the value of price corresponding to type for each row
df = df.join(dhl_price_map, on=df['product_weight'] >= dhl_price_map['Weight'], how='left')
df = df.withColumn('dhl_fee', F.expr(""price[type]""))
df.show()


+--------------+----+-----------+------+--------------------+-------+
|product_weight|type|        id_|Weight|               price|dhl_fee|
+--------------+----+-----------+------+--------------------+-------+
|             1|   B| 8589934592|     1|{A -> 16.78, B ->...|  17.05|
|             2|   D|25769803776|     1|{A -> 16.78, B ->...|   40.1|
|             2|   D|25769803776|     2|{A -> 16.78, B ->...|  58.07|
|             4|   A|34359738368|     1|{A -> 16.78, B ->...|  16.78|
|             4|   A|34359738368|     2|{A -> 16.78, B ->...|  16.78|
|             4|   A|34359738368|     3|{A -> 18.43, B ->...|  18.43|
|             4|   A|34359738368|     4|{A -> 20.08, B ->...|  20.08|
|            30|   A|51539607552|     1|{A -> 16.78, B ->...|  16.78|
|            30|   A|51539607552|     2|{A -> 16.78, B ->...|  16.78|
|            30|   A|51539607552|     3|{A -> 18.43, B ->...|  18.43|
|            30|   A|51539607552|     4|{A -> 20.08, B ->...|  20.08|
|            30|   A|51539607552|    30|{A -> 20.08, B ->...|  20.08|
|           100|   C|60129542144|     1|{A -> 16.78, B ->...|  20.23|
|           100|   C|60129542144|     2|{A -> 16.78, B ->...|  20.23|
|           100|   C|60129542144|     3|{A -> 18.43, B ->...|   25.0|
|           100|   C|60129542144|     4|{A -> 20.08, B ->...|  29.77|
|           100|   C|60129542144|    30|{A -> 20.08, B ->...|  29.77|
|           100|   C|60129542144|    40|{A -> 21.08, B ->...|  30.77|
+--------------+----+-----------+------+--------------------+-------+

Create a window specification to rank the Weight per unique row in the requests, and then filter the rows to retain only those corresponding to the maximum weight in dhl_price.
W = Window.partitionBy('id_').orderBy(F.desc('Weight'))
df = df.withColumn('rank', F.dense_rank().over(W)).filter('rank == 1')

+--------------+----+-----------+------+--------------------+-------+----+
|product_weight|type|        id_|Weight|               price|dhl_fee|rank|
+--------------+----+-----------+------+--------------------+-------+----+
|             1|   B| 8589934592|     1|{A -> 16.78, B ->...|  17.05|   1|
|             2|   D|25769803776|     2|{A -> 16.78, B ->...|  58.07|   1|
|             4|   A|34359738368|     4|{A -> 20.08, B ->...|  20.08|   1|
|            30|   A|51539607552|    30|{A -> 20.08, B ->...|  20.08|   1|
|           100|   C|60129542144|    40|{A -> 21.08, B ->...|  30.77|   1|
+--------------+----+-----------+------+--------------------+-------+----+

Drop the extra columns
df = df.select(*requests.columns, 'dhl_fee')
df.show()

+--------------+----+-------+
|product_weight|type|dhl_fee|
+--------------+----+-------+
|             1|   B|  17.05|
|             2|   D|  58.07|
|             4|   A|  20.08|
|            30|   A|  20.08|
|           100|   C|  30.77|
+--------------+----+-------+

",
76397895,Error while compiling Spring boot maven poject,"Hi im learning Sping Boot and when I compile the maven project im getting the next error:
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-resources-plugin:3.3.0:resources (default-resources) on project backweb-api: filtering D:\***\***\backweb\backweb-api\src\main\resources\application-test.properties to D:\***\***\backweb\backweb-api\target\classes\application-test.properties failed with MalformedInputException: Input length = 1 -> [Help 1]
Compile output
application-test.properties
Full trace mvn compile -3
-I reviewed the application-test.properties file and I think that it's ok.
-I cleanned the target folders and re compiled maven project.


[ERROR] Failed to execute goal org.apache.maven.plugins:maven-resources-plugin:3.3.0:resources (default-resources) on project backweb-api: filtering D:\***\***\backweb\backweb-api\src\main\resources\application-test.propert
ies to D:\***\***\backweb\backweb-api\target\classes\application-test.properties failed with MalformedInputException: Input length = 1 -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-resources-plugin:3.3.0:resources (default-resources) on project backweb-api: filtering D:\***\***\backweb\backweb
-api\src\main\resources\application-test.properties to D:\***\***\backweb\backweb-api\target\classes\application-test.properties failed with MalformedInputException: Input length = 1
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:568)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: org.apache.maven.plugin.MojoExecutionException: filtering D:\***\***\backweb\backweb-api\src\main\resources\application-test.properties to D:\***\***\backweb\backweb-api\target\classes\application-test.p
roperties failed with MalformedInputException: Input length = 1
    at org.apache.maven.plugins.resources.ResourcesMojo.execute (ResourcesMojo.java:362)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:568)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: org.apache.maven.shared.filtering.MavenFilteringException: filtering D:\***\***\backweb\backweb-api\src\main\resources\application-test.properties to D:\***\***\backweb\backweb-api\target\classes\applica
tion-test.properties failed with MalformedInputException: Input length = 1
    at org.apache.maven.shared.filtering.DefaultMavenFileFilter.copyFile (DefaultMavenFileFilter.java:118)
    at org.apache.maven.shared.filtering.DefaultMavenResourcesFiltering.filterResources (DefaultMavenResourcesFiltering.java:277)
    at org.apache.maven.plugins.resources.ResourcesMojo.execute (ResourcesMojo.java:356)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:568)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
    at java.nio.charset.CoderResult.throwException (CoderResult.java:274)
    at sun.nio.cs.StreamDecoder.implRead (StreamDecoder.java:326)
    at sun.nio.cs.StreamDecoder.read (StreamDecoder.java:188)
    at java.io.InputStreamReader.read (InputStreamReader.java:177)
    at java.io.BufferedReader.read1 (BufferedReader.java:211)
    at java.io.BufferedReader.read (BufferedReader.java:287)
    at java.io.BufferedReader.fill (BufferedReader.java:162)
    at java.io.BufferedReader.read (BufferedReader.java:183)
    at org.apache.maven.shared.filtering.BoundedReader.read (BoundedReader.java:85)
    at org.apache.maven.shared.filtering.MultiDelimiterInterpolatorFilterReaderLineEnding.read (MultiDelimiterInterpolatorFilterReaderLineEnding.java:235)
    at org.apache.maven.shared.filtering.MultiDelimiterInterpolatorFilterReaderLineEnding.read (MultiDelimiterInterpolatorFilterReaderLineEnding.java:197)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:568)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
[ERROR]
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :backweb-api



",76398039,"Your application-test.properties contains non-UTF8 characters (e.g. the accent on the configuration o). This causes this error
",
76396209,Table only appears for a split second on button click,"I have a button and a table on my webpage. I want to view the table when I click that button. I have tried it using javascript but the table only appears for a split second and then disappears. following is the code for my table:


var button = document.getElementById('BtnNextInsured'); // Assumes element with id='button'

button.onclick = function() {
  var div = document.getElementById('tblContact');
  if (div.style.display !== 'none') {
    div.style.display = 'none';
  } else {
    div.style.display = 'block';
  }
};
<button id=""BtnNextInsured"" class=""btn btn-md btn-primary"">Next</button> Table to show:

<table id=""tblContact"" class=""table table-light table-borderless"" style=""display:none;"">
  <tr>
    <td class=""cellContent"">
      <input id=""txtEmail"" class=""txt"" placeholder=""Email"" />
    </td>
    <td class=""cellContent"">
      <input id=""txtCell"" class=""txt"" placeholder=""Cell Phone"" />
    </td>
    <td class=""cellContent"">
      <input id=""txtLandline"" class=""txt"" placeholder=""Landline"" />
    </td>
  </tr>

  <tr>
    <td colspan=""3"" style=""text-align: right"">
      <button id=""BtnNextContact"" class=""btn btn-md btn-primary"">Next</button>
    </td>
  </tr>
</table>



I don't understand what's wrong with my code. I am a beginner to javascript.
",76396902,"as comments saying better using eventlistener.
with the eventlistene, you can use the event and like here stop it to avoid double click.
to appear disappear use a class 'hide' so you can toggle it, easier than than checking display none.


const tableAppear = () => {
  document.querySelector('#BtnNextInsured').addEventListener('click', evt => {
    evt.stopImmediatePropagation();
    document.querySelector('#tblContact').classList.toggle('hide');
  });
}
window.addEventListener('load', tableAppear);
.hide {
  display: none;
}
<button id=""BtnNextInsured"" class=""btn btn-md btn-primary"">Next</button> Table to show:

<table id=""tblContact"" class=""table table-light table-borderless hide"">
  <tr>
    <td class=""cellContent"">
      <input id=""txtEmail"" class=""txt"" placeholder=""Email"" />
    </td>
    <td class=""cellContent"">
      <input id=""txtCell"" class=""txt"" placeholder=""Cell Phone"" />
    </td>
    <td class=""cellContent"">
      <input id=""txtLandline"" class=""txt"" placeholder=""Landline"" />
    </td>
  </tr>

  <tr>
    <td colspan=""3"" style=""text-align: right"">
      <button id=""BtnNextContact"" class=""btn btn-md btn-primary"">Next</button>
    </td>
  </tr>
</table>



",
76395403,"Why am I able to use C# lists without writing ""using System.Collections.Generic;"" at the beginning of my file?","Take a look at this simple C# program:
using System;

namespace testProgram
{
    internal class Program
    {
        static void Main(string[] args)
        {
            List<string> list = new List<string>();
            list.Add(""List element."");
            Console.WriteLine(list[0]);
        }
    }
}

Output:
List element.

You can see that it uses a list. I always saw on the Internet that in order to use a list, I need to add ""using System.Collections.Generic;"" at the beginning of my file. However, the program can run without this line, why?
",76395424,"Look at your project file. I strongly suspect it will include this:
<ImplicitUsings>enable</ImplicitUsings>

The ImplicitUsings feature is described here:

The ImplicitUsings property can be used to enable and disable implicit global using directives in C# projects that target .NET 6 or a later version and C# 10 or a later version. When the feature is enabled, the .NET SDK adds global using directives for a set of default namespaces based on the type of project SDK. Set this property to true or enable to enable implicit global using directives. To disable implicit global using directives, remove the property or set it to false or disable.

Note that that means you don't need using System; either.
Combined with top-level statements, your whole file could actually be:
List<string> list = new List<string>();
list.Add(""List element."");
Console.WriteLine(list[0]);

Or if the namespace is important to you:
namespace testProgram;

List<string> list = new List<string>();
list.Add(""List element."");
Console.WriteLine(list[0]);

",
76396701,import python libraries (eg: rapidjson) in airflow,"I want to use the Python library rapidjson in my Airflow DAG. My code repo is hosted on Git. Whenever I merge something into the master or test branch, the changes are automatically configured to reflect on the Airflow UI.
My Airflow is hosted as a VM on AWS EC2. Under the EC2 instances, I see three different instances for: scheduler, webserver, workers.
I connected to these 3 individually via Session Manager. Once the terminal opened, I installed the library using
pip install python-rapidjson

I also verified the installation using pip list. Now, I import the library in my dag's code simply like this:
import rapidjson

However, when I open the Airflow UI, my DAG has an error that:
No module named 'rapidjson'

Are there additional steps that I am missing out on? Do I need to import it into my Airflow code base in any other way as well?
Within my Airflow git repository, I also have a ""requirements.txt"" file. I tried to include
python-rapidjson==1.5.5
this there as well but I do not know how to actually install this.
I tried this:
pip install requirements.txt
within the session manager's terminal as well. However, the terminal is not able to locate this file. In fact, when I do ""ls"", I don't see anything.
pwd
/var/snap/amazon-ssm-agent/6522

",76396926,"Have you tried using the PythonVirtualEnvOperator ?
It will allow you to install the library at runtime so you don't need to make changes on the server just for one job.
To run a function called my_callable, simply use the following:
from airflow.operators.python import PythonVirtualenvOperator


my_task = PythonVirtualenvOperator(
        task_id=""my_task "",
        requirements=""python-rapidjson==1.5.5"",
        python_callable=my_callable,
    )

I still recommend updating your server environment for core libs, but this is a best practice when using special libs for a small minority of jobs.
",
76397951,Is there a way to set a ConsoleColor value equal to a variable?,"I am working on creating a class to support a console app I am developing, and I would like to create a method within to change both the background and foreground color.  Is there a way to set a ConsoleColor value (which I believe is an enum) to another variable so this can easily be changed by the user at runtime?  For instance, I am hoping for something like the following.
Public class ConsoleOutput
{
  private var consoleBackground = ConsoleColor.White;
  private var consoleForeground = ConsoleColor.Black;
  
  Public ConsoleOutput
  {
    Console.BackgroundColor = consoleBackground
    Console.ForegroundColor = consoleForeground
  }
}

This, however, did not work.
",76398063,"You don't seem to ever write to the console in your program, which you obviously need to do. Other then that you also need a way to change the colors during runtime using e.g. setters or a function like ChangeColors().
Here a working sample program for reference:
namespace MyProgram;

class Program
{
    static void Main(string[] args)
    {
        var coloredPrinter = new ColoredPrinter(ConsoleColor.White, ConsoleColor.Blue);
        coloredPrinter.WriteLine(""This is white text on blue background"");
        coloredPrinter.ChangeColors(ConsoleColor.Yellow, ConsoleColor.Red);
        coloredPrinter.WriteLine(""This is yellow text on red background"");
        Console.WriteLine(""This is the default"");
    }
}

class ColoredPrinter
{
    public ConsoleColor ForegroundColor { get; set; }
    public ConsoleColor BackgroundColor { get; set; }

    public ColoredPrinter(ConsoleColor foregroundColor, ConsoleColor backgroundColor)
    {
        ChangeColors(foregroundColor, backgroundColor);
    }

    public void ChangeColors(ConsoleColor foregroundColor, ConsoleColor backgroundColor)
    {
        ForegroundColor = foregroundColor;
        BackgroundColor = backgroundColor;
    }

    public void WriteLine(string text)
    {
        Console.ResetColor();
        Console.BackgroundColor = BackgroundColor;
        Console.ForegroundColor = ForegroundColor;
        Console.WriteLine(text);
        Console.ResetColor();
    }
}


This prints out the following:

",
76396427,How to destroy an initiated GameObject on collision?,"I have created a script that spawns bullets/projectiles that moves forward with a certain force. However, the bullet itself does not disappear on collision. Since it is a instantiated object, how do I make it disappear? I've tried OnCollisionEnter() but to no avail. The code below is how i created the bullet.
// Instantiate bullet/projectile
GameObject currentBullet = Instantiate(bullet, attackPoint.position, Quaternion.identity);
        
// Rotate bullet to shoot direction
currentBullet.transform.forward = directionWithSpread.normalized;

// Add forces to bullet
currentBullet.GetComponent<Rigidbody>().AddForce(directionWithSpread.normalized * shootForce, ForceMode.Impulse);
currentBullet.GetComponent<Rigidbody>().AddForce(fpsCam.transform.up * upwardForce, ForceMode.Impulse);

EDIT
Perhaps, is there anyway to assign a script to a prefab that will be instantiated?
",76396935,"You can choose two paths to achieve this
1) Adding the script to the prefab (done in the editor)
You can edit the prefab (opening it from the editor) and add the script to it.
2) Adding the script to the instance (done programmatically)
In this case your first instantiate the prefab and then programmatically add the script to it with something like this:
GameObject currentBullet = Instantiate(bullet, attackPoint.position, Quaternion.identity);

currentBullet.AddComponent<YourScript>();

3? Using RayCasting (this option in my opinion fits your needs)
Usually the best way to handle bullets is using RayCast
In this case the bullet does not need the code to handle its collision but it is just the visual representation of the bullet, the real collision detection and other actions linked to it are controlled by the raycasting, in this way you save some memory
Hope this helps
",
76395282,reading input from .txt file in C,"john smith 21 VETERAN 1   
I have  a .txt file writing this . I want to read this .txt file and take john smith as a variable . but I can't read with whitespaces.
edit: I want to take john smith and print it in the console with
 printf(""%s"",name);
I tried code below but didnt work. only takes smith.
while (fscanf(file, ""%s, %d %s %d"", name, &age, priorityGroup, &priorityLevel) == 4)

edit2:
",76395427,"The task is not easy for beginners learning C.
There can be different approaches.
I can suggest the following approach.
At first a full record is read from the file using standard C function fgets as for example
char record[100];

fgets( record, sizeof( record ), fp );

Now let's assume that the record is already read. Then to output the first two words in record you can use standard C function sscanf as shown in the demonstration program below.
#include <stdio.h>

int main( void )
{
    char record[] = ""john smith 21 VETERAN 1\n"";

    int n = 0;

    for (size_t i = 0; i < 2; i++)
    {
        const char *p = record + n;
        int m;

        sscanf( p, ""%*s%n"", &m );

        n += m;
    }

    printf( ""\""%.*s\""\n"", n, record );
}

The program output is
""john smith""

Or you could make a string of the two words just writing
record[n] = '\0';
printf( ""\""%s\""\n"", record );

Or if your compiler supports variable length arrays you could write
#include <string.h>

//...

char name[n + 1];

memcpy( name, record, n );
name[n] = '\0';

printf( ""\""%s\""\n"", name );

If the compiler does not support variable length arrays then you will need to allocate an array dynamically like for example
#include <string.h>
#include <stdlib.h>

//...

char *name = malloc( n + 1 );

memcpy( name, record, n );
name[n] = '\0';

printf( ""\""%s\""\n"", name );

In this case do not forget to free the allocated memory when it will not be required any more
free( name );

In the demonstration program the string literal used as an initializer of the array record is appended with the new line character '\n' because the function fgets itself can store this character in the destination array.
",
76398040,"ForEach method skips Console.WriteLine() on each iteration, then dumps all the logs when loop is done. Why?","I have this piece of code:
[Test]
public void LinqConsoleOutputTest() {
    Enumerable.Range(0, 10).ToList().ForEach(_ => {
        Thread.Sleep(500);
        Console.WriteLine($""Slept..."");
    });
}

And to my surprise the code was executing around 5 seconds as expected, but it did not print to the console. It just dumped all 10 logs when it finished ForEach loop.
And I am just curious why does it work that way? Is it related to ForEach/Linq or maybe to the Console object?
",76398085,"Console.WriteLine() writes to whatever is the Standard Output which might not be a console in case of a test. Run the same code in a simple ""normal"" C# program and it will print as expected.
Consider this program:
namespace MyProgram;

class Program
{
    static void Main(string[] args)
    {
        using var fileStream = File.OpenWrite(""./consoleOutput.txt"");
        using var streamWriter = new StreamWriter(fileStream);
        // redirect standard output
        Console.SetOut(streamWriter);
        Enumerable.Range(0, 10).ToList().ForEach(_ =>
        {
            Thread.Sleep(500);
            Console.WriteLine($""Slept... {DateTime.UtcNow}"");
        });
        Console.WriteLine(""Done"");
    }
}

What this program does is, it redirects the standard output to write to a FileStream of a file called consoleOutput.txt. So when you run the program it creates the file and writes everything you would normally print to the console to the file. If you have a look at the file you can see that in fact it contains the whole output:
consoleOutput.txt
Slept... 6/3/2023 9:34:42 PM
Slept... 6/3/2023 9:34:42 PM
Slept... 6/3/2023 9:34:43 PM
Slept... 6/3/2023 9:34:43 PM
Slept... 6/3/2023 9:34:44 PM
Slept... 6/3/2023 9:34:44 PM
Slept... 6/3/2023 9:34:45 PM
Slept... 6/3/2023 9:34:45 PM
Slept... 6/3/2023 9:34:46 PM
Slept... 6/3/2023 9:34:46 PM
Done

",
76396924,"Nested for loop not looping on the first set, Python","I have written this code in python. In the end I would like to use this to get the indices to cut up a 100x100 matrix into squares that overlap by 10. However, at the bottom there is a nested loop and the y values print how I think they should but not the x-values, the x-values never change... Can anyone help? Thanks
x_split = np.linspace(0, 100, 4 + 1, dtype=int)
x_start = x_split[:-1] - 5
x_start[0] = 0
x_end = x_split[1:] + 5
x_end[-1] = 100

y_split = np.linspace(0, 100, 4 + 1, dtype=int)
y_start = y_split[:-1] - 5
y_start[0] = 0
y_end = y_split[1:] + 5
y_end[-1] = 100

x_inds = zip(x_start, x_end)
y_inds = zip(y_start, y_end)

i = 0
for start_x, end_x in x_inds:
    for start_y, end_y in y_inds:
        i += 1
        print(f""i = {i}"")
        print(f""x = {start_x} {end_x}"")
        print(f""y = {start_y} {end_y}"")
        print("""")

Current output:
i = 1
x = 0 30
y = 0 30

i = 2
x = 0 30
y = 20 55

i = 3
x = 0 30
y = 45 80

i = 4
x = 0 30
y = 70 100

And then stops. I want to to continue...
i = 5
x = 20 55
y = 0 30

i = 6
x = 20 55
y = 20 55

...

",76396974,"zip(y_start, y_end) returns an iterator.  After the first series of Xs, y_inds is exhausted and yields no more values for the subsequent values of X.
Make it y_inds a list, so it can be reused on subsequence X rows:
*y_inds, = zip(y_start, y_end)

You could also let numpy do the iterating for you to produce a matrix of square coordinates:
coords  = np.repeat(x_start,y_start.size)[:,None], \
          np.repeat(x_end,  y_start.size)[:,None], \
          np.tile(y_start,x_start.size)[:,None],   \
          np.tile(y_end,  x_start.size)[:,None]
squares = np.concatenate(coords,axis=1)
print(squares)
[[  0  30   0  30]
 [  0  30  20  55]
 [  0  30  45  80]
 [  0  30  70 100]
 [ 20  55   0  30]
 [ 20  55  20  55]
 [ 20  55  45  80]
 [ 20  55  70 100]
 [ 45  80   0  30]
 [ 45  80  20  55]
 [ 45  80  45  80]
 [ 45  80  70 100]
 [ 70 100   0  30]
 [ 70 100  20  55]
 [ 70 100  45  80]
 [ 70 100  70 100]]

",
76387478,Flutter - in_app_purchase plugin - iOS - works in sim but not on device,"Building an iOS app, _inAppPurchase.queryProductDetails(productIds); returns product details in a simulator (ipad, iphone) and everything works. But if I run on a physical iPhone productDetailResponse.productDetails.isEmpty is true and my product ids are in notFoundIDs. Same if I 'flutter build ipa' and try in TestFlight. Using synced StoreKit.storekit.. in_app_purchase: ^3.1.7. Why this may be?
",76395430,"Getting all ""Agreements, Tax and Banking"" bits sorted and ""Active"" in App Store Connect has fixed it. Things started working the same on the device, simulator and TestFlight as soon as the forms got reviewed and approved.
",
76398059,Not explicitly saying `return` gives error: `match` arms have incompatible types,"I am very new to Rust.
The following function:
async fn create(body: String) -> impl IntoResponse {
    let j = match serde_json::from_str::<CreationJSON>(&body) {
        Ok(j) => j,
        Err (_) => (
            StatusCode::UNPROCESSABLE_ENTITY,
            ""body is invalid"".to_string(),
        ),
    };
    println!(""{:?}"", j.record_stringified);

    (
        StatusCode::CREATED,
        ""created"".to_string(),
    )
}

gives error:
error[E0308]: `match` arms have incompatible types
   --> src/main.rs:128:20
    |
126 |       let j = match serde_json::from_str::<CreationJSON>(&body) {
    |               ------------------------------------------------- `match` arms have incompatible types
127 |           Ok(j) => j,
    |                    - this is found to be of type `CreationJSON`
128 |           Err (_) => (
    |  ____________________^
129 | |             StatusCode::UNPROCESSABLE_ENTITY,
130 | |             ""body is invalid"".to_string(),
131 | |         ),
    | |_________^ expected `CreationJSON`, found `(StatusCode, String)`
    |
    = note: expected struct `CreationJSON`
                found tuple `(axum::http::StatusCode, std::string::String)`

However, if I add a ""return"" to the Err arm, then it works fine:
async fn create(body: String) -> impl IntoResponse {
    let j = match serde_json::from_str::<CreationJSON>(&body) {
        Ok(j) => j,
        Err (_) => return (
            StatusCode::UNPROCESSABLE_ENTITY,
            ""body is invalid"".to_string(),
        ),
    };
    println!(""{:?}"", j.record_stringified);

    (
        StatusCode::CREATED,
        ""created"".to_string(),
    )
}

Why do I need to add the return in the Err arm? I thought a function with no semi-colon was supposed to automatically return in Rust?
",76398086,"return returns from the function. If you don't use the return keyword, then it'll ""return"" from the statement. In your first example, you're trying to assign the tuple to the variable j, while in the second you return from the function.
",
76397592,Why is the 'Growth Estimates' table not being detected by beautifulsoup on this website?,"I tried to webscrape the data from the below url to get the data from the ""Growth Estimates"" table using beautiful soup & requests but it can't seem to pick the table up. However when using the inspection tool I can see there is a table there to pull data from and I couldn't see anything about it being pulled dynamically, but I could be wrong.
url = https://finance.yahoo.com/quote/AAPL/analysis?p=AAPL
Is someone able to explain the issue and offer a solution?
Thank you!
import requests
from bs4 import BeautifulSoup

def get_growth_data(symbol):
    url = ""https://finance.yahoo.com/quote/{symbol}/analysis?p={symbol}""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, ""html.parser"")

    # Find the table containing the growth data
    table = soup.find(""table"", class_=""W(100%) M(0) BdB Bdc($seperatorColor) Mb(25px)"")

    if table is None:
        print(""Table not found."")
        return []

    # Extract the growth values from the table
    growth_values = []
    rows = table.find_all(""tr"")
    for row in rows:
        columns = row.find_all(""td"")
        if len(columns) >= 2:
            growth_values.append(columns[1].text)

    return growth_values

symbol = 'AAPL'
growth_data = get_growth_data(symbol)
print(growth_data)


",76398115,"To get correct response from the server set User-Agent HTTP header in your request:
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = 'https://finance.yahoo.com/quote/AAPL/analysis?p=AAPL'
headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/113.0'}
soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')

table = soup.select_one('table:-soup-contains(""Growth Estimates"")')
df = pd.read_html(str(table))[0]

print(df)

Prints:
           Growth Estimates    AAPL  Industry  Sector(s)  S&P 500
0              Current Qtr.  -0.80%       NaN        NaN      NaN
1                 Next Qtr.   5.40%       NaN        NaN      NaN
2              Current Year  -2.30%       NaN        NaN      NaN
3                 Next Year   9.90%       NaN        NaN      NaN
4  Next 5 Years (per annum)   8.02%       NaN        NaN      NaN
5  Past 5 Years (per annum)  23.64%       NaN        NaN      NaN

",
76396938,OptInt type function in Python,"import pandas as pd
import json

    mass=[]
    fall=[]
    year=[]

req = requests.get(""https://data.nasa.gov/resource/y77d-th95.json"")
response =req.json()

for i in range(0,len(response)):
    mass.append(response[i]['mass']) 
    fall.append(response[i]['fall'])
    year.append(response[i]['year'])

I handle keyError with the help of Exception handling. If i got keyerror i added NAN value.
As java have function like OptString whenever we get keyerror it add default value.
I just want to get all data in list but via using optString type method.
Can you give any example ?
Thanks ,
",76397014,"You can use dict.get to provide a default value as the second argument.
For example:
from math import nan

for obj in response:
    mass.append(obj.get('mass', nan))
    fall.append(obj.get('fall', nan))
    year.append(obj.get('year', nan))

",
76396255,Owner information not available from Google Cloud Storage blob,"I am using get_blob using the python API. All kinds of information about the blob is available, but owner is set to None. My application needs to know who last updated the file in GCS.  Any help would be appreciated!
I tried calling reload with projection set to full, as suggested elsewhere, but it didn’t help. I tried setting acl as well as uniform permissions on the bucket, but it was the same result.
I would be willing to change the bucket permissions to anything (other than making it public) that would allow me to get the file updater/owner information.
",76397019,"In Google Cloud Storage, buckets are owned by the project. Users (IAM principals) are not owners of a bucket. Permissions are granted to users to access a bucket and its objects. The logs record who created a bucket.
For updates, review Audit Logging. Google records changes to a resource's metadata.
Usage logs & storage logs
",
76395432,Fix error RecyclerView: No adapter attached; skipping layout,"Just implemented RecyclerView in my code, replacing ListView.
Everything works fine. The data is displayed.
But error messages are being logged:
RecyclerView: No adapter attached; skipping layout
I have read other questions related to the same problem but none of them help.
",76395438,"i have this problem , a few time problem is recycleView put in ScrollView object
After checking implementation, the reason appears to be the following. If RecyclerView gets put into a ScrollView, then during measure step its height is unspecified (because ScrollView allows any height) and, as a result, gets equal to minimum height (as per implementation) which is apparently zero.
You have couple of options for fixing this:
Set a certain height to RecyclerView
Set ScrollView.fillViewport to true
Or keep RecyclerView outside of ScrollView. In my opinion, this is the best option by far. If RecyclerView height is not limited - which is the case when it's put into ScrollView - then all Adapter's views have enough place vertically and get created all at once. There is no view recycling anymore which kinda breaks the purpose of RecyclerView .
(Can be followed for android.support.v4.widget.NestedScrollView as well)
",
76395305,Pygame get_rect() is not running,"get_rect() is not running
I was trying to make a simple game for educational purposes using the pygame module. I encountered this error. I would be glad if you can help
import pygame 
import random
import sys 
import os 

pygame.init()
balikci_konum = ""E:/E/Python/Python-eski/Oyuncalismalari/balik_avlama_oyunu/textures/balikci.png""
genislik = 1000
yukseklik = 600 

ekran = pygame.display.set_mode((genislik,yukseklik))

# ...

class Balik(pygame.sprite.Sprite):
    def __init__(self,x,y,resim,tip):
        super().__init__()
        self.image = resim 
        self.rect = self.image.get_rect()
        self.rect.topleft = (x,y)
        self.tip = tip 
        self.hiz = random.randint(0,13)
        self.yonx= random.choice([1,-1])
        self.yony= random.choice([1,-1])
        
    def update(self):
        self.rect.x = self.hiz*self.yonx
        self.rect.y = self.hiz*self.yony
        
        if self.rect.left <=0 or self.rect.right >= genislik : 
            self.yonx *=-1
            
        if self.rect.top <=0 or self.rect.bottom >= yukseklik : 
            self.yony *=-1
     
#Balık Grupları 
balik1 = pygame.image.load(""E:/E/Python/Python-eski/Oyuncalismalari/balik_avlama_oyunu/textures/balik1.png"")
balik2 = pygame.image.load(""E:/E/Python/Python-eski/Oyuncalismalari/balik_avlama_oyunu/textures/balik2.png"")

balik_grup = pygame.sprite.Group()
balik = Balik(random.randint(0,genislik-32),random.randint(0,yukseklik-32),balik1,0)
balik_grup.add(balik)

balik = Balik(random.randint(0,genislik-32),random.randint(0,yukseklik-32),balik2,0)
balik_grup.add(balik)

# ...

that is all of my code
i tried relocating objects but still get_rect() function doesn't work
",76395440,"To move the objects you have to change the position with += instead of setting the position with an assignment (=):
self.rect.x = self.hiz*self.yonx
self.rect.y = self.hiz*self.yony
self.rect.x += self.hiz*self.yonx
self.rect.y += self.hiz*self.yony

",
76398116,Javascript construct trap not working in class returning proxy,"I am trying to create a little javascript two way form binder using proxies. I am stuck on how I can intercept 'new' calls. I use a 'construct' trap but it doesn't fire. Here is my code, I have removed the stuff that is not relivant for my specific problem
    class BoundObject {

        constructor(object, element) {

            // distribute object properties into ""this""
            for (const prop in object) {
                this[prop] = object[prop]
            }

            return new Proxy(this, {

                construct:(target, args) => {
                    console.log(`Creating a new ${target.name}`) // why is this not fired?
                    return Reflect.construct(...args)
                },
                set: (target, prop, val, receiver) => {
                    console.log(`attempting to set ${prop}=${val} type ${typeof val}`);

                    return Reflect.set(target, prop, val) 
                }
            })
        }
    }

    // create an instance of our BoundObject class, passing in an object and an HTML element
    const user = new BoundObject({name:'fred'},document.querySelector('#user-form'))    // why is the 'construct' call not intercepted?
    user.name = 'mary' // set user name. The 'set' call is sucessfully intercepted

The set trap works, but the construct trap fails to fire.  I suspect this is to do with javascript deep magic around 'this' but cannot figure it out
How can I intercept the construction of the proxy object my class returns?
",76398122,"The construct trap is only called when the [[Construct]] internal method is invoked on the proxy itself. This could be caused by using the new operator. However, in this case, the Proxy is returned as a result of calling new on the BoundObject constructor; new was not called on the proxy itself.
Here is an example of where the construct trap could be called.


function MyObj() {}
const proxy = new Proxy(MyObj, {
  construct(target, args) {
    console.log(args);
    return new target(...args);
  }
});
new proxy('something');



",
76396518,How can I fix line breaks output by script(1) utility?,"I have the following lftp script to copy files from a remote to local:
env TERM=dumb script -a $LOGSTDOUT -c ""$(cat <<- EOF
    lftp $PROTOCOL://$URL -u ${USER},${PASS} << EOFF
    set dns:fatal-timeout never
    set sftp:auto-confirm yes
    set mirror:use-pget-n 50
    set mirror:parallel-transfer-count 2
    set mirror:parallel-directories yes
    set mirror:include-regex $REGEX
    set log:enabled/xfer yes
    set log:file/xfer $LOG
    set xfer:use-temp-file yes
    set xfer:temp-file-name *.lftp
    mirror -c -v --loop --Remove-source-dirs ""$REMOTEDIR"" ""$LOCALDIR""
    quit
    EOFF
EOF
)""

I am capturing terminal output with the script(1) utility. The env TERM=dumb is just a random piece of code I found to disable ANSI escape codes.
My problem is that the line breaks of the output log file get quiet mangled. It seems to be using CR and LF. I discovered more information here and it seems this is by design. Though I'm not sure how to fix it.
These line endings cause issues when viewing the logs in lnav:

The reason for this becomes quickly apparent upon inspecting the raw text:

I have thought of some potential options, but not sure how to implement:

Fix the output of the script(1) utility so that single CR are converted to LF. Maybe this can be achieved with piping or some arguement?
A hack for lnav to treat CR as LF when displaying in the GUI.

Anyone know how I can fix these line breaks so it shows correctly in lnav?
",76397042,"Try replacing
... script -a $LOGSTDOUT ...

with
... script -a >(tr -d '\r' >""$LOGSTDOUT"") ...


See the Process Substitution section on the Bash Reference Manual for an explanation of >(...).
Note that ALL_UPPERCASE variable names (like LOGSTDOUT) are best avoided because there is a danger of clashes with the large number of special ALL_UPPERCASE variables that are used in shell programming.  See Correct Bash and shell script variable capitalization.
The quotes on $LOGSTDOUT are necessary in general.  Use Shellcheck to find common problems with shell code, including missing quotes.

",
76397068,PySpark and Postgres: JDBC connection error,"Issues connecting PySpark & Postgres
I've scoured the Apache docs, Stackoverflow and watched youtube tutorials but can't seem to find an issue to my exact issue. I have clearly pointed to the postgres executable jar file but it doesn't I can't seem to be able to read from the db. Below is an extract from my script. The properties parameter contains the user, password and driver (""org.postgresql.Driver"") details in the form of a dictionary.
Anybody know what I am missing? I get the error:

IllegalArgumentException: requirement failed: the driver could not open a JDBC connection. Check the URL

Added context: I am using Postgres 15, Spark 3.4.0 and Python 3.10
spark = SparkSession\
        .builder\
        .config('spark.jars','C:\Program Files\Spark\spark-3.4.0-bin-hadoop3\jars\postgresql-42.6.0.jar')\
        .getOrCreate()

    spark.read.jdbc(
        url=f'jdbc:postgresql//localhost:5432/ag',
        table='customers',
        properties=props).load()

",76397087,"Seems like your URL is missing a : character.
You've written
jdbc:postgresql//localhost...

instead of
jdbc:postgresql://localhost...

",
76397037,django full text search taggit,"My application - the basics
I have a simple django application which allows for storing information about certain items and I'm trying to implement a search view/functionality.
I'm using django-taggit to tag the items by their functionality/features.
What I want to implement
I want to implement a full text search which allows to search across all the fields of the items, including their tags.
The problem(s)

On the results view, the tagged items are showing up multiple times (one occurence per tag)
The ranking is correct when I specify * only a single* tag in the search field, but when I specify multiple tag names, I will get unexpected ranking results.

I suspect the SearchVector() does not resolve the tags relation as I expected it to do. The tags should be treated just like a list of words in this case.
Example Code
models.py
from django.db import models
from taggit.managers import TaggableManager

class Item(models.Model):
    identifier = models.SlugField('ID', unique=True, editable=False)
    short_text = models.CharField('Short Text', max_length=100, blank=True)
    serial_number = models.CharField('Serial Number', max_length=30, blank=True)
    revision = models.CharField('Revision/Version', max_length=30, blank=True)
    part_number = models.CharField('Part Number', max_length=30, blank=True)
    manufacturer = models.CharField('Manufacturer', max_length=30, blank=True)
    description = models.TextField('Description', blank=True)
    tags = TaggableManager('Tags', blank=True)
    is_active = models.BooleanField('Active', default=True)

forms.py
from django import forms

class SearchForm(forms.Form):
    search = forms.CharField(max_length=200, required=False)
    active_only = forms.BooleanField(initial=True, label='Show active items only', required=False)

views.py
from django.views.generic.list import ListView
from django.contrib.postgres.search import SearchQuery, SearchVector, SearchRank

from . import models
from . import forms

class ItemListView(ListView):
    form_class = forms.SearchForm
    model = models.Item
    fields = ['serial_number', 'part_number', 'manufacturer', 'tags', 'is_active']
    template_name_suffix = '_list'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['form'] = self.form_class(self.request.GET)
        return context

    def get_queryset(self):
        queryset = super().get_queryset()
        form = self.form_class(self.request.GET)
        if form.is_valid():
            if form.cleaned_data['active_only']:
                queryset = queryset.filter(is_active=True)

            if not form.cleaned_data['search']:
                return super().get_queryset()

            search_vector = SearchVector('identifier', 'short_text', 'serial_number', 'revision', 'part_number',
                                         'manufacturer', 'description', 'tags')
            search_query = SearchQuery(form.cleaned_data['search'], search_type='websearch')
            return (
                queryset.annotate(
                    search=search_vector, rank=SearchRank(search_vector, search_query)
                )
                # .filter(search=search_query)
                .order_by(""-rank"").distinct()
            ) #.filter(search__icontains=form.cleaned_data['search'],)
        return super().get_queryset()

",76397104,"Your problem is that you're adding the tags field directly to your SearchVector
Lets concatenate the tags with Django's StringAgg into a single string and then use that string in your SearchVector
first we import StringAgg
from django.contrib.postgres.aggregates import StringAgg

then this is how you have to change your get_queryset function
def get_queryset(self):
    queryset = super().get_queryset()
    form = self.form_class(self.request.GET)
    if form.is_valid():
        if form.cleaned_data['active_only']:
            queryset = queryset.filter(is_active=True)

        if not form.cleaned_data['search']:
            return queryset

        queryset = queryset.annotate(tags_str=StringAgg('tags__name', delimiter=' '))
        search_vector = SearchVector('identifier', 'short_text', 'serial_number', 'revision', 'part_number',
                                     'manufacturer', 'description', 'tags_str')
        search_query = SearchQuery(form.cleaned_data['search'], search_type='websearch')
        return (
            queryset.annotate(
                search=search_vector, rank=SearchRank(search_vector, search_query)
            )
            .order_by(""-rank"").distinct()
        )
    return queryset

",
76398027,"What constitutes a directive in C++, and how can I use them effectively?","How do you exactly define a directive in programming?
#include is a directive, and using namespace std is a directive as well, assuming that I  am correct.
What actually makes a sentence or word a directive?
I tried reading from different sources, but all lead to no avail. I  am new to programming and I hope to grasp a good understanding of a directive.
",76398125,"You mentioned two different programming constructs in your question: 1) the #include pre-processing directive and 2) the using namespace directive.
Each of these constructs is literally referred to as a ""directive"" in the documentation, but they are completely different things. I can see how that could cause confusion.
Both the pre-processing directives and the using directive are not something that the user gets to define. They have a fixed meaning in the language. They can be used with different arguments to achieve different results.
Directives like #include can be used to direct what source code is used in the compilation. For example, #include <iostream> will cause the file iostream to be included. There are numerous other pre-processor directives, but for starting out, #include is by far the most important.
A directive like using std::cout instructs the compiler to bring the std::cout symbol into the current namespace so it can be referenced as simply cout.
I hope this helps clear up the confusion.
",
76395200,Grouping dictionary data from a list and storing in another dictionary,"I am currently working on a big data set where the data is stored into list with dictionaries (format shown below)
List = [{ID001: Report-1, ID002: Report-1, ID003: Report-1}, {ID001: Report-2, ID005: Report-5}…..]

I am trying to group and print a list with one dictionary that contains all ID’s from all dictionaries in the above list with related values (in this case report-1, report-2…)
result_List = {ID001: [Report-1, Report-2, Report-5], ID002: [Report-3, Report-6, Report-23]}

I thought of using for loop, but once the first dictionary is done looping through all the other dictionaries in list, it has to store unique ID’s in the result_List.
The code itself contains lot of loops, which I believe too complex for a list of 200 dictionaries with data.
I am looking for something with Pandas or Numpy with relatively less complex. Can anyone suggest the best way to do this kind of data crunch?
",76395444,"This way it can be done.
data =  [{ID001: Report-1, ID002: Report-1, ID003: Report-1}, {ID001: Report-2, ID005: Report-5}…..]
d = pd.DataFrame.from_records(data)
res = d.to_dict(""list"")
print(res)

",
76397033,Retrieve issue number from string for comic,"I'm not great with RegEx.  I need to retrieve the issue number from the long title of a comic book that includes its name and sometimes the artist's name.  Usually the issue number is the last number in the string, but not always.  Here are 6 examples that capture the range of variations I'm looking at:
STAR WARS: DOCTOR APHRA 32 CHRIS SPROUSE RETURN OF THE JEDI 40TH ANNIVERSARY VARIANT
DEADPOOL 7
X-23: DEADLY REGENESIS 3 GERALD PAREL VARIANT
SPIDER-MAN 2099: DARK GENESIS 5
THE GODFORSAKEN 99 OF KRONOS 2 KEN GRAGGINS VARIANT
Teenage Mutant Ninja Turtles: Saturday Morning Adventures (2023-) #1 Variant RI (10) (Dooney)

I'm using VBA and this is my current function:
Function ExtractText(c As Range) As String
    Dim rgx As RegExp
    Dim match As match
    Dim mc As MatchCollection
    Dim sComicNo As String, sPattern As Variant
    Dim lPos As Long, x As Long

    sComicNo = """"
    sPattern = Array("" [0-9] "", "" #[0-9] "", "" [0-9][0-9] "", "" #[0-9][0-9] "", "" [0-9][0-9]"", "" #[0-9][0-9]"", "" [0-9]"", "" #[0-9]"")
    lPos = 0
    
    Set rgx = New RegExp
    
    On Error GoTo ErrHandler
    
    Do While sComicNo = """"
        
        With rgx
            .Pattern = sPattern(x)
            .Global = True
        
            If .Test(c.Value) Then
                Set mc = .Execute(c.Value)
                
                If mc.Count > 0 Then
                    Set match = mc.Item(mc.Count - 1)
                Else
                    ExtractText = """"
                End If
                
                lPos = match.FirstIndex
                sComicNo = WorksheetFunction.Trim(match.Value) & ""|"" & lPos
            Else
                sComicNo = """"
            End If
            
            x = x + 1
            
            If x > 8 Then
                ExtractText = sComicNo
                Exit Function
            End If
        
        End With
        
    Loop
    
    ExtractText = sComicNo
    
ErrHandler:

    Exit Function
    
End Function

This pattern matches all my examples except the Spider-Man 2099, but there are other possible variations I'm overlooking.  It is also retrieving the position of the match for a separate purpose.  I'm trying to be as restrictive as possible by using patterns in a sequence that will retrieve very specific cases and gradually work up from there.
",76397135,"I don't know VBA, but by trying to comprehend the code I think your regex can be simplified to this:
\s      # Match a whitespace,
#?      # an optional '#', then
\d\d?   # 1 or 2 digits, followed by
\b      # a word boundary (prevents numbers with 3+ digits from being matched).

Try it on regex101.com.
Alternatively, you can use a lookbehind to skip the trimming part:
(?<=\s)#?\d\d?\b

...in which (?<=\s) means ""match something preceded by a whitespace"".
Try it on regex101.com.
",
76395338,Why does Javascript add an extra backslash when I capture input values?,"I have a standard HTML input form. The issue occurs when I enter something like \n in the input. I'm aware it's a regex that means newline, but a user may for example have it in their password, and it should remain unchanged.
The behaviour I'm observing:
const value = $('#myInput').val()
console.log(value)   // Console shows: hello\n (which is indeed what the user typed in)
const outputData = { password: value }
console.log(outputData)   // Console shows: { password: ""hello\\n""}

It would be fine if it were just a display issue, but that data is being sent to the server with the double backslash, so there is a mismatch between what the user types in and what gets sent to the server (which of course breaks things).
It's also worth noting this doesn't happen (everything works as expected) when I send a request body via Postman

Why is this happening?
How can I avoid it?

",76395466,"Re the code comments in the code you shared:


const value = $('#myInput').val()
console.log(value)   // Console shows: hello\n (which is indeed what the user typed in)
const outputData = { password: value }
console.log(outputData)   // Console shows: { password: ""hello\\n""}
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js""></script>
<input type=""text"" id=""myInput"" value=""hello\n"">



I assume you're referring to this style of console output:

That's just the difference between how the Chrome console (and some others) displays the output when you pass it a string vs. when you pass it an object. JavaScript is not adding anything to the string. When you pass a string to console.log, it just shows the characters of the string (which in your case are h, e, l, l, o, \, and n). But when you pass it an object, it shows you something that looks like an object literal, and since the object literal contains a string literal for password, the backslash in the string literal has to be shown escaped. (Otherwise, it would be an escape character.) But there is only one backslash in the string, not two, exactly like this:


console.log(""hello\\n""); // <== Notice the escaped backslash



The backslash is escaped in the string literal, because otherwise it wouldn't be a backslash, it would be an escape character changing the meaning of the next character (in this case, putting a newline in the string rather than \ and n.) There is only one backslash in the string.

It would be fine if it were just a display issue, but that data is being sent to the server with the double backslash...

Two possibilities here:

The extra backslash is being added by code you haven't shown, either the code preparing the string to send to your server, or the code on the server writing it to the database. For instance, another answer posted to this question suggested doing JSON.stringify(value), but that would add an extra backslash to it for the same reason we put on in a string literal: you have to escape backslashes in JSON strings. The extra one would be removed if the server code parsed the JSON, but it wouldn't be if the server code used the string as-is. I'm not saying you're doing that, just that it's the kind of thing you could be doing to see what you're describing.

Or — with apologies! :-) — it's possible that you're misinterpreting what you're seeing for what's in the database in the same way you misinterpeting the object literal in the console and there is actually only one backslash in the database.


Your best bet for finding out what's causing the problem is to carefully audit the code processing the string, in particular:

Check the actual value of the string with a debugger just prior to sending it to the server.
Check how you're encoding what you're sending the server.
Check exactly what your code on the server is receiving.
Check what that code is doing to the string when adding the value to the database.

Somewhere along the line, you'll either determine there isn't actually a problem (#2 above), or you'll find out what's mistakenly adding the backslash (#1 above).
",
76396891,How to make RTL to support right to left languages only in a column in XAML WPF VB.NET,"If using FlowDirection=""RightToLeft"" will change the whole datagrid right to left and solves the problem.
But my grid has both LTR and RTL contents. Some columns are LTR and some other column are RTL.
So please help me on this that how can I only set one column as RTL?
Thanks.
",76397169,"At last I found the answer.
Here it is:
   <DataGridTextColumn ...>
        <DataGridTextColumn.ElementStyle>
            <Style TargetType=""TextBlock"">
                <Setter Property=""FlowDirection"" Value=""LeftToRight"" />
            </Style>
        </DataGridTextColumn.ElementStyle>
        <DataGridTextColumn.EditingElementStyle>
            <Style TargetType=""TextBox"">
                <Setter Property=""FlowDirection"" Value=""LeftToRight"" />
            </Style>
        </DataGridTextColumn.EditingElementStyle>
    </DataGridTextColumn>

",
76397866,make two combo box related in javaFx,"I Want to change the second comboBox items based on the user select of item in another comboBox.
I do have courseCB comboBox that the user have to select first and then selct from mealCB comboBox;
mealCB items should change based of courseCB select item.
   @FXML
    private ComboBox<String> courseCB;

    @FXML
    private ComboBox<String> mealCB;
  @FXML
    void initialize() {
     //   courseCB.
        ObservableList<String> courseList = FXCollections.observableArrayList(fileReader.getMealCourseArray());
        courseCB.setItems(courseList);
        courseCB.setValue(""main meal"");
        

 courseCB.getSelectionModel().selectedItemProperty().addListener(new ChangeListener<String>() {
        @Override
        public void changed(ObservableValue<? extends String> observableValue, String s, String t1) {
            switch (t1){
                case  ""first meal"": {
                    ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""first meal""));
                    mealCB.setItems(mealByCourse);
                }
                case ""main meal"": {
                    ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""main meal""));
                    mealCB.setItems(mealByCourse);
                }
                case ""drink"": {
                    ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""drink""));
                    mealCB.setItems(mealByCourse);
                }
                break;
            }
        }
    });
}


the code of getMealBycourse:
 public static String[] getMealsByCourse(String mealCourse){
        ArrayList<String> mealsList = new ArrayList<>();
        Meal[] meals ;
        fileReader filereader= new fileReader();
        meals = filereader.getMealArray();
        for (int i = 0; i < meals.length ; i++) {
            if ((meals[i].getMealCourse()).compareTo(mealCourse)==0) {
                mealsList.add(meals[i].getMealName());
            }
        }
        String [] mealsByCourse = mealsList.toArray(new String[mealsList.size()]);
        return mealsByCourse;
    }


fileReader.getMealCourseArray() will return an array of items of courseMeal and populate the courseCB (comboBox) with it.
the problem is that that mealCB is getting values of drinks only.
the method of get mealByCourse is working very well by testing it.
so what is needed to change in the addListener to make it work.
",76398139,"This is related to these answers so also study them:

Javafx Cascading dropdown based on selection
combobox dependent on another combobox - JavaFX

You are using a similar approach to the first referenced answer, but have a logic error in your switch statement.
You don't break after every condition, instead you break only at the end of the switch. So the other conditions are ""working"" as in doing what they are asked to do, but the subsequent conditions run too and only the last one applies.
You can fix your logic error with the following code:
switch (t1) {
    case ""first meal"": {
        ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""first meal""));
        mealCB.setItems(mealByCourse);
    }
    break; 
    case ""main meal"": {
        ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""main meal""));
        mealCB.setItems(mealByCourse);
    }
    break;
    case ""drink"": {
        ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""drink""));
        mealCB.setItems(mealByCourse);
    }
    break;
}

Note that I added break statements in between each case statement.
That way, only the block of a single case statement will be executed for each case.
I advise you to go back and do some study on the Java language basics.  Study how switch statements work. That article is for Java 8. There are improved switches in later Java versions, but you aren't using the improved switch syntax, so the old article still applies for the old syntax that you use.
If you want to switch to using a more modern syntax (requires Java 14+), then you can write instead:
switch (t1) {
    case ""first meal"" -> {
        ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""first meal""));
        mealCB.setItems(mealByCourse);
    }

    case ""main meal"" -> {
        ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""main meal""));
        mealCB.setItems(mealByCourse);
    }

    case ""drink"" -> {
        ObservableList<String> mealByCourse = FXCollections.observableArrayList(fileReader.getMealsByCourse(""drink""));
        mealCB.setItems(mealByCourse);
    }
}

This will do the same as the previous example, but is IMO, less error prone.  It uses the new -> syntax for cases in switches rather than the old : syntax.  The new syntax does not fall through cases and does not require explicit break statements.
The switch can be converted to an expression to allow a more functional style of programming, which I often prefer:
String[] mealsByCourse = switch (t1) {
    case ""first meal"" -> fileReader.getMealsByCourse(""first meal"");
    case ""main meal"" -> fileReader.getMealsByCourse(""main meal"");
    case ""drink"" -> fileReader.getMealsByCourse(""drink"");
    default -> new String[0];
);

mealCB.setItems(
    FXCollections.observableArrayList(
            mealsByCourse
    )
);

But then when you do that it becomes clear that you are just switching on a value and applying that value.
So you can instead just eliminate the switch statement and use the simple solution proposed by Anon in comments:
ObservableList<String> mealsByCourse = FXCollections.observableArrayList(
        fileReader.getMealsByCourse(t1)
);                     

mealCB.setItems(mealsByCourse);

This final approach is the same as one of those proposed solutions in this answer to a related question:

combobox dependent on another combobox - JavaFX

",